[
  {
    "objectID": "posterior-predictive.html",
    "href": "posterior-predictive.html",
    "title": "Posterior Predictive Checks",
    "section": "",
    "text": "You’ve chosen a prior, computed the posterior, maybe even compared models. But there’s a question you haven’t asked: does the model actually fit the data?\nFrequentists check model fit with residual plots, Q-Q plots, and goodness-of-fit tests. Bayesians have their own tool: posterior predictive checks (PPCs).\nThe idea is beautifully simple: if your model is correct, then data simulated from it should look like the real data. If they don’t, something is wrong.\n\n\n\n\n\n\nExample: You fit a normal model to income data. The model says incomes are symmetric. But real income data has a long right tail — a few people earn much more than average. Simulated datasets from your model won’t have those extreme values. The mismatch tells you the normal model is wrong.",
    "crumbs": [
      "Model Checking",
      "Posterior Predictive Checks"
    ]
  },
  {
    "objectID": "posterior-predictive.html#does-the-model-fit",
    "href": "posterior-predictive.html#does-the-model-fit",
    "title": "Posterior Predictive Checks",
    "section": "",
    "text": "You’ve chosen a prior, computed the posterior, maybe even compared models. But there’s a question you haven’t asked: does the model actually fit the data?\nFrequentists check model fit with residual plots, Q-Q plots, and goodness-of-fit tests. Bayesians have their own tool: posterior predictive checks (PPCs).\nThe idea is beautifully simple: if your model is correct, then data simulated from it should look like the real data. If they don’t, something is wrong.\n\n\n\n\n\n\nExample: You fit a normal model to income data. The model says incomes are symmetric. But real income data has a long right tail — a few people earn much more than average. Simulated datasets from your model won’t have those extreme values. The mismatch tells you the normal model is wrong.",
    "crumbs": [
      "Model Checking",
      "Posterior Predictive Checks"
    ]
  },
  {
    "objectID": "posterior-predictive.html#how-it-works",
    "href": "posterior-predictive.html#how-it-works",
    "title": "Posterior Predictive Checks",
    "section": "How it works",
    "text": "How it works\nThe posterior predictive distribution generates “fake data” from your fitted model:\n\nDraw \\(\\theta^*\\) from the posterior \\(p(\\theta \\mid \\mathbf{y})\\)\nGenerate \\(\\mathbf{y}^{\\text{rep}} \\sim p(\\mathbf{y} \\mid \\theta^*)\\)\nRepeat many times to get many replicated datasets\n\nEach \\(\\mathbf{y}^{\\text{rep}}\\) is a dataset that could have been generated by the model, given what you learned from the data. If the model is right, the real data should look like a “typical” replicated dataset.\nTo make this concrete, pick a test statistic \\(T\\) — anything that captures a feature you care about (the max, the skewness, the standard deviation) — and compare:\n\\[\nT(\\mathbf{y}^{\\text{obs}}) \\quad \\text{vs} \\quad T(\\mathbf{y}^{\\text{rep}}_1),\\; T(\\mathbf{y}^{\\text{rep}}_2),\\; \\ldots\n\\]\nThe posterior predictive p-value is:\n\\[\np_{pp} = P\\!\\left(T(\\mathbf{y}^{\\text{rep}}) \\geq T(\\mathbf{y}^{\\text{obs}})\\right)\n\\]\nIf \\(p_{pp}\\) is near 0 or 1, the observed statistic is extreme relative to what the model predicts — evidence of model misfit.\n\n\n\n\n\n\nNot a “real” p-value: The posterior predictive p-value uses the data twice (once to fit, once to compare), so it’s not a proper significance test. Think of it as a diagnostic, not a hypothesis test. Values near 0.5 mean the model is consistent with the data on that feature.\n\n\n\n\n\nSimulation: Is the normal model right?\nFit a normal model (\\(y \\sim N(\\mu, \\sigma^2)\\)) to data that may or may not be normal. The posterior predictive check reveals the mismatch.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good  { color: #27ae60; font-weight: bold; }\n    .bad   { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"dgp\", \"True DGP:\",\n                  choices = c(\"Normal\", \"Heavy-tailed (t, df=3)\", \"Skewed (exp)\"),\n                  selected = \"Normal\"),\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 30, max = 500, value = 100, step = 10),\n\n      selectInput(\"stat\", \"Test statistic T:\",\n                  choices = c(\"Max\" = \"max\", \"Skewness\" = \"skew\", \"SD\" = \"sd\"),\n                  selected = \"max\"),\n\n      actionButton(\"go\", \"New data\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"data_plot\", height = \"420px\")),\n        column(6, plotOutput(\"ppc_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  n_rep &lt;- 500  # number of replicated datasets\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    dgp &lt;- input$dgp\n\n    # Generate observed data from true DGP\n    y_obs &lt;- if (dgp == \"Normal\") {\n      rnorm(n, 5, 2)\n    } else if (dgp == \"Heavy-tailed (t, df=3)\") {\n      5 + 2 * rt(n, df = 3)\n    } else {\n      rexp(n, rate = 0.5)  # mean = 2, skewed right\n    }\n\n    # Fit normal model: posterior for mu and sigma\n    # Using conjugate with vague priors:\n    # mu | sigma, y ~ N(ybar, sigma^2/n)\n    # sigma^2 | y ~ InvGamma(...)\n    ybar &lt;- mean(y_obs)\n    s2   &lt;- var(y_obs)\n\n    # Compute test statistic\n    calc_stat &lt;- function(y, type) {\n      if (type == \"max\") return(max(y))\n      if (type == \"skew\") {\n        m &lt;- mean(y); s &lt;- sd(y)\n        return(mean(((y - m) / s)^3))\n      }\n      if (type == \"sd\") return(sd(y))\n    }\n\n    stat_type &lt;- input$stat\n    t_obs &lt;- calc_stat(y_obs, stat_type)\n\n    # Generate replicated datasets from posterior predictive\n    t_rep &lt;- numeric(n_rep)\n    y_rep_curves &lt;- list()\n    n_curves &lt;- 50  # store some for visual overlay\n\n    for (r in 1:n_rep) {\n      # Draw sigma^2 from scaled inv-chi-sq (approximate with posterior)\n      sig2_star &lt;- s2 * (n - 1) / rchisq(1, df = n - 1)\n      sig_star  &lt;- sqrt(sig2_star)\n\n      # Draw mu from conditional posterior\n      mu_star &lt;- rnorm(1, ybar, sig_star / sqrt(n))\n\n      # Generate replicated data\n      y_r &lt;- rnorm(n, mu_star, sig_star)\n      t_rep[r] &lt;- calc_stat(y_r, stat_type)\n\n      if (r &lt;= n_curves) {\n        y_rep_curves[[r]] &lt;- y_r\n      }\n    }\n\n    # Posterior predictive p-value\n    p_pp &lt;- mean(t_rep &gt;= t_obs)\n\n    list(y_obs = y_obs, t_obs = t_obs, t_rep = t_rep,\n         y_rep_curves = y_rep_curves, p_pp = p_pp,\n         stat_type = stat_type, dgp = dgp, ybar = ybar, s2 = s2)\n  })\n\n  output$data_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Histogram of observed data\n    h &lt;- hist(d$y_obs, breaks = 25, plot = FALSE)\n\n    hist(d$y_obs, breaks = 25, freq = FALSE,\n         col = \"#3498db40\", border = \"#3498db\",\n         xlab = \"y\", main = \"Observed Data + Posterior Predictive\",\n         ylim = c(0, max(h$density) * 1.5))\n\n    # Overlay density curves from replicated datasets\n    x_range &lt;- range(c(d$y_obs, unlist(d$y_rep_curves)))\n    x_seq &lt;- seq(x_range[1] - 1, x_range[2] + 1, length.out = 200)\n\n    for (i in seq_along(d$y_rep_curves)) {\n      dens &lt;- density(d$y_rep_curves[[i]], from = x_range[1] - 1,\n                      to = x_range[2] + 1, n = 200)\n      lines(dens$x, dens$y, col = \"#e74c3c15\", lwd = 1)\n    }\n\n    # Observed density on top\n    dens_obs &lt;- density(d$y_obs)\n    lines(dens_obs$x, dens_obs$y, col = \"#2c3e50\", lwd = 2.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Observed data\", \"Replicated (model)\"),\n           col = c(\"#2c3e50\", \"#e74c3c80\"),\n           lwd = c(2.5, 1.5))\n  })\n\n  output$ppc_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    stat_label &lt;- switch(d$stat_type,\n                         \"max\" = \"max(y)\",\n                         \"skew\" = \"skewness(y)\",\n                         \"sd\" = \"sd(y)\")\n\n    h &lt;- hist(d$t_rep, breaks = 30, plot = FALSE)\n\n    # Color bins by whether they're &gt;= t_obs\n    bin_cols &lt;- ifelse(h$mids &gt;= d$t_obs, \"#e74c3c80\", \"#bdc3c780\")\n\n    hist(d$t_rep, breaks = 30, freq = FALSE,\n         col = bin_cols, border = \"white\",\n         xlab = paste0(\"T = \", stat_label),\n         main = paste0(\"Posterior Predictive Check: \", stat_label))\n\n    # Vertical line at observed\n    abline(v = d$t_obs, col = \"#2c3e50\", lwd = 2.5, lty = 1)\n    text(d$t_obs, max(h$density) * 0.9, pos = 4, cex = 0.85,\n         col = \"#2c3e50\", font = 2,\n         labels = paste0(\"T(obs) = \", round(d$t_obs, 3)))\n\n    # p-value label\n    p_label &lt;- if (d$p_pp &lt; 0.05 || d$p_pp &gt; 0.95) {\n      paste0(\"p_pp = \", round(d$p_pp, 3), \"  MISFIT\")\n    } else {\n      paste0(\"p_pp = \", round(d$p_pp, 3), \"  OK\")\n    }\n    p_col &lt;- if (d$p_pp &lt; 0.05 || d$p_pp &gt; 0.95) \"#e74c3c\" else \"#27ae60\"\n    mtext(p_label, side = 3, line = -1.5, cex = 0.9, col = p_col, font = 2)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.8,\n           legend = c(\"T(y_rep) distribution\",\n                      paste0(\"T(y_obs) = \", round(d$t_obs, 3)),\n                      \"p-value region\"),\n           col = c(\"#bdc3c7\", \"#2c3e50\", \"#e74c3c80\"),\n           pch = c(15, NA, 15), lwd = c(NA, 2.5, NA), lty = c(NA, 1, NA))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n\n    stat_label &lt;- switch(d$stat_type,\n                         \"max\" = \"max(y)\",\n                         \"skew\" = \"skewness\",\n                         \"sd\" = \"sd(y)\")\n\n    verdict &lt;- if (d$p_pp &lt; 0.05 || d$p_pp &gt; 0.95) {\n      \"&lt;span class='bad'&gt;FAIL — model misfit&lt;/span&gt;\"\n    } else {\n      \"&lt;span class='good'&gt;PASS — consistent&lt;/span&gt;\"\n    }\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;T(y_obs):&lt;/b&gt; \", round(d$t_obs, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Mean T(y_rep):&lt;/b&gt; \", round(mean(d$t_rep), 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;p&lt;sub&gt;pp&lt;/sub&gt;:&lt;/b&gt; \", round(d$p_pp, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Verdict:&lt;/b&gt; \", verdict, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;DGP:&lt;/b&gt; \", d$dgp, \"&lt;br&gt;\",\n        \"&lt;b&gt;Statistic:&lt;/b&gt; \", stat_label\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nNormal DGP, any statistic: the check passes. The model is correct, so \\(T(\\mathbf{y}^{\\text{obs}})\\) falls comfortably within the replicated distribution. The posterior predictive p-value is near 0.5.\nHeavy-tailed (t, df=3), test stat = Max: the normal model can’t produce the extreme values that a heavy-tailed distribution generates. \\(\\max(y)\\) is far into the tail of the replicated distribution. The check fails.\nSkewed (exponential), test stat = Skewness: the normal model is symmetric, so replicated datasets have skewness near 0. But the exponential data is heavily right-skewed. The check catches this.\nLarge n makes failures sharper: with n = 500, the replicated distribution concentrates tightly, making any mismatch glaringly obvious. Small samples are more forgiving.\nLeft panel clue: when the model is wrong, the light red replicated curves don’t match the shape of the observed histogram. That visual mismatch is what the p-value quantifies.\nSD statistic, Normal DGP: a good check that the variance is well-captured. Switch to heavy-tailed data — the SD check may or may not fail depending on the specific draw.",
    "crumbs": [
      "Model Checking",
      "Posterior Predictive Checks"
    ]
  },
  {
    "objectID": "posterior-predictive.html#in-stata",
    "href": "posterior-predictive.html#in-stata",
    "title": "Posterior Predictive Checks",
    "section": "In Stata",
    "text": "In Stata\n* Fit the Bayesian model\nbayes: reg y x1 x2\n\n* Posterior predictive p-values for test statistics\nbayesstats ppvalues (mean) (sd) (max)\n\n* If p_pp near 0 or 1, the model struggles\n* to reproduce that feature of the data\n\n\n\n\n\n\nWhich statistics to check? Pick statistics that matter for your research question. If you care about tail behavior (risk analysis), check max and min. If you’re modeling skewed outcomes (wages, prices), check skewness. If your model should capture variability, check sd.",
    "crumbs": [
      "Model Checking",
      "Posterior Predictive Checks"
    ]
  },
  {
    "objectID": "posterior-predictive.html#did-you-know",
    "href": "posterior-predictive.html#did-you-know",
    "title": "Posterior Predictive Checks",
    "section": "Did you know?",
    "text": "Did you know?\n\nRubin (1984) introduced posterior predictive checks as a general framework for model criticism. The key insight: use the model to generate data, then see if the generated data resemble reality. Simple, powerful, and applicable to any model.\nGelman, Meng & Stern (1996) formalized posterior predictive p-values and showed how to choose test statistics that target specific model assumptions. Their paper established PPCs as a standard tool in applied Bayesian work.\nConnection to cross-validation: LOO-CV (leave-one-out cross-validation) is another model checking tool. While PPCs ask “can the model reproduce features of the data?”, LOO-CV asks “can the model predict held-out observations?” Both catch model problems, but from different angles. The loo package in R and Stata’s bayesstats ic with WAIC approximate LOO-CV efficiently.",
    "crumbs": [
      "Model Checking",
      "Posterior Predictive Checks"
    ]
  },
  {
    "objectID": "shrinkage.html",
    "href": "shrinkage.html",
    "title": "Bayesian Shrinkage",
    "section": "",
    "text": "Imagine you’re a baseball scout. It’s early in the season and you need to estimate the true batting average for 50 players, each with only 20 at-bats.\nOne player went 10-for-20 (.500). Another went 1-for-20 (.050). Are those their true abilities? Probably not — with only 20 at-bats, there’s a ton of noise. The .500 hitter probably got lucky. The .050 hitter probably got unlucky.\nShrinkage says: don’t take the raw numbers at face value. Pull (“shrink”) every estimate toward the overall average. The more uncertain you are about an individual estimate, the more you pull.\n\\[\\hat{\\theta}_i^{shrunk} = w_i \\cdot \\bar{\\theta}_{overall} + (1 - w_i) \\cdot \\hat{\\theta}_i^{raw}\\]\nThis is the core of empirical Bayes and James-Stein estimation. It sounds like you’re adding bias — and you are — but you’re reducing variance by more than enough to compensate. The result: better predictions overall.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_players\", \"Number of players:\",\n                  min = 10, max = 100, value = 40, step = 5),\n\n      sliderInput(\"at_bats\", \"At-bats per player:\",\n                  min = 5, max = 200, value = 20, step = 5),\n\n      sliderInput(\"true_spread\", \"True talent spread (SD):\",\n                  min = 0.01, max = 0.08, value = 0.03, step = 0.005),\n\n      actionButton(\"go\", \"New season\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"shrinkage_plot\", height = \"420px\")),\n        column(6, plotOutput(\"mse_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    k   &lt;- input$n_players\n    n   &lt;- input$at_bats\n    tau &lt;- input$true_spread\n\n    # True batting averages (centered around .260)\n    true_avg &lt;- rnorm(k, mean = 0.260, sd = tau)\n    true_avg &lt;- pmin(pmax(true_avg, 0.100), 0.400)\n\n    # Observed: hits in n at-bats\n    hits &lt;- rbinom(k, size = n, prob = true_avg)\n    obs_avg &lt;- hits / n\n\n    # Grand mean\n    grand_mean &lt;- mean(obs_avg)\n\n    # Empirical Bayes shrinkage\n    # Estimate prior variance from data\n    obs_var &lt;- var(obs_avg)\n    sampling_var &lt;- mean(obs_avg * (1 - obs_avg) / n)\n    prior_var &lt;- max(obs_var - sampling_var, 0.0001)\n\n    # Shrinkage weight (toward grand mean)\n    w &lt;- sampling_var / (sampling_var + prior_var)\n    shrunk_avg &lt;- w * grand_mean + (1 - w) * obs_avg\n\n    # MSE\n    mse_raw   &lt;- mean((obs_avg - true_avg)^2)\n    mse_shrunk &lt;- mean((shrunk_avg - true_avg)^2)\n\n    # Future performance (another n at-bats from true ability)\n    future_hits &lt;- rbinom(k, size = n, prob = true_avg)\n    future_avg  &lt;- future_hits / n\n\n    pred_err_raw   &lt;- mean((obs_avg - future_avg)^2)\n    pred_err_shrunk &lt;- mean((shrunk_avg - future_avg)^2)\n\n    list(true_avg = true_avg, obs_avg = obs_avg, shrunk_avg = shrunk_avg,\n         grand_mean = grand_mean, w = w,\n         mse_raw = mse_raw, mse_shrunk = mse_shrunk,\n         pred_err_raw = pred_err_raw, pred_err_shrunk = pred_err_shrunk,\n         k = k, n = n)\n  })\n\n  output$shrinkage_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ord &lt;- order(d$obs_avg)\n\n    plot(d$obs_avg[ord], seq_along(ord), pch = 16, col = \"#e74c3c\",\n         xlab = \"Batting average\", ylab = \"Player (sorted by raw avg)\",\n         main = \"Shrinkage in Action\",\n         xlim = range(c(d$obs_avg, d$shrunk_avg, d$true_avg)))\n\n    points(d$shrunk_avg[ord], seq_along(ord), pch = 17, col = \"#3498db\")\n    points(d$true_avg[ord], seq_along(ord), pch = 4, col = \"#27ae60\", cex = 0.8)\n\n    # Draw arrows from raw to shrunk\n    arrows(d$obs_avg[ord], seq_along(ord),\n           d$shrunk_avg[ord], seq_along(ord),\n           length = 0.05, col = \"#bdc3c780\", lwd = 1)\n\n    # Grand mean\n    abline(v = d$grand_mean, lty = 2, col = \"gray50\", lwd = 1.5)\n\n    legend(\"bottomright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Raw average\", \"Shrunk estimate\",\n                      \"True ability\", \"Grand mean\"),\n           col = c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"gray50\"),\n           pch = c(16, 17, 4, NA),\n           lty = c(NA, NA, NA, 2), lwd = c(NA, NA, NA, 1.5))\n  })\n\n  output$mse_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 6, 3, 1))\n\n    vals &lt;- c(d$mse_raw, d$mse_shrunk, d$pred_err_raw, d$pred_err_shrunk)\n    cols &lt;- c(\"#e74c3c\", \"#3498db\", \"#e74c3c80\", \"#3498db80\")\n    labels &lt;- c(\"Raw\\nvs truth\", \"Shrunk\\nvs truth\",\n                \"Raw\\nvs future\", \"Shrunk\\nvs future\")\n\n    bp &lt;- barplot(vals, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.8,\n                  main = \"Mean Squared Error\",\n                  ylab = \"MSE\", las = 1)\n    text(bp, vals + max(vals) * 0.03, round(vals, 5), cex = 0.8)\n\n    pct1 &lt;- round((1 - d$mse_shrunk / d$mse_raw) * 100, 0)\n    pct2 &lt;- round((1 - d$pred_err_shrunk / d$pred_err_raw) * 100, 0)\n\n    mtext(paste0(\"Shrinkage reduces estimation error by ~\", pct1, \"%\"),\n          side = 1, line = 3.5, cex = 0.85, col = \"#2c3e50\")\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    pct_est &lt;- round((1 - d$mse_shrunk / d$mse_raw) * 100, 1)\n    pct_pred &lt;- round((1 - d$pred_err_shrunk / d$pred_err_raw) * 100, 1)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Shrinkage weight:&lt;/b&gt; \", round(d$w * 100, 1),\n        \"% toward grand mean&lt;br&gt;\",\n        \"&lt;b&gt;Grand mean:&lt;/b&gt; \", round(d$grand_mean, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;MSE (raw):&lt;/b&gt; \", round(d$mse_raw, 5), \"&lt;br&gt;\",\n        \"&lt;b&gt;MSE (shrunk):&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$mse_shrunk, 5), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Improvement:&lt;/b&gt; &lt;span class='good'&gt;\", pct_est, \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Prediction error (raw):&lt;/b&gt; \", round(d$pred_err_raw, 5), \"&lt;br&gt;\",\n        \"&lt;b&gt;Prediction error (shrunk):&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$pred_err_shrunk, 5), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Improvement:&lt;/b&gt; &lt;span class='good'&gt;\", pct_pred, \"%&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nAt-bats = 5: extreme noise. Raw averages are all over the place (some players show .000 or .600). Shrinkage pulls them heavily toward the mean — and the green crosses (true ability) confirm the shrunk estimates are closer.\nAt-bats = 200: lots of data per player. Shrinkage is minimal because the raw averages are already precise. With enough data, shrinkage vanishes.\nLook at the MSE bars: shrinkage almost always wins, especially with small samples. It also predicts future performance better.\nTrue talent spread = 0.01 (everyone is similar): shrinkage is aggressive because individual differences are small relative to noise.\nTrue talent spread = 0.08 (wide range of talent): shrinkage is lighter because individual differences are real, not noise.",
    "crumbs": [
      "Hierarchical Modeling",
      "Shrinkage"
    ]
  },
  {
    "objectID": "shrinkage.html#what-is-shrinkage",
    "href": "shrinkage.html#what-is-shrinkage",
    "title": "Bayesian Shrinkage",
    "section": "",
    "text": "Imagine you’re a baseball scout. It’s early in the season and you need to estimate the true batting average for 50 players, each with only 20 at-bats.\nOne player went 10-for-20 (.500). Another went 1-for-20 (.050). Are those their true abilities? Probably not — with only 20 at-bats, there’s a ton of noise. The .500 hitter probably got lucky. The .050 hitter probably got unlucky.\nShrinkage says: don’t take the raw numbers at face value. Pull (“shrink”) every estimate toward the overall average. The more uncertain you are about an individual estimate, the more you pull.\n\\[\\hat{\\theta}_i^{shrunk} = w_i \\cdot \\bar{\\theta}_{overall} + (1 - w_i) \\cdot \\hat{\\theta}_i^{raw}\\]\nThis is the core of empirical Bayes and James-Stein estimation. It sounds like you’re adding bias — and you are — but you’re reducing variance by more than enough to compensate. The result: better predictions overall.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_players\", \"Number of players:\",\n                  min = 10, max = 100, value = 40, step = 5),\n\n      sliderInput(\"at_bats\", \"At-bats per player:\",\n                  min = 5, max = 200, value = 20, step = 5),\n\n      sliderInput(\"true_spread\", \"True talent spread (SD):\",\n                  min = 0.01, max = 0.08, value = 0.03, step = 0.005),\n\n      actionButton(\"go\", \"New season\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"shrinkage_plot\", height = \"420px\")),\n        column(6, plotOutput(\"mse_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    k   &lt;- input$n_players\n    n   &lt;- input$at_bats\n    tau &lt;- input$true_spread\n\n    # True batting averages (centered around .260)\n    true_avg &lt;- rnorm(k, mean = 0.260, sd = tau)\n    true_avg &lt;- pmin(pmax(true_avg, 0.100), 0.400)\n\n    # Observed: hits in n at-bats\n    hits &lt;- rbinom(k, size = n, prob = true_avg)\n    obs_avg &lt;- hits / n\n\n    # Grand mean\n    grand_mean &lt;- mean(obs_avg)\n\n    # Empirical Bayes shrinkage\n    # Estimate prior variance from data\n    obs_var &lt;- var(obs_avg)\n    sampling_var &lt;- mean(obs_avg * (1 - obs_avg) / n)\n    prior_var &lt;- max(obs_var - sampling_var, 0.0001)\n\n    # Shrinkage weight (toward grand mean)\n    w &lt;- sampling_var / (sampling_var + prior_var)\n    shrunk_avg &lt;- w * grand_mean + (1 - w) * obs_avg\n\n    # MSE\n    mse_raw   &lt;- mean((obs_avg - true_avg)^2)\n    mse_shrunk &lt;- mean((shrunk_avg - true_avg)^2)\n\n    # Future performance (another n at-bats from true ability)\n    future_hits &lt;- rbinom(k, size = n, prob = true_avg)\n    future_avg  &lt;- future_hits / n\n\n    pred_err_raw   &lt;- mean((obs_avg - future_avg)^2)\n    pred_err_shrunk &lt;- mean((shrunk_avg - future_avg)^2)\n\n    list(true_avg = true_avg, obs_avg = obs_avg, shrunk_avg = shrunk_avg,\n         grand_mean = grand_mean, w = w,\n         mse_raw = mse_raw, mse_shrunk = mse_shrunk,\n         pred_err_raw = pred_err_raw, pred_err_shrunk = pred_err_shrunk,\n         k = k, n = n)\n  })\n\n  output$shrinkage_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ord &lt;- order(d$obs_avg)\n\n    plot(d$obs_avg[ord], seq_along(ord), pch = 16, col = \"#e74c3c\",\n         xlab = \"Batting average\", ylab = \"Player (sorted by raw avg)\",\n         main = \"Shrinkage in Action\",\n         xlim = range(c(d$obs_avg, d$shrunk_avg, d$true_avg)))\n\n    points(d$shrunk_avg[ord], seq_along(ord), pch = 17, col = \"#3498db\")\n    points(d$true_avg[ord], seq_along(ord), pch = 4, col = \"#27ae60\", cex = 0.8)\n\n    # Draw arrows from raw to shrunk\n    arrows(d$obs_avg[ord], seq_along(ord),\n           d$shrunk_avg[ord], seq_along(ord),\n           length = 0.05, col = \"#bdc3c780\", lwd = 1)\n\n    # Grand mean\n    abline(v = d$grand_mean, lty = 2, col = \"gray50\", lwd = 1.5)\n\n    legend(\"bottomright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Raw average\", \"Shrunk estimate\",\n                      \"True ability\", \"Grand mean\"),\n           col = c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"gray50\"),\n           pch = c(16, 17, 4, NA),\n           lty = c(NA, NA, NA, 2), lwd = c(NA, NA, NA, 1.5))\n  })\n\n  output$mse_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 6, 3, 1))\n\n    vals &lt;- c(d$mse_raw, d$mse_shrunk, d$pred_err_raw, d$pred_err_shrunk)\n    cols &lt;- c(\"#e74c3c\", \"#3498db\", \"#e74c3c80\", \"#3498db80\")\n    labels &lt;- c(\"Raw\\nvs truth\", \"Shrunk\\nvs truth\",\n                \"Raw\\nvs future\", \"Shrunk\\nvs future\")\n\n    bp &lt;- barplot(vals, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.8,\n                  main = \"Mean Squared Error\",\n                  ylab = \"MSE\", las = 1)\n    text(bp, vals + max(vals) * 0.03, round(vals, 5), cex = 0.8)\n\n    pct1 &lt;- round((1 - d$mse_shrunk / d$mse_raw) * 100, 0)\n    pct2 &lt;- round((1 - d$pred_err_shrunk / d$pred_err_raw) * 100, 0)\n\n    mtext(paste0(\"Shrinkage reduces estimation error by ~\", pct1, \"%\"),\n          side = 1, line = 3.5, cex = 0.85, col = \"#2c3e50\")\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    pct_est &lt;- round((1 - d$mse_shrunk / d$mse_raw) * 100, 1)\n    pct_pred &lt;- round((1 - d$pred_err_shrunk / d$pred_err_raw) * 100, 1)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Shrinkage weight:&lt;/b&gt; \", round(d$w * 100, 1),\n        \"% toward grand mean&lt;br&gt;\",\n        \"&lt;b&gt;Grand mean:&lt;/b&gt; \", round(d$grand_mean, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;MSE (raw):&lt;/b&gt; \", round(d$mse_raw, 5), \"&lt;br&gt;\",\n        \"&lt;b&gt;MSE (shrunk):&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$mse_shrunk, 5), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Improvement:&lt;/b&gt; &lt;span class='good'&gt;\", pct_est, \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Prediction error (raw):&lt;/b&gt; \", round(d$pred_err_raw, 5), \"&lt;br&gt;\",\n        \"&lt;b&gt;Prediction error (shrunk):&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$pred_err_shrunk, 5), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Improvement:&lt;/b&gt; &lt;span class='good'&gt;\", pct_pred, \"%&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nAt-bats = 5: extreme noise. Raw averages are all over the place (some players show .000 or .600). Shrinkage pulls them heavily toward the mean — and the green crosses (true ability) confirm the shrunk estimates are closer.\nAt-bats = 200: lots of data per player. Shrinkage is minimal because the raw averages are already precise. With enough data, shrinkage vanishes.\nLook at the MSE bars: shrinkage almost always wins, especially with small samples. It also predicts future performance better.\nTrue talent spread = 0.01 (everyone is similar): shrinkage is aggressive because individual differences are small relative to noise.\nTrue talent spread = 0.08 (wide range of talent): shrinkage is lighter because individual differences are real, not noise.",
    "crumbs": [
      "Hierarchical Modeling",
      "Shrinkage"
    ]
  },
  {
    "objectID": "shrinkage.html#why-does-this-work",
    "href": "shrinkage.html#why-does-this-work",
    "title": "Bayesian Shrinkage",
    "section": "Why does this work?",
    "text": "Why does this work?\nIt seems wrong to move estimates away from the data. But consider what happens without shrinkage:\n\nPlayers who got lucky are overestimated\nPlayers who got unlucky are underestimated\nThese errors don’t cancel — they inflate the overall MSE\n\nShrinkage dampens both overestimates and underestimates simultaneously. The small bias it introduces (pulling everyone toward the mean) is more than offset by the massive reduction in variance. This is the bias-variance tradeoff in action.\n\nWhere you see this in practice\n\n\n\nMethod\nWhat gets shrunk\n\n\n\n\nRidge regression\nCoefficients toward zero\n\n\nLASSO\nCoefficients toward zero (with selection)\n\n\nRandom effects models\nGroup means toward grand mean\n\n\nEmpirical Bayes\nIndividual estimates toward overall mean\n\n\nBayesian priors\nPosteriors toward prior mean\n\n\n\nThey all share the same logic: when you have many noisy estimates, borrowing strength across them improves every single one.",
    "crumbs": [
      "Hierarchical Modeling",
      "Shrinkage"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Thinking",
    "section": "",
    "text": "Bayesian inference — updating beliefs with data. Start with a prior, observe data, get a posterior. Just the core logic, with simulations.\nBuilds on: Statistical Inference",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#the-workflow",
    "href": "index.html#the-workflow",
    "title": "Bayesian Thinking",
    "section": "The workflow",
    "text": "The workflow\nEvery Bayesian analysis follows the same loop:\n\n\n\nStep\nWhat you do\nWhere to learn it\n\n\n\n\n1. Write down the likelihood\nWhat’s your data model? Normal, binomial, Poisson?\nBayes’ Theorem — shows how the likelihood plugs into the updating formula\n\n\n2. Choose a prior\nWhat do you believe before seeing data? Vague or informative?\nPriors & Posteriors — watch different priors get updated by data\n\n\n3. Compute the posterior\nConjugate pair → formula. Otherwise → MCMC.\nPriors & Posteriors for closed-form; MCMC for sampling when no formula exists\n\n\n4. Summarize & interpret\nPosterior mean, credible intervals, posterior probabilities\nBayesian Regression — credible intervals and how to read Bayesian output\n\n\n5. Check sensitivity\nHow much do results change with different priors?\nShrinkage — why the prior pulls estimates and when that helps vs hurts\n\n\n6. Scale up\nMultiple groups? Partial pooling via hierarchical models.\nHierarchical Models — let groups borrow strength from each other\n\n\n7. Compare models\nWhich model fits better? Bayes factors and BIC.\nModel Comparison — Bayes factors, BIC, and Lindley’s paradox\n\n\n8. Check model fit\nDoes the model actually describe the data?\nPosterior Predictive Checks — simulate from the model and compare\n\n\n\nThe core loop: likelihood + prior → posterior → interpret → check.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#foundations",
    "href": "index.html#foundations",
    "title": "Bayesian Thinking",
    "section": "Foundations",
    "text": "Foundations\n\nBayes’ Theorem — The engine behind everything: how evidence updates beliefs\nPriors & Posteriors — Watch your prior get overwhelmed by data\nBayesian vs Frequentist — Same question, two philosophies, different answers\nBayesian Regression — From Stata’s reg y x to full posterior inference on coefficients",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#computation",
    "href": "index.html#computation",
    "title": "Bayesian Thinking",
    "section": "Computation",
    "text": "Computation\n\nMCMC — Sampling from posteriors when closed-form solutions don’t exist",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#hierarchical-modeling",
    "href": "index.html#hierarchical-modeling",
    "title": "Bayesian Thinking",
    "section": "Hierarchical Modeling",
    "text": "Hierarchical Modeling\n\nShrinkage — Why pulling estimates toward the mean beats taking them at face value\nHierarchical Models — Partial pooling: the killer app of Bayesian inference",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#model-checking",
    "href": "index.html#model-checking",
    "title": "Bayesian Thinking",
    "section": "Model Checking",
    "text": "Model Checking\n\nModel Comparison — Bayes factors, BIC, and choosing between models\nPosterior Predictive Checks — Does the model fit? Simulate and check",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#application",
    "href": "index.html#application",
    "title": "Bayesian Thinking",
    "section": "Application",
    "text": "Application\n\nReturns to Education — Full worked example: OLS vs Bayesian on the Mincer equation",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#how-does-bayesian-inference-relate-to-causal-inference",
    "href": "index.html#how-does-bayesian-inference-relate-to-causal-inference",
    "title": "Bayesian Thinking",
    "section": "How does Bayesian inference relate to causal inference?",
    "text": "How does Bayesian inference relate to causal inference?\nThey’re different questions:\n\n\n\n\n\n\n\n\n\nBayesian inference\nCausal inference\n\n\n\n\nQuestion\nWhat should I believe given the data?\nDoes X cause Y?\n\n\nFramework\nPrior + likelihood = posterior\nPotential outcomes, DAGs\n\n\nKey concept\nUpdating beliefs\nCounterfactuals\n\n\n\nYou can combine them — Bayesian causal inference uses Bayesian methods to estimate causal effects — but each stands on its own.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "model-comparison.html",
    "href": "model-comparison.html",
    "title": "Model Comparison",
    "section": "",
    "text": "You’ve built a model — maybe a regression with three predictors. A colleague suggests dropping one. Frequentists compare models with F-tests or likelihood ratio tests. Bayesians have their own answer: Bayes factors.\nThe idea is simple. Compute the marginal likelihood of each model — how well the model predicted the data, averaging over all possible parameter values — and take the ratio:\n\\[\n\\text{BF}_{10} = \\frac{p(\\mathbf{y} \\mid M_1)}{p(\\mathbf{y} \\mid M_0)}\n\\]\nwhere \\(p(\\mathbf{y} \\mid M_k) = \\int p(\\mathbf{y} \\mid \\theta, M_k) \\, p(\\theta \\mid M_k) \\, d\\theta\\).\nA Bayes factor of 10 means the data are 10 times more likely under \\(M_1\\) than \\(M_0\\). Unlike a p-value, Bayes factors can support the null — a BF of 0.1 means the data favor \\(M_0\\) by a factor of 10.\nTo get posterior odds between models, multiply the Bayes factor by your prior odds:\n\\[\n\\underbrace{\\frac{P(M_1 \\mid \\mathbf{y})}{P(M_0 \\mid \\mathbf{y})}}_{\\text{posterior odds}}\n= \\underbrace{\\frac{P(M_1)}{P(M_0)}}_{\\text{prior odds}}\n\\times \\underbrace{\\text{BF}_{10}}_{\\text{Bayes factor}}\n\\]\nWith equal prior odds (50/50 between models), the posterior odds equal the Bayes factor.\nJeffreys’ interpretation scale:\n\n\n\nBF₁₀\nEvidence for M₁\n\n\n\n\n1–3\nAnecdotal\n\n\n3–10\nSubstantial\n\n\n10–30\nStrong\n\n\n30–100\nVery strong\n\n\n&gt; 100\nDecisive\n\n\n\n\n\n\n\n\n\nReading BF &lt; 1: If BF₁₀ = 0.05, that’s the same as BF₀₁ = 20 — strong evidence for the null. Bayes factors are symmetric: just flip the ratio.",
    "crumbs": [
      "Model Checking",
      "Model Comparison"
    ]
  },
  {
    "objectID": "model-comparison.html#which-model-is-better",
    "href": "model-comparison.html#which-model-is-better",
    "title": "Model Comparison",
    "section": "",
    "text": "You’ve built a model — maybe a regression with three predictors. A colleague suggests dropping one. Frequentists compare models with F-tests or likelihood ratio tests. Bayesians have their own answer: Bayes factors.\nThe idea is simple. Compute the marginal likelihood of each model — how well the model predicted the data, averaging over all possible parameter values — and take the ratio:\n\\[\n\\text{BF}_{10} = \\frac{p(\\mathbf{y} \\mid M_1)}{p(\\mathbf{y} \\mid M_0)}\n\\]\nwhere \\(p(\\mathbf{y} \\mid M_k) = \\int p(\\mathbf{y} \\mid \\theta, M_k) \\, p(\\theta \\mid M_k) \\, d\\theta\\).\nA Bayes factor of 10 means the data are 10 times more likely under \\(M_1\\) than \\(M_0\\). Unlike a p-value, Bayes factors can support the null — a BF of 0.1 means the data favor \\(M_0\\) by a factor of 10.\nTo get posterior odds between models, multiply the Bayes factor by your prior odds:\n\\[\n\\underbrace{\\frac{P(M_1 \\mid \\mathbf{y})}{P(M_0 \\mid \\mathbf{y})}}_{\\text{posterior odds}}\n= \\underbrace{\\frac{P(M_1)}{P(M_0)}}_{\\text{prior odds}}\n\\times \\underbrace{\\text{BF}_{10}}_{\\text{Bayes factor}}\n\\]\nWith equal prior odds (50/50 between models), the posterior odds equal the Bayes factor.\nJeffreys’ interpretation scale:\n\n\n\nBF₁₀\nEvidence for M₁\n\n\n\n\n1–3\nAnecdotal\n\n\n3–10\nSubstantial\n\n\n10–30\nStrong\n\n\n30–100\nVery strong\n\n\n&gt; 100\nDecisive\n\n\n\n\n\n\n\n\n\nReading BF &lt; 1: If BF₁₀ = 0.05, that’s the same as BF₀₁ = 20 — strong evidence for the null. Bayes factors are symmetric: just flip the ratio.",
    "crumbs": [
      "Model Checking",
      "Model Comparison"
    ]
  },
  {
    "objectID": "model-comparison.html#connection-to-bic",
    "href": "model-comparison.html#connection-to-bic",
    "title": "Model Comparison",
    "section": "Connection to BIC",
    "text": "Connection to BIC\nComputing the marginal likelihood exactly requires integrating over the prior — often intractable. But there’s a shortcut you already have.\nThe Bayesian Information Criterion (BIC) approximates the log marginal likelihood:\n\\[\n\\text{BIC} \\approx -2 \\log p(\\mathbf{y} \\mid M) + \\text{const}\n\\]\nSo the difference in BIC between two models approximates the log Bayes factor:\n\\[\n\\Delta\\text{BIC} = \\text{BIC}_1 - \\text{BIC}_0 \\approx -2 \\log \\text{BF}_{10}\n\\]\nwhich gives us:\n\\[\n\\text{BF}_{10} \\approx \\exp\\!\\left(-\\tfrac{\\Delta\\text{BIC}}{2}\\right)\n\\]\n\n\n\n\n\n\nBIC is the lazy Bayes factor. It doesn’t require specifying priors — it uses an implicit “unit information prior.” You already get BIC from Stata’s estat ic. To approximate a Bayes factor, just compute \\(\\exp(-\\Delta\\text{BIC}/2)\\).\n\n\n\nThree approaches compared:\n\n\n\nApproach\nFavors M₁ when\nCan support null?\nRequires prior?\n\n\n\n\np-value\np &lt; 0.05\nNo (only rejects)\nNo\n\n\nBIC\nΔBIC &lt; 0\nYes (ΔBIC &gt; 0)\nNo (implicit)\n\n\nBayes factor\nBF₁₀ &gt; 1\nYes (BF₁₀ &lt; 1)\nYes (explicit)\n\n\n\n\n\nSimulation: Does X belong in the model?\nCompare two models: \\(M_0\\): \\(y = \\text{noise}\\) vs \\(M_1\\): \\(y = \\beta x + \\text{noise}\\). The Bayes factor tells you whether the data support including \\(x\\).\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good  { color: #27ae60; font-weight: bold; }\n    .bad   { color: #e74c3c; font-weight: bold; }\n    .muted { color: #7f8c8d; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_beta\", HTML(\"True &beta;:\"),\n                  min = 0, max = 3, value = 0, step = 0.25),\n\n      sliderInput(\"prior_sd\", \"Prior SD on &beta;:\",\n                  min = 0.1, max = 5, value = 1, step = 0.1),\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 10, max = 300, value = 50, step = 10),\n\n      sliderInput(\"sigma\", HTML(\"Noise (&sigma;):\"),\n                  min = 0.5, max = 5, value = 2, step = 0.5),\n\n      actionButton(\"go\", \"New data\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"density_plot\", height = \"420px\")),\n        column(6, plotOutput(\"bf_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    true_beta &lt;- input$true_beta\n    prior_sd  &lt;- input$prior_sd\n    n         &lt;- input$n\n    sigma     &lt;- input$sigma\n\n    # Generate data\n    x &lt;- rnorm(n, 0, 1)\n    y &lt;- true_beta * x + rnorm(n, 0, sigma)\n\n    # OLS estimate under M1\n    beta_ols &lt;- sum(x * y) / sum(x^2)\n    se_ols   &lt;- sigma / sqrt(sum(x^2))\n\n    # Bayesian posterior under M1 (prior: beta ~ N(0, prior_sd^2))\n    prior_prec &lt;- 1 / prior_sd^2\n    data_prec  &lt;- sum(x^2) / sigma^2\n    post_prec  &lt;- prior_prec + data_prec\n    post_sd    &lt;- 1 / sqrt(post_prec)\n    post_mean  &lt;- (0 * prior_prec + beta_ols * data_prec) / post_prec\n\n    # Savage-Dickey BF: BF10 = prior density at 0 / posterior density at 0\n    prior_at_0 &lt;- dnorm(0, 0, prior_sd)\n    post_at_0  &lt;- dnorm(0, post_mean, post_sd)\n    bf_10      &lt;- prior_at_0 / post_at_0\n\n    # BIC approximation\n    # Under M0: RSS = sum(y^2); Under M1: RSS = sum((y - beta_ols*x)^2)\n    rss0 &lt;- sum(y^2)\n    rss1 &lt;- sum((y - beta_ols * x)^2)\n    bic0 &lt;- n * log(rss0 / n) + 0 * log(n)\n    bic1 &lt;- n * log(rss1 / n) + 1 * log(n)\n    delta_bic &lt;- bic1 - bic0\n    bf_bic    &lt;- exp(-delta_bic / 2)\n\n    # Jeffreys label\n    abf &lt;- bf_10\n    label &lt;- if (abf &gt; 100) \"Decisive for M1\"\n             else if (abf &gt; 30) \"Very strong for M1\"\n             else if (abf &gt; 10) \"Strong for M1\"\n             else if (abf &gt; 3) \"Substantial for M1\"\n             else if (abf &gt; 1) \"Anecdotal for M1\"\n             else if (abf &gt; 1/3) \"Anecdotal for M0\"\n             else if (abf &gt; 1/10) \"Substantial for M0\"\n             else if (abf &gt; 1/30) \"Strong for M0\"\n             else if (abf &gt; 1/100) \"Very strong for M0\"\n             else \"Decisive for M0\"\n\n    list(x = x, y = y, beta_ols = beta_ols, se_ols = se_ols,\n         prior_sd = prior_sd, post_mean = post_mean, post_sd = post_sd,\n         prior_at_0 = prior_at_0, post_at_0 = post_at_0,\n         bf_10 = bf_10, delta_bic = delta_bic, bf_bic = bf_bic,\n         label = label, true_beta = true_beta)\n  })\n\n  output$density_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Plot range\n    xlim &lt;- c(min(-3 * d$prior_sd, d$post_mean - 4 * d$post_sd),\n              max(3 * d$prior_sd, d$post_mean + 4 * d$post_sd))\n    x_seq &lt;- seq(xlim[1], xlim[2], length.out = 400)\n\n    prior_y &lt;- dnorm(x_seq, 0, d$prior_sd)\n    post_y  &lt;- dnorm(x_seq, d$post_mean, d$post_sd)\n\n    ylim &lt;- c(0, max(c(prior_y, post_y)) * 1.2)\n\n    plot(NULL, xlim = xlim, ylim = ylim,\n         xlab = expression(beta), ylab = \"Density\",\n         main = \"Savage-Dickey: Prior vs Posterior at \\u03b2 = 0\")\n\n    # Prior (red dashed)\n    lines(x_seq, prior_y, col = \"#e74c3c\", lwd = 2, lty = 2)\n\n    # Posterior (blue filled)\n    polygon(c(x_seq, rev(x_seq)), c(post_y, rep(0, length(x_seq))),\n            col = \"#3498db30\", border = NA)\n    lines(x_seq, post_y, col = \"#3498db\", lwd = 2.5)\n\n    # Mark densities at beta = 0\n    segments(0, 0, 0, d$prior_at_0, col = \"#e74c3c\", lwd = 2, lty = 3)\n    points(0, d$prior_at_0, pch = 16, col = \"#e74c3c\", cex = 1.5)\n\n    segments(0, 0, 0, d$post_at_0, col = \"#3498db\", lwd = 2, lty = 3)\n    points(0, d$post_at_0, pch = 16, col = \"#3498db\", cex = 1.5)\n\n    # Labels\n    text(0, d$prior_at_0, pos = 4, cex = 0.8, col = \"#e74c3c\",\n         labels = paste0(\"Prior(0) = \", round(d$prior_at_0, 3)))\n    text(0, d$post_at_0, pos = 4, cex = 0.8, col = \"#3498db\",\n         labels = paste0(\"Post(0) = \", round(d$post_at_0, 3)))\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Prior\", \"Posterior\"),\n           col = c(\"#e74c3c\", \"#3498db\"),\n           lwd = c(2, 2.5), lty = c(2, 1))\n  })\n\n  output$bf_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 6, 3, 1))\n\n    log_bf  &lt;- log10(d$bf_10)\n    log_bic &lt;- log10(d$bf_bic)\n\n    vals &lt;- c(log_bf, log_bic)\n    xlim &lt;- range(c(vals, -2.5, 2.5))\n    xlim &lt;- c(min(xlim[1], -2.5), max(xlim[2], 2.5))\n\n    bp &lt;- barplot(vals, horiz = TRUE,\n                  names.arg = c(\"Bayes Factor\", \"BIC approx\"),\n                  col = c(\"#3498db\", \"#f39c12\"),\n                  xlim = xlim, las = 1, border = NA,\n                  main = expression(\"log\"[10] * \"(BF\"[10] * \")\"),\n                  xlab = expression(\"log\"[10] * \"(BF\"[10] * \")\"))\n\n    # Jeffreys reference lines\n    abline(v = 0, lty = 1, col = \"#2c3e50\", lwd = 1.5)\n    ref_vals &lt;- c(log10(3), log10(10), log10(30), log10(100),\n                  -log10(3), -log10(10), -log10(30), -log10(100))\n    ref_labs &lt;- c(\"3\", \"10\", \"30\", \"100\", \"1/3\", \"1/10\", \"1/30\", \"1/100\")\n\n    for (i in seq_along(ref_vals)) {\n      if (ref_vals[i] &gt;= xlim[1] && ref_vals[i] &lt;= xlim[2]) {\n        abline(v = ref_vals[i], lty = 3, col = \"#bdc3c7\")\n      }\n    }\n\n    # Label regions\n    text(xlim[2] * 0.85, mean(bp), \"Favors M1 \\u2192\", cex = 0.8, col = \"#27ae60\")\n    text(xlim[1] * 0.85, mean(bp), \"\\u2190 Favors M0\", cex = 0.8, col = \"#e74c3c\")\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    label_class &lt;- if (d$bf_10 &gt;= 3) \"good\"\n                   else if (d$bf_10 &lt;= 1/3) \"bad\"\n                   else \"muted\"\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;BF\\u2081\\u2080:&lt;/b&gt; \", round(d$bf_10, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;log\\u2081\\u2080(BF):&lt;/b&gt; \", round(log10(d$bf_10), 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;\\u0394BIC:&lt;/b&gt; \", round(d$delta_bic, 2), \"&lt;br&gt;\",\n        \"&lt;b&gt;BIC-approx BF:&lt;/b&gt; \", round(d$bf_bic, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Verdict:&lt;/b&gt; &lt;span class='\", label_class, \"'&gt;\",\n        d$label, \"&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nTrue β = 0 (null is true): the Bayes factor favors \\(M_0\\). Unlike a p-value that can only “fail to reject,” the BF actively supports the null.\nTrue β = 2: the BF strongly favors \\(M_1\\). Increase n to make the evidence decisive.\nWide prior (prior SD = 5): even when β is truly nonzero, a very wide prior penalizes \\(M_1\\). The prior spreads probability over implausible values, wasting “predictive budget.” This is Lindley’s paradox: a vague Bayesian can reject a true effect.\nNarrow prior (prior SD = 0.5) centered at 0: a tight prior makes \\(M_1\\) more efficient — if the truth is near 0, \\(M_1\\) predicts nearly as well as \\(M_0\\).\nWatch the left panel: the Savage-Dickey ratio is literally prior density at 0 divided by posterior density at 0. When the posterior shifts away from 0, the denominator shrinks and the BF grows.\nCompare BF and BIC: they usually agree in direction but the magnitudes differ. BIC is a large-sample approximation, so they converge with bigger n.",
    "crumbs": [
      "Model Checking",
      "Model Comparison"
    ]
  },
  {
    "objectID": "model-comparison.html#in-stata",
    "href": "model-comparison.html#in-stata",
    "title": "Model Comparison",
    "section": "In Stata",
    "text": "In Stata\n* Fit two competing models\nbayes: reg y x1 x2\nbayesstats ic\n\nbayes: reg y x1\nbayesstats ic\n\n* Compare DIC/WAIC across models\n* Lower DIC or WAIC = better predictive fit\n* ΔDIC works like ΔBIC: bigger gap = stronger preference\n\n\n\n\n\n\nQuick BIC-based Bayes factor: Run frequentist models, use estat ic to get BIC, then compute display exp(-(BIC1 - BIC0)/2) for an approximate Bayes factor. No MCMC needed.",
    "crumbs": [
      "Model Checking",
      "Model Comparison"
    ]
  },
  {
    "objectID": "model-comparison.html#did-you-know",
    "href": "model-comparison.html#did-you-know",
    "title": "Model Comparison",
    "section": "Did you know?",
    "text": "Did you know?\n\nJeffreys (1961) proposed the interpretation scale for Bayes factors in Theory of Probability. His categories — anecdotal, substantial, strong, very strong, decisive — remain the standard reference, though some Bayesians caution against rigid thresholds (just as with p = 0.05).\nLindley’s paradox (1957): with a sufficiently large sample, a vague prior can lead the Bayes factor to favor the null even when the p-value is tiny. The resolution: the BF is sensitive to the spread of the prior, not just its center. A prior of \\(N(0, 1000^2)\\) says you think β could be anywhere from -3000 to 3000 — that’s a real claim, and the data punish it.\nKass & Raftery (1995) wrote the definitive review of Bayes factors, proposing modified guidelines and advocating BIC as a practical approximation. Their paper remains one of the most cited in Bayesian statistics.",
    "crumbs": [
      "Model Checking",
      "Model Comparison"
    ]
  },
  {
    "objectID": "bayes-vs-freq.html",
    "href": "bayes-vs-freq.html",
    "title": "Bayesian vs Frequentist",
    "section": "",
    "text": "Bayesian and frequentist statistics look at the same data but ask different questions:\n\n\n\n\n\n\n\n\n\nFrequentist\nBayesian\n\n\n\n\nParameters are…\nFixed but unknown\nRandom variables with distributions\n\n\nProbability means…\nLong-run frequency\nDegree of belief\n\n\nResult\nPoint estimate + confidence interval\nFull posterior distribution\n\n\n“There’s a 95% chance…”\n…that this procedure captures the true value\n…that the true value is in this interval\n\n\n\nThe frequentist says: “If I repeated this experiment forever, 95% of my CIs would contain the true value.” The Bayesian says: “Given what I’ve seen, I’m 95% sure the true value is in this range.”\nMost people actually think like Bayesians (“what’s the probability the parameter is between A and B?”) but compute like frequentists (p-values, CIs).\n\n\nThe simulation below runs the same experiment and shows both the frequentist confidence interval and the Bayesian credible interval. Watch how they differ — especially with small samples and informative priors.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_mu\", HTML(\"True &mu;:\"),\n                  min = -3, max = 3, value = 1, step = 0.5),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 2, max = 200, value = 10, step = 1),\n\n      sliderInput(\"prior_mu\", \"Bayesian prior mean:\",\n                  min = -3, max = 3, value = 0, step = 0.5),\n\n      sliderInput(\"prior_sd\", \"Prior SD:\",\n                  min = 0.5, max = 10, value = 2, step = 0.5),\n\n      actionButton(\"go\", \"New experiment\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"interval_plot\", height = \"420px\")),\n        column(6, plotOutput(\"repeat_plot\",   height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    true_mu  &lt;- input$true_mu\n    n        &lt;- input$n\n    prior_mu &lt;- input$prior_mu\n    prior_sd &lt;- input$prior_sd\n    sigma    &lt;- 2\n\n    y &lt;- rnorm(n, mean = true_mu, sd = sigma)\n    y_bar &lt;- mean(y)\n    se &lt;- sigma / sqrt(n)\n\n    # Frequentist 95% CI\n    freq_lo &lt;- y_bar - 1.96 * se\n    freq_hi &lt;- y_bar + 1.96 * se\n\n    # Bayesian posterior\n    prior_prec &lt;- 1 / prior_sd^2\n    data_prec  &lt;- n / sigma^2\n    post_prec  &lt;- prior_prec + data_prec\n    post_sd    &lt;- 1 / sqrt(post_prec)\n    post_mu    &lt;- (prior_prec * prior_mu + data_prec * y_bar) / post_prec\n\n    bayes_lo &lt;- qnorm(0.025, post_mu, post_sd)\n    bayes_hi &lt;- qnorm(0.975, post_mu, post_sd)\n\n    # Repeated experiments for right panel\n    k &lt;- 50\n    reps &lt;- t(replicate(k, {\n      yy &lt;- rnorm(n, mean = true_mu, sd = sigma)\n      yy_bar &lt;- mean(yy)\n      f_lo &lt;- yy_bar - 1.96 * se\n      f_hi &lt;- yy_bar + 1.96 * se\n\n      d_prec &lt;- n / sigma^2\n      p_prec &lt;- prior_prec + d_prec\n      p_sd &lt;- 1 / sqrt(p_prec)\n      p_mu &lt;- (prior_prec * prior_mu + d_prec * yy_bar) / p_prec\n      b_lo &lt;- qnorm(0.025, p_mu, p_sd)\n      b_hi &lt;- qnorm(0.975, p_mu, p_sd)\n\n      c(yy_bar, f_lo, f_hi, p_mu, b_lo, b_hi)\n    }))\n\n    list(true_mu = true_mu, y_bar = y_bar,\n         freq_lo = freq_lo, freq_hi = freq_hi,\n         post_mu = post_mu, post_sd = post_sd,\n         bayes_lo = bayes_lo, bayes_hi = bayes_hi,\n         reps = reps, prior_mu = prior_mu)\n  })\n\n  output$interval_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 8, 3, 1))\n    xlim &lt;- range(c(d$freq_lo, d$freq_hi, d$bayes_lo, d$bayes_hi, d$true_mu)) +\n            c(-0.5, 0.5)\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, 2.5),\n         yaxt = \"n\", ylab = \"\", xlab = expression(mu),\n         main = \"This Experiment\")\n    axis(2, at = 1:2, labels = c(\"Frequentist\\n95% CI\", \"Bayesian\\n95% CrI\"),\n         las = 1, cex.axis = 0.85)\n\n    # Frequentist\n    segments(d$freq_lo, 1, d$freq_hi, 1, lwd = 4, col = \"#e74c3c\")\n    points(d$y_bar, 1, pch = 19, cex = 1.5, col = \"#e74c3c\")\n\n    # Bayesian\n    segments(d$bayes_lo, 2, d$bayes_hi, 2, lwd = 4, col = \"#3498db\")\n    points(d$post_mu, 2, pch = 19, cex = 1.5, col = \"#3498db\")\n\n    # True value\n    abline(v = d$true_mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n    text(d$true_mu, 2.4, expression(\"True \" * mu), cex = 0.9, col = \"#2c3e50\")\n  })\n\n  output$repeat_plot &lt;- renderPlot({\n    d &lt;- dat()\n    k &lt;- nrow(d$reps)\n\n    par(mar = c(4.5, 4, 3, 1))\n\n    freq_covers &lt;- d$reps[, 2] &lt;= d$true_mu & d$reps[, 3] &gt;= d$true_mu\n    bayes_covers &lt;- d$reps[, 5] &lt;= d$true_mu & d$reps[, 6] &gt;= d$true_mu\n\n    xlim &lt;- range(d$reps[, 2:6], d$true_mu) + c(-0.5, 0.5)\n\n    plot(NULL, xlim = xlim, ylim = c(1, k),\n         xlab = expression(mu), ylab = \"Experiment #\",\n         main = paste0(k, \" repeated experiments\"))\n\n    for (i in seq_len(k)) {\n      # Frequentist (left-shifted slightly)\n      clr_f &lt;- if (freq_covers[i]) \"#e74c3c\" else \"#e74c3c40\"\n      segments(d$reps[i, 2], i - 0.15, d$reps[i, 3], i - 0.15,\n               lwd = 1.5, col = clr_f)\n\n      # Bayesian (right-shifted slightly)\n      clr_b &lt;- if (bayes_covers[i]) \"#3498db\" else \"#3498db40\"\n      segments(d$reps[i, 5], i + 0.15, d$reps[i, 6], i + 0.15,\n               lwd = 1.5, col = clr_b)\n    }\n\n    abline(v = d$true_mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\n             paste0(\"Freq CI (\", sum(freq_covers), \"/\", k, \" cover)\"),\n             paste0(\"Bayes CrI (\", sum(bayes_covers), \"/\", k, \" cover)\")\n           ),\n           col = c(\"#e74c3c\", \"#3498db\"), lwd = 3)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Frequentist:&lt;/b&gt;&lt;br&gt;\",\n        \"Estimate: \", round(d$y_bar, 3), \"&lt;br&gt;\",\n        \"95% CI: [\", round(d$freq_lo, 3), \", \", round(d$freq_hi, 3), \"]&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Bayesian:&lt;/b&gt;&lt;br&gt;\",\n        \"Posterior mean: \", round(d$post_mu, 3), \"&lt;br&gt;\",\n        \"95% CrI: [\", round(d$bayes_lo, 3), \", \", round(d$bayes_hi, 3), \"]&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;CrI is narrower because the prior adds information.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nn = 5, prior centered at 0, true mu = 1: the Bayesian CrI is narrower but pulled toward 0 (shrinkage). The frequentist CI is wider but centered on the data.\nn = 200: both intervals are nearly identical. With lots of data, the prior washes out and Bayesian = frequentist.\nSet a wrong prior (prior mean = -3, true mu = 2, n = 5): the Bayesian interval gets pulled toward -3. A bad prior hurts with small samples. Slide n up — the data corrects it.\nRight panel: the frequentist CI is designed so that ~95% of the red intervals cover the truth across repetitions. The Bayesian CrI coverage depends on how good the prior is.\n\n\n\n\n\n\n\n\n\n\n\nUse frequentist when…\nUse Bayesian when…\n\n\n\n\nYou want procedure guarantees (coverage)\nYou want direct probability statements\n\n\nYou have no prior information\nYou have real prior knowledge\n\n\nRegulatory/peer review expects it\nSmall samples, need to borrow strength\n\n\nSimple problems\nComplex hierarchical models\n\n\n\nIn practice, most applied researchers use frequentist methods but interpret them like Bayesians. Understanding both helps you know what your numbers actually mean.",
    "crumbs": [
      "Foundations",
      "Bayesian vs Frequentist"
    ]
  },
  {
    "objectID": "bayes-vs-freq.html#same-data-different-questions",
    "href": "bayes-vs-freq.html#same-data-different-questions",
    "title": "Bayesian vs Frequentist",
    "section": "",
    "text": "Bayesian and frequentist statistics look at the same data but ask different questions:\n\n\n\n\n\n\n\n\n\nFrequentist\nBayesian\n\n\n\n\nParameters are…\nFixed but unknown\nRandom variables with distributions\n\n\nProbability means…\nLong-run frequency\nDegree of belief\n\n\nResult\nPoint estimate + confidence interval\nFull posterior distribution\n\n\n“There’s a 95% chance…”\n…that this procedure captures the true value\n…that the true value is in this interval\n\n\n\nThe frequentist says: “If I repeated this experiment forever, 95% of my CIs would contain the true value.” The Bayesian says: “Given what I’ve seen, I’m 95% sure the true value is in this range.”\nMost people actually think like Bayesians (“what’s the probability the parameter is between A and B?”) but compute like frequentists (p-values, CIs).\n\n\nThe simulation below runs the same experiment and shows both the frequentist confidence interval and the Bayesian credible interval. Watch how they differ — especially with small samples and informative priors.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_mu\", HTML(\"True &mu;:\"),\n                  min = -3, max = 3, value = 1, step = 0.5),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 2, max = 200, value = 10, step = 1),\n\n      sliderInput(\"prior_mu\", \"Bayesian prior mean:\",\n                  min = -3, max = 3, value = 0, step = 0.5),\n\n      sliderInput(\"prior_sd\", \"Prior SD:\",\n                  min = 0.5, max = 10, value = 2, step = 0.5),\n\n      actionButton(\"go\", \"New experiment\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"interval_plot\", height = \"420px\")),\n        column(6, plotOutput(\"repeat_plot\",   height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    true_mu  &lt;- input$true_mu\n    n        &lt;- input$n\n    prior_mu &lt;- input$prior_mu\n    prior_sd &lt;- input$prior_sd\n    sigma    &lt;- 2\n\n    y &lt;- rnorm(n, mean = true_mu, sd = sigma)\n    y_bar &lt;- mean(y)\n    se &lt;- sigma / sqrt(n)\n\n    # Frequentist 95% CI\n    freq_lo &lt;- y_bar - 1.96 * se\n    freq_hi &lt;- y_bar + 1.96 * se\n\n    # Bayesian posterior\n    prior_prec &lt;- 1 / prior_sd^2\n    data_prec  &lt;- n / sigma^2\n    post_prec  &lt;- prior_prec + data_prec\n    post_sd    &lt;- 1 / sqrt(post_prec)\n    post_mu    &lt;- (prior_prec * prior_mu + data_prec * y_bar) / post_prec\n\n    bayes_lo &lt;- qnorm(0.025, post_mu, post_sd)\n    bayes_hi &lt;- qnorm(0.975, post_mu, post_sd)\n\n    # Repeated experiments for right panel\n    k &lt;- 50\n    reps &lt;- t(replicate(k, {\n      yy &lt;- rnorm(n, mean = true_mu, sd = sigma)\n      yy_bar &lt;- mean(yy)\n      f_lo &lt;- yy_bar - 1.96 * se\n      f_hi &lt;- yy_bar + 1.96 * se\n\n      d_prec &lt;- n / sigma^2\n      p_prec &lt;- prior_prec + d_prec\n      p_sd &lt;- 1 / sqrt(p_prec)\n      p_mu &lt;- (prior_prec * prior_mu + d_prec * yy_bar) / p_prec\n      b_lo &lt;- qnorm(0.025, p_mu, p_sd)\n      b_hi &lt;- qnorm(0.975, p_mu, p_sd)\n\n      c(yy_bar, f_lo, f_hi, p_mu, b_lo, b_hi)\n    }))\n\n    list(true_mu = true_mu, y_bar = y_bar,\n         freq_lo = freq_lo, freq_hi = freq_hi,\n         post_mu = post_mu, post_sd = post_sd,\n         bayes_lo = bayes_lo, bayes_hi = bayes_hi,\n         reps = reps, prior_mu = prior_mu)\n  })\n\n  output$interval_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 8, 3, 1))\n    xlim &lt;- range(c(d$freq_lo, d$freq_hi, d$bayes_lo, d$bayes_hi, d$true_mu)) +\n            c(-0.5, 0.5)\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, 2.5),\n         yaxt = \"n\", ylab = \"\", xlab = expression(mu),\n         main = \"This Experiment\")\n    axis(2, at = 1:2, labels = c(\"Frequentist\\n95% CI\", \"Bayesian\\n95% CrI\"),\n         las = 1, cex.axis = 0.85)\n\n    # Frequentist\n    segments(d$freq_lo, 1, d$freq_hi, 1, lwd = 4, col = \"#e74c3c\")\n    points(d$y_bar, 1, pch = 19, cex = 1.5, col = \"#e74c3c\")\n\n    # Bayesian\n    segments(d$bayes_lo, 2, d$bayes_hi, 2, lwd = 4, col = \"#3498db\")\n    points(d$post_mu, 2, pch = 19, cex = 1.5, col = \"#3498db\")\n\n    # True value\n    abline(v = d$true_mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n    text(d$true_mu, 2.4, expression(\"True \" * mu), cex = 0.9, col = \"#2c3e50\")\n  })\n\n  output$repeat_plot &lt;- renderPlot({\n    d &lt;- dat()\n    k &lt;- nrow(d$reps)\n\n    par(mar = c(4.5, 4, 3, 1))\n\n    freq_covers &lt;- d$reps[, 2] &lt;= d$true_mu & d$reps[, 3] &gt;= d$true_mu\n    bayes_covers &lt;- d$reps[, 5] &lt;= d$true_mu & d$reps[, 6] &gt;= d$true_mu\n\n    xlim &lt;- range(d$reps[, 2:6], d$true_mu) + c(-0.5, 0.5)\n\n    plot(NULL, xlim = xlim, ylim = c(1, k),\n         xlab = expression(mu), ylab = \"Experiment #\",\n         main = paste0(k, \" repeated experiments\"))\n\n    for (i in seq_len(k)) {\n      # Frequentist (left-shifted slightly)\n      clr_f &lt;- if (freq_covers[i]) \"#e74c3c\" else \"#e74c3c40\"\n      segments(d$reps[i, 2], i - 0.15, d$reps[i, 3], i - 0.15,\n               lwd = 1.5, col = clr_f)\n\n      # Bayesian (right-shifted slightly)\n      clr_b &lt;- if (bayes_covers[i]) \"#3498db\" else \"#3498db40\"\n      segments(d$reps[i, 5], i + 0.15, d$reps[i, 6], i + 0.15,\n               lwd = 1.5, col = clr_b)\n    }\n\n    abline(v = d$true_mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\n             paste0(\"Freq CI (\", sum(freq_covers), \"/\", k, \" cover)\"),\n             paste0(\"Bayes CrI (\", sum(bayes_covers), \"/\", k, \" cover)\")\n           ),\n           col = c(\"#e74c3c\", \"#3498db\"), lwd = 3)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Frequentist:&lt;/b&gt;&lt;br&gt;\",\n        \"Estimate: \", round(d$y_bar, 3), \"&lt;br&gt;\",\n        \"95% CI: [\", round(d$freq_lo, 3), \", \", round(d$freq_hi, 3), \"]&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Bayesian:&lt;/b&gt;&lt;br&gt;\",\n        \"Posterior mean: \", round(d$post_mu, 3), \"&lt;br&gt;\",\n        \"95% CrI: [\", round(d$bayes_lo, 3), \", \", round(d$bayes_hi, 3), \"]&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;CrI is narrower because the prior adds information.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nn = 5, prior centered at 0, true mu = 1: the Bayesian CrI is narrower but pulled toward 0 (shrinkage). The frequentist CI is wider but centered on the data.\nn = 200: both intervals are nearly identical. With lots of data, the prior washes out and Bayesian = frequentist.\nSet a wrong prior (prior mean = -3, true mu = 2, n = 5): the Bayesian interval gets pulled toward -3. A bad prior hurts with small samples. Slide n up — the data corrects it.\nRight panel: the frequentist CI is designed so that ~95% of the red intervals cover the truth across repetitions. The Bayesian CrI coverage depends on how good the prior is.\n\n\n\n\n\n\n\n\n\n\n\nUse frequentist when…\nUse Bayesian when…\n\n\n\n\nYou want procedure guarantees (coverage)\nYou want direct probability statements\n\n\nYou have no prior information\nYou have real prior knowledge\n\n\nRegulatory/peer review expects it\nSmall samples, need to borrow strength\n\n\nSimple problems\nComplex hierarchical models\n\n\n\nIn practice, most applied researchers use frequentist methods but interpret them like Bayesians. Understanding both helps you know what your numbers actually mean.",
    "crumbs": [
      "Foundations",
      "Bayesian vs Frequentist"
    ]
  },
  {
    "objectID": "mcmc.html",
    "href": "mcmc.html",
    "title": "Markov Chain Monte Carlo",
    "section": "",
    "text": "The name breaks down into two parts:\n\nMonte Carlo: using random sampling to approximate something you can’t compute exactly. Instead of solving an integral analytically, you draw random samples and use their average as an approximation. (Named after the Monte Carlo casino — it’s fundamentally about randomness.)\nMarkov Chain: a sequence of random values where each value depends only on the previous one — not on the full history. The “chain” is a random walk through parameter space, where each step proposes a new value based on where you currently are.\n\nPut them together: MCMC constructs a Markov chain whose long-run distribution equals the posterior. Run it long enough, and the samples you collect are (approximately) draws from \\(p(\\theta \\mid y)\\) — even though you never computed that distribution directly.\n\n\n\n\n\n\nThe blindfolded hiker. Imagine you’re blindfolded on a hilly landscape and want to map out the shape of a mountain. You can’t see the whole thing, but at each step you can feel whether the ground goes up or down. You take a step — if it’s uphill (higher probability), you go. If it’s downhill, you might go (sometimes you need to cross valleys to find higher peaks). After thousands of steps, plot everywhere you’ve been on a map — the places you visited most often are the peaks. You’ve mapped the mountain without ever seeing it. That’s MCMC.\n\n\n\n\n\n\n\n\n\nIsn’t this just optimization? It looks like it — both climb hills. But an optimizer (like maximum likelihood in Stata’s reg) only goes uphill, finds the peak, and stops. That gives you one number: the best \\(\\hat\\beta\\). MCMC deliberately goes downhill sometimes (step 4 above: accepting worse proposals with some probability). That’s what lets it explore the full shape of the posterior — not just where the peak is, but how wide, how skewed, and whether there are multiple peaks. Turn off the downhill moves and you’d have a hill-climbing optimizer. Keep them on and you have a sampler.\n\n\n\n\n\nThe chain wanders through parameter space. Early on, the histogram of visited values looks nothing like the target. But as samples accumulate, the histogram converges to the true posterior — the chain has “learned” the distribution just by random walking.\n#| standalone: true\n#| viewerHeight: 480\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 8px; font-size: 13px; line-height: 1.7;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"show_n\", \"Samples to show:\",\n                  min = 10, max = 5000, value = 50, step = 10,\n                  animate = animationOptions(interval = 80, loop = FALSE)),\n\n      actionButton(\"go_intro\", \"New chain\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_intro\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"trace_intro\", height = \"340px\")),\n        column(6, plotOutput(\"hist_intro\", height = \"340px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  chain_data &lt;- reactive({\n    input$go_intro\n\n    # Target: posterior is N(2, 0.5^2) — we pretend we can't compute this\n    target_mu &lt;- 2\n    target_sd &lt;- 0.5\n    log_target &lt;- function(x) dnorm(x, target_mu, target_sd, log = TRUE)\n\n    # Run a long chain once\n    n_total &lt;- 5000\n    chain &lt;- numeric(n_total)\n    chain[1] &lt;- -1  # start far from the target\n    prop_sd &lt;- 0.4\n\n    for (t in 2:n_total) {\n      proposal &lt;- rnorm(1, chain[t - 1], prop_sd)\n      log_r &lt;- log_target(proposal) - log_target(chain[t - 1])\n      if (log(runif(1)) &lt; log_r) {\n        chain[t] &lt;- proposal\n      } else {\n        chain[t] &lt;- chain[t - 1]\n      }\n    }\n\n    list(chain = chain, target_mu = target_mu, target_sd = target_sd)\n  })\n\n  output$trace_intro &lt;- renderPlot({\n    d &lt;- chain_data()\n    n_show &lt;- min(input$show_n, length(d$chain))\n    par(mar = c(4, 4.5, 3, 1))\n\n    plot(1:n_show, d$chain[1:n_show], type = \"l\",\n         col = \"#3498db80\", lwd = 0.6,\n         xlim = c(1, max(200, n_show)),\n         ylim = range(d$chain),\n         xlab = \"Step\", ylab = expression(theta),\n         main = \"The chain explores\")\n\n    # Show current position\n    points(n_show, d$chain[n_show], pch = 19, col = \"#e74c3c\", cex = 1.5)\n\n    abline(h = d$target_mu, lty = 2, col = \"#27ae60\", lwd = 1.5)\n    text(max(200, n_show) * 0.95, d$target_mu,\n         expression(\"Target \" * mu), pos = 3, cex = 0.8, col = \"#27ae60\")\n  })\n\n  output$hist_intro &lt;- renderPlot({\n    d &lt;- chain_data()\n    n_show &lt;- min(input$show_n, length(d$chain))\n    samples &lt;- d$chain[1:n_show]\n    par(mar = c(4, 4.5, 3, 1))\n\n    xlim &lt;- c(d$target_mu - 3 * d$target_sd - 1,\n              d$target_mu + 3 * d$target_sd + 1)\n\n    hist(samples, breaks = seq(xlim[1], xlim[2], length.out = 40),\n         freq = FALSE, col = \"#3498db40\", border = \"#3498db\",\n         xlim = xlim, ylim = c(0, 1.2),\n         xlab = expression(theta), main = \"Histogram vs true posterior\")\n\n    # True target\n    x_seq &lt;- seq(xlim[1], xlim[2], length.out = 300)\n    lines(x_seq, dnorm(x_seq, d$target_mu, d$target_sd),\n          col = \"#27ae60\", lwd = 2.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(paste0(\"MCMC samples (n=\", n_show, \")\"),\n                      \"True posterior\"),\n           col = c(\"#3498db\", \"#27ae60\"),\n           lwd = c(NA, 2.5), pch = c(15, NA), pt.cex = c(1.5, NA))\n  })\n\n  output$results_intro &lt;- renderUI({\n    d &lt;- chain_data()\n    n_show &lt;- min(input$show_n, length(d$chain))\n    samples &lt;- d$chain[1:n_show]\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Samples:&lt;/b&gt; \", n_show, \"&lt;br&gt;\",\n        \"&lt;b&gt;MCMC mean:&lt;/b&gt; \", round(mean(samples), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;True mean:&lt;/b&gt; \", d$target_mu, \"&lt;br&gt;\",\n        \"&lt;b&gt;Error:&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(abs(mean(samples) - d$target_mu), 3), \"&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\nDrag the slider (or hit the play button) and watch:\n\n10 samples: the histogram is jagged, nothing like the green curve. The chain hasn’t explored enough.\n100 samples: the shape starts to emerge. The chain has found the high-probability region.\n1000+ samples: the histogram matches the true posterior almost exactly. The chain has “learned” the distribution through random walking alone.\n\nThe chain never knew the formula for the green curve. It only knew how to evaluate the posterior at any single point and compare “is this new spot better or worse?” That’s enough.",
    "crumbs": [
      "Computation",
      "MCMC"
    ]
  },
  {
    "objectID": "mcmc.html#what-is-mcmc",
    "href": "mcmc.html#what-is-mcmc",
    "title": "Markov Chain Monte Carlo",
    "section": "",
    "text": "The name breaks down into two parts:\n\nMonte Carlo: using random sampling to approximate something you can’t compute exactly. Instead of solving an integral analytically, you draw random samples and use their average as an approximation. (Named after the Monte Carlo casino — it’s fundamentally about randomness.)\nMarkov Chain: a sequence of random values where each value depends only on the previous one — not on the full history. The “chain” is a random walk through parameter space, where each step proposes a new value based on where you currently are.\n\nPut them together: MCMC constructs a Markov chain whose long-run distribution equals the posterior. Run it long enough, and the samples you collect are (approximately) draws from \\(p(\\theta \\mid y)\\) — even though you never computed that distribution directly.\n\n\n\n\n\n\nThe blindfolded hiker. Imagine you’re blindfolded on a hilly landscape and want to map out the shape of a mountain. You can’t see the whole thing, but at each step you can feel whether the ground goes up or down. You take a step — if it’s uphill (higher probability), you go. If it’s downhill, you might go (sometimes you need to cross valleys to find higher peaks). After thousands of steps, plot everywhere you’ve been on a map — the places you visited most often are the peaks. You’ve mapped the mountain without ever seeing it. That’s MCMC.\n\n\n\n\n\n\n\n\n\nIsn’t this just optimization? It looks like it — both climb hills. But an optimizer (like maximum likelihood in Stata’s reg) only goes uphill, finds the peak, and stops. That gives you one number: the best \\(\\hat\\beta\\). MCMC deliberately goes downhill sometimes (step 4 above: accepting worse proposals with some probability). That’s what lets it explore the full shape of the posterior — not just where the peak is, but how wide, how skewed, and whether there are multiple peaks. Turn off the downhill moves and you’d have a hill-climbing optimizer. Keep them on and you have a sampler.\n\n\n\n\n\nThe chain wanders through parameter space. Early on, the histogram of visited values looks nothing like the target. But as samples accumulate, the histogram converges to the true posterior — the chain has “learned” the distribution just by random walking.\n#| standalone: true\n#| viewerHeight: 480\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 8px; font-size: 13px; line-height: 1.7;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"show_n\", \"Samples to show:\",\n                  min = 10, max = 5000, value = 50, step = 10,\n                  animate = animationOptions(interval = 80, loop = FALSE)),\n\n      actionButton(\"go_intro\", \"New chain\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_intro\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"trace_intro\", height = \"340px\")),\n        column(6, plotOutput(\"hist_intro\", height = \"340px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  chain_data &lt;- reactive({\n    input$go_intro\n\n    # Target: posterior is N(2, 0.5^2) — we pretend we can't compute this\n    target_mu &lt;- 2\n    target_sd &lt;- 0.5\n    log_target &lt;- function(x) dnorm(x, target_mu, target_sd, log = TRUE)\n\n    # Run a long chain once\n    n_total &lt;- 5000\n    chain &lt;- numeric(n_total)\n    chain[1] &lt;- -1  # start far from the target\n    prop_sd &lt;- 0.4\n\n    for (t in 2:n_total) {\n      proposal &lt;- rnorm(1, chain[t - 1], prop_sd)\n      log_r &lt;- log_target(proposal) - log_target(chain[t - 1])\n      if (log(runif(1)) &lt; log_r) {\n        chain[t] &lt;- proposal\n      } else {\n        chain[t] &lt;- chain[t - 1]\n      }\n    }\n\n    list(chain = chain, target_mu = target_mu, target_sd = target_sd)\n  })\n\n  output$trace_intro &lt;- renderPlot({\n    d &lt;- chain_data()\n    n_show &lt;- min(input$show_n, length(d$chain))\n    par(mar = c(4, 4.5, 3, 1))\n\n    plot(1:n_show, d$chain[1:n_show], type = \"l\",\n         col = \"#3498db80\", lwd = 0.6,\n         xlim = c(1, max(200, n_show)),\n         ylim = range(d$chain),\n         xlab = \"Step\", ylab = expression(theta),\n         main = \"The chain explores\")\n\n    # Show current position\n    points(n_show, d$chain[n_show], pch = 19, col = \"#e74c3c\", cex = 1.5)\n\n    abline(h = d$target_mu, lty = 2, col = \"#27ae60\", lwd = 1.5)\n    text(max(200, n_show) * 0.95, d$target_mu,\n         expression(\"Target \" * mu), pos = 3, cex = 0.8, col = \"#27ae60\")\n  })\n\n  output$hist_intro &lt;- renderPlot({\n    d &lt;- chain_data()\n    n_show &lt;- min(input$show_n, length(d$chain))\n    samples &lt;- d$chain[1:n_show]\n    par(mar = c(4, 4.5, 3, 1))\n\n    xlim &lt;- c(d$target_mu - 3 * d$target_sd - 1,\n              d$target_mu + 3 * d$target_sd + 1)\n\n    hist(samples, breaks = seq(xlim[1], xlim[2], length.out = 40),\n         freq = FALSE, col = \"#3498db40\", border = \"#3498db\",\n         xlim = xlim, ylim = c(0, 1.2),\n         xlab = expression(theta), main = \"Histogram vs true posterior\")\n\n    # True target\n    x_seq &lt;- seq(xlim[1], xlim[2], length.out = 300)\n    lines(x_seq, dnorm(x_seq, d$target_mu, d$target_sd),\n          col = \"#27ae60\", lwd = 2.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(paste0(\"MCMC samples (n=\", n_show, \")\"),\n                      \"True posterior\"),\n           col = c(\"#3498db\", \"#27ae60\"),\n           lwd = c(NA, 2.5), pch = c(15, NA), pt.cex = c(1.5, NA))\n  })\n\n  output$results_intro &lt;- renderUI({\n    d &lt;- chain_data()\n    n_show &lt;- min(input$show_n, length(d$chain))\n    samples &lt;- d$chain[1:n_show]\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Samples:&lt;/b&gt; \", n_show, \"&lt;br&gt;\",\n        \"&lt;b&gt;MCMC mean:&lt;/b&gt; \", round(mean(samples), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;True mean:&lt;/b&gt; \", d$target_mu, \"&lt;br&gt;\",\n        \"&lt;b&gt;Error:&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(abs(mean(samples) - d$target_mu), 3), \"&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\nDrag the slider (or hit the play button) and watch:\n\n10 samples: the histogram is jagged, nothing like the green curve. The chain hasn’t explored enough.\n100 samples: the shape starts to emerge. The chain has found the high-probability region.\n1000+ samples: the histogram matches the true posterior almost exactly. The chain has “learned” the distribution through random walking alone.\n\nThe chain never knew the formula for the green curve. It only knew how to evaluate the posterior at any single point and compare “is this new spot better or worse?” That’s enough.",
    "crumbs": [
      "Computation",
      "MCMC"
    ]
  },
  {
    "objectID": "mcmc.html#why-do-we-need-it",
    "href": "mcmc.html#why-do-we-need-it",
    "title": "Markov Chain Monte Carlo",
    "section": "Why do we need it?",
    "text": "Why do we need it?\nOn the Priors & Posteriors page, we used conjugate priors — special prior-likelihood pairs where the posterior has a closed-form solution. That’s elegant but limiting. Most real models don’t have conjugate posteriors:\n\nLogistic regression with a prior on coefficients\nHierarchical models with multiple levels of parameters\nAny model where the posterior \\(p(\\theta \\mid y) \\propto p(y \\mid \\theta) \\, p(\\theta)\\) doesn’t simplify to a known distribution\n\nFor these models, we can’t write down the posterior analytically. We need to sample from it numerically. That’s what MCMC does.",
    "crumbs": [
      "Computation",
      "MCMC"
    ]
  },
  {
    "objectID": "mcmc.html#the-metropolis-hastings-algorithm",
    "href": "mcmc.html#the-metropolis-hastings-algorithm",
    "title": "Markov Chain Monte Carlo",
    "section": "The Metropolis-Hastings algorithm",
    "text": "The Metropolis-Hastings algorithm\nThe most foundational MCMC algorithm. The core idea is beautifully simple:\n\nStart at some value \\(\\theta_0\\).\nPropose a new value \\(\\theta^*\\) from a proposal distribution \\(q(\\theta^* \\mid \\theta_t)\\) — typically \\(\\theta^* \\sim N(\\theta_t, \\sigma_{prop}^2)\\).\nCompute the acceptance ratio: \\[\\alpha = \\min\\left(1, \\, \\frac{p(\\theta^* \\mid y) \\; q(\\theta_t \\mid \\theta^*)}{p(\\theta_t \\mid y) \\; q(\\theta^* \\mid \\theta_t)}\\right)\\]\nAccept \\(\\theta^*\\) with probability \\(\\alpha\\) (set \\(\\theta_{t+1} = \\theta^*\\)). Otherwise stay: \\(\\theta_{t+1} = \\theta_t\\).\nRepeat.\n\nWhen the proposal is symmetric — e.g. \\(q(\\theta^* \\mid \\theta_t) = N(\\theta_t, \\sigma_{prop}^2)\\) — the proposal ratio \\(q(\\theta_t \\mid \\theta^*) / q(\\theta^* \\mid \\theta_t) = 1\\) and the acceptance ratio simplifies to \\(\\alpha = \\min\\!\\left(1,\\; p(\\theta^* \\mid y) \\,/\\, p(\\theta_t \\mid y)\\right)\\). This is the original Metropolis algorithm. The Hastings (1970) generalization adds the proposal ratio so that asymmetric proposals can be used.\nKey insight: you never need to compute the normalizing constant \\(p(y) = \\int p(y \\mid \\theta) \\, p(\\theta) \\, d\\theta\\). The ratio \\(p(\\theta^* \\mid y) / p(\\theta_t \\mid y)\\) cancels it out. You only need to evaluate the unnormalized posterior — the numerator of Bayes’ theorem.\nAfter enough iterations, the chain converges to the posterior distribution. The samples \\(\\theta_1, \\theta_2, \\ldots\\) are (correlated) draws from \\(p(\\theta \\mid y)\\).\n\nSimulation 1: Metropolis-Hastings in action\nEstimate the mean \\(\\mu\\) of normally distributed data with unknown true value. The proposal width controls exploration: too narrow and the chain moves slowly; too wide and most proposals get rejected.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_mu\", HTML(\"True &mu;:\"),\n                  min = -3, max = 3, value = 1, step = 0.5),\n\n      sliderInput(\"n_data\", \"Data points:\",\n                  min = 5, max = 100, value = 20, step = 5),\n\n      sliderInput(\"prop_sd\", \"Proposal width (SD):\",\n                  min = 0.05, max = 5, value = 0.5, step = 0.05),\n\n      sliderInput(\"n_iter\", \"Iterations:\",\n                  min = 500, max = 5000, value = 2000, step = 500),\n\n      actionButton(\"go\", \"Run chain\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"trace_plot\", height = \"420px\")),\n        column(6, plotOutput(\"hist_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    true_mu  &lt;- input$true_mu\n    n_data   &lt;- input$n_data\n    prop_sd  &lt;- input$prop_sd\n    n_iter   &lt;- input$n_iter\n    sigma    &lt;- 2  # known SD\n\n    # Generate data\n    y &lt;- rnorm(n_data, mean = true_mu, sd = sigma)\n\n    # Log posterior (unnormalized): normal likelihood + flat prior\n    log_post &lt;- function(mu) {\n      sum(dnorm(y, mean = mu, sd = sigma, log = TRUE))\n    }\n\n    # Metropolis-Hastings\n    chain &lt;- numeric(n_iter)\n    chain[1] &lt;- 0  # start at 0\n    accepted &lt;- 0\n\n    for (t in 2:n_iter) {\n      proposal &lt;- rnorm(1, mean = chain[t - 1], sd = prop_sd)\n      log_ratio &lt;- log_post(proposal) - log_post(chain[t - 1])\n\n      if (log(runif(1)) &lt; log_ratio) {\n        chain[t] &lt;- proposal\n        accepted &lt;- accepted + 1\n      } else {\n        chain[t] &lt;- chain[t - 1]\n      }\n    }\n\n    accept_rate &lt;- accepted / (n_iter - 1)\n\n    # Analytic posterior for comparison (conjugate: flat prior + normal)\n    post_mean &lt;- mean(y)\n    post_sd   &lt;- sigma / sqrt(n_data)\n\n    list(chain = chain, accept_rate = accept_rate,\n         true_mu = true_mu, post_mean = post_mean, post_sd = post_sd,\n         n_iter = n_iter, prop_sd = prop_sd)\n  })\n\n  output$trace_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$chain, type = \"l\", col = \"#3498db80\", lwd = 0.5,\n         xlab = \"Iteration\", ylab = expression(mu),\n         main = \"Trace Plot\")\n\n    abline(h = d$true_mu, lty = 2, col = \"#e74c3c\", lwd = 2)\n    abline(h = d$post_mean, lty = 3, col = \"#27ae60\", lwd = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(expression(\"True \" * mu),\n                      \"Posterior mean (analytic)\"),\n           col = c(\"#e74c3c\", \"#27ae60\"),\n           lty = c(2, 3), lwd = 2)\n  })\n\n  output$hist_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Discard first 20% as burn-in\n    burnin &lt;- floor(d$n_iter * 0.2)\n    samples &lt;- d$chain[(burnin + 1):d$n_iter]\n\n    hist(samples, breaks = 40, freq = FALSE,\n         col = \"#3498db40\", border = \"#3498db\",\n         xlab = expression(mu), main = \"Posterior Distribution\",\n         xlim = range(c(samples, d$true_mu - 0.5, d$true_mu + 0.5)))\n\n    # Analytic posterior\n    x_seq &lt;- seq(min(samples) - 0.5, max(samples) + 0.5, length.out = 200)\n    lines(x_seq, dnorm(x_seq, d$post_mean, d$post_sd),\n          col = \"#27ae60\", lwd = 2.5)\n\n    abline(v = d$true_mu, lty = 2, col = \"#e74c3c\", lwd = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\"MCMC samples\", \"Analytic posterior\",\n                      expression(\"True \" * mu)),\n           col = c(\"#3498db\", \"#27ae60\", \"#e74c3c\"),\n           lwd = c(NA, 2.5, 2), lty = c(NA, 1, 2),\n           pch = c(15, NA, NA), pt.cex = c(1.5, NA, NA))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    burnin &lt;- floor(d$n_iter * 0.2)\n    samples &lt;- d$chain[(burnin + 1):d$n_iter]\n\n    rate_class &lt;- if (d$accept_rate &gt; 0.15 && d$accept_rate &lt; 0.5) \"good\" else \"bad\"\n    rate_note &lt;- if (d$accept_rate &lt; 0.15) {\n      \"Too low — proposal too wide\"\n    } else if (d$accept_rate &gt; 0.5) {\n      \"Too high — proposal too narrow\"\n    } else {\n      \"Good range (0.15-0.50)\"\n    }\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Acceptance rate:&lt;/b&gt; &lt;span class='\", rate_class, \"'&gt;\",\n        round(d$accept_rate * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;small&gt;\", rate_note, \"&lt;/small&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;MCMC posterior mean:&lt;/b&gt; \", round(mean(samples), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Analytic posterior mean:&lt;/b&gt; \", round(d$post_mean, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;True &mu;:&lt;/b&gt; \", d$true_mu, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Proposal width:&lt;/b&gt; \", d$prop_sd\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nProposal width = 0.5: a well-tuned chain. The trace plot shows good mixing (bouncing around the posterior), and the histogram matches the analytic posterior (green curve). Acceptance rate is in the sweet spot (20–40%).\nProposal width = 0.05: too narrow. The chain takes tiny steps — the trace plot shows slow, random-walk behavior. Acceptance rate is near 100% (almost every proposal is accepted because it’s barely different). The chain explores the posterior very slowly.\nProposal width = 5: too wide. Most proposals jump far from the current value and land in low-probability regions — they get rejected. The trace plot shows long flat stretches (the chain is stuck). Acceptance rate is very low.\nThe goldilocks principle: you want a proposal width that’s “just right” — large enough to explore, small enough to get accepted. The theoretical optimum for 1D is an acceptance rate around 44% (Roberts et al., 1997).",
    "crumbs": [
      "Computation",
      "MCMC"
    ]
  },
  {
    "objectID": "mcmc.html#burn-in-and-convergence",
    "href": "mcmc.html#burn-in-and-convergence",
    "title": "Markov Chain Monte Carlo",
    "section": "Burn-in and convergence",
    "text": "Burn-in and convergence\nA practical concern: the chain starts at an arbitrary value (\\(\\theta_0 = 0\\) above). The early samples reflect the starting point, not the posterior. You need to discard these initial samples — the “burn-in” period.\nHow do you know the chain has converged? Run multiple chains from different starting points. If they all end up exploring the same region, you have evidence of convergence. If they’re stuck in different places, the chains haven’t converged and you need more iterations.\n\nSimulation 2: Multiple chains and convergence\n#| standalone: true\n#| viewerHeight: 580\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_iter2\", \"Iterations:\",\n                  min = 200, max = 3000, value = 1000, step = 200),\n\n      sliderInput(\"burnin\", \"Burn-in (discard first %):\",\n                  min = 0, max = 50, value = 20, step = 5),\n\n      sliderInput(\"prop_sd2\", \"Proposal width:\",\n                  min = 0.1, max = 3, value = 0.5, step = 0.1),\n\n      actionButton(\"go2\", \"Run chains\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"multi_trace\", height = \"420px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat2 &lt;- reactive({\n    input$go2\n    n_iter  &lt;- input$n_iter2\n    burnin  &lt;- input$burnin / 100\n    prop_sd &lt;- input$prop_sd2\n    sigma   &lt;- 2\n    true_mu &lt;- 1.5\n\n    # Generate data once\n    y &lt;- rnorm(30, mean = true_mu, sd = sigma)\n\n    log_post &lt;- function(mu) {\n      sum(dnorm(y, mean = mu, sd = sigma, log = TRUE))\n    }\n\n    # Run 4 chains from different starting points\n    starts &lt;- c(-5, -2, 4, 7)\n    chain_cols &lt;- c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"#f39c12\")\n    chains &lt;- matrix(0, nrow = n_iter, ncol = 4)\n\n    for (ch in 1:4) {\n      chains[1, ch] &lt;- starts[ch]\n      for (t in 2:n_iter) {\n        proposal &lt;- rnorm(1, chains[t - 1, ch], prop_sd)\n        log_r &lt;- log_post(proposal) - log_post(chains[t - 1, ch])\n        if (log(runif(1)) &lt; log_r) {\n          chains[t, ch] &lt;- proposal\n        } else {\n          chains[t, ch] &lt;- chains[t - 1, ch]\n        }\n      }\n    }\n\n    # Posterior (analytic)\n    post_mean &lt;- mean(y)\n\n    list(chains = chains, starts = starts, chain_cols = chain_cols,\n         n_iter = n_iter, burnin = burnin, true_mu = true_mu,\n         post_mean = post_mean)\n  })\n\n  output$multi_trace &lt;- renderPlot({\n    d &lt;- dat2()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    burnin_line &lt;- floor(d$n_iter * d$burnin)\n\n    ylim &lt;- range(d$chains)\n    plot(NULL, xlim = c(1, d$n_iter), ylim = ylim,\n         xlab = \"Iteration\", ylab = expression(mu),\n         main = \"Four Chains from Different Starting Points\")\n\n    # Shade burn-in region\n    if (burnin_line &gt; 0) {\n      rect(0, ylim[1] - 1, burnin_line, ylim[2] + 1,\n           col = \"#f0f0f080\", border = NA)\n      abline(v = burnin_line, lty = 2, col = \"gray40\", lwd = 1.5)\n      text(burnin_line, ylim[2], \"burn-in\", pos = 2,\n           cex = 0.8, col = \"gray40\")\n    }\n\n    for (ch in 1:4) {\n      lines(d$chains[, ch], col = paste0(d$chain_cols[ch], \"90\"),\n            lwd = 0.8)\n    }\n\n    abline(h = d$true_mu, lty = 2, col = \"#2c3e50\", lwd = 2)\n    text(d$n_iter * 0.98, d$true_mu, expression(\"True \" * mu),\n         pos = 3, cex = 0.85, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.75,\n           legend = paste0(\"Chain \", 1:4, \" (start = \", d$starts, \")\"),\n           col = d$chain_cols, lwd = 2)\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- dat2()\n    burnin_n &lt;- floor(d$n_iter * d$burnin)\n\n    # Post-burnin means per chain\n    if (burnin_n &lt; d$n_iter) {\n      post_samples &lt;- d$chains[(burnin_n + 1):d$n_iter, ]\n      chain_means &lt;- round(colMeans(post_samples), 3)\n      spread &lt;- round(max(chain_means) - min(chain_means), 3)\n    } else {\n      chain_means &lt;- rep(NA, 4)\n      spread &lt;- NA\n    }\n\n    converged &lt;- !is.na(spread) && spread &lt; 0.3\n    conv_class &lt;- if (converged) \"good\" else \"bad\"\n    conv_label &lt;- if (converged) \"Chains agree\" else \"Chains disagree\"\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Post-burn-in means:&lt;/b&gt;&lt;br&gt;\",\n        paste0(\"Chain \", 1:4, \": \", chain_means, collapse = \"&lt;br&gt;\"), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Spread:&lt;/b&gt; &lt;span class='\", conv_class, \"'&gt;\",\n        spread, \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;span class='\", conv_class, \"'&gt;\", conv_label, \"&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nDefault settings (1000 iterations, burn-in = 20%): all four chains start at different values (-5, -2, 4, 7) but converge to the same region within ~100 iterations. After burn-in, all chain means agree. This is convergence.\nBurn-in = 0%: the early samples (from the starting points) contaminate the posterior. The chain means diverge because each chain’s average is pulled toward its start.\nProposal width = 0.1: very slow exploration. The chains take longer to converge — you can see them creeping slowly toward the true value. With only 1000 iterations, they might not fully converge. Increase iterations to fix this.\nProposal width = 3: the chains converge quickly but the trace plot shows many flat stretches (rejected proposals). The posterior is explored but inefficiently.",
    "crumbs": [
      "Computation",
      "MCMC"
    ]
  },
  {
    "objectID": "mcmc.html#the-key-message",
    "href": "mcmc.html#the-key-message",
    "title": "Markov Chain Monte Carlo",
    "section": "The key message",
    "text": "The key message\nMCMC lets you compute posteriors for any model — not just conjugate ones. Specify the likelihood and the prior, and the algorithm samples from the posterior. This is what makes Bayesian inference practical for real-world problems like hierarchical models, where closed-form posteriors don’t exist.\nThis is also the engine behind Bayesian regression. When you type bayes: reg y x in Stata, it runs Metropolis-Hastings to sample from the posterior over regression coefficients — the same algorithm you just watched above, applied to a model you already know.\nModern tools like Stan, JAGS, and PyMC automate this — you specify the model and they handle the sampling. But understanding the basics (proposal tuning, burn-in, convergence checks) helps you diagnose problems when things go wrong.",
    "crumbs": [
      "Computation",
      "MCMC"
    ]
  },
  {
    "objectID": "mcmc.html#did-you-know",
    "href": "mcmc.html#did-you-know",
    "title": "Markov Chain Monte Carlo",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe Metropolis algorithm was developed at Los Alamos National Laboratory by Nicholas Metropolis, Arianna Rosenbluth, Marshall Rosenbluth, Augusta Teller, and Edward Teller (1953) — originally for simulating equations of state in statistical mechanics, not statistics. W.K. Hastings (1970) generalized it to asymmetric proposal distributions.\nMCMC was named one of the top 10 algorithms of the 20th century by Computing in Science & Engineering (2000). It’s used across physics, chemistry, biology, statistics, and machine learning.\nModern Bayesian computation has largely moved beyond basic Metropolis-Hastings to Hamiltonian Monte Carlo (HMC) and its adaptive variant NUTS (Hoffman & Gelman, 2014). HMC uses gradient information to make smarter proposals, dramatically improving efficiency in high-dimensional problems. This is what Stan uses under the hood.",
    "crumbs": [
      "Computation",
      "MCMC"
    ]
  },
  {
    "objectID": "bayesian-regression.html",
    "href": "bayesian-regression.html",
    "title": "Bayesian Regression",
    "section": "",
    "text": "On the MCMC page, you saw how to sample from any posterior — even when no closed-form solution exists. Now we apply that machinery to something familiar: regression.\nIn Stata, you type reg y x and get a point estimate \\(\\hat{\\beta}\\), a standard error, and a 95% confidence interval. That’s OLS — it picks the single line that minimizes the sum of squared residuals.\nBayesian regression starts from the same place (the same likelihood) but adds one ingredient: a prior distribution on \\(\\beta\\). Instead of a single best-fit line, you get a full posterior distribution — a curve showing how plausible each value of \\(\\beta\\) is, given both the data and your prior beliefs.\n\n\n\n\n\n\nExample: You’re estimating the effect of an extra year of education on hourly wages. Before seeing your data, you’ve read the literature — most estimates land between $0.50 and $1.50 per year. A Bayesian approach lets you encode this: set a prior of \\(\\beta \\sim N(1.0, 0.5^2)\\), centered on $1.00 with moderate uncertainty. OLS ignores this information entirely. The Bayesian estimate blends it with your data.\n\n\n\n\n\n\n\n\n\nWhat “a prior on \\(\\beta\\)” means: it’s a probability distribution expressing your beliefs about the slope before seeing the data. A prior of \\(N(0, 10^2)\\) says “I think \\(\\beta\\) is probably near 0 but I’m very uncertain” (vague). A prior of \\(N(1, 0.3^2)\\) says “I’m fairly confident \\(\\beta\\) is close to 1” (informative). After seeing data, the prior gets updated to a posterior.\n\n\n\nThe math side-by-side:\n\n\n\n\n\n\n\n\n\nOLS\nBayesian\n\n\n\n\nModel\n\\(y = X\\beta + \\varepsilon, \\;\\; \\varepsilon \\sim N(0, \\sigma^2 I)\\)\nSame likelihood + prior \\(\\beta \\sim N(\\mu_0, \\sigma_0^2)\\)\n\n\nEstimate\n\\(\\hat{\\beta}_{OLS} = (X'X)^{-1}X'y\\)\nPosterior mean \\(= w \\cdot \\hat{\\beta}_{OLS} + (1-w) \\cdot \\mu_0\\)\n\n\nUncertainty\nSE, confidence interval\nFull posterior distribution, credible interval\n\n\nResult\nOne number + interval\nEntire distribution\n\n\n\nThe posterior mean is a precision-weighted average of the prior mean and the OLS estimate, where \\(w = \\frac{\\text{data precision}}{\\text{data precision} + \\text{prior precision}}\\). The more data you have, the more the posterior looks like OLS. The stronger the prior (smaller \\(\\sigma_0^2\\)), the more the posterior is pulled toward the prior mean.\n\n\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_beta\", HTML(\"True &beta;:\"),\n                  min = -3, max = 3, value = 1.5, step = 0.5),\n\n      sliderInput(\"prior_mean\", \"Prior mean:\",\n                  min = -3, max = 3, value = 0, step = 0.5),\n\n      sliderInput(\"prior_sd\", \"Prior SD:\",\n                  min = 0.1, max = 5, value = 1, step = 0.1),\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 5, max = 200, value = 20, step = 5),\n\n      sliderInput(\"sigma\", HTML(\"Noise (&sigma;):\"),\n                  min = 0.5, max = 5, value = 2, step = 0.5),\n\n      actionButton(\"go\", \"New data\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"scatter_plot\", height = \"420px\")),\n        column(6, plotOutput(\"density_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    true_beta  &lt;- input$true_beta\n    prior_mean &lt;- input$prior_mean\n    prior_sd   &lt;- input$prior_sd\n    n          &lt;- input$n\n    sigma      &lt;- input$sigma\n\n    # Generate data: y = true_beta * x + noise (centered x, no intercept)\n    x &lt;- rnorm(n, 0, 1)\n    y &lt;- true_beta * x + rnorm(n, 0, sigma)\n\n    # OLS estimate\n    beta_ols &lt;- sum(x * y) / sum(x^2)\n    se_ols   &lt;- sigma / sqrt(sum(x^2))\n\n    # Bayesian posterior (conjugate normal-normal, known sigma)\n    prior_prec &lt;- 1 / prior_sd^2\n    data_prec  &lt;- sum(x^2) / sigma^2\n    post_prec  &lt;- prior_prec + data_prec\n    post_sd    &lt;- 1 / sqrt(post_prec)\n    post_mean  &lt;- (prior_mean * prior_prec + beta_ols * data_prec) / post_prec\n\n    # Weights\n    w_data  &lt;- data_prec / post_prec\n    w_prior &lt;- prior_prec / post_prec\n\n    list(x = x, y = y, true_beta = true_beta,\n         beta_ols = beta_ols, se_ols = se_ols,\n         prior_mean = prior_mean, prior_sd = prior_sd,\n         post_mean = post_mean, post_sd = post_sd,\n         w_data = w_data, w_prior = w_prior, sigma = sigma)\n  })\n\n  output$scatter_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$y, pch = 16, col = \"#2c3e5060\", cex = 1.2,\n         xlab = \"x\", ylab = \"y\", main = \"Data + Regression Lines\")\n\n    # True line (green, dashed)\n    abline(0, d$true_beta, col = \"#27ae60\", lwd = 2.5, lty = 2)\n    # OLS line (red)\n    abline(0, d$beta_ols, col = \"#e74c3c\", lwd = 2.5)\n    # Bayesian line (blue)\n    abline(0, d$post_mean, col = \"#3498db\", lwd = 2.5)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\n             paste0(\"True: \", d$true_beta),\n             paste0(\"OLS: \", round(d$beta_ols, 3)),\n             paste0(\"Bayesian: \", round(d$post_mean, 3))\n           ),\n           col = c(\"#27ae60\", \"#e74c3c\", \"#3498db\"),\n           lwd = 2.5, lty = c(2, 1, 1))\n  })\n\n  output$density_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Range for plotting\n    all_means &lt;- c(d$prior_mean, d$beta_ols, d$post_mean)\n    all_sds   &lt;- c(d$prior_sd, d$se_ols, d$post_sd)\n    xlim &lt;- range(c(all_means - 3.5 * all_sds, all_means + 3.5 * all_sds))\n    x_seq &lt;- seq(xlim[1], xlim[2], length.out = 300)\n\n    # Densities\n    prior_y &lt;- dnorm(x_seq, d$prior_mean, d$prior_sd)\n    lik_y   &lt;- dnorm(x_seq, d$beta_ols, d$se_ols)\n    post_y  &lt;- dnorm(x_seq, d$post_mean, d$post_sd)\n\n    ylim &lt;- c(0, max(c(prior_y, lik_y, post_y)) * 1.15)\n\n    plot(NULL, xlim = xlim, ylim = ylim,\n         xlab = expression(beta), ylab = \"Density\",\n         main = expression(\"Prior, Likelihood, & Posterior for \" * beta))\n\n    # Prior (red, dashed)\n    lines(x_seq, prior_y, col = \"#e74c3c\", lwd = 2, lty = 2)\n\n    # Likelihood (gray, dotted)\n    lines(x_seq, lik_y, col = \"gray50\", lwd = 2, lty = 3)\n\n    # Posterior (blue, shaded)\n    polygon(c(x_seq, rev(x_seq)), c(post_y, rep(0, length(x_seq))),\n            col = \"#3498db30\", border = NA)\n    lines(x_seq, post_y, col = \"#3498db\", lwd = 2.5)\n\n    # True beta\n    abline(v = d$true_beta, lty = 2, col = \"#27ae60\", lwd = 1.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Prior\", \"Likelihood (data)\",\n                      \"Posterior\", expression(\"True \" * beta)),\n           col = c(\"#e74c3c\", \"gray50\", \"#3498db\", \"#27ae60\"),\n           lwd = c(2, 2, 2.5, 1.5),\n           lty = c(2, 3, 1, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;OLS estimate:&lt;/b&gt; \", round(d$beta_ols, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Posterior mean:&lt;/b&gt; \", round(d$post_mean, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;True &beta;:&lt;/b&gt; \", d$true_beta, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Weight on data:&lt;/b&gt; \", round(d$w_data * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;b&gt;Weight on prior:&lt;/b&gt; \", round(d$w_prior * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;OLS SE:&lt;/b&gt; \", round(d$se_ols, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Posterior SD:&lt;/b&gt; \", round(d$post_sd, 3)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nVague prior (prior SD = 5): the posterior almost exactly matches OLS. With a wide prior, the data dominates — the weight on prior drops near 0%. A vague Bayesian is a frequentist.\nStrong prior (prior SD = 0.3): the posterior is pulled heavily toward the prior mean. Even if the data says otherwise, the posterior barely moves. This is shrinkage in action.\nLarge n (n = 200): regardless of the prior, the posterior converges to the OLS estimate. Data overwhelms the prior. Watch the weight on data approach 100%.\nWrong prior (prior mean = -2, true beta = 1.5, n = 10): the Bayesian estimate is biased toward -2. Now increase n — the data corrects the bad prior. This is why Bayesian inference is “self-correcting” with enough data.\nCompare the right panel: the posterior (blue) always sits between the prior (red) and the likelihood (gray). It’s literally a compromise between what you believed and what you saw.",
    "crumbs": [
      "Foundations",
      "Bayesian Regression"
    ]
  },
  {
    "objectID": "bayesian-regression.html#from-reg-y-x-to-bayesian-regression",
    "href": "bayesian-regression.html#from-reg-y-x-to-bayesian-regression",
    "title": "Bayesian Regression",
    "section": "",
    "text": "On the MCMC page, you saw how to sample from any posterior — even when no closed-form solution exists. Now we apply that machinery to something familiar: regression.\nIn Stata, you type reg y x and get a point estimate \\(\\hat{\\beta}\\), a standard error, and a 95% confidence interval. That’s OLS — it picks the single line that minimizes the sum of squared residuals.\nBayesian regression starts from the same place (the same likelihood) but adds one ingredient: a prior distribution on \\(\\beta\\). Instead of a single best-fit line, you get a full posterior distribution — a curve showing how plausible each value of \\(\\beta\\) is, given both the data and your prior beliefs.\n\n\n\n\n\n\nExample: You’re estimating the effect of an extra year of education on hourly wages. Before seeing your data, you’ve read the literature — most estimates land between $0.50 and $1.50 per year. A Bayesian approach lets you encode this: set a prior of \\(\\beta \\sim N(1.0, 0.5^2)\\), centered on $1.00 with moderate uncertainty. OLS ignores this information entirely. The Bayesian estimate blends it with your data.\n\n\n\n\n\n\n\n\n\nWhat “a prior on \\(\\beta\\)” means: it’s a probability distribution expressing your beliefs about the slope before seeing the data. A prior of \\(N(0, 10^2)\\) says “I think \\(\\beta\\) is probably near 0 but I’m very uncertain” (vague). A prior of \\(N(1, 0.3^2)\\) says “I’m fairly confident \\(\\beta\\) is close to 1” (informative). After seeing data, the prior gets updated to a posterior.\n\n\n\nThe math side-by-side:\n\n\n\n\n\n\n\n\n\nOLS\nBayesian\n\n\n\n\nModel\n\\(y = X\\beta + \\varepsilon, \\;\\; \\varepsilon \\sim N(0, \\sigma^2 I)\\)\nSame likelihood + prior \\(\\beta \\sim N(\\mu_0, \\sigma_0^2)\\)\n\n\nEstimate\n\\(\\hat{\\beta}_{OLS} = (X'X)^{-1}X'y\\)\nPosterior mean \\(= w \\cdot \\hat{\\beta}_{OLS} + (1-w) \\cdot \\mu_0\\)\n\n\nUncertainty\nSE, confidence interval\nFull posterior distribution, credible interval\n\n\nResult\nOne number + interval\nEntire distribution\n\n\n\nThe posterior mean is a precision-weighted average of the prior mean and the OLS estimate, where \\(w = \\frac{\\text{data precision}}{\\text{data precision} + \\text{prior precision}}\\). The more data you have, the more the posterior looks like OLS. The stronger the prior (smaller \\(\\sigma_0^2\\)), the more the posterior is pulled toward the prior mean.\n\n\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_beta\", HTML(\"True &beta;:\"),\n                  min = -3, max = 3, value = 1.5, step = 0.5),\n\n      sliderInput(\"prior_mean\", \"Prior mean:\",\n                  min = -3, max = 3, value = 0, step = 0.5),\n\n      sliderInput(\"prior_sd\", \"Prior SD:\",\n                  min = 0.1, max = 5, value = 1, step = 0.1),\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 5, max = 200, value = 20, step = 5),\n\n      sliderInput(\"sigma\", HTML(\"Noise (&sigma;):\"),\n                  min = 0.5, max = 5, value = 2, step = 0.5),\n\n      actionButton(\"go\", \"New data\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"scatter_plot\", height = \"420px\")),\n        column(6, plotOutput(\"density_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    true_beta  &lt;- input$true_beta\n    prior_mean &lt;- input$prior_mean\n    prior_sd   &lt;- input$prior_sd\n    n          &lt;- input$n\n    sigma      &lt;- input$sigma\n\n    # Generate data: y = true_beta * x + noise (centered x, no intercept)\n    x &lt;- rnorm(n, 0, 1)\n    y &lt;- true_beta * x + rnorm(n, 0, sigma)\n\n    # OLS estimate\n    beta_ols &lt;- sum(x * y) / sum(x^2)\n    se_ols   &lt;- sigma / sqrt(sum(x^2))\n\n    # Bayesian posterior (conjugate normal-normal, known sigma)\n    prior_prec &lt;- 1 / prior_sd^2\n    data_prec  &lt;- sum(x^2) / sigma^2\n    post_prec  &lt;- prior_prec + data_prec\n    post_sd    &lt;- 1 / sqrt(post_prec)\n    post_mean  &lt;- (prior_mean * prior_prec + beta_ols * data_prec) / post_prec\n\n    # Weights\n    w_data  &lt;- data_prec / post_prec\n    w_prior &lt;- prior_prec / post_prec\n\n    list(x = x, y = y, true_beta = true_beta,\n         beta_ols = beta_ols, se_ols = se_ols,\n         prior_mean = prior_mean, prior_sd = prior_sd,\n         post_mean = post_mean, post_sd = post_sd,\n         w_data = w_data, w_prior = w_prior, sigma = sigma)\n  })\n\n  output$scatter_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$y, pch = 16, col = \"#2c3e5060\", cex = 1.2,\n         xlab = \"x\", ylab = \"y\", main = \"Data + Regression Lines\")\n\n    # True line (green, dashed)\n    abline(0, d$true_beta, col = \"#27ae60\", lwd = 2.5, lty = 2)\n    # OLS line (red)\n    abline(0, d$beta_ols, col = \"#e74c3c\", lwd = 2.5)\n    # Bayesian line (blue)\n    abline(0, d$post_mean, col = \"#3498db\", lwd = 2.5)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\n             paste0(\"True: \", d$true_beta),\n             paste0(\"OLS: \", round(d$beta_ols, 3)),\n             paste0(\"Bayesian: \", round(d$post_mean, 3))\n           ),\n           col = c(\"#27ae60\", \"#e74c3c\", \"#3498db\"),\n           lwd = 2.5, lty = c(2, 1, 1))\n  })\n\n  output$density_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Range for plotting\n    all_means &lt;- c(d$prior_mean, d$beta_ols, d$post_mean)\n    all_sds   &lt;- c(d$prior_sd, d$se_ols, d$post_sd)\n    xlim &lt;- range(c(all_means - 3.5 * all_sds, all_means + 3.5 * all_sds))\n    x_seq &lt;- seq(xlim[1], xlim[2], length.out = 300)\n\n    # Densities\n    prior_y &lt;- dnorm(x_seq, d$prior_mean, d$prior_sd)\n    lik_y   &lt;- dnorm(x_seq, d$beta_ols, d$se_ols)\n    post_y  &lt;- dnorm(x_seq, d$post_mean, d$post_sd)\n\n    ylim &lt;- c(0, max(c(prior_y, lik_y, post_y)) * 1.15)\n\n    plot(NULL, xlim = xlim, ylim = ylim,\n         xlab = expression(beta), ylab = \"Density\",\n         main = expression(\"Prior, Likelihood, & Posterior for \" * beta))\n\n    # Prior (red, dashed)\n    lines(x_seq, prior_y, col = \"#e74c3c\", lwd = 2, lty = 2)\n\n    # Likelihood (gray, dotted)\n    lines(x_seq, lik_y, col = \"gray50\", lwd = 2, lty = 3)\n\n    # Posterior (blue, shaded)\n    polygon(c(x_seq, rev(x_seq)), c(post_y, rep(0, length(x_seq))),\n            col = \"#3498db30\", border = NA)\n    lines(x_seq, post_y, col = \"#3498db\", lwd = 2.5)\n\n    # True beta\n    abline(v = d$true_beta, lty = 2, col = \"#27ae60\", lwd = 1.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Prior\", \"Likelihood (data)\",\n                      \"Posterior\", expression(\"True \" * beta)),\n           col = c(\"#e74c3c\", \"gray50\", \"#3498db\", \"#27ae60\"),\n           lwd = c(2, 2, 2.5, 1.5),\n           lty = c(2, 3, 1, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;OLS estimate:&lt;/b&gt; \", round(d$beta_ols, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Posterior mean:&lt;/b&gt; \", round(d$post_mean, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;True &beta;:&lt;/b&gt; \", d$true_beta, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Weight on data:&lt;/b&gt; \", round(d$w_data * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;b&gt;Weight on prior:&lt;/b&gt; \", round(d$w_prior * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;OLS SE:&lt;/b&gt; \", round(d$se_ols, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Posterior SD:&lt;/b&gt; \", round(d$post_sd, 3)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nVague prior (prior SD = 5): the posterior almost exactly matches OLS. With a wide prior, the data dominates — the weight on prior drops near 0%. A vague Bayesian is a frequentist.\nStrong prior (prior SD = 0.3): the posterior is pulled heavily toward the prior mean. Even if the data says otherwise, the posterior barely moves. This is shrinkage in action.\nLarge n (n = 200): regardless of the prior, the posterior converges to the OLS estimate. Data overwhelms the prior. Watch the weight on data approach 100%.\nWrong prior (prior mean = -2, true beta = 1.5, n = 10): the Bayesian estimate is biased toward -2. Now increase n — the data corrects the bad prior. This is why Bayesian inference is “self-correcting” with enough data.\nCompare the right panel: the posterior (blue) always sits between the prior (red) and the likelihood (gray). It’s literally a compromise between what you believed and what you saw.",
    "crumbs": [
      "Foundations",
      "Bayesian Regression"
    ]
  },
  {
    "objectID": "bayesian-regression.html#credible-intervals-vs-confidence-intervals",
    "href": "bayesian-regression.html#credible-intervals-vs-confidence-intervals",
    "title": "Bayesian Regression",
    "section": "Credible intervals vs confidence intervals",
    "text": "Credible intervals vs confidence intervals\nBoth give you a range of plausible values for \\(\\beta\\). But they answer different questions:\n\n\n\n\n\n\n\n\n\nConfidence interval (frequentist)\nCredible interval (Bayesian)\n\n\n\n\nStatement\n“If I repeated this experiment many times, 95% of my CIs would contain the true \\(\\beta\\)”\n“Given the data and my prior, there’s a 95% probability that \\(\\beta\\) is in this interval”\n\n\nAbout\nThe procedure\nThe parameter\n\n\nDepends on prior?\nNo\nYes\n\n\nInterpretation\nFrequency guarantee across experiments\nDirect probability statement for this experiment\n\n\n\nThe credible interval says what most people think the confidence interval says. See Bayesian vs Frequentist for more on this distinction.",
    "crumbs": [
      "Foundations",
      "Bayesian Regression"
    ]
  },
  {
    "objectID": "bayesian-regression.html#the-prior-as-regularization",
    "href": "bayesian-regression.html#the-prior-as-regularization",
    "title": "Bayesian Regression",
    "section": "The prior as regularization",
    "text": "The prior as regularization\nA prior isn’t just a philosophical stance — it has a direct mathematical connection to regularization:\n\n\n\n\n\n\n\n\nPrior on \\(\\beta\\)\nEquivalent to\nPenalty\n\n\n\n\n\\(N(0, \\sigma_0^2)\\)\nRidge regression\n\\(\\lambda = \\sigma^2 / \\sigma_0^2\\)\n\n\nLaplace\\((0, b)\\)\nLASSO\n\\(\\lambda = \\sigma^2 / b\\)\n\n\nFlat (improper)\nOLS\nNo penalty\n\n\n\n\n\n\n\n\n\nMAP vs full Bayesian: The maximum a posteriori (MAP) estimate — the peak of the posterior — equals the ridge/LASSO solution exactly. But full Bayesian inference gives you the entire posterior, not just the peak. You get uncertainty for free.\n\n\n\nA tighter prior (smaller \\(\\sigma_0^2\\)) implies stronger regularization (larger \\(\\lambda\\)). This is why Bayesian regression with an informative prior produces shrinkage — it pulls estimates toward the prior mean, just like ridge regression pulls coefficients toward zero.\n\nMAP vs MLE: where the prior shows up in optimization\nThree ways to estimate \\(\\beta\\), each using a different objective:\n\n\n\n\n\n\n\n\n\n\nMLE\nMAP\nFull Bayesian\n\n\n\n\nObjective\n\\(\\arg\\max_\\beta \\; p(y \\mid \\beta)\\)\n\\(\\arg\\max_\\beta \\; p(y \\mid \\beta)\\,p(\\beta)\\)\nCompute \\(p(\\beta \\mid y)\\)\n\n\nWhat you get\nPoint estimate\nPoint estimate\nEntire distribution\n\n\nUncertainty?\nOnly via SE / asymptotics\nNo (just the peak)\nYes — built in\n\n\nEquivalent to\nOLS\nRidge / LASSO\n—\n\n\n\nIn log space the relationship is transparent:\n\\[\\log p(\\beta \\mid y) = \\underbrace{\\log p(y \\mid \\beta)}_{\\text{log-likelihood}} + \\underbrace{\\log p(\\beta)}_{\\text{log-prior}} - \\text{const}\\]\n\nMLE maximizes the first term alone.\nMAP maximizes the first plus the second — the prior acts as a penalty.\n\nWhat does that penalty look like?\n\nNormal prior \\(\\beta \\sim N(0, \\sigma_0^2)\\): \\(\\log p(\\beta) = -\\beta^2 / 2\\sigma_0^2 + \\text{const}\\) — that’s the ridge (\\(L_2\\)) penalty.\nLaplace prior \\(\\beta \\sim \\text{Laplace}(0, b)\\): \\(\\log p(\\beta) = -|\\beta| / b + \\text{const}\\) — that’s the LASSO (\\(L_1\\)) penalty.\n\nSo MAP = regularized MLE. Full Bayesian = MAP + uncertainty. The posterior mean, posterior median, and MAP (posterior mode) are three different summaries of the same posterior distribution. For symmetric posteriors (like the normal-normal case in the simulation above) they coincide, but in general they can differ.\n\n\n\n\n\n\nConnecting to the simulation above: the OLS line is the MLE. For the symmetric normal posterior in this simulation, the posterior mean essentially equals the MAP. Try a strong prior (SD = 0.3) and watch both get pulled toward the prior mean — that’s the ridge penalty at work. Then set the prior SD to 5 and watch MAP converge to MLE — a vague prior means almost no penalty.\n\n\n\n\n\nSimulation: Repeated experiments — CI vs CrI coverage\nRun the same experiment many times. Each repetition generates new data from the true model, computes both a frequentist CI and a Bayesian CrI, and checks whether each contains the true \\(\\beta\\). The prior is centered at 0 (like ridge regression).\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_beta2\", HTML(\"True &beta;:\"),\n                  min = -3, max = 3, value = 1.5, step = 0.5),\n\n      sliderInput(\"prior_sd2\", \"Prior SD:\",\n                  min = 0.1, max = 5, value = 1, step = 0.1),\n\n      sliderInput(\"n2\", \"Sample size (n):\",\n                  min = 5, max = 200, value = 20, step = 5),\n\n      sliderInput(\"n_reps\", \"Repetitions:\",\n                  min = 10, max = 100, value = 40, step = 10),\n\n      actionButton(\"go2\", \"Run experiments\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"interval_plot\", height = \"420px\")),\n        column(6, plotOutput(\"dot_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat2 &lt;- reactive({\n    input$go2\n    true_beta  &lt;- input$true_beta2\n    prior_sd   &lt;- input$prior_sd2\n    n          &lt;- input$n2\n    n_reps     &lt;- input$n_reps\n    sigma      &lt;- 2\n    prior_mean &lt;- 0  # centered at 0, like ridge\n\n    prior_prec &lt;- 1 / prior_sd^2\n\n    # Storage: ols_est, ols_lo, ols_hi, bayes_est, bayes_lo, bayes_hi\n    results &lt;- matrix(NA, nrow = n_reps, ncol = 6)\n\n    for (r in 1:n_reps) {\n      x &lt;- rnorm(n, 0, 1)\n      y &lt;- true_beta * x + rnorm(n, 0, sigma)\n\n      beta_ols &lt;- sum(x * y) / sum(x^2)\n      se_ols   &lt;- sigma / sqrt(sum(x^2))\n\n      data_prec &lt;- sum(x^2) / sigma^2\n      post_prec &lt;- prior_prec + data_prec\n      post_sd   &lt;- 1 / sqrt(post_prec)\n      post_mean &lt;- (prior_mean * prior_prec + beta_ols * data_prec) / post_prec\n\n      results[r, ] &lt;- c(\n        beta_ols,\n        beta_ols - 1.96 * se_ols,\n        beta_ols + 1.96 * se_ols,\n        post_mean,\n        qnorm(0.025, post_mean, post_sd),\n        qnorm(0.975, post_mean, post_sd)\n      )\n    }\n\n    # Coverage\n    ci_covers  &lt;- results[, 2] &lt;= true_beta & results[, 3] &gt;= true_beta\n    cri_covers &lt;- results[, 5] &lt;= true_beta & results[, 6] &gt;= true_beta\n\n    # Average widths\n    ci_width  &lt;- mean(results[, 3] - results[, 2])\n    cri_width &lt;- mean(results[, 6] - results[, 5])\n\n    # MSE\n    mse_ols   &lt;- mean((results[, 1] - true_beta)^2)\n    mse_bayes &lt;- mean((results[, 4] - true_beta)^2)\n\n    # Implied ridge lambda\n    lambda &lt;- sigma^2 / prior_sd^2\n\n    list(results = results, true_beta = true_beta, n_reps = n_reps,\n         ci_covers = ci_covers, cri_covers = cri_covers,\n         ci_width = ci_width, cri_width = cri_width,\n         mse_ols = mse_ols, mse_bayes = mse_bayes, lambda = lambda)\n  })\n\n  output$interval_plot &lt;- renderPlot({\n    d &lt;- dat2()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    n_show &lt;- min(d$n_reps, 40)\n    xlim &lt;- range(c(d$results[1:n_show, 2:3],\n                    d$results[1:n_show, 5:6], d$true_beta)) + c(-0.3, 0.3)\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, n_show + 0.5),\n         xlab = expression(beta), ylab = \"Experiment #\",\n         main = \"95% Intervals Across Experiments\")\n\n    for (i in 1:n_show) {\n      # CI (red)\n      ci_col &lt;- if (d$ci_covers[i]) \"#e74c3c\" else \"#e74c3c40\"\n      segments(d$results[i, 2], i - 0.15, d$results[i, 3], i - 0.15,\n               lwd = 2, col = ci_col)\n\n      # CrI (blue)\n      cri_col &lt;- if (d$cri_covers[i]) \"#3498db\" else \"#3498db40\"\n      segments(d$results[i, 5], i + 0.15, d$results[i, 6], i + 0.15,\n               lwd = 2, col = cri_col)\n    }\n\n    abline(v = d$true_beta, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\n             paste0(\"CI (\", sum(d$ci_covers[1:n_show]), \"/\", n_show, \" cover)\"),\n             paste0(\"CrI (\", sum(d$cri_covers[1:n_show]), \"/\", n_show, \" cover)\")\n           ),\n           col = c(\"#e74c3c\", \"#3498db\"), lwd = 3)\n  })\n\n  output$dot_plot &lt;- renderPlot({\n    d &lt;- dat2()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    n_show &lt;- min(d$n_reps, 40)\n    xlim &lt;- range(c(d$results[1:n_show, 1],\n                    d$results[1:n_show, 4], d$true_beta)) + c(-0.3, 0.3)\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, n_show + 0.5),\n         xlab = expression(hat(beta)), ylab = \"Experiment #\",\n         main = \"Point Estimates Across Experiments\")\n\n    for (i in 1:n_show) {\n      points(d$results[i, 1], i - 0.12, pch = 16, col = \"#e74c3c\", cex = 0.9)\n      points(d$results[i, 4], i + 0.12, pch = 17, col = \"#3498db\", cex = 0.9)\n    }\n\n    abline(v = d$true_beta, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\"OLS estimate\", \"Posterior mean\"),\n           col = c(\"#e74c3c\", \"#3498db\"),\n           pch = c(16, 17))\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- dat2()\n\n    ci_rate  &lt;- round(mean(d$ci_covers) * 100, 1)\n    cri_rate &lt;- round(mean(d$cri_covers) * 100, 1)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;CI coverage:&lt;/b&gt; \", ci_rate, \"%&lt;br&gt;\",\n        \"&lt;b&gt;CrI coverage:&lt;/b&gt; \", cri_rate, \"%&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Avg CI width:&lt;/b&gt; \", round(d$ci_width, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Avg CrI width:&lt;/b&gt; \", round(d$cri_width, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;MSE (OLS):&lt;/b&gt; \", round(d$mse_ols, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;MSE (Bayes):&lt;/b&gt; \", round(d$mse_bayes, 4), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Implied ridge &lambda;:&lt;/b&gt; \", round(d$lambda, 3)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nLarge n (n = 200): both intervals are nearly identical. CI and CrI coverage, width, and MSE all converge. With lots of data, Bayesian = frequentist.\nStrong prior (prior SD = 0.3), true beta = 1.5: the CrI is narrower but the prior pulls estimates toward 0. Some CrIs miss the true value — the coverage drops below 95%. A strong wrong prior hurts.\nVague prior (prior SD = 5): the CrI and CI are virtually the same. The implied ridge \\(\\lambda\\) is tiny — almost no regularization.\nTrue beta = 0, prior SD = 1: the prior is correct! The CrI has excellent coverage and is narrower than the CI. The Bayesian MSE beats OLS because the prior genuinely helps.\nRight panel: notice how the blue dots (Bayesian) cluster tighter around the true value than the red dots (OLS) when the prior is reasonable. That’s the bias-variance tradeoff — a little bias from the prior reduces variance enough to lower overall MSE.",
    "crumbs": [
      "Foundations",
      "Bayesian Regression"
    ]
  },
  {
    "objectID": "bayesian-regression.html#in-stata-bayes-reg-y-x",
    "href": "bayesian-regression.html#in-stata-bayes-reg-y-x",
    "title": "Bayesian Regression",
    "section": "In Stata: bayes: reg y x",
    "text": "In Stata: bayes: reg y x\nStata makes Bayesian regression straightforward. Just prepend bayes: to your regression command:\n* Frequentist OLS\nreg wage education experience\n\n* Bayesian regression (default priors)\nbayes: reg wage education experience\n\n* With custom priors\nbayes, prior({wage:education}, normal(1, 0.25)) ///\n      prior({wage:experience}, normal(0.5, 1))  ///\n      : reg wage education experience\nThe output looks different from OLS — instead of a coefficient table with p-values, you get posterior means, standard deviations, and credible intervals. Under the hood, Stata uses the Metropolis-Hastings algorithm (see MCMC) to sample from the posterior.\n\n\n\n\n\n\nTry it: run reg y x and bayes: reg y x on the same dataset. With Stata’s default (vague) priors and moderate sample sizes, the results will be nearly identical — confirming that the prior washes out with enough data.",
    "crumbs": [
      "Foundations",
      "Bayesian Regression"
    ]
  },
  {
    "objectID": "bayesian-regression.html#did-you-know",
    "href": "bayesian-regression.html#did-you-know",
    "title": "Bayesian Regression",
    "section": "Did you know?",
    "text": "Did you know?\n\nLindley & Smith (1972) formalized the conjugate Bayesian linear model, showing how hierarchical priors on regression coefficients lead to shrinkage estimators. Their work unified the Bayesian and empirical Bayes approaches to regression.\nHoerl & Kennard (1970) introduced ridge regression as a purely frequentist technique for handling multicollinearity. The Bayesian interpretation — that ridge is equivalent to a normal prior — came later, connecting two literatures that developed independently.\nModern frontiers: Bayesian regression ideas power some of today’s most flexible methods. BART (Bayesian Additive Regression Trees) uses priors on tree structures for nonparametric regression. Bayesian model averaging puts priors on competing models themselves, not just parameters, to account for model uncertainty.",
    "crumbs": [
      "Foundations",
      "Bayesian Regression"
    ]
  },
  {
    "objectID": "hierarchical.html",
    "href": "hierarchical.html",
    "title": "Hierarchical Models",
    "section": "",
    "text": "You have data from \\(K\\) groups — schools, hospitals, factories, regions — and you want to estimate a parameter (say, the mean) for each group. Two extreme approaches:\n\nNo pooling: estimate each group separately using only its own data. With small groups, the estimates are noisy and unreliable.\nComplete pooling: ignore groups entirely and estimate a single grand mean. This throws away real group differences.\n\nNeither is satisfying. No pooling overfits to noise. Complete pooling underfits by ignoring structure. You want something in between.\n\n\n\n\n\n\nExample: Uber driver ratings. Uber has millions of drivers. A new driver with 3 rides and a perfect 5.0 rating — is she really the best driver on the platform? A veteran with 2,000 rides and a 4.7 — is he worse? Common sense says no: the 5.0 is mostly luck (small sample), and the 4.7 is a reliable signal (large sample). Partial pooling formalizes this: shrink the new driver’s 5.0 toward the platform average (say, 4.8), but barely touch the veteran’s 4.7. Every driver’s estimate improves — especially the ones with few rides.",
    "crumbs": [
      "Hierarchical Modeling",
      "Hierarchical Models"
    ]
  },
  {
    "objectID": "hierarchical.html#the-problem",
    "href": "hierarchical.html#the-problem",
    "title": "Hierarchical Models",
    "section": "",
    "text": "You have data from \\(K\\) groups — schools, hospitals, factories, regions — and you want to estimate a parameter (say, the mean) for each group. Two extreme approaches:\n\nNo pooling: estimate each group separately using only its own data. With small groups, the estimates are noisy and unreliable.\nComplete pooling: ignore groups entirely and estimate a single grand mean. This throws away real group differences.\n\nNeither is satisfying. No pooling overfits to noise. Complete pooling underfits by ignoring structure. You want something in between.\n\n\n\n\n\n\nExample: Uber driver ratings. Uber has millions of drivers. A new driver with 3 rides and a perfect 5.0 rating — is she really the best driver on the platform? A veteran with 2,000 rides and a 4.7 — is he worse? Common sense says no: the 5.0 is mostly luck (small sample), and the 4.7 is a reliable signal (large sample). Partial pooling formalizes this: shrink the new driver’s 5.0 toward the platform average (say, 4.8), but barely touch the veteran’s 4.7. Every driver’s estimate improves — especially the ones with few rides.",
    "crumbs": [
      "Hierarchical Modeling",
      "Hierarchical Models"
    ]
  },
  {
    "objectID": "hierarchical.html#hierarchical-models-partial-pooling",
    "href": "hierarchical.html#hierarchical-models-partial-pooling",
    "title": "Hierarchical Models",
    "section": "Hierarchical models: partial pooling",
    "text": "Hierarchical models: partial pooling\nA hierarchical (multilevel) model learns the group-level variation from the data and uses it to shrink each group’s estimate toward the grand mean — more for small groups, less for large groups.\nThe model:\n\\[y_{ij} \\sim N(\\theta_j, \\sigma^2) \\quad \\text{(data within group } j\\text{)}\\] \\[\\theta_j \\sim N(\\mu, \\tau^2) \\quad \\text{(group means come from a population)}\\]\n\n\\(\\theta_j\\) is the true mean for group \\(j\\)\n\\(\\mu\\) is the overall population mean\n\\(\\tau^2\\) is the between-group variance (how different groups really are)\n\\(\\sigma^2\\) is the within-group variance (noise)\n\nThe posterior for each \\(\\theta_j\\) is a weighted average of the group’s own data and the grand mean:\n\\[\\hat{\\theta}_j^{partial} = w_j \\cdot \\bar{y}_j + (1 - w_j) \\cdot \\hat{\\mu}\\]\nwhere the weight \\(w_j = \\frac{n_j / \\sigma^2}{n_j / \\sigma^2 + 1/\\tau^2}\\) depends on the group sample size \\(n_j\\). Small groups shrink more because their data is less informative relative to the population prior.\nThis is the same shrinkage logic from the shrinkage page, but now applied within a formal model that estimates \\(\\tau^2\\) from the data.\n\nSimulation: School test scores\n\\(K\\) schools, each with \\(n_k\\) students. Compare three estimates for each school’s true mean:\n\nNo pooling (red): each school’s raw sample mean\nComplete pooling (gray): the grand mean for all schools\nPartial pooling (blue): the hierarchical Bayes estimate\n\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"K\", \"Number of schools:\",\n                  min = 5, max = 30, value = 12, step = 1),\n\n      sliderInput(\"n_per\", \"Students per school:\",\n                  min = 5, max = 100, value = 15, step = 5),\n\n      sliderInput(\"tau\", \"Between-school SD (\\u03C4):\",\n                  min = 1, max = 15, value = 5, step = 1),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"school_plot\", height = \"450px\")),\n        column(6, plotOutput(\"mse_plot\",    height = \"450px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    K     &lt;- input$K\n    n_per &lt;- input$n_per\n    tau   &lt;- input$tau\n    sigma &lt;- 10  # within-school SD (fixed)\n\n    # True school means\n    mu &lt;- 70  # grand mean\n    theta_true &lt;- rnorm(K, mean = mu, sd = tau)\n\n    # Generate student scores\n    y_bar &lt;- numeric(K)\n    for (j in 1:K) {\n      scores &lt;- rnorm(n_per, mean = theta_true[j], sd = sigma)\n      y_bar[j] &lt;- mean(scores)\n    }\n\n    # No pooling: raw means\n    no_pool &lt;- y_bar\n\n    # Complete pooling: grand mean\n    grand_mean &lt;- mean(y_bar)\n    complete_pool &lt;- rep(grand_mean, K)\n\n    # Partial pooling (empirical Bayes)\n    # Estimate tau from data\n    between_var &lt;- var(y_bar)\n    within_var  &lt;- sigma^2 / n_per\n    tau_hat_sq  &lt;- max(between_var - within_var, 0.01)\n\n    w &lt;- (n_per / sigma^2) / (n_per / sigma^2 + 1 / tau_hat_sq)\n    partial_pool &lt;- w * y_bar + (1 - w) * grand_mean\n\n    # MSE\n    mse_no   &lt;- mean((no_pool - theta_true)^2)\n    mse_comp &lt;- mean((complete_pool - theta_true)^2)\n    mse_part &lt;- mean((partial_pool - theta_true)^2)\n\n    list(theta_true = theta_true, no_pool = no_pool,\n         complete_pool = complete_pool, partial_pool = partial_pool,\n         grand_mean = grand_mean, w = w,\n         mse_no = mse_no, mse_comp = mse_comp, mse_part = mse_part,\n         K = K, n_per = n_per, tau = tau)\n  })\n\n  output$school_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 5.5, 3, 1))\n\n    K &lt;- d$K\n    ord &lt;- order(d$no_pool)\n\n    xlim &lt;- range(c(d$no_pool, d$partial_pool, d$theta_true, d$grand_mean))\n    xlim &lt;- xlim + c(-2, 2)\n\n    plot(NULL, xlim = xlim, ylim = c(1, K),\n         yaxt = \"n\", xlab = \"Score\",\n         ylab = \"\", main = \"School Mean Estimates\")\n    axis(2, at = 1:K, labels = paste0(\"School \", ord), las = 1, cex.axis = 0.7)\n\n    # Grand mean line\n    abline(v = d$grand_mean, lty = 2, col = \"gray50\", lwd = 1.5)\n\n    for (i in 1:K) {\n      j &lt;- ord[i]\n\n      # Arrow from no-pooling to partial-pooling\n      arrows(d$no_pool[j], i, d$partial_pool[j], i,\n             length = 0.04, col = \"#bdc3c780\", lwd = 1)\n\n      # Points\n      points(d$no_pool[j], i, pch = 16, col = \"#e74c3c\", cex = 1.2)\n      points(d$partial_pool[j], i, pch = 17, col = \"#3498db\", cex = 1.2)\n      points(d$theta_true[j], i, pch = 4, col = \"#27ae60\", cex = 1, lwd = 2)\n    }\n\n    legend(\"bottomright\", bty = \"n\", cex = 0.8,\n           legend = c(\"No pooling (raw mean)\", \"Partial pooling\",\n                      \"True mean\", \"Grand mean\"),\n           col = c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"gray50\"),\n           pch = c(16, 17, 4, NA),\n           lty = c(NA, NA, NA, 2), lwd = c(NA, NA, 2, 1.5))\n  })\n\n  output$mse_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4, 3, 1))\n\n    vals &lt;- c(d$mse_no, d$mse_comp, d$mse_part)\n    cols &lt;- c(\"#e74c3c\", \"gray60\", \"#3498db\")\n    labels &lt;- c(\"No pooling\", \"Complete\\npooling\", \"Partial\\npooling\")\n\n    bp &lt;- barplot(vals, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.85,\n                  main = \"Mean Squared Error\",\n                  ylab = \"MSE\")\n    text(bp, vals + max(vals) * 0.03, round(vals, 1), cex = 0.85, font = 2)\n\n    pct &lt;- round((1 - d$mse_part / d$mse_no) * 100, 0)\n    mtext(paste0(\"Partial pooling reduces MSE by ~\", pct, \"% vs no pooling\"),\n          side = 1, line = 3.5, cex = 0.85, col = \"#2c3e50\")\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    pct_np &lt;- round((1 - d$mse_part / d$mse_no) * 100, 1)\n    pct_cp &lt;- round((1 - d$mse_part / d$mse_comp) * 100, 1)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Shrinkage weight:&lt;/b&gt; \", round(d$w * 100, 1),\n        \"% on group data&lt;br&gt;\",\n        \"&lt;small&gt;(\", round((1 - d$w) * 100, 1), \"% on grand mean)&lt;/small&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;MSE no pooling:&lt;/b&gt; &lt;span class='bad'&gt;\",\n        round(d$mse_no, 1), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;MSE complete pooling:&lt;/b&gt; \",\n        round(d$mse_comp, 1), \"&lt;br&gt;\",\n        \"&lt;b&gt;MSE partial pooling:&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$mse_part, 1), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;vs no pooling:&lt;/b&gt; &lt;span class='good'&gt;\",\n        pct_np, \"% lower&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;vs complete pooling:&lt;/b&gt; &lt;span class='good'&gt;\",\n        pct_cp, \"% lower&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nStudents per school = 5, between-school SD = 5: small samples, real group differences. The no-pooling estimates (red) are scattered — some far from the truth. Partial pooling (blue triangles) shrinks them toward the grand mean, landing closer to the true values (green crosses). MSE drops substantially.\nStudents per school = 100: with lots of data per group, the raw means are already precise. Shrinkage is minimal — blue and red nearly overlap. With enough data, partial pooling = no pooling.\nBetween-school SD = 1: schools are very similar. Complete pooling is nearly optimal because the group differences are tiny. Partial pooling agrees — it shrinks almost entirely to the grand mean.\nBetween-school SD = 15: schools differ a lot. Complete pooling is terrible because it ignores real differences. No pooling is better, and partial pooling is best — it respects both the data and the group structure.\nLook at the left plot: the arrows show shrinkage. Extreme schools (raw means far from the grand mean) shrink the most. Schools near the middle barely move. This is adaptive — the model learns how much shrinkage is appropriate.",
    "crumbs": [
      "Hierarchical Modeling",
      "Hierarchical Models"
    ]
  },
  {
    "objectID": "hierarchical.html#why-partial-pooling-wins",
    "href": "hierarchical.html#why-partial-pooling-wins",
    "title": "Hierarchical Models",
    "section": "Why partial pooling wins",
    "text": "Why partial pooling wins\nThe logic is identical to the shrinkage page, but in a structured model:\n\nNo pooling has zero bias but high variance (each estimate uses only local data).\nComplete pooling has high bias but zero variance across groups.\nPartial pooling trades a little bias for a large reduction in variance. The bias-variance tradeoff is optimized by the model.\n\nThe more groups you have, the better the model estimates \\(\\tau^2\\) (the between-group variance), and the more precisely it calibrates the amount of shrinkage.\n\nWhere hierarchical models show up\n\n\n\nApplication\nGroups\nWhat gets shrunk\n\n\n\n\nEducation\nSchools / districts\nTest score means\n\n\nSports analytics\nPlayers / teams\nBatting averages, win rates\n\n\nClinical trials\nStudy sites / subgroups\nTreatment effects\n\n\nMarketing\nRegions / customer segments\nResponse rates\n\n\nEcology\nSpecies / habitats\nPopulation parameters\n\n\n\nIn each case, the hierarchical structure lets you borrow strength across groups — improving estimates for every group, especially the small ones.\n\n\nWhen is your data hierarchical?\nWhenever units are nested inside groups, and you expect units in the same group to be more similar to each other than to units in other groups.\n\n\n\n\n\n\nExample: dollar stores and obesity. Say you’re studying whether dollar store presence affects obesity rates. Your data has census tracts nested within counties. That’s hierarchical — tracts in the same county share the local food environment, demographics, and policy context.\n\nLevel 1 (tracts): each tract has its own obesity rate, influenced by dollar store presence + tract-level covariates\nLevel 2 (counties): tract-level effects come from a county-level distribution — tracts in the same county are more similar to each other than to tracts in other counties\n\nPartial pooling matters for small counties with few tracts. If a county has only 2 tracts, the raw county average is noisy. The hierarchical model shrinks it toward the overall pattern — borrowing strength from larger counties.\n\n\n\nWithout a hierarchical model, you’d have to choose:\n\n\n\n\n\n\n\nApproach\nProblem\n\n\n\n\nIgnore counties, pool all tracts\nMisses that tracts in the same county share unobserved factors\n\n\nCounty fixed effects\nEats degrees of freedom; can’t estimate county-level predictors; small counties get noisy estimates\n\n\nHierarchical model\nBest of both — partial pooling, can include county-level predictors, small counties get regularized\n\n\n\nThe rule of thumb: if your research question involves variation across groups (does the effect differ by county? do rural vs urban counties respond differently?), a hierarchical model is the natural fit. If you’re just estimating one overall effect and groups are a nuisance, fixed effects or clustered SEs might be enough.",
    "crumbs": [
      "Hierarchical Modeling",
      "Hierarchical Models"
    ]
  },
  {
    "objectID": "hierarchical.html#did-you-know",
    "href": "hierarchical.html#did-you-know",
    "title": "Hierarchical Models",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe “8 schools” example from Rubin (1981) and Gelman et al. (2013, Bayesian Data Analysis) is the canonical textbook example of hierarchical modeling. Eight schools ran a coaching program; the hierarchical model showed that the most impressive result (School A, +28 points) was largely noise, and the true effects were more modest and similar across schools.\nHierarchical models are the Bayesian counterpart of random effects models in frequentist statistics. The key difference: Bayesian hierarchical models provide full posterior distributions for each group parameter, while frequentist random effects models typically give only point estimates (BLUPs — Best Linear Unbiased Predictors).\nStein’s paradox (1956) proved that when estimating 3 or more means simultaneously, the sample means are inadmissible — there always exists an estimator with lower total MSE. The James-Stein estimator and hierarchical models both exploit this: by shrinking toward a common mean, they beat the “obvious” estimator that uses each group’s data alone.",
    "crumbs": [
      "Hierarchical Modeling",
      "Hierarchical Models"
    ]
  },
  {
    "objectID": "application.html",
    "href": "application.html",
    "title": "Application: Returns to Education",
    "section": "",
    "text": "You’ve learned the pieces — Bayes’ theorem, priors and posteriors, shrinkage, MCMC, Bayesian regression. Now let’s use them on a real question.\nWhat’s the causal return to an extra year of education on wages?\nThis is one of the most studied questions in labor economics. The standard approach uses the Mincer equation:\n\\[\n\\log(\\text{wage}) = \\beta_0 + \\beta_1 \\cdot \\text{educ} + \\beta_2 \\cdot \\text{exper} + \\beta_3 \\cdot \\text{exper}^2 + \\varepsilon\n\\]\nWe’ll simulate data from this model and walk through the full Bayesian workflow: estimate, interpret, check sensitivity, and compare to OLS.\n\n\n\n\n\n\nWhat the literature says: Estimates of the return to education typically range from 5% to 15% per year (β₁ ≈ 0.05–0.15 in log points). Card (1999) found ~10% using instrumental variables. We’ll use this as our prior information.\n\n\n\n\n\n\nWalk through all five steps. The left panel shows the posterior for the education coefficient with the prior and OLS estimate. The right panel shows how the posterior shifts as you change the prior strength.\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 13px; line-height: 1.8;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good  { color: #27ae60; font-weight: bold; }\n    .bad   { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      tags$h5(\"Data\"),\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 30, max = 1000, value = 200, step = 10),\n\n      sliderInput(\"true_beta\", HTML(\"True &beta;&lt;sub&gt;educ&lt;/sub&gt;:\"),\n                  min = 0.01, max = 0.20, value = 0.10, step = 0.01),\n\n      tags$hr(),\n      tags$h5(\"Prior\"),\n\n      sliderInput(\"prior_mean\", HTML(\"Prior mean for &beta;&lt;sub&gt;educ&lt;/sub&gt;:\"),\n                  min = 0, max = 0.20, value = 0.10, step = 0.01),\n\n      sliderInput(\"prior_sd\", HTML(\"Prior SD for &beta;&lt;sub&gt;educ&lt;/sub&gt;:\"),\n                  min = 0.005, max = 0.10, value = 0.03, step = 0.005),\n\n      actionButton(\"go\", \"New data\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"scatter_plot\", height = \"300px\")),\n        column(6, plotOutput(\"posterior_plot\", height = \"300px\"))\n      ),\n      fluidRow(\n        column(12, plotOutput(\"sensitivity_plot\", height = \"280px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n          &lt;- input$n\n    true_beta  &lt;- input$true_beta\n    prior_mean &lt;- input$prior_mean\n    prior_sd   &lt;- input$prior_sd\n\n    # Simulate Mincer equation\n    educ  &lt;- round(rnorm(n, 13, 2.5))   # years of education ~13 mean\n    educ  &lt;- pmax(educ, 8)               # floor at 8 years\n    exper &lt;- round(pmax(rnorm(n, 15, 8), 0))  # years of experience\n    sigma &lt;- 0.4                          # residual SD in log wages\n\n    log_wage &lt;- 1.0 + true_beta * educ + 0.03 * exper - 0.0005 * exper^2 +\n                rnorm(n, 0, sigma)\n\n    # OLS: regress log_wage on educ, exper, exper^2\n    X &lt;- cbind(1, educ, exper, exper^2)\n    XtX_inv &lt;- solve(t(X) %*% X)\n    beta_ols &lt;- as.numeric(XtX_inv %*% t(X) %*% log_wage)\n    resid &lt;- log_wage - X %*% beta_ols\n    s2 &lt;- sum(resid^2) / (n - 4)\n    se_ols &lt;- sqrt(s2 * diag(XtX_inv))\n\n    # Extract education coefficient (index 2)\n    b_ols  &lt;- beta_ols[2]\n    se_b   &lt;- se_ols[2]\n\n    # Bayesian posterior (conjugate, focusing on educ coeff)\n    prior_prec &lt;- 1 / prior_sd^2\n    data_prec  &lt;- 1 / se_b^2\n    post_prec  &lt;- prior_prec + data_prec\n    post_sd    &lt;- 1 / sqrt(post_prec)\n    post_mean  &lt;- (prior_mean * prior_prec + b_ols * data_prec) / post_prec\n\n    w_prior &lt;- prior_prec / post_prec\n    w_data  &lt;- data_prec / post_prec\n\n    # CrI\n    cri_lo &lt;- qnorm(0.025, post_mean, post_sd)\n    cri_hi &lt;- qnorm(0.975, post_mean, post_sd)\n\n    # CI\n    ci_lo &lt;- b_ols - 1.96 * se_b\n    ci_hi &lt;- b_ols + 1.96 * se_b\n\n    # P(beta &gt; 0 | data)\n    p_positive &lt;- 1 - pnorm(0, post_mean, post_sd)\n\n    # P(beta &gt; 0.05 | data)\n    p_above_05 &lt;- 1 - pnorm(0.05, post_mean, post_sd)\n\n    # Sensitivity: how posterior mean changes with prior SD\n    prior_sd_grid &lt;- seq(0.005, 0.15, length.out = 100)\n    sens_means &lt;- numeric(length(prior_sd_grid))\n    sens_lo    &lt;- numeric(length(prior_sd_grid))\n    sens_hi    &lt;- numeric(length(prior_sd_grid))\n\n    for (i in seq_along(prior_sd_grid)) {\n      pp &lt;- 1 / prior_sd_grid[i]^2\n      posp &lt;- pp + data_prec\n      pm &lt;- (prior_mean * pp + b_ols * data_prec) / posp\n      ps &lt;- 1 / sqrt(posp)\n      sens_means[i] &lt;- pm\n      sens_lo[i] &lt;- qnorm(0.025, pm, ps)\n      sens_hi[i] &lt;- qnorm(0.975, pm, ps)\n    }\n\n    list(educ = educ, log_wage = log_wage, exper = exper,\n         beta_ols_all = beta_ols, se_ols_all = se_ols,\n         b_ols = b_ols, se_b = se_b, ci_lo = ci_lo, ci_hi = ci_hi,\n         prior_mean = prior_mean, prior_sd = prior_sd,\n         post_mean = post_mean, post_sd = post_sd,\n         cri_lo = cri_lo, cri_hi = cri_hi,\n         w_prior = w_prior, w_data = w_data,\n         p_positive = p_positive, p_above_05 = p_above_05,\n         true_beta = true_beta, sigma = sigma,\n         prior_sd_grid = prior_sd_grid, sens_means = sens_means,\n         sens_lo = sens_lo, sens_hi = sens_hi)\n  })\n\n  # --- Left: scatterplot with OLS and Bayesian regression lines ---\n  output$scatter_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$educ, d$log_wage, pch = 16, col = \"#2c3e5040\", cex = 0.9,\n         xlab = \"Years of education\", ylab = \"log(wage)\",\n         main = \"Data + Regression Lines\")\n\n    # Predict at education values, holding experience at mean\n    educ_seq &lt;- seq(min(d$educ), max(d$educ), length.out = 100)\n    mean_exp &lt;- mean(d$exper)\n\n    # OLS fitted line (red)\n    ols_pred &lt;- d$beta_ols_all[1] + d$beta_ols_all[2] * educ_seq +\n                d$beta_ols_all[3] * mean_exp + d$beta_ols_all[4] * mean_exp^2\n    lines(educ_seq, ols_pred, col = \"#e74c3c\", lwd = 2.5)\n\n    # Bayesian fitted line (blue) — same intercept/exper coeffs, Bayesian educ slope\n    bayes_pred &lt;- d$beta_ols_all[1] + d$post_mean * educ_seq +\n                  d$beta_ols_all[3] * mean_exp + d$beta_ols_all[4] * mean_exp^2\n    lines(educ_seq, bayes_pred, col = \"#3498db\", lwd = 2.5)\n\n    # True line (green dashed)\n    true_pred &lt;- 1.0 + d$true_beta * educ_seq +\n                 0.03 * mean_exp - 0.0005 * mean_exp^2\n    lines(educ_seq, true_pred, col = \"#27ae60\", lwd = 2, lty = 2)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.8,\n           legend = c(\n             paste0(\"OLS: \", round(d$b_ols, 4)),\n             paste0(\"Bayesian: \", round(d$post_mean, 4)),\n             paste0(\"True: \", d$true_beta)\n           ),\n           col = c(\"#e74c3c\", \"#3498db\", \"#27ae60\"),\n           lwd = c(2.5, 2.5, 2), lty = c(1, 1, 2))\n  })\n\n  # --- Right: posterior density with OLS comparison ---\n  output$posterior_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Range for plotting\n    all_means &lt;- c(d$prior_mean, d$b_ols, d$post_mean)\n    all_sds   &lt;- c(d$prior_sd, d$se_b, d$post_sd)\n    xlim &lt;- range(c(all_means - 3.5 * all_sds, all_means + 3.5 * all_sds))\n    x_seq &lt;- seq(xlim[1], xlim[2], length.out = 400)\n\n    prior_y &lt;- dnorm(x_seq, d$prior_mean, d$prior_sd)\n    lik_y   &lt;- dnorm(x_seq, d$b_ols, d$se_b)\n    post_y  &lt;- dnorm(x_seq, d$post_mean, d$post_sd)\n\n    ylim &lt;- c(0, max(c(prior_y, lik_y, post_y)) * 1.25)\n\n    plot(NULL, xlim = xlim, ylim = ylim,\n         xlab = expression(beta[educ]),\n         ylab = \"Density\",\n         main = expression(\"OLS vs Bayesian: \" * beta[educ]))\n\n    # Prior (red dashed)\n    lines(x_seq, prior_y, col = \"#e74c3c\", lwd = 2, lty = 2)\n\n    # Likelihood / OLS (gray dotted)\n    lines(x_seq, lik_y, col = \"gray50\", lwd = 2, lty = 3)\n\n    # Posterior (blue filled)\n    polygon(c(x_seq, rev(x_seq)), c(post_y, rep(0, length(x_seq))),\n            col = \"#3498db30\", border = NA)\n    lines(x_seq, post_y, col = \"#3498db\", lwd = 2.5)\n\n    # True value\n    abline(v = d$true_beta, lty = 2, col = \"#27ae60\", lwd = 1.5)\n\n    # OLS CI bracket (red)\n    y_ci &lt;- max(post_y) * 0.05\n    arrows(d$ci_lo, y_ci, d$ci_hi, y_ci,\n           code = 3, angle = 90, length = 0.04, lwd = 2, col = \"#e74c3c\")\n    text(d$b_ols, y_ci + max(post_y) * 0.07, \"95% CI\", cex = 0.7,\n         col = \"#e74c3c\", font = 2)\n\n    # Bayesian CrI bracket (blue)\n    y_cri &lt;- max(post_y) * 0.15\n    arrows(d$cri_lo, y_cri, d$cri_hi, y_cri,\n           code = 3, angle = 90, length = 0.04, lwd = 2, col = \"#3498db\")\n    text(d$post_mean, y_cri + max(post_y) * 0.07, \"95% CrI\", cex = 0.7,\n         col = \"#3498db\", font = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.75,\n           legend = c(\"Prior\", \"OLS (likelihood)\",\n                      \"Posterior\", expression(\"True \" * beta[educ])),\n           col = c(\"#e74c3c\", \"gray50\", \"#3498db\", \"#27ae60\"),\n           lwd = c(2, 2, 2.5, 1.5),\n           lty = c(2, 3, 1, 2))\n  })\n\n  # --- Bottom: sensitivity to prior strength ---\n  output$sensitivity_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 2.5, 1))\n\n    ylim &lt;- range(c(d$sens_lo, d$sens_hi, d$b_ols, d$true_beta))\n\n    plot(d$prior_sd_grid, d$sens_means, type = \"l\",\n         lwd = 2.5, col = \"#3498db\",\n         xlab = expression(\"Prior SD for \" * beta[educ]),\n         ylab = expression(\"Posterior mean of \" * beta[educ]),\n         main = \"Sensitivity: How Much Does the Prior Matter?\",\n         ylim = ylim)\n\n    # CrI band\n    polygon(c(d$prior_sd_grid, rev(d$prior_sd_grid)),\n            c(d$sens_lo, rev(d$sens_hi)),\n            col = \"#3498db15\", border = NA)\n    lines(d$prior_sd_grid, d$sens_lo, col = \"#3498db50\", lty = 2)\n    lines(d$prior_sd_grid, d$sens_hi, col = \"#3498db50\", lty = 2)\n\n    # OLS reference\n    abline(h = d$b_ols, col = \"#e74c3c\", lwd = 1.5, lty = 3)\n\n    # True value\n    abline(h = d$true_beta, col = \"#27ae60\", lwd = 1.5, lty = 2)\n\n    # Prior mean reference\n    abline(h = d$prior_mean, col = \"#f39c12\", lwd = 1.5, lty = 3)\n\n    # Current prior SD\n    abline(v = d$prior_sd, col = \"#2c3e50\", lwd = 1, lty = 3)\n    points(d$prior_sd, d$post_mean, pch = 16, col = \"#2c3e50\", cex = 1.5)\n\n    legend(\"bottomright\", bty = \"n\", cex = 0.75,\n           legend = c(\"Posterior mean\", \"95% CrI\",\n                      \"OLS estimate\", expression(\"True \" * beta),\n                      \"Prior mean\", \"You are here\"),\n           col = c(\"#3498db\", \"#3498db50\", \"#e74c3c\",\n                   \"#27ae60\", \"#f39c12\", \"#2c3e50\"),\n           lwd = c(2.5, 1.5, 1.5, 1.5, 1.5, NA),\n           lty = c(1, 2, 3, 2, 3, NA),\n           pch = c(NA, NA, NA, NA, NA, 16))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;OLS:&lt;/b&gt; \", round(d$b_ols, 4),\n        \" [\", round(d$ci_lo, 4), \", \", round(d$ci_hi, 4), \"]&lt;br&gt;\",\n        \"&lt;b&gt;Posterior:&lt;/b&gt; \", round(d$post_mean, 4),\n        \" [\", round(d$cri_lo, 4), \", \", round(d$cri_hi, 4), \"]&lt;br&gt;\",\n        \"&lt;b&gt;True:&lt;/b&gt; \", d$true_beta, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Prior weight:&lt;/b&gt; \", round(d$w_prior * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;b&gt;Data weight:&lt;/b&gt; \", round(d$w_data * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;P(&beta; &gt; 0 | data):&lt;/b&gt; \",\n        round(d$p_positive, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;P(&beta; &gt; 0.05 | data):&lt;/b&gt; \",\n        round(d$p_above_05, 4)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nStrong correct prior (prior mean = 0.10, prior SD = 0.02, true β = 0.10): the CrI is tighter than the OLS CI. Incorporating good prior information genuinely improves precision. This is the Bayesian advantage.\nWrong prior + small n (prior mean = 0.02, true β = 0.10, n = 30): the posterior is pulled toward 0.02 — the prior biases the estimate. Now increase n to 500 — the data overwhelm the wrong prior.\nLarge n (n = 1000): the posterior and OLS converge regardless of the prior. The right panel shows the sensitivity curve flattening — the prior SD barely matters.\nP(β &gt; 0 | data): this direct probability statement is impossible with frequentist methods. A 95% CI of [0.03, 0.12] doesn’t tell you the probability that β &gt; 0. The posterior does.\nRight panel insight: as prior SD increases (prior gets vaguer), the posterior mean converges to OLS. The x-axis is “how much you trust the prior” and the curve shows the bias-precision tradeoff.",
    "crumbs": [
      "Application",
      "Returns to Education"
    ]
  },
  {
    "objectID": "application.html#putting-it-all-together-returns-to-education",
    "href": "application.html#putting-it-all-together-returns-to-education",
    "title": "Application: Returns to Education",
    "section": "",
    "text": "You’ve learned the pieces — Bayes’ theorem, priors and posteriors, shrinkage, MCMC, Bayesian regression. Now let’s use them on a real question.\nWhat’s the causal return to an extra year of education on wages?\nThis is one of the most studied questions in labor economics. The standard approach uses the Mincer equation:\n\\[\n\\log(\\text{wage}) = \\beta_0 + \\beta_1 \\cdot \\text{educ} + \\beta_2 \\cdot \\text{exper} + \\beta_3 \\cdot \\text{exper}^2 + \\varepsilon\n\\]\nWe’ll simulate data from this model and walk through the full Bayesian workflow: estimate, interpret, check sensitivity, and compare to OLS.\n\n\n\n\n\n\nWhat the literature says: Estimates of the return to education typically range from 5% to 15% per year (β₁ ≈ 0.05–0.15 in log points). Card (1999) found ~10% using instrumental variables. We’ll use this as our prior information.\n\n\n\n\n\n\nWalk through all five steps. The left panel shows the posterior for the education coefficient with the prior and OLS estimate. The right panel shows how the posterior shifts as you change the prior strength.\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 13px; line-height: 1.8;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good  { color: #27ae60; font-weight: bold; }\n    .bad   { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      tags$h5(\"Data\"),\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 30, max = 1000, value = 200, step = 10),\n\n      sliderInput(\"true_beta\", HTML(\"True &beta;&lt;sub&gt;educ&lt;/sub&gt;:\"),\n                  min = 0.01, max = 0.20, value = 0.10, step = 0.01),\n\n      tags$hr(),\n      tags$h5(\"Prior\"),\n\n      sliderInput(\"prior_mean\", HTML(\"Prior mean for &beta;&lt;sub&gt;educ&lt;/sub&gt;:\"),\n                  min = 0, max = 0.20, value = 0.10, step = 0.01),\n\n      sliderInput(\"prior_sd\", HTML(\"Prior SD for &beta;&lt;sub&gt;educ&lt;/sub&gt;:\"),\n                  min = 0.005, max = 0.10, value = 0.03, step = 0.005),\n\n      actionButton(\"go\", \"New data\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"scatter_plot\", height = \"300px\")),\n        column(6, plotOutput(\"posterior_plot\", height = \"300px\"))\n      ),\n      fluidRow(\n        column(12, plotOutput(\"sensitivity_plot\", height = \"280px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n          &lt;- input$n\n    true_beta  &lt;- input$true_beta\n    prior_mean &lt;- input$prior_mean\n    prior_sd   &lt;- input$prior_sd\n\n    # Simulate Mincer equation\n    educ  &lt;- round(rnorm(n, 13, 2.5))   # years of education ~13 mean\n    educ  &lt;- pmax(educ, 8)               # floor at 8 years\n    exper &lt;- round(pmax(rnorm(n, 15, 8), 0))  # years of experience\n    sigma &lt;- 0.4                          # residual SD in log wages\n\n    log_wage &lt;- 1.0 + true_beta * educ + 0.03 * exper - 0.0005 * exper^2 +\n                rnorm(n, 0, sigma)\n\n    # OLS: regress log_wage on educ, exper, exper^2\n    X &lt;- cbind(1, educ, exper, exper^2)\n    XtX_inv &lt;- solve(t(X) %*% X)\n    beta_ols &lt;- as.numeric(XtX_inv %*% t(X) %*% log_wage)\n    resid &lt;- log_wage - X %*% beta_ols\n    s2 &lt;- sum(resid^2) / (n - 4)\n    se_ols &lt;- sqrt(s2 * diag(XtX_inv))\n\n    # Extract education coefficient (index 2)\n    b_ols  &lt;- beta_ols[2]\n    se_b   &lt;- se_ols[2]\n\n    # Bayesian posterior (conjugate, focusing on educ coeff)\n    prior_prec &lt;- 1 / prior_sd^2\n    data_prec  &lt;- 1 / se_b^2\n    post_prec  &lt;- prior_prec + data_prec\n    post_sd    &lt;- 1 / sqrt(post_prec)\n    post_mean  &lt;- (prior_mean * prior_prec + b_ols * data_prec) / post_prec\n\n    w_prior &lt;- prior_prec / post_prec\n    w_data  &lt;- data_prec / post_prec\n\n    # CrI\n    cri_lo &lt;- qnorm(0.025, post_mean, post_sd)\n    cri_hi &lt;- qnorm(0.975, post_mean, post_sd)\n\n    # CI\n    ci_lo &lt;- b_ols - 1.96 * se_b\n    ci_hi &lt;- b_ols + 1.96 * se_b\n\n    # P(beta &gt; 0 | data)\n    p_positive &lt;- 1 - pnorm(0, post_mean, post_sd)\n\n    # P(beta &gt; 0.05 | data)\n    p_above_05 &lt;- 1 - pnorm(0.05, post_mean, post_sd)\n\n    # Sensitivity: how posterior mean changes with prior SD\n    prior_sd_grid &lt;- seq(0.005, 0.15, length.out = 100)\n    sens_means &lt;- numeric(length(prior_sd_grid))\n    sens_lo    &lt;- numeric(length(prior_sd_grid))\n    sens_hi    &lt;- numeric(length(prior_sd_grid))\n\n    for (i in seq_along(prior_sd_grid)) {\n      pp &lt;- 1 / prior_sd_grid[i]^2\n      posp &lt;- pp + data_prec\n      pm &lt;- (prior_mean * pp + b_ols * data_prec) / posp\n      ps &lt;- 1 / sqrt(posp)\n      sens_means[i] &lt;- pm\n      sens_lo[i] &lt;- qnorm(0.025, pm, ps)\n      sens_hi[i] &lt;- qnorm(0.975, pm, ps)\n    }\n\n    list(educ = educ, log_wage = log_wage, exper = exper,\n         beta_ols_all = beta_ols, se_ols_all = se_ols,\n         b_ols = b_ols, se_b = se_b, ci_lo = ci_lo, ci_hi = ci_hi,\n         prior_mean = prior_mean, prior_sd = prior_sd,\n         post_mean = post_mean, post_sd = post_sd,\n         cri_lo = cri_lo, cri_hi = cri_hi,\n         w_prior = w_prior, w_data = w_data,\n         p_positive = p_positive, p_above_05 = p_above_05,\n         true_beta = true_beta, sigma = sigma,\n         prior_sd_grid = prior_sd_grid, sens_means = sens_means,\n         sens_lo = sens_lo, sens_hi = sens_hi)\n  })\n\n  # --- Left: scatterplot with OLS and Bayesian regression lines ---\n  output$scatter_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$educ, d$log_wage, pch = 16, col = \"#2c3e5040\", cex = 0.9,\n         xlab = \"Years of education\", ylab = \"log(wage)\",\n         main = \"Data + Regression Lines\")\n\n    # Predict at education values, holding experience at mean\n    educ_seq &lt;- seq(min(d$educ), max(d$educ), length.out = 100)\n    mean_exp &lt;- mean(d$exper)\n\n    # OLS fitted line (red)\n    ols_pred &lt;- d$beta_ols_all[1] + d$beta_ols_all[2] * educ_seq +\n                d$beta_ols_all[3] * mean_exp + d$beta_ols_all[4] * mean_exp^2\n    lines(educ_seq, ols_pred, col = \"#e74c3c\", lwd = 2.5)\n\n    # Bayesian fitted line (blue) — same intercept/exper coeffs, Bayesian educ slope\n    bayes_pred &lt;- d$beta_ols_all[1] + d$post_mean * educ_seq +\n                  d$beta_ols_all[3] * mean_exp + d$beta_ols_all[4] * mean_exp^2\n    lines(educ_seq, bayes_pred, col = \"#3498db\", lwd = 2.5)\n\n    # True line (green dashed)\n    true_pred &lt;- 1.0 + d$true_beta * educ_seq +\n                 0.03 * mean_exp - 0.0005 * mean_exp^2\n    lines(educ_seq, true_pred, col = \"#27ae60\", lwd = 2, lty = 2)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.8,\n           legend = c(\n             paste0(\"OLS: \", round(d$b_ols, 4)),\n             paste0(\"Bayesian: \", round(d$post_mean, 4)),\n             paste0(\"True: \", d$true_beta)\n           ),\n           col = c(\"#e74c3c\", \"#3498db\", \"#27ae60\"),\n           lwd = c(2.5, 2.5, 2), lty = c(1, 1, 2))\n  })\n\n  # --- Right: posterior density with OLS comparison ---\n  output$posterior_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Range for plotting\n    all_means &lt;- c(d$prior_mean, d$b_ols, d$post_mean)\n    all_sds   &lt;- c(d$prior_sd, d$se_b, d$post_sd)\n    xlim &lt;- range(c(all_means - 3.5 * all_sds, all_means + 3.5 * all_sds))\n    x_seq &lt;- seq(xlim[1], xlim[2], length.out = 400)\n\n    prior_y &lt;- dnorm(x_seq, d$prior_mean, d$prior_sd)\n    lik_y   &lt;- dnorm(x_seq, d$b_ols, d$se_b)\n    post_y  &lt;- dnorm(x_seq, d$post_mean, d$post_sd)\n\n    ylim &lt;- c(0, max(c(prior_y, lik_y, post_y)) * 1.25)\n\n    plot(NULL, xlim = xlim, ylim = ylim,\n         xlab = expression(beta[educ]),\n         ylab = \"Density\",\n         main = expression(\"OLS vs Bayesian: \" * beta[educ]))\n\n    # Prior (red dashed)\n    lines(x_seq, prior_y, col = \"#e74c3c\", lwd = 2, lty = 2)\n\n    # Likelihood / OLS (gray dotted)\n    lines(x_seq, lik_y, col = \"gray50\", lwd = 2, lty = 3)\n\n    # Posterior (blue filled)\n    polygon(c(x_seq, rev(x_seq)), c(post_y, rep(0, length(x_seq))),\n            col = \"#3498db30\", border = NA)\n    lines(x_seq, post_y, col = \"#3498db\", lwd = 2.5)\n\n    # True value\n    abline(v = d$true_beta, lty = 2, col = \"#27ae60\", lwd = 1.5)\n\n    # OLS CI bracket (red)\n    y_ci &lt;- max(post_y) * 0.05\n    arrows(d$ci_lo, y_ci, d$ci_hi, y_ci,\n           code = 3, angle = 90, length = 0.04, lwd = 2, col = \"#e74c3c\")\n    text(d$b_ols, y_ci + max(post_y) * 0.07, \"95% CI\", cex = 0.7,\n         col = \"#e74c3c\", font = 2)\n\n    # Bayesian CrI bracket (blue)\n    y_cri &lt;- max(post_y) * 0.15\n    arrows(d$cri_lo, y_cri, d$cri_hi, y_cri,\n           code = 3, angle = 90, length = 0.04, lwd = 2, col = \"#3498db\")\n    text(d$post_mean, y_cri + max(post_y) * 0.07, \"95% CrI\", cex = 0.7,\n         col = \"#3498db\", font = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.75,\n           legend = c(\"Prior\", \"OLS (likelihood)\",\n                      \"Posterior\", expression(\"True \" * beta[educ])),\n           col = c(\"#e74c3c\", \"gray50\", \"#3498db\", \"#27ae60\"),\n           lwd = c(2, 2, 2.5, 1.5),\n           lty = c(2, 3, 1, 2))\n  })\n\n  # --- Bottom: sensitivity to prior strength ---\n  output$sensitivity_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 2.5, 1))\n\n    ylim &lt;- range(c(d$sens_lo, d$sens_hi, d$b_ols, d$true_beta))\n\n    plot(d$prior_sd_grid, d$sens_means, type = \"l\",\n         lwd = 2.5, col = \"#3498db\",\n         xlab = expression(\"Prior SD for \" * beta[educ]),\n         ylab = expression(\"Posterior mean of \" * beta[educ]),\n         main = \"Sensitivity: How Much Does the Prior Matter?\",\n         ylim = ylim)\n\n    # CrI band\n    polygon(c(d$prior_sd_grid, rev(d$prior_sd_grid)),\n            c(d$sens_lo, rev(d$sens_hi)),\n            col = \"#3498db15\", border = NA)\n    lines(d$prior_sd_grid, d$sens_lo, col = \"#3498db50\", lty = 2)\n    lines(d$prior_sd_grid, d$sens_hi, col = \"#3498db50\", lty = 2)\n\n    # OLS reference\n    abline(h = d$b_ols, col = \"#e74c3c\", lwd = 1.5, lty = 3)\n\n    # True value\n    abline(h = d$true_beta, col = \"#27ae60\", lwd = 1.5, lty = 2)\n\n    # Prior mean reference\n    abline(h = d$prior_mean, col = \"#f39c12\", lwd = 1.5, lty = 3)\n\n    # Current prior SD\n    abline(v = d$prior_sd, col = \"#2c3e50\", lwd = 1, lty = 3)\n    points(d$prior_sd, d$post_mean, pch = 16, col = \"#2c3e50\", cex = 1.5)\n\n    legend(\"bottomright\", bty = \"n\", cex = 0.75,\n           legend = c(\"Posterior mean\", \"95% CrI\",\n                      \"OLS estimate\", expression(\"True \" * beta),\n                      \"Prior mean\", \"You are here\"),\n           col = c(\"#3498db\", \"#3498db50\", \"#e74c3c\",\n                   \"#27ae60\", \"#f39c12\", \"#2c3e50\"),\n           lwd = c(2.5, 1.5, 1.5, 1.5, 1.5, NA),\n           lty = c(1, 2, 3, 2, 3, NA),\n           pch = c(NA, NA, NA, NA, NA, 16))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;OLS:&lt;/b&gt; \", round(d$b_ols, 4),\n        \" [\", round(d$ci_lo, 4), \", \", round(d$ci_hi, 4), \"]&lt;br&gt;\",\n        \"&lt;b&gt;Posterior:&lt;/b&gt; \", round(d$post_mean, 4),\n        \" [\", round(d$cri_lo, 4), \", \", round(d$cri_hi, 4), \"]&lt;br&gt;\",\n        \"&lt;b&gt;True:&lt;/b&gt; \", d$true_beta, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Prior weight:&lt;/b&gt; \", round(d$w_prior * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;b&gt;Data weight:&lt;/b&gt; \", round(d$w_data * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;P(&beta; &gt; 0 | data):&lt;/b&gt; \",\n        round(d$p_positive, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;P(&beta; &gt; 0.05 | data):&lt;/b&gt; \",\n        round(d$p_above_05, 4)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nStrong correct prior (prior mean = 0.10, prior SD = 0.02, true β = 0.10): the CrI is tighter than the OLS CI. Incorporating good prior information genuinely improves precision. This is the Bayesian advantage.\nWrong prior + small n (prior mean = 0.02, true β = 0.10, n = 30): the posterior is pulled toward 0.02 — the prior biases the estimate. Now increase n to 500 — the data overwhelm the wrong prior.\nLarge n (n = 1000): the posterior and OLS converge regardless of the prior. The right panel shows the sensitivity curve flattening — the prior SD barely matters.\nP(β &gt; 0 | data): this direct probability statement is impossible with frequentist methods. A 95% CI of [0.03, 0.12] doesn’t tell you the probability that β &gt; 0. The posterior does.\nRight panel insight: as prior SD increases (prior gets vaguer), the posterior mean converges to OLS. The x-axis is “how much you trust the prior” and the curve shows the bias-precision tradeoff.",
    "crumbs": [
      "Application",
      "Returns to Education"
    ]
  },
  {
    "objectID": "application.html#the-stata-workflow",
    "href": "application.html#the-stata-workflow",
    "title": "Application: Returns to Education",
    "section": "The Stata workflow",
    "text": "The Stata workflow\nHere’s the complete analysis in Stata, mirroring each step above:\n* ----- Step 1: Look at the data -----\nsummarize wage education experience\nscatter wage education\n\n* ----- Step 2: OLS -----\ngen log_wage = log(wage)\ngen exper2 = experience * experience\nreg log_wage education experience exper2\n\n* ----- Step 3: Bayesian with default (vague) priors -----\nbayes: reg log_wage education experience exper2\n\n* ----- Step 4: Bayesian with informative prior -----\n* Literature says ~10% return, we're fairly confident\nbayes, prior({log_wage:education}, normal(0.10, 0.01)) : ///\n    reg log_wage education experience exper2\n\n* Note: normal(0.10, 0.01) means prior variance 0.01,\n* so prior SD = 0.1 — centered at 10% with moderate spread\n\n* ----- Step 5: Check convergence -----\nbayesgraph diagnostics {log_wage:education}\n* Look for: stationary trace, high ESS, R-hat near 1\n\n* ----- Step 6: Compare models -----\nbayesstats ic\n* Compare DIC/WAIC to the default-prior model\n\n\n\n\n\n\nInterpreting Stata output: The key columns are Mean (posterior mean), Std. Dev. (posterior SD), and the Equal-tailed credible interval. Compare these directly to the OLS coefficient, standard error, and confidence interval. With vague priors and moderate n, they’ll be nearly identical.",
    "crumbs": [
      "Application",
      "Returns to Education"
    ]
  },
  {
    "objectID": "application.html#what-did-we-learn",
    "href": "application.html#what-did-we-learn",
    "title": "Application: Returns to Education",
    "section": "What did we learn?",
    "text": "What did we learn?\n\nWith a reasonable prior, Bayesian and OLS agree — especially with large samples. The prior adds almost no bias but can reduce uncertainty.\nThe prior adds value when samples are small or when you have genuine outside information (literature, expert knowledge, previous studies).\nThe posterior gives direct probability statements that OLS can’t: \\(P(\\beta &gt; 0.05 \\mid \\text{data})\\) answers the question “how confident should I be that the return exceeds 5%?” directly.\nSensitivity analysis is easy and important. The right panel shows exactly how your conclusions depend on the prior. If the answer doesn’t change much across reasonable priors, your results are robust.\n\nWhere to go deeper:\n\nBayes’ theorem — the updating formula behind everything\nPriors & Posteriors — mechanics of conjugate updating\nShrinkage — why the prior pulls estimates and when it helps\nBayesian Regression — the full OLS vs Bayesian comparison\nModel Comparison — Bayes factors for choosing between models\nPosterior Predictive Checks — does the model fit?",
    "crumbs": [
      "Application",
      "Returns to Education"
    ]
  },
  {
    "objectID": "application.html#did-you-know",
    "href": "application.html#did-you-know",
    "title": "Application: Returns to Education",
    "section": "Did you know?",
    "text": "Did you know?\n\nMincer (1974) introduced the log-linear earnings equation in Schooling, Experience, and Earnings. The specification — log wages regressed on education and a quadratic in experience — became the workhorse of labor economics. Nearly every study of returns to education starts from this framework.\nCard (1999, 2001) used instrumental variables (geographic proximity to colleges) to address the endogeneity of education, finding returns of about 10% per year. His work highlighted that OLS may underestimate the return for marginal students — those on the fence about attending college.\nBayesian approaches in development economics: researchers studying returns to education in developing countries often face small samples and noisy data. Informative priors based on findings from similar countries can stabilize estimates — a natural application of the framework you just practiced.",
    "crumbs": [
      "Application",
      "Returns to Education"
    ]
  },
  {
    "objectID": "priors-posteriors.html",
    "href": "priors-posteriors.html",
    "title": "Priors & Posteriors",
    "section": "",
    "text": "Imagine you’re trying to estimate something — say, the average effect of a tutoring program on test scores. In frequentist statistics, you collect data, compute a point estimate, and that’s your answer.\nIn Bayesian statistics, you do something different:\n\nStart with a prior — what you believed before seeing data. Maybe from past studies, expert opinion, or just “I have no idea” (a flat prior).\nObserve data — the likelihood tells you how probable the data is for each possible value of the parameter.\nCombine them — Bayes’ theorem multiplies the prior by the likelihood to give you the posterior: your updated belief after seeing the data.\n\n\\[\\underbrace{P(\\theta \\mid \\text{data})}_{\\text{posterior}} \\propto \\underbrace{P(\\text{data} \\mid \\theta)}_{\\text{likelihood}} \\times \\underbrace{P(\\theta)}_{\\text{prior}}\\]\n\n\n\n\n\n\nExample: diagnosing a headache. A doctor sees a patient with a headache. Before any tests, her prior is: 99.9% chance it’s a tension headache, 0.1% chance it’s a brain tumor (base rates from experience). Then an MRI shows something unusual — that’s the data. The posterior updates: maybe now it’s 95% tension headache, 5% tumor. The prior didn’t disappear — it got updated by the evidence. A second MRI (more data) updates it further. With enough evidence, even a strong prior gets overwhelmed. That’s Bayesian inference: start with what you know, update with what you see.\n\n\n\nThe simulation below makes this tangible. You’re estimating the true mean of a normal distribution. Set a prior, generate data, and watch the posterior form.\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_mu\", HTML(\"True &mu; (unknown to you):\"),\n                  min = -5, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"prior_mu\", \"Prior mean:\",\n                  min = -5, max = 5, value = 0, step = 0.5),\n\n      sliderInput(\"prior_sd\", \"Prior SD (certainty):\",\n                  min = 0.5, max = 10, value = 3, step = 0.5),\n\n      sliderInput(\"n\", \"Sample size (data):\",\n                  min = 1, max = 200, value = 5, step = 1),\n\n      sliderInput(\"sigma\", HTML(\"Data noise (&sigma;):\"),\n                  min = 0.5, max = 5, value = 2, step = 0.5),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"posterior_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n\n    true_mu  &lt;- input$true_mu\n    prior_mu &lt;- input$prior_mu\n    prior_sd &lt;- input$prior_sd\n    n        &lt;- input$n\n    sigma    &lt;- input$sigma\n\n    # Generate data\n    y &lt;- rnorm(n, mean = true_mu, sd = sigma)\n    y_bar &lt;- mean(y)\n\n    # Posterior (conjugate normal-normal)\n    prior_prec &lt;- 1 / prior_sd^2\n    data_prec  &lt;- n / sigma^2\n    post_prec  &lt;- prior_prec + data_prec\n    post_sd    &lt;- 1 / sqrt(post_prec)\n    post_mu    &lt;- (prior_prec * prior_mu + data_prec * y_bar) / post_prec\n\n    # Shrinkage weight on prior\n    w_prior &lt;- prior_prec / post_prec\n\n    list(true_mu = true_mu, prior_mu = prior_mu, prior_sd = prior_sd,\n         y_bar = y_bar, sigma = sigma, n = n,\n         post_mu = post_mu, post_sd = post_sd, w_prior = w_prior)\n  })\n\n  output$posterior_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    xmin &lt;- min(d$prior_mu - 3.5 * d$prior_sd, d$post_mu - 4 * d$post_sd, d$true_mu - 2)\n    xmax &lt;- max(d$prior_mu + 3.5 * d$prior_sd, d$post_mu + 4 * d$post_sd, d$true_mu + 2)\n    x &lt;- seq(xmin, xmax, length.out = 500)\n\n    y_prior &lt;- dnorm(x, d$prior_mu, d$prior_sd)\n    y_like  &lt;- dnorm(x, d$y_bar, d$sigma / sqrt(d$n))\n    y_post  &lt;- dnorm(x, d$post_mu, d$post_sd)\n\n    ylim &lt;- c(0, max(y_prior, y_like, y_post) * 1.15)\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(x, y_prior, type = \"l\", lwd = 2.5, col = \"#e74c3c\",\n         xlab = expression(mu), ylab = \"Density\",\n         main = \"Prior + Likelihood = Posterior\",\n         ylim = ylim)\n    lines(x, y_like, lwd = 2.5, col = \"#3498db\")\n    lines(x, y_post, lwd = 3, col = \"#27ae60\")\n\n    # Shade posterior\n    polygon(c(x, rev(x)),\n            c(y_post, rep(0, length(x))),\n            col = adjustcolor(\"#27ae60\", 0.2), border = NA)\n\n    # True value\n    abline(v = d$true_mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.9,\n           legend = c(\"Prior (your belief before data)\",\n                      \"Likelihood (what the data says)\",\n                      \"Posterior (updated belief)\",\n                      expression(\"True \" * mu)),\n           col = c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"#2c3e50\"),\n           lwd = c(2.5, 2.5, 3, 2),\n           lty = c(1, 1, 1, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Prior mean:&lt;/b&gt; \", d$prior_mu, \"&lt;br&gt;\",\n        \"&lt;b&gt;Data mean:&lt;/b&gt; \", round(d$y_bar, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Posterior mean:&lt;/b&gt; \", round(d$post_mu, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Posterior SD:&lt;/b&gt; \", round(d$post_sd, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Weight on prior:&lt;/b&gt; \", round(d$w_prior * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;b&gt;Weight on data:&lt;/b&gt; \", round((1 - d$w_prior) * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;small&gt;Posterior = weighted average of prior & data&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nn = 1: the posterior is mostly the prior (red). You barely have data.\nSlide n to 100: the posterior (green) collapses onto the data mean (blue). Data overwhelms the prior. With enough data, the prior doesn’t matter.\nSet prior SD = 0.5 (strong prior) with n = 5: the posterior is pulled toward the prior. This is shrinkage — the prior is “shrinking” your estimate away from the data and toward your prior belief.\nSet prior SD = 10 (vague prior): the posterior tracks the data almost exactly. A flat prior says “I have no opinion” and lets the data speak.\nWatch the weight on prior in the sidebar — it shows exactly how much the posterior is a compromise between prior and data.",
    "crumbs": [
      "Foundations",
      "Priors & Posteriors"
    ]
  },
  {
    "objectID": "priors-posteriors.html#what-is-bayesian-inference-really",
    "href": "priors-posteriors.html#what-is-bayesian-inference-really",
    "title": "Priors & Posteriors",
    "section": "",
    "text": "Imagine you’re trying to estimate something — say, the average effect of a tutoring program on test scores. In frequentist statistics, you collect data, compute a point estimate, and that’s your answer.\nIn Bayesian statistics, you do something different:\n\nStart with a prior — what you believed before seeing data. Maybe from past studies, expert opinion, or just “I have no idea” (a flat prior).\nObserve data — the likelihood tells you how probable the data is for each possible value of the parameter.\nCombine them — Bayes’ theorem multiplies the prior by the likelihood to give you the posterior: your updated belief after seeing the data.\n\n\\[\\underbrace{P(\\theta \\mid \\text{data})}_{\\text{posterior}} \\propto \\underbrace{P(\\text{data} \\mid \\theta)}_{\\text{likelihood}} \\times \\underbrace{P(\\theta)}_{\\text{prior}}\\]\n\n\n\n\n\n\nExample: diagnosing a headache. A doctor sees a patient with a headache. Before any tests, her prior is: 99.9% chance it’s a tension headache, 0.1% chance it’s a brain tumor (base rates from experience). Then an MRI shows something unusual — that’s the data. The posterior updates: maybe now it’s 95% tension headache, 5% tumor. The prior didn’t disappear — it got updated by the evidence. A second MRI (more data) updates it further. With enough evidence, even a strong prior gets overwhelmed. That’s Bayesian inference: start with what you know, update with what you see.\n\n\n\nThe simulation below makes this tangible. You’re estimating the true mean of a normal distribution. Set a prior, generate data, and watch the posterior form.\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_mu\", HTML(\"True &mu; (unknown to you):\"),\n                  min = -5, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"prior_mu\", \"Prior mean:\",\n                  min = -5, max = 5, value = 0, step = 0.5),\n\n      sliderInput(\"prior_sd\", \"Prior SD (certainty):\",\n                  min = 0.5, max = 10, value = 3, step = 0.5),\n\n      sliderInput(\"n\", \"Sample size (data):\",\n                  min = 1, max = 200, value = 5, step = 1),\n\n      sliderInput(\"sigma\", HTML(\"Data noise (&sigma;):\"),\n                  min = 0.5, max = 5, value = 2, step = 0.5),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"posterior_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n\n    true_mu  &lt;- input$true_mu\n    prior_mu &lt;- input$prior_mu\n    prior_sd &lt;- input$prior_sd\n    n        &lt;- input$n\n    sigma    &lt;- input$sigma\n\n    # Generate data\n    y &lt;- rnorm(n, mean = true_mu, sd = sigma)\n    y_bar &lt;- mean(y)\n\n    # Posterior (conjugate normal-normal)\n    prior_prec &lt;- 1 / prior_sd^2\n    data_prec  &lt;- n / sigma^2\n    post_prec  &lt;- prior_prec + data_prec\n    post_sd    &lt;- 1 / sqrt(post_prec)\n    post_mu    &lt;- (prior_prec * prior_mu + data_prec * y_bar) / post_prec\n\n    # Shrinkage weight on prior\n    w_prior &lt;- prior_prec / post_prec\n\n    list(true_mu = true_mu, prior_mu = prior_mu, prior_sd = prior_sd,\n         y_bar = y_bar, sigma = sigma, n = n,\n         post_mu = post_mu, post_sd = post_sd, w_prior = w_prior)\n  })\n\n  output$posterior_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    xmin &lt;- min(d$prior_mu - 3.5 * d$prior_sd, d$post_mu - 4 * d$post_sd, d$true_mu - 2)\n    xmax &lt;- max(d$prior_mu + 3.5 * d$prior_sd, d$post_mu + 4 * d$post_sd, d$true_mu + 2)\n    x &lt;- seq(xmin, xmax, length.out = 500)\n\n    y_prior &lt;- dnorm(x, d$prior_mu, d$prior_sd)\n    y_like  &lt;- dnorm(x, d$y_bar, d$sigma / sqrt(d$n))\n    y_post  &lt;- dnorm(x, d$post_mu, d$post_sd)\n\n    ylim &lt;- c(0, max(y_prior, y_like, y_post) * 1.15)\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(x, y_prior, type = \"l\", lwd = 2.5, col = \"#e74c3c\",\n         xlab = expression(mu), ylab = \"Density\",\n         main = \"Prior + Likelihood = Posterior\",\n         ylim = ylim)\n    lines(x, y_like, lwd = 2.5, col = \"#3498db\")\n    lines(x, y_post, lwd = 3, col = \"#27ae60\")\n\n    # Shade posterior\n    polygon(c(x, rev(x)),\n            c(y_post, rep(0, length(x))),\n            col = adjustcolor(\"#27ae60\", 0.2), border = NA)\n\n    # True value\n    abline(v = d$true_mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.9,\n           legend = c(\"Prior (your belief before data)\",\n                      \"Likelihood (what the data says)\",\n                      \"Posterior (updated belief)\",\n                      expression(\"True \" * mu)),\n           col = c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"#2c3e50\"),\n           lwd = c(2.5, 2.5, 3, 2),\n           lty = c(1, 1, 1, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Prior mean:&lt;/b&gt; \", d$prior_mu, \"&lt;br&gt;\",\n        \"&lt;b&gt;Data mean:&lt;/b&gt; \", round(d$y_bar, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Posterior mean:&lt;/b&gt; \", round(d$post_mu, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Posterior SD:&lt;/b&gt; \", round(d$post_sd, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Weight on prior:&lt;/b&gt; \", round(d$w_prior * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;b&gt;Weight on data:&lt;/b&gt; \", round((1 - d$w_prior) * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;small&gt;Posterior = weighted average of prior & data&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nn = 1: the posterior is mostly the prior (red). You barely have data.\nSlide n to 100: the posterior (green) collapses onto the data mean (blue). Data overwhelms the prior. With enough data, the prior doesn’t matter.\nSet prior SD = 0.5 (strong prior) with n = 5: the posterior is pulled toward the prior. This is shrinkage — the prior is “shrinking” your estimate away from the data and toward your prior belief.\nSet prior SD = 10 (vague prior): the posterior tracks the data almost exactly. A flat prior says “I have no opinion” and lets the data speak.\nWatch the weight on prior in the sidebar — it shows exactly how much the posterior is a compromise between prior and data.",
    "crumbs": [
      "Foundations",
      "Priors & Posteriors"
    ]
  },
  {
    "objectID": "priors-posteriors.html#shrinkage-the-bayesian-superpower",
    "href": "priors-posteriors.html#shrinkage-the-bayesian-superpower",
    "title": "Priors & Posteriors",
    "section": "Shrinkage: the Bayesian superpower",
    "text": "Shrinkage: the Bayesian superpower\nLook at the “weight on prior” number in the sidebar. The posterior mean is literally a weighted average:\n\\[\\mu_{post} = w \\cdot \\mu_{prior} + (1 - w) \\cdot \\bar{y}\\]\nwhere \\(w\\) depends on how confident your prior is relative to how much data you have.\nThis is shrinkage: the posterior “shrinks” the data estimate toward the prior. When is this useful?\n\nSmall samples: noisy data gets regularized toward a sensible default.\nMany groups: estimating batting averages for 500 baseball players? Shrink extreme estimates toward the league average. A player who went 3-for-3 on opening day probably isn’t a .1000 hitter.\nHierarchical models: borrow strength across groups by shrinking toward a common mean.\n\nShrinkage isn’t bias — it’s a bias-variance tradeoff. You add a little bias but reduce variance a lot, often improving overall accuracy.",
    "crumbs": [
      "Foundations",
      "Priors & Posteriors"
    ]
  },
  {
    "objectID": "priors-posteriors.html#why-did-the-math-work-out-so-cleanly",
    "href": "priors-posteriors.html#why-did-the-math-work-out-so-cleanly",
    "title": "Priors & Posteriors",
    "section": "Why did the math work out so cleanly?",
    "text": "Why did the math work out so cleanly?\nThe simulation above computes the posterior instantly — no sampling, no iteration, just a formula. That’s because we used a conjugate prior: a special prior-likelihood pair where the posterior has the same distributional form as the prior.\nHere, the prior is Normal, the likelihood is Normal, and the posterior is Normal too. You just update the mean and variance:\n\\[\\mu_{post} = \\frac{\\frac{\\mu_0}{\\sigma_0^2} + \\frac{n\\bar{y}}{\\sigma^2}}{\\frac{1}{\\sigma_0^2} + \\frac{n}{\\sigma^2}}\\]\nPlug in numbers, get the answer. No algorithm required.\n\nCommon conjugate pairs\n\n\n\nLikelihood\nConjugate prior\nPosterior\nExample\n\n\n\n\nNormal (known \\(\\sigma\\))\nNormal\nNormal\nEstimating a mean (this page)\n\n\nBinomial\nBeta\nBeta\nEstimating a proportion (Bayes’ Theorem)\n\n\nPoisson\nGamma\nGamma\nEstimating a rate\n\n\n\n\n\nThe problem: most real models aren’t conjugate\nConjugacy is elegant but limited. It only works for these specific combinations. The moment your model gets realistic — logistic regression with priors on coefficients, hierarchical models with multiple levels, non-standard likelihoods — there’s no conjugate solution. The posterior is some high-dimensional surface with no closed-form expression.\n\n\n\n\n\n\nWhat does “closed-form” mean? A closed-form solution is one you can write as a finite formula using standard operations (addition, multiplication, exponents, etc.) and evaluate directly.\nClosed-form (conjugate case): the posterior mean above — plug in \\(\\mu_0\\), \\(\\sigma_0\\), \\(n\\), \\(\\bar{y}\\), \\(\\sigma\\), do arithmetic, get the exact answer. Done.\nNot closed-form (non-conjugate case): say you want the posterior for a logistic regression coefficient \\(\\theta\\) with a normal prior:\n\\[p(\\theta \\mid y) = \\frac{\\prod_{i=1}^n \\frac{1}{1 + e^{-\\theta x_i}} \\cdot e^{-\\theta^2/2}}{\\int_{-\\infty}^{\\infty} \\prod_{i=1}^n \\frac{1}{1 + e^{-\\theta x_i}} \\cdot e^{-\\theta^2/2} \\, d\\theta}\\]\nThat integral in the denominator? There’s no formula for it. You can’t simplify it to “plug in numbers.” You’d have to numerically approximate it — which is exactly what MCMC does (it sidesteps the integral entirely by sampling).\n\n\n\nThat’s why MCMC exists: when you can’t write down the posterior, you sample from it instead. The progression in this course:\n\nThis page: conjugate priors — exact, instant posteriors (the special case)\nMCMC: numerical sampling — posteriors for any model (the general case)\nHierarchical Models: the reason you need MCMC in practice",
    "crumbs": [
      "Foundations",
      "Priors & Posteriors"
    ]
  },
  {
    "objectID": "bayes-theorem.html",
    "href": "bayes-theorem.html",
    "title": "Bayes’ Theorem",
    "section": "",
    "text": "\\[P(H \\mid D) = \\frac{P(D \\mid H) \\cdot P(H)}{P(D)}\\]\nIn words:\n\\[\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Evidence}}\\]\nThat looks abstract. Let’s make it concrete.",
    "crumbs": [
      "Foundations",
      "Bayes' Theorem"
    ]
  },
  {
    "objectID": "bayes-theorem.html#the-formula",
    "href": "bayes-theorem.html#the-formula",
    "title": "Bayes’ Theorem",
    "section": "",
    "text": "\\[P(H \\mid D) = \\frac{P(D \\mid H) \\cdot P(H)}{P(D)}\\]\nIn words:\n\\[\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Evidence}}\\]\nThat looks abstract. Let’s make it concrete.",
    "crumbs": [
      "Foundations",
      "Bayes' Theorem"
    ]
  },
  {
    "objectID": "bayes-theorem.html#the-medical-test-example",
    "href": "bayes-theorem.html#the-medical-test-example",
    "title": "Bayes’ Theorem",
    "section": "The medical test example",
    "text": "The medical test example\nImagine a disease that affects 1 in 1,000 people. A test for it is 99% accurate — if you have the disease it says positive 99% of the time, and if you don’t have it, it says negative 99% of the time.\nYou test positive. What’s the probability you actually have the disease?\nMost people say 99%. The real answer is about 9%. This is not a trick — it’s Bayes’ theorem. The disease is so rare that even a good test produces more false positives than true positives.\nThe simulator below lets you adjust the base rate and test accuracy and watch how the posterior probability changes.\n#| standalone: true\n#| viewerHeight: 580\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .result-box {\n      background: #f0f4f8; border-radius: 6px; padding: 16px;\n      margin-top: 14px; font-size: 15px; line-height: 2;\n      text-align: center;\n    }\n    .result-box .big {\n      font-size: 32px; color: #e74c3c; font-weight: bold;\n    }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"prev\", \"Base rate (prevalence):\",\n                  min = 0.001, max = 0.20, value = 0.001, step = 0.001),\n\n      sliderInput(\"sens\", \"Sensitivity (true positive rate):\",\n                  min = 0.50, max = 1.00, value = 0.99, step = 0.01),\n\n      sliderInput(\"spec\", \"Specificity (true negative rate):\",\n                  min = 0.50, max = 1.00, value = 0.99, step = 0.01),\n\n      uiOutput(\"result_box\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"tree_plot\", height = \"420px\")),\n        column(6, plotOutput(\"icon_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  vals &lt;- reactive({\n    prev &lt;- input$prev\n    sens &lt;- input$sens\n    spec &lt;- input$spec\n\n    # Out of 10,000 people\n    N &lt;- 10000\n    sick &lt;- round(N * prev)\n    healthy &lt;- N - sick\n\n    true_pos  &lt;- round(sick * sens)\n    false_neg &lt;- sick - true_pos\n    false_pos &lt;- round(healthy * (1 - spec))\n    true_neg  &lt;- healthy - false_pos\n\n    total_pos &lt;- true_pos + false_pos\n    ppv &lt;- if (total_pos &gt; 0) true_pos / total_pos else 0\n\n    list(N = N, sick = sick, healthy = healthy,\n         true_pos = true_pos, false_neg = false_neg,\n         false_pos = false_pos, true_neg = true_neg,\n         total_pos = total_pos, ppv = ppv)\n  })\n\n  output$tree_plot &lt;- renderPlot({\n    v &lt;- vals()\n    par(mar = c(1, 1, 3, 1))\n\n    plot(NULL, xlim = c(0, 10), ylim = c(0, 10), axes = FALSE,\n         xlab = \"\", ylab = \"\", main = \"What happens to 10,000 people?\")\n\n    # Population\n    text(5, 9.5, paste0(\"Population: \", v$N), cex = 1.2, font = 2)\n\n    # Sick vs Healthy\n    text(2.5, 7.5, paste0(\"Sick: \", v$sick), cex = 1.1, col = \"#e74c3c\")\n    text(7.5, 7.5, paste0(\"Healthy: \", v$healthy), cex = 1.1, col = \"#3498db\")\n    segments(5, 9, 2.5, 8, lwd = 2)\n    segments(5, 9, 7.5, 8, lwd = 2)\n\n    # Test results for sick\n    text(1.2, 5.2, paste0(\"Test +\\n\", v$true_pos), cex = 1, col = \"#27ae60\", font = 2)\n    text(3.8, 5.2, paste0(\"Test -\\n\", v$false_neg), cex = 1, col = \"#7f8c8d\")\n    segments(2.5, 7, 1.2, 5.8, lwd = 1.5)\n    segments(2.5, 7, 3.8, 5.8, lwd = 1.5)\n\n    # Test results for healthy\n    text(6.2, 5.2, paste0(\"Test +\\n\", v$false_pos), cex = 1, col = \"#e74c3c\", font = 2)\n    text(8.8, 5.2, paste0(\"Test -\\n\", v$true_neg), cex = 1, col = \"#7f8c8d\")\n    segments(7.5, 7, 6.2, 5.8, lwd = 1.5)\n    segments(7.5, 7, 8.8, 5.8, lwd = 1.5)\n\n    # Total positives\n    text(3.7, 3, paste0(\"Total positive tests: \", v$total_pos), cex = 1.1, font = 2)\n    text(3.7, 2, paste0(\"Of these, truly sick: \", v$true_pos), cex = 1.1,\n         col = \"#27ae60\", font = 2)\n    text(3.7, 1, paste0(\"P(sick | test+) = \",\n         v$true_pos, \"/\", v$total_pos, \" = \",\n         round(v$ppv * 100, 1), \"%\"), cex = 1.2, font = 2, col = \"#e74c3c\")\n  })\n\n  output$icon_plot &lt;- renderPlot({\n    v &lt;- vals()\n    par(mar = c(1, 1, 3, 1))\n\n    # Show total positive tests as dots\n    n_show &lt;- min(v$total_pos, 200)\n    n_true &lt;- round(n_show * v$ppv)\n    n_false &lt;- n_show - n_true\n\n    cols &lt;- c(rep(\"#27ae60\", n_true), rep(\"#e74c3c\", n_false))\n    cols &lt;- sample(cols)\n\n    ncol &lt;- ceiling(sqrt(n_show))\n    nrow &lt;- ceiling(n_show / ncol)\n\n    plot(NULL, xlim = c(0, ncol + 1), ylim = c(0, nrow + 1),\n         axes = FALSE, xlab = \"\", ylab = \"\",\n         main = paste0(\"All \", v$total_pos, \" positive tests\"))\n\n    if (n_show &gt; 0) {\n      x &lt;- rep(seq_len(ncol), times = nrow)[seq_len(n_show)]\n      y &lt;- rep(seq(nrow, 1), each = ncol)[seq_len(n_show)]\n      points(x, y, pch = 15, cex = max(0.5, 3 - n_show / 50), col = cols)\n    }\n\n    legend(\"bottom\", bty = \"n\", horiz = TRUE, cex = 0.95,\n           legend = c(paste0(\"Truly sick (\", n_true, \")\"),\n                      paste0(\"False alarm (\", n_false, \")\")),\n           col = c(\"#27ae60\", \"#e74c3c\"), pch = 15, pt.cex = 1.5)\n  })\n\n  output$result_box &lt;- renderUI({\n    v &lt;- vals()\n    tags$div(class = \"result-box\",\n      HTML(paste0(\n        \"If you test positive,&lt;br&gt;\",\n        \"the probability you're sick is:&lt;br&gt;\",\n        \"&lt;span class='big'&gt;\", round(v$ppv * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;small&gt;not \", round(input$sens * 100), \"%!&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nDefault settings (prevalence 0.1%, test 99% accurate): only ~9% of positive tests are truly sick. The base rate dominates.\nSlide prevalence up to 5%: now ~84% of positives are real. The prior matters!\nSlide prevalence to 50%: the posterior is ~99%. When the disease is common, a positive test is very informative.\nLower specificity to 90%: false positives explode. Watch the right plot fill with red dots.\n\n\n\nThe lesson\nBayes’ theorem tells you: don’t just look at the test accuracy — look at how common the thing is. A 99% accurate test is nearly useless for a 1-in-1,000 disease because most positives are false alarms. This is the base rate fallacy, and Bayes’ theorem is the cure.",
    "crumbs": [
      "Foundations",
      "Bayes' Theorem"
    ]
  },
  {
    "objectID": "bayes-theorem.html#the-coin-flip-your-first-conjugate-model",
    "href": "bayes-theorem.html#the-coin-flip-your-first-conjugate-model",
    "title": "Bayes’ Theorem",
    "section": "The coin-flip: your first conjugate model",
    "text": "The coin-flip: your first conjugate model\nThe medical test was a discrete example — disease or no disease. Now let’s do the same logic with a continuous parameter.\nYou have a coin and want to estimate \\(\\theta = P(\\text{heads})\\). Before flipping, you express your belief about \\(\\theta\\) as a Beta distribution:\n\\[\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\]\nThe Beta distribution lives on \\([0, 1]\\) — perfect for a probability. The parameters \\(\\alpha\\) and \\(\\beta\\) control the shape:\n\n\\(\\alpha = \\beta = 1\\): flat (uniform) — “I have no idea”\n\\(\\alpha = \\beta = 5\\): peaked at 0.5 — “I think it’s fair”\n\\(\\alpha = 2, \\beta = 8\\): peaked near 0.2 — “I think it’s biased toward tails”\n\nNow flip the coin \\(n\\) times and observe \\(k\\) heads. The likelihood is binomial:\n\\[k \\sim \\text{Binomial}(n, \\theta)\\]\nBayes’ theorem gives the posterior — and because the Beta prior is conjugate to the binomial likelihood, the posterior is also a Beta:\n\\[\\theta \\mid k \\sim \\text{Beta}(\\alpha + k, \\;\\; \\beta + n - k)\\]\nThat’s it. The prior “counts” (\\(\\alpha, \\beta\\)) get updated by the data counts (\\(k\\) heads, \\(n - k\\) tails). No MCMC needed — the answer is a formula.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"alpha\", HTML(\"Prior &alpha;:\"),\n                  min = 1, max = 10, value = 1, step = 1),\n\n      sliderInput(\"beta_param\", HTML(\"Prior &beta;:\"),\n                  min = 1, max = 10, value = 1, step = 1),\n\n      sliderInput(\"n_flips\", \"Number of flips (n):\",\n                  min = 1, max = 100, value = 20, step = 1),\n\n      sliderInput(\"true_theta\", HTML(\"True &theta;:\"),\n                  min = 0.01, max = 0.99, value = 0.6, step = 0.01),\n\n      actionButton(\"flip\", \"Flip coins\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"stats_box\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"density_plot\", height = \"420px\")),\n        column(6, plotOutput(\"sequential_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  sim &lt;- reactive({\n    input$flip\n    a &lt;- input$alpha\n    b &lt;- input$beta_param\n    n &lt;- input$n_flips\n    theta &lt;- input$true_theta\n\n    # Simulate coin flips\n    flips &lt;- rbinom(n, 1, theta)\n    k &lt;- sum(flips)\n\n    # Posterior parameters\n    a_post &lt;- a + k\n    b_post &lt;- b + (n - k)\n\n    # Sequential updating: posterior mean after each flip\n    seq_means &lt;- numeric(n)\n    cum_heads &lt;- cumsum(flips)\n    for (i in seq_len(n)) {\n      seq_means[i] &lt;- (a + cum_heads[i]) / (a + b + i)\n    }\n\n    list(a = a, b = b, n = n, k = k, theta = theta,\n         a_post = a_post, b_post = b_post,\n         flips = flips, seq_means = seq_means)\n  })\n\n  output$density_plot &lt;- renderPlot({\n    d &lt;- sim()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    x &lt;- seq(0, 1, length.out = 300)\n    prior_y &lt;- dbeta(x, d$a, d$b)\n    post_y  &lt;- dbeta(x, d$a_post, d$b_post)\n\n    ylim &lt;- c(0, max(c(prior_y, post_y)) * 1.15)\n\n    plot(NULL, xlim = c(0, 1), ylim = ylim,\n         xlab = expression(theta), ylab = \"Density\",\n         main = expression(\"Prior & Posterior for \" * theta))\n\n    # Prior\n    lines(x, prior_y, col = \"#e74c3c\", lwd = 2, lty = 2)\n\n    # Posterior (shaded)\n    polygon(c(x, rev(x)), c(post_y, rep(0, length(x))),\n            col = \"#3498db30\", border = NA)\n    lines(x, post_y, col = \"#3498db\", lwd = 2.5)\n\n    # True theta\n    abline(v = d$theta, lty = 2, col = \"#27ae60\", lwd = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\n             paste0(\"Prior: Beta(\", d$a, \", \", d$b, \")\"),\n             paste0(\"Posterior: Beta(\", d$a_post, \", \", d$b_post, \")\"),\n             expression(\"True \" * theta)\n           ),\n           col = c(\"#e74c3c\", \"#3498db\", \"#27ae60\"),\n           lwd = c(2, 2.5, 2), lty = c(2, 1, 2))\n  })\n\n  output$sequential_plot &lt;- renderPlot({\n    d &lt;- sim()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    prior_mean &lt;- d$a / (d$a + d$b)\n\n    plot(seq_len(d$n), d$seq_means, type = \"l\", lwd = 2, col = \"#3498db\",\n         xlab = \"Flip number\", ylab = expression(\"Posterior mean of \" * theta),\n         main = \"Sequential Updating\",\n         ylim = c(0, 1))\n\n    # True theta\n    abline(h = d$theta, lty = 2, col = \"#27ae60\", lwd = 1.5)\n\n    # Prior mean\n    abline(h = prior_mean, lty = 3, col = \"#e74c3c\", lwd = 1.5)\n\n    # MLE line (cumulative k/n)\n    cum_heads &lt;- cumsum(d$flips)\n    mle_seq &lt;- cum_heads / seq_len(d$n)\n    lines(seq_len(d$n), mle_seq, col = \"gray50\", lwd = 1.5, lty = 3)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Posterior mean\", \"MLE (k/n)\",\n                      expression(\"True \" * theta), \"Prior mean\"),\n           col = c(\"#3498db\", \"gray50\", \"#27ae60\", \"#e74c3c\"),\n           lwd = c(2, 1.5, 1.5, 1.5), lty = c(1, 3, 2, 3))\n  })\n\n  output$stats_box &lt;- renderUI({\n    d &lt;- sim()\n    prior_mean &lt;- d$a / (d$a + d$b)\n    post_mean  &lt;- d$a_post / (d$a_post + d$b_post)\n    mle        &lt;- if (d$n &gt; 0) d$k / d$n else NA\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Heads:&lt;/b&gt; \", d$k, \" / \", d$n, \"&lt;br&gt;\",\n        \"&lt;b&gt;MLE (k/n):&lt;/b&gt; \", round(mle, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Prior mean:&lt;/b&gt; \", round(prior_mean, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Posterior mean:&lt;/b&gt; \", round(post_mean, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;True &theta;:&lt;/b&gt; \", d$theta\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nFlat prior (\\(\\alpha = \\beta = 1\\)): the posterior is driven entirely by the data. The posterior mean equals the MLE. With no prior information, the Bayesian is the frequentist.\nStrong prior (\\(\\alpha = \\beta = 10\\)): the prior is concentrated near 0.5. With few flips, the posterior barely moves. Increase \\(n\\) and watch the data overwhelm the prior.\nLarge \\(n\\) (100 flips): the prior becomes irrelevant. Prior and posterior mean converge to the MLE. This is the “prior washes out” property.\nSequential plot: watch how the posterior mean (blue) starts at the prior mean and drifts toward the true \\(\\theta\\) as data accumulates. Early flips matter more; later flips change the estimate less.",
    "crumbs": [
      "Foundations",
      "Bayes' Theorem"
    ]
  },
  {
    "objectID": "bayes-theorem.html#why-this-matters",
    "href": "bayes-theorem.html#why-this-matters",
    "title": "Bayes’ Theorem",
    "section": "Why this matters",
    "text": "Why this matters\nThe coin-flip model is Bayesian inference in its simplest form: prior \\(\\times\\) likelihood \\(=\\) posterior, all in closed form. The same logic — start with a belief, update with data — drives everything else on this site.\nThe coin-flip uses a Beta prior on a probability. The next page shows the same updating with different distributions. Bayesian Regression uses a Normal prior on a slope. The math changes; the logic doesn’t.",
    "crumbs": [
      "Foundations",
      "Bayes' Theorem"
    ]
  }
]