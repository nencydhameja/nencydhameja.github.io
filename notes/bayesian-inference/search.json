[
  {
    "objectID": "bayes-theorem.html",
    "href": "bayes-theorem.html",
    "title": "Bayes’ Theorem",
    "section": "",
    "text": "\\[P(H \\mid D) = \\frac{P(D \\mid H) \\cdot P(H)}{P(D)}\\]\nIn words:\n\\[\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Evidence}}\\]\nThat looks abstract. Let’s make it concrete.",
    "crumbs": [
      "Bayes' Theorem"
    ]
  },
  {
    "objectID": "bayes-theorem.html#the-formula",
    "href": "bayes-theorem.html#the-formula",
    "title": "Bayes’ Theorem",
    "section": "",
    "text": "\\[P(H \\mid D) = \\frac{P(D \\mid H) \\cdot P(H)}{P(D)}\\]\nIn words:\n\\[\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Evidence}}\\]\nThat looks abstract. Let’s make it concrete.",
    "crumbs": [
      "Bayes' Theorem"
    ]
  },
  {
    "objectID": "bayes-theorem.html#the-medical-test-example",
    "href": "bayes-theorem.html#the-medical-test-example",
    "title": "Bayes’ Theorem",
    "section": "The medical test example",
    "text": "The medical test example\nImagine a disease that affects 1 in 1,000 people. A test for it is 99% accurate — if you have the disease it says positive 99% of the time, and if you don’t have it, it says negative 99% of the time.\nYou test positive. What’s the probability you actually have the disease?\nMost people say 99%. The real answer is about 9%. This is not a trick — it’s Bayes’ theorem. The disease is so rare that even a good test produces more false positives than true positives.\nThe simulator below lets you adjust the base rate and test accuracy and watch how the posterior probability changes.\n#| standalone: true\n#| viewerHeight: 580\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .result-box {\n      background: #f0f4f8; border-radius: 6px; padding: 16px;\n      margin-top: 14px; font-size: 15px; line-height: 2;\n      text-align: center;\n    }\n    .result-box .big {\n      font-size: 32px; color: #e74c3c; font-weight: bold;\n    }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"prev\", \"Base rate (prevalence):\",\n                  min = 0.001, max = 0.20, value = 0.001, step = 0.001),\n\n      sliderInput(\"sens\", \"Sensitivity (true positive rate):\",\n                  min = 0.50, max = 1.00, value = 0.99, step = 0.01),\n\n      sliderInput(\"spec\", \"Specificity (true negative rate):\",\n                  min = 0.50, max = 1.00, value = 0.99, step = 0.01),\n\n      uiOutput(\"result_box\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"tree_plot\", height = \"420px\")),\n        column(6, plotOutput(\"icon_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  vals &lt;- reactive({\n    prev &lt;- input$prev\n    sens &lt;- input$sens\n    spec &lt;- input$spec\n\n    # Out of 10,000 people\n    N &lt;- 10000\n    sick &lt;- round(N * prev)\n    healthy &lt;- N - sick\n\n    true_pos  &lt;- round(sick * sens)\n    false_neg &lt;- sick - true_pos\n    false_pos &lt;- round(healthy * (1 - spec))\n    true_neg  &lt;- healthy - false_pos\n\n    total_pos &lt;- true_pos + false_pos\n    ppv &lt;- if (total_pos &gt; 0) true_pos / total_pos else 0\n\n    list(N = N, sick = sick, healthy = healthy,\n         true_pos = true_pos, false_neg = false_neg,\n         false_pos = false_pos, true_neg = true_neg,\n         total_pos = total_pos, ppv = ppv)\n  })\n\n  output$tree_plot &lt;- renderPlot({\n    v &lt;- vals()\n    par(mar = c(1, 1, 3, 1))\n\n    plot(NULL, xlim = c(0, 10), ylim = c(0, 10), axes = FALSE,\n         xlab = \"\", ylab = \"\", main = \"What happens to 10,000 people?\")\n\n    # Population\n    text(5, 9.5, paste0(\"Population: \", v$N), cex = 1.2, font = 2)\n\n    # Sick vs Healthy\n    text(2.5, 7.5, paste0(\"Sick: \", v$sick), cex = 1.1, col = \"#e74c3c\")\n    text(7.5, 7.5, paste0(\"Healthy: \", v$healthy), cex = 1.1, col = \"#3498db\")\n    segments(5, 9, 2.5, 8, lwd = 2)\n    segments(5, 9, 7.5, 8, lwd = 2)\n\n    # Test results for sick\n    text(1.2, 5.2, paste0(\"Test +\\n\", v$true_pos), cex = 1, col = \"#27ae60\", font = 2)\n    text(3.8, 5.2, paste0(\"Test -\\n\", v$false_neg), cex = 1, col = \"#7f8c8d\")\n    segments(2.5, 7, 1.2, 5.8, lwd = 1.5)\n    segments(2.5, 7, 3.8, 5.8, lwd = 1.5)\n\n    # Test results for healthy\n    text(6.2, 5.2, paste0(\"Test +\\n\", v$false_pos), cex = 1, col = \"#e74c3c\", font = 2)\n    text(8.8, 5.2, paste0(\"Test -\\n\", v$true_neg), cex = 1, col = \"#7f8c8d\")\n    segments(7.5, 7, 6.2, 5.8, lwd = 1.5)\n    segments(7.5, 7, 8.8, 5.8, lwd = 1.5)\n\n    # Total positives\n    text(3.7, 3, paste0(\"Total positive tests: \", v$total_pos), cex = 1.1, font = 2)\n    text(3.7, 2, paste0(\"Of these, truly sick: \", v$true_pos), cex = 1.1,\n         col = \"#27ae60\", font = 2)\n    text(3.7, 1, paste0(\"P(sick | test+) = \",\n         v$true_pos, \"/\", v$total_pos, \" = \",\n         round(v$ppv * 100, 1), \"%\"), cex = 1.2, font = 2, col = \"#e74c3c\")\n  })\n\n  output$icon_plot &lt;- renderPlot({\n    v &lt;- vals()\n    par(mar = c(1, 1, 3, 1))\n\n    # Show total positive tests as dots\n    n_show &lt;- min(v$total_pos, 200)\n    n_true &lt;- round(n_show * v$ppv)\n    n_false &lt;- n_show - n_true\n\n    cols &lt;- c(rep(\"#27ae60\", n_true), rep(\"#e74c3c\", n_false))\n    cols &lt;- sample(cols)\n\n    ncol &lt;- ceiling(sqrt(n_show))\n    nrow &lt;- ceiling(n_show / ncol)\n\n    plot(NULL, xlim = c(0, ncol + 1), ylim = c(0, nrow + 1),\n         axes = FALSE, xlab = \"\", ylab = \"\",\n         main = paste0(\"All \", v$total_pos, \" positive tests\"))\n\n    if (n_show &gt; 0) {\n      x &lt;- rep(seq_len(ncol), times = nrow)[seq_len(n_show)]\n      y &lt;- rep(seq(nrow, 1), each = ncol)[seq_len(n_show)]\n      points(x, y, pch = 15, cex = max(0.5, 3 - n_show / 50), col = cols)\n    }\n\n    legend(\"bottom\", bty = \"n\", horiz = TRUE, cex = 0.95,\n           legend = c(paste0(\"Truly sick (\", n_true, \")\"),\n                      paste0(\"False alarm (\", n_false, \")\")),\n           col = c(\"#27ae60\", \"#e74c3c\"), pch = 15, pt.cex = 1.5)\n  })\n\n  output$result_box &lt;- renderUI({\n    v &lt;- vals()\n    tags$div(class = \"result-box\",\n      HTML(paste0(\n        \"If you test positive,&lt;br&gt;\",\n        \"the probability you're sick is:&lt;br&gt;\",\n        \"&lt;span class='big'&gt;\", round(v$ppv * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;small&gt;not \", round(input$sens * 100), \"%!&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nDefault settings (prevalence 0.1%, test 99% accurate): only ~9% of positive tests are truly sick. The base rate dominates.\nSlide prevalence up to 5%: now ~84% of positives are real. The prior matters!\nSlide prevalence to 50%: the posterior is ~99%. When the disease is common, a positive test is very informative.\nLower specificity to 90%: false positives explode. Watch the right plot fill with red dots.\n\n\n\nThe lesson\nBayes’ theorem tells you: don’t just look at the test accuracy — look at how common the thing is. A 99% accurate test is nearly useless for a 1-in-1,000 disease because most positives are false alarms. This is the base rate fallacy, and Bayes’ theorem is the cure.",
    "crumbs": [
      "Bayes' Theorem"
    ]
  },
  {
    "objectID": "priors-posteriors.html",
    "href": "priors-posteriors.html",
    "title": "Priors & Posteriors",
    "section": "",
    "text": "Imagine you’re trying to estimate something — say, the average effect of a tutoring program on test scores. In frequentist statistics, you collect data, compute a point estimate, and that’s your answer.\nIn Bayesian statistics, you do something different:\n\nStart with a prior — what you believed before seeing data. Maybe from past studies, expert opinion, or just “I have no idea” (a flat prior).\nObserve data — the likelihood tells you how probable the data is for each possible value of the parameter.\nCombine them — Bayes’ theorem multiplies the prior by the likelihood to give you the posterior: your updated belief after seeing the data.\n\n\\[\\underbrace{P(\\theta \\mid \\text{data})}_{\\text{posterior}} \\propto \\underbrace{P(\\text{data} \\mid \\theta)}_{\\text{likelihood}} \\times \\underbrace{P(\\theta)}_{\\text{prior}}\\]\nThe simulation below makes this tangible. You’re estimating the true mean of a normal distribution. Set a prior, generate data, and watch the posterior form.\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_mu\", HTML(\"True &mu; (unknown to you):\"),\n                  min = -5, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"prior_mu\", \"Prior mean:\",\n                  min = -5, max = 5, value = 0, step = 0.5),\n\n      sliderInput(\"prior_sd\", \"Prior SD (certainty):\",\n                  min = 0.5, max = 10, value = 3, step = 0.5),\n\n      sliderInput(\"n\", \"Sample size (data):\",\n                  min = 1, max = 200, value = 5, step = 1),\n\n      sliderInput(\"sigma\", HTML(\"Data noise (&sigma;):\"),\n                  min = 0.5, max = 5, value = 2, step = 0.5),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"posterior_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n\n    true_mu  &lt;- input$true_mu\n    prior_mu &lt;- input$prior_mu\n    prior_sd &lt;- input$prior_sd\n    n        &lt;- input$n\n    sigma    &lt;- input$sigma\n\n    # Generate data\n    y &lt;- rnorm(n, mean = true_mu, sd = sigma)\n    y_bar &lt;- mean(y)\n\n    # Posterior (conjugate normal-normal)\n    prior_prec &lt;- 1 / prior_sd^2\n    data_prec  &lt;- n / sigma^2\n    post_prec  &lt;- prior_prec + data_prec\n    post_sd    &lt;- 1 / sqrt(post_prec)\n    post_mu    &lt;- (prior_prec * prior_mu + data_prec * y_bar) / post_prec\n\n    # Shrinkage weight on prior\n    w_prior &lt;- prior_prec / post_prec\n\n    list(true_mu = true_mu, prior_mu = prior_mu, prior_sd = prior_sd,\n         y_bar = y_bar, sigma = sigma, n = n,\n         post_mu = post_mu, post_sd = post_sd, w_prior = w_prior)\n  })\n\n  output$posterior_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    xmin &lt;- min(d$prior_mu - 3.5 * d$prior_sd, d$post_mu - 4 * d$post_sd, d$true_mu - 2)\n    xmax &lt;- max(d$prior_mu + 3.5 * d$prior_sd, d$post_mu + 4 * d$post_sd, d$true_mu + 2)\n    x &lt;- seq(xmin, xmax, length.out = 500)\n\n    y_prior &lt;- dnorm(x, d$prior_mu, d$prior_sd)\n    y_like  &lt;- dnorm(x, d$y_bar, d$sigma / sqrt(d$n))\n    y_post  &lt;- dnorm(x, d$post_mu, d$post_sd)\n\n    ylim &lt;- c(0, max(y_prior, y_like, y_post) * 1.15)\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(x, y_prior, type = \"l\", lwd = 2.5, col = \"#e74c3c\",\n         xlab = expression(mu), ylab = \"Density\",\n         main = \"Prior + Likelihood = Posterior\",\n         ylim = ylim)\n    lines(x, y_like, lwd = 2.5, col = \"#3498db\")\n    lines(x, y_post, lwd = 3, col = \"#27ae60\")\n\n    # Shade posterior\n    polygon(c(x, rev(x)),\n            c(y_post, rep(0, length(x))),\n            col = adjustcolor(\"#27ae60\", 0.2), border = NA)\n\n    # True value\n    abline(v = d$true_mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.9,\n           legend = c(\"Prior (your belief before data)\",\n                      \"Likelihood (what the data says)\",\n                      \"Posterior (updated belief)\",\n                      expression(\"True \" * mu)),\n           col = c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"#2c3e50\"),\n           lwd = c(2.5, 2.5, 3, 2),\n           lty = c(1, 1, 1, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Prior mean:&lt;/b&gt; \", d$prior_mu, \"&lt;br&gt;\",\n        \"&lt;b&gt;Data mean:&lt;/b&gt; \", round(d$y_bar, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Posterior mean:&lt;/b&gt; \", round(d$post_mu, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Posterior SD:&lt;/b&gt; \", round(d$post_sd, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Weight on prior:&lt;/b&gt; \", round(d$w_prior * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;b&gt;Weight on data:&lt;/b&gt; \", round((1 - d$w_prior) * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;small&gt;Posterior = weighted average of prior & data&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nn = 1: the posterior is mostly the prior (red). You barely have data.\nSlide n to 100: the posterior (green) collapses onto the data mean (blue). Data overwhelms the prior. With enough data, the prior doesn’t matter.\nSet prior SD = 0.5 (strong prior) with n = 5: the posterior is pulled toward the prior. This is shrinkage — the prior is “shrinking” your estimate away from the data and toward your prior belief.\nSet prior SD = 10 (vague prior): the posterior tracks the data almost exactly. A flat prior says “I have no opinion” and lets the data speak.\nWatch the weight on prior in the sidebar — it shows exactly how much the posterior is a compromise between prior and data.",
    "crumbs": [
      "Priors & Posteriors"
    ]
  },
  {
    "objectID": "priors-posteriors.html#what-is-bayesian-inference-really",
    "href": "priors-posteriors.html#what-is-bayesian-inference-really",
    "title": "Priors & Posteriors",
    "section": "",
    "text": "Imagine you’re trying to estimate something — say, the average effect of a tutoring program on test scores. In frequentist statistics, you collect data, compute a point estimate, and that’s your answer.\nIn Bayesian statistics, you do something different:\n\nStart with a prior — what you believed before seeing data. Maybe from past studies, expert opinion, or just “I have no idea” (a flat prior).\nObserve data — the likelihood tells you how probable the data is for each possible value of the parameter.\nCombine them — Bayes’ theorem multiplies the prior by the likelihood to give you the posterior: your updated belief after seeing the data.\n\n\\[\\underbrace{P(\\theta \\mid \\text{data})}_{\\text{posterior}} \\propto \\underbrace{P(\\text{data} \\mid \\theta)}_{\\text{likelihood}} \\times \\underbrace{P(\\theta)}_{\\text{prior}}\\]\nThe simulation below makes this tangible. You’re estimating the true mean of a normal distribution. Set a prior, generate data, and watch the posterior form.\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_mu\", HTML(\"True &mu; (unknown to you):\"),\n                  min = -5, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"prior_mu\", \"Prior mean:\",\n                  min = -5, max = 5, value = 0, step = 0.5),\n\n      sliderInput(\"prior_sd\", \"Prior SD (certainty):\",\n                  min = 0.5, max = 10, value = 3, step = 0.5),\n\n      sliderInput(\"n\", \"Sample size (data):\",\n                  min = 1, max = 200, value = 5, step = 1),\n\n      sliderInput(\"sigma\", HTML(\"Data noise (&sigma;):\"),\n                  min = 0.5, max = 5, value = 2, step = 0.5),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"posterior_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n\n    true_mu  &lt;- input$true_mu\n    prior_mu &lt;- input$prior_mu\n    prior_sd &lt;- input$prior_sd\n    n        &lt;- input$n\n    sigma    &lt;- input$sigma\n\n    # Generate data\n    y &lt;- rnorm(n, mean = true_mu, sd = sigma)\n    y_bar &lt;- mean(y)\n\n    # Posterior (conjugate normal-normal)\n    prior_prec &lt;- 1 / prior_sd^2\n    data_prec  &lt;- n / sigma^2\n    post_prec  &lt;- prior_prec + data_prec\n    post_sd    &lt;- 1 / sqrt(post_prec)\n    post_mu    &lt;- (prior_prec * prior_mu + data_prec * y_bar) / post_prec\n\n    # Shrinkage weight on prior\n    w_prior &lt;- prior_prec / post_prec\n\n    list(true_mu = true_mu, prior_mu = prior_mu, prior_sd = prior_sd,\n         y_bar = y_bar, sigma = sigma, n = n,\n         post_mu = post_mu, post_sd = post_sd, w_prior = w_prior)\n  })\n\n  output$posterior_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    xmin &lt;- min(d$prior_mu - 3.5 * d$prior_sd, d$post_mu - 4 * d$post_sd, d$true_mu - 2)\n    xmax &lt;- max(d$prior_mu + 3.5 * d$prior_sd, d$post_mu + 4 * d$post_sd, d$true_mu + 2)\n    x &lt;- seq(xmin, xmax, length.out = 500)\n\n    y_prior &lt;- dnorm(x, d$prior_mu, d$prior_sd)\n    y_like  &lt;- dnorm(x, d$y_bar, d$sigma / sqrt(d$n))\n    y_post  &lt;- dnorm(x, d$post_mu, d$post_sd)\n\n    ylim &lt;- c(0, max(y_prior, y_like, y_post) * 1.15)\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(x, y_prior, type = \"l\", lwd = 2.5, col = \"#e74c3c\",\n         xlab = expression(mu), ylab = \"Density\",\n         main = \"Prior + Likelihood = Posterior\",\n         ylim = ylim)\n    lines(x, y_like, lwd = 2.5, col = \"#3498db\")\n    lines(x, y_post, lwd = 3, col = \"#27ae60\")\n\n    # Shade posterior\n    polygon(c(x, rev(x)),\n            c(y_post, rep(0, length(x))),\n            col = adjustcolor(\"#27ae60\", 0.2), border = NA)\n\n    # True value\n    abline(v = d$true_mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.9,\n           legend = c(\"Prior (your belief before data)\",\n                      \"Likelihood (what the data says)\",\n                      \"Posterior (updated belief)\",\n                      expression(\"True \" * mu)),\n           col = c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"#2c3e50\"),\n           lwd = c(2.5, 2.5, 3, 2),\n           lty = c(1, 1, 1, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Prior mean:&lt;/b&gt; \", d$prior_mu, \"&lt;br&gt;\",\n        \"&lt;b&gt;Data mean:&lt;/b&gt; \", round(d$y_bar, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Posterior mean:&lt;/b&gt; \", round(d$post_mu, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Posterior SD:&lt;/b&gt; \", round(d$post_sd, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Weight on prior:&lt;/b&gt; \", round(d$w_prior * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;b&gt;Weight on data:&lt;/b&gt; \", round((1 - d$w_prior) * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;small&gt;Posterior = weighted average of prior & data&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nn = 1: the posterior is mostly the prior (red). You barely have data.\nSlide n to 100: the posterior (green) collapses onto the data mean (blue). Data overwhelms the prior. With enough data, the prior doesn’t matter.\nSet prior SD = 0.5 (strong prior) with n = 5: the posterior is pulled toward the prior. This is shrinkage — the prior is “shrinking” your estimate away from the data and toward your prior belief.\nSet prior SD = 10 (vague prior): the posterior tracks the data almost exactly. A flat prior says “I have no opinion” and lets the data speak.\nWatch the weight on prior in the sidebar — it shows exactly how much the posterior is a compromise between prior and data.",
    "crumbs": [
      "Priors & Posteriors"
    ]
  },
  {
    "objectID": "priors-posteriors.html#shrinkage-the-bayesian-superpower",
    "href": "priors-posteriors.html#shrinkage-the-bayesian-superpower",
    "title": "Priors & Posteriors",
    "section": "Shrinkage: the Bayesian superpower",
    "text": "Shrinkage: the Bayesian superpower\nLook at the “weight on prior” number in the sidebar. The posterior mean is literally a weighted average:\n\\[\\mu_{post} = w \\cdot \\mu_{prior} + (1 - w) \\cdot \\bar{y}\\]\nwhere \\(w\\) depends on how confident your prior is relative to how much data you have.\nThis is shrinkage: the posterior “shrinks” the data estimate toward the prior. When is this useful?\n\nSmall samples: noisy data gets regularized toward a sensible default.\nMany groups: estimating batting averages for 500 baseball players? Shrink extreme estimates toward the league average. A player who went 3-for-3 on opening day probably isn’t a .1000 hitter.\nHierarchical models: borrow strength across groups by shrinking toward a common mean.\n\nShrinkage isn’t bias — it’s a bias-variance tradeoff. You add a little bias but reduce variance a lot, often improving overall accuracy.",
    "crumbs": [
      "Priors & Posteriors"
    ]
  },
  {
    "objectID": "priors-posteriors.html#why-did-the-math-work-out-so-cleanly",
    "href": "priors-posteriors.html#why-did-the-math-work-out-so-cleanly",
    "title": "Priors & Posteriors",
    "section": "Why did the math work out so cleanly?",
    "text": "Why did the math work out so cleanly?\nThe simulation above computes the posterior instantly — no sampling, no iteration, just a formula. That’s because we used a conjugate prior: a special prior-likelihood pair where the posterior has the same distributional form as the prior.\nHere, the prior is Normal, the likelihood is Normal, and the posterior is Normal too. You just update the mean and variance:\n\\[\\mu_{post} = \\frac{\\frac{\\mu_0}{\\sigma_0^2} + \\frac{n\\bar{y}}{\\sigma^2}}{\\frac{1}{\\sigma_0^2} + \\frac{n}{\\sigma^2}}\\]\nPlug in numbers, get the answer. No algorithm required.\n\nCommon conjugate pairs\n\n\n\nLikelihood\nConjugate prior\nPosterior\nExample\n\n\n\n\nNormal (known \\(\\sigma\\))\nNormal\nNormal\nEstimating a mean (this page)\n\n\nBinomial\nBeta\nBeta\nEstimating a proportion (Bayes’ Theorem)\n\n\nPoisson\nGamma\nGamma\nEstimating a rate\n\n\n\n\n\nThe problem: most real models aren’t conjugate\nConjugacy is elegant but limited. It only works for these specific combinations. The moment your model gets realistic — logistic regression with priors on coefficients, hierarchical models with multiple levels, non-standard likelihoods — there’s no conjugate solution. The posterior is some high-dimensional surface with no closed-form expression.\n\n\n\n\n\n\nWhat does “closed-form” mean? A closed-form solution is one you can write as a finite formula using standard operations (addition, multiplication, exponents, etc.) and evaluate directly.\nClosed-form (conjugate case): the posterior mean above — plug in \\(\\mu_0\\), \\(\\sigma_0\\), \\(n\\), \\(\\bar{y}\\), \\(\\sigma\\), do arithmetic, get the exact answer. Done.\nNot closed-form (non-conjugate case): say you want the posterior for a logistic regression coefficient \\(\\theta\\) with a normal prior:\n\\[p(\\theta \\mid y) = \\frac{\\prod_{i=1}^n \\frac{1}{1 + e^{-\\theta x_i}} \\cdot e^{-\\theta^2/2}}{\\int_{-\\infty}^{\\infty} \\prod_{i=1}^n \\frac{1}{1 + e^{-\\theta x_i}} \\cdot e^{-\\theta^2/2} \\, d\\theta}\\]\nThat integral in the denominator? There’s no formula for it. You can’t simplify it to “plug in numbers.” You’d have to numerically approximate it — which is exactly what MCMC does (it sidesteps the integral entirely by sampling).\n\n\n\nThat’s why MCMC exists: when you can’t write down the posterior, you sample from it instead. The progression in this course:\n\nThis page: conjugate priors — exact, instant posteriors (the special case)\nMCMC: numerical sampling — posteriors for any model (the general case)\nHierarchical Models: the reason you need MCMC in practice",
    "crumbs": [
      "Priors & Posteriors"
    ]
  },
  {
    "objectID": "hierarchical.html",
    "href": "hierarchical.html",
    "title": "Hierarchical Models",
    "section": "",
    "text": "You have data from \\(K\\) groups — schools, hospitals, factories, regions — and you want to estimate a parameter (say, the mean) for each group. Two extreme approaches:\n\nNo pooling: estimate each group separately using only its own data. With small groups, the estimates are noisy and unreliable.\nComplete pooling: ignore groups entirely and estimate a single grand mean. This throws away real group differences.\n\nNeither is satisfying. No pooling overfits to noise. Complete pooling underfits by ignoring structure. You want something in between.",
    "crumbs": [
      "Beyond Conjugates",
      "Hierarchical Models"
    ]
  },
  {
    "objectID": "hierarchical.html#the-problem",
    "href": "hierarchical.html#the-problem",
    "title": "Hierarchical Models",
    "section": "",
    "text": "You have data from \\(K\\) groups — schools, hospitals, factories, regions — and you want to estimate a parameter (say, the mean) for each group. Two extreme approaches:\n\nNo pooling: estimate each group separately using only its own data. With small groups, the estimates are noisy and unreliable.\nComplete pooling: ignore groups entirely and estimate a single grand mean. This throws away real group differences.\n\nNeither is satisfying. No pooling overfits to noise. Complete pooling underfits by ignoring structure. You want something in between.",
    "crumbs": [
      "Beyond Conjugates",
      "Hierarchical Models"
    ]
  },
  {
    "objectID": "hierarchical.html#hierarchical-models-partial-pooling",
    "href": "hierarchical.html#hierarchical-models-partial-pooling",
    "title": "Hierarchical Models",
    "section": "Hierarchical models: partial pooling",
    "text": "Hierarchical models: partial pooling\nA hierarchical (multilevel) model learns the group-level variation from the data and uses it to shrink each group’s estimate toward the grand mean — more for small groups, less for large groups.\nThe model:\n\\[y_{ij} \\sim N(\\theta_j, \\sigma^2) \\quad \\text{(data within group } j\\text{)}\\] \\[\\theta_j \\sim N(\\mu, \\tau^2) \\quad \\text{(group means come from a population)}\\]\n\n\\(\\theta_j\\) is the true mean for group \\(j\\)\n\\(\\mu\\) is the overall population mean\n\\(\\tau^2\\) is the between-group variance (how different groups really are)\n\\(\\sigma^2\\) is the within-group variance (noise)\n\nThe posterior for each \\(\\theta_j\\) is a weighted average of the group’s own data and the grand mean:\n\\[\\hat{\\theta}_j^{partial} = w_j \\cdot \\bar{y}_j + (1 - w_j) \\cdot \\hat{\\mu}\\]\nwhere the weight \\(w_j = \\frac{n_j / \\sigma^2}{n_j / \\sigma^2 + 1/\\tau^2}\\) depends on the group sample size \\(n_j\\). Small groups shrink more because their data is less informative relative to the population prior.\nThis is the same shrinkage logic from the shrinkage page, but now applied within a formal model that estimates \\(\\tau^2\\) from the data.\n\nSimulation: School test scores\n\\(K\\) schools, each with \\(n_k\\) students. Compare three estimates for each school’s true mean:\n\nNo pooling (red): each school’s raw sample mean\nComplete pooling (gray): the grand mean for all schools\nPartial pooling (blue): the hierarchical Bayes estimate\n\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"K\", \"Number of schools:\",\n                  min = 5, max = 30, value = 12, step = 1),\n\n      sliderInput(\"n_per\", \"Students per school:\",\n                  min = 5, max = 100, value = 15, step = 5),\n\n      sliderInput(\"tau\", \"Between-school SD (\\u03C4):\",\n                  min = 1, max = 15, value = 5, step = 1),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"school_plot\", height = \"450px\")),\n        column(6, plotOutput(\"mse_plot\",    height = \"450px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    K     &lt;- input$K\n    n_per &lt;- input$n_per\n    tau   &lt;- input$tau\n    sigma &lt;- 10  # within-school SD (fixed)\n\n    # True school means\n    mu &lt;- 70  # grand mean\n    theta_true &lt;- rnorm(K, mean = mu, sd = tau)\n\n    # Generate student scores\n    y_bar &lt;- numeric(K)\n    for (j in 1:K) {\n      scores &lt;- rnorm(n_per, mean = theta_true[j], sd = sigma)\n      y_bar[j] &lt;- mean(scores)\n    }\n\n    # No pooling: raw means\n    no_pool &lt;- y_bar\n\n    # Complete pooling: grand mean\n    grand_mean &lt;- mean(y_bar)\n    complete_pool &lt;- rep(grand_mean, K)\n\n    # Partial pooling (empirical Bayes)\n    # Estimate tau from data\n    between_var &lt;- var(y_bar)\n    within_var  &lt;- sigma^2 / n_per\n    tau_hat_sq  &lt;- max(between_var - within_var, 0.01)\n\n    w &lt;- (n_per / sigma^2) / (n_per / sigma^2 + 1 / tau_hat_sq)\n    partial_pool &lt;- w * y_bar + (1 - w) * grand_mean\n\n    # MSE\n    mse_no   &lt;- mean((no_pool - theta_true)^2)\n    mse_comp &lt;- mean((complete_pool - theta_true)^2)\n    mse_part &lt;- mean((partial_pool - theta_true)^2)\n\n    list(theta_true = theta_true, no_pool = no_pool,\n         complete_pool = complete_pool, partial_pool = partial_pool,\n         grand_mean = grand_mean, w = w,\n         mse_no = mse_no, mse_comp = mse_comp, mse_part = mse_part,\n         K = K, n_per = n_per, tau = tau)\n  })\n\n  output$school_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 5.5, 3, 1))\n\n    K &lt;- d$K\n    ord &lt;- order(d$no_pool)\n\n    xlim &lt;- range(c(d$no_pool, d$partial_pool, d$theta_true, d$grand_mean))\n    xlim &lt;- xlim + c(-2, 2)\n\n    plot(NULL, xlim = xlim, ylim = c(1, K),\n         yaxt = \"n\", xlab = \"Score\",\n         ylab = \"\", main = \"School Mean Estimates\")\n    axis(2, at = 1:K, labels = paste0(\"School \", ord), las = 1, cex.axis = 0.7)\n\n    # Grand mean line\n    abline(v = d$grand_mean, lty = 2, col = \"gray50\", lwd = 1.5)\n\n    for (i in 1:K) {\n      j &lt;- ord[i]\n\n      # Arrow from no-pooling to partial-pooling\n      arrows(d$no_pool[j], i, d$partial_pool[j], i,\n             length = 0.04, col = \"#bdc3c780\", lwd = 1)\n\n      # Points\n      points(d$no_pool[j], i, pch = 16, col = \"#e74c3c\", cex = 1.2)\n      points(d$partial_pool[j], i, pch = 17, col = \"#3498db\", cex = 1.2)\n      points(d$theta_true[j], i, pch = 4, col = \"#27ae60\", cex = 1, lwd = 2)\n    }\n\n    legend(\"bottomright\", bty = \"n\", cex = 0.8,\n           legend = c(\"No pooling (raw mean)\", \"Partial pooling\",\n                      \"True mean\", \"Grand mean\"),\n           col = c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"gray50\"),\n           pch = c(16, 17, 4, NA),\n           lty = c(NA, NA, NA, 2), lwd = c(NA, NA, 2, 1.5))\n  })\n\n  output$mse_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4, 3, 1))\n\n    vals &lt;- c(d$mse_no, d$mse_comp, d$mse_part)\n    cols &lt;- c(\"#e74c3c\", \"gray60\", \"#3498db\")\n    labels &lt;- c(\"No pooling\", \"Complete\\npooling\", \"Partial\\npooling\")\n\n    bp &lt;- barplot(vals, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.85,\n                  main = \"Mean Squared Error\",\n                  ylab = \"MSE\")\n    text(bp, vals + max(vals) * 0.03, round(vals, 1), cex = 0.85, font = 2)\n\n    pct &lt;- round((1 - d$mse_part / d$mse_no) * 100, 0)\n    mtext(paste0(\"Partial pooling reduces MSE by ~\", pct, \"% vs no pooling\"),\n          side = 1, line = 3.5, cex = 0.85, col = \"#2c3e50\")\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    pct_np &lt;- round((1 - d$mse_part / d$mse_no) * 100, 1)\n    pct_cp &lt;- round((1 - d$mse_part / d$mse_comp) * 100, 1)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Shrinkage weight:&lt;/b&gt; \", round(d$w * 100, 1),\n        \"% on group data&lt;br&gt;\",\n        \"&lt;small&gt;(\", round((1 - d$w) * 100, 1), \"% on grand mean)&lt;/small&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;MSE no pooling:&lt;/b&gt; &lt;span class='bad'&gt;\",\n        round(d$mse_no, 1), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;MSE complete pooling:&lt;/b&gt; \",\n        round(d$mse_comp, 1), \"&lt;br&gt;\",\n        \"&lt;b&gt;MSE partial pooling:&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$mse_part, 1), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;vs no pooling:&lt;/b&gt; &lt;span class='good'&gt;\",\n        pct_np, \"% lower&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;vs complete pooling:&lt;/b&gt; &lt;span class='good'&gt;\",\n        pct_cp, \"% lower&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nStudents per school = 5, between-school SD = 5: small samples, real group differences. The no-pooling estimates (red) are scattered — some far from the truth. Partial pooling (blue triangles) shrinks them toward the grand mean, landing closer to the true values (green crosses). MSE drops substantially.\nStudents per school = 100: with lots of data per group, the raw means are already precise. Shrinkage is minimal — blue and red nearly overlap. With enough data, partial pooling = no pooling.\nBetween-school SD = 1: schools are very similar. Complete pooling is nearly optimal because the group differences are tiny. Partial pooling agrees — it shrinks almost entirely to the grand mean.\nBetween-school SD = 15: schools differ a lot. Complete pooling is terrible because it ignores real differences. No pooling is better, and partial pooling is best — it respects both the data and the group structure.\nLook at the left plot: the arrows show shrinkage. Extreme schools (raw means far from the grand mean) shrink the most. Schools near the middle barely move. This is adaptive — the model learns how much shrinkage is appropriate.",
    "crumbs": [
      "Beyond Conjugates",
      "Hierarchical Models"
    ]
  },
  {
    "objectID": "hierarchical.html#why-partial-pooling-wins",
    "href": "hierarchical.html#why-partial-pooling-wins",
    "title": "Hierarchical Models",
    "section": "Why partial pooling wins",
    "text": "Why partial pooling wins\nThe logic is identical to the shrinkage page, but in a structured model:\n\nNo pooling has zero bias but high variance (each estimate uses only local data).\nComplete pooling has high bias but zero variance across groups.\nPartial pooling trades a little bias for a large reduction in variance. The bias-variance tradeoff is optimized by the model.\n\nThe more groups you have, the better the model estimates \\(\\tau^2\\) (the between-group variance), and the more precisely it calibrates the amount of shrinkage.\n\nWhere hierarchical models show up\n\n\n\nApplication\nGroups\nWhat gets shrunk\n\n\n\n\nEducation\nSchools / districts\nTest score means\n\n\nSports analytics\nPlayers / teams\nBatting averages, win rates\n\n\nClinical trials\nStudy sites / subgroups\nTreatment effects\n\n\nMarketing\nRegions / customer segments\nResponse rates\n\n\nEcology\nSpecies / habitats\nPopulation parameters\n\n\n\nIn each case, the hierarchical structure lets you borrow strength across groups — improving estimates for every group, especially the small ones.",
    "crumbs": [
      "Beyond Conjugates",
      "Hierarchical Models"
    ]
  },
  {
    "objectID": "hierarchical.html#did-you-know",
    "href": "hierarchical.html#did-you-know",
    "title": "Hierarchical Models",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe “8 schools” example from Rubin (1981) and Gelman et al. (2013, Bayesian Data Analysis) is the canonical textbook example of hierarchical modeling. Eight schools ran a coaching program; the hierarchical model showed that the most impressive result (School A, +28 points) was largely noise, and the true effects were more modest and similar across schools.\nHierarchical models are the Bayesian counterpart of random effects models in frequentist statistics. The key difference: Bayesian hierarchical models provide full posterior distributions for each group parameter, while frequentist random effects models typically give only point estimates (BLUPs — Best Linear Unbiased Predictors).\nStein’s paradox (1956) proved that when estimating 3 or more means simultaneously, the sample means are inadmissible — there always exists an estimator with lower total MSE. The James-Stein estimator and hierarchical models both exploit this: by shrinking toward a common mean, they beat the “obvious” estimator that uses each group’s data alone.",
    "crumbs": [
      "Beyond Conjugates",
      "Hierarchical Models"
    ]
  },
  {
    "objectID": "mcmc.html",
    "href": "mcmc.html",
    "title": "Markov Chain Monte Carlo",
    "section": "",
    "text": "The name breaks down into two parts:\n\nMonte Carlo: using random sampling to approximate something you can’t compute exactly. Instead of solving an integral analytically, you draw random samples and use their average as an approximation. (Named after the Monte Carlo casino — it’s fundamentally about randomness.)\nMarkov Chain: a sequence of random values where each value depends only on the previous one — not on the full history. The “chain” is a random walk through parameter space, where each step proposes a new value based on where you currently are.\n\nPut them together: MCMC constructs a Markov chain whose long-run distribution equals the posterior. Run it long enough, and the samples you collect are (approximately) draws from \\(p(\\theta \\mid y)\\) — even though you never computed that distribution directly.",
    "crumbs": [
      "Beyond Conjugates",
      "MCMC"
    ]
  },
  {
    "objectID": "mcmc.html#why-mcmc",
    "href": "mcmc.html#why-mcmc",
    "title": "Markov Chain Monte Carlo",
    "section": "",
    "text": "On the Priors & Posteriors page, we used conjugate priors — special prior-likelihood pairs where the posterior has a closed-form solution. That’s elegant but limiting. Most real models don’t have conjugate posteriors:\n\nLogistic regression with a prior on coefficients\nHierarchical models with multiple levels of parameters\nAny model where the posterior \\(p(\\theta \\mid y) \\propto p(y \\mid \\theta) \\, p(\\theta)\\) doesn’t simplify to a known distribution\n\nFor these models, we can’t write down the posterior analytically. We need to sample from it numerically. That’s what MCMC does.",
    "crumbs": [
      "Beyond Conjugates",
      "MCMC"
    ]
  },
  {
    "objectID": "mcmc.html#the-metropolis-hastings-algorithm",
    "href": "mcmc.html#the-metropolis-hastings-algorithm",
    "title": "Markov Chain Monte Carlo",
    "section": "The Metropolis-Hastings algorithm",
    "text": "The Metropolis-Hastings algorithm\nThe most foundational MCMC algorithm. The core idea is beautifully simple:\n\nStart at some value \\(\\theta_0\\).\nPropose a new value \\(\\theta^*\\) from a proposal distribution \\(q(\\theta^* \\mid \\theta_t)\\) — typically \\(\\theta^* \\sim N(\\theta_t, \\sigma_{prop}^2)\\).\nCompute the acceptance ratio: \\[\\alpha = \\min\\left(1, \\, \\frac{p(\\theta^* \\mid y)}{p(\\theta_t \\mid y)}\\right)\\]\nAccept \\(\\theta^*\\) with probability \\(\\alpha\\) (set \\(\\theta_{t+1} = \\theta^*\\)). Otherwise stay: \\(\\theta_{t+1} = \\theta_t\\).\nRepeat.\n\nKey insight: you never need to compute the normalizing constant \\(p(y) = \\int p(y \\mid \\theta) \\, p(\\theta) \\, d\\theta\\). The ratio \\(p(\\theta^* \\mid y) / p(\\theta_t \\mid y)\\) cancels it out. You only need to evaluate the unnormalized posterior — the numerator of Bayes’ theorem.\nAfter enough iterations, the chain converges to the posterior distribution. The samples \\(\\theta_1, \\theta_2, \\ldots\\) are (correlated) draws from \\(p(\\theta \\mid y)\\).\n\nSimulation 1: Metropolis-Hastings in action\nEstimate the mean \\(\\mu\\) of normally distributed data with unknown true value. The proposal width controls exploration: too narrow and the chain moves slowly; too wide and most proposals get rejected.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_mu\", HTML(\"True &mu;:\"),\n                  min = -3, max = 3, value = 1, step = 0.5),\n\n      sliderInput(\"n_data\", \"Data points:\",\n                  min = 5, max = 100, value = 20, step = 5),\n\n      sliderInput(\"prop_sd\", \"Proposal width (SD):\",\n                  min = 0.05, max = 5, value = 0.5, step = 0.05),\n\n      sliderInput(\"n_iter\", \"Iterations:\",\n                  min = 500, max = 5000, value = 2000, step = 500),\n\n      actionButton(\"go\", \"Run chain\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"trace_plot\", height = \"420px\")),\n        column(6, plotOutput(\"hist_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    true_mu  &lt;- input$true_mu\n    n_data   &lt;- input$n_data\n    prop_sd  &lt;- input$prop_sd\n    n_iter   &lt;- input$n_iter\n    sigma    &lt;- 2  # known SD\n\n    # Generate data\n    y &lt;- rnorm(n_data, mean = true_mu, sd = sigma)\n\n    # Log posterior (unnormalized): normal likelihood + flat prior\n    log_post &lt;- function(mu) {\n      sum(dnorm(y, mean = mu, sd = sigma, log = TRUE))\n    }\n\n    # Metropolis-Hastings\n    chain &lt;- numeric(n_iter)\n    chain[1] &lt;- 0  # start at 0\n    accepted &lt;- 0\n\n    for (t in 2:n_iter) {\n      proposal &lt;- rnorm(1, mean = chain[t - 1], sd = prop_sd)\n      log_ratio &lt;- log_post(proposal) - log_post(chain[t - 1])\n\n      if (log(runif(1)) &lt; log_ratio) {\n        chain[t] &lt;- proposal\n        accepted &lt;- accepted + 1\n      } else {\n        chain[t] &lt;- chain[t - 1]\n      }\n    }\n\n    accept_rate &lt;- accepted / (n_iter - 1)\n\n    # Analytic posterior for comparison (conjugate: flat prior + normal)\n    post_mean &lt;- mean(y)\n    post_sd   &lt;- sigma / sqrt(n_data)\n\n    list(chain = chain, accept_rate = accept_rate,\n         true_mu = true_mu, post_mean = post_mean, post_sd = post_sd,\n         n_iter = n_iter, prop_sd = prop_sd)\n  })\n\n  output$trace_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$chain, type = \"l\", col = \"#3498db80\", lwd = 0.5,\n         xlab = \"Iteration\", ylab = expression(mu),\n         main = \"Trace Plot\")\n\n    abline(h = d$true_mu, lty = 2, col = \"#e74c3c\", lwd = 2)\n    abline(h = d$post_mean, lty = 3, col = \"#27ae60\", lwd = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(expression(\"True \" * mu),\n                      \"Posterior mean (analytic)\"),\n           col = c(\"#e74c3c\", \"#27ae60\"),\n           lty = c(2, 3), lwd = 2)\n  })\n\n  output$hist_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Discard first 20% as burn-in\n    burnin &lt;- floor(d$n_iter * 0.2)\n    samples &lt;- d$chain[(burnin + 1):d$n_iter]\n\n    hist(samples, breaks = 40, freq = FALSE,\n         col = \"#3498db40\", border = \"#3498db\",\n         xlab = expression(mu), main = \"Posterior Distribution\",\n         xlim = range(c(samples, d$true_mu - 0.5, d$true_mu + 0.5)))\n\n    # Analytic posterior\n    x_seq &lt;- seq(min(samples) - 0.5, max(samples) + 0.5, length.out = 200)\n    lines(x_seq, dnorm(x_seq, d$post_mean, d$post_sd),\n          col = \"#27ae60\", lwd = 2.5)\n\n    abline(v = d$true_mu, lty = 2, col = \"#e74c3c\", lwd = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\"MCMC samples\", \"Analytic posterior\",\n                      expression(\"True \" * mu)),\n           col = c(\"#3498db\", \"#27ae60\", \"#e74c3c\"),\n           lwd = c(NA, 2.5, 2), lty = c(NA, 1, 2),\n           pch = c(15, NA, NA), pt.cex = c(1.5, NA, NA))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    burnin &lt;- floor(d$n_iter * 0.2)\n    samples &lt;- d$chain[(burnin + 1):d$n_iter]\n\n    rate_class &lt;- if (d$accept_rate &gt; 0.15 && d$accept_rate &lt; 0.5) \"good\" else \"bad\"\n    rate_note &lt;- if (d$accept_rate &lt; 0.15) {\n      \"Too low — proposal too wide\"\n    } else if (d$accept_rate &gt; 0.5) {\n      \"Too high — proposal too narrow\"\n    } else {\n      \"Good range (0.15-0.50)\"\n    }\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Acceptance rate:&lt;/b&gt; &lt;span class='\", rate_class, \"'&gt;\",\n        round(d$accept_rate * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;small&gt;\", rate_note, \"&lt;/small&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;MCMC posterior mean:&lt;/b&gt; \", round(mean(samples), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Analytic posterior mean:&lt;/b&gt; \", round(d$post_mean, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;True &mu;:&lt;/b&gt; \", d$true_mu, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Proposal width:&lt;/b&gt; \", d$prop_sd\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nProposal width = 0.5: a well-tuned chain. The trace plot shows good mixing (bouncing around the posterior), and the histogram matches the analytic posterior (green curve). Acceptance rate is in the sweet spot (20–40%).\nProposal width = 0.05: too narrow. The chain takes tiny steps — the trace plot shows slow, random-walk behavior. Acceptance rate is near 100% (almost every proposal is accepted because it’s barely different). The chain explores the posterior very slowly.\nProposal width = 5: too wide. Most proposals jump far from the current value and land in low-probability regions — they get rejected. The trace plot shows long flat stretches (the chain is stuck). Acceptance rate is very low.\nThe goldilocks principle: you want a proposal width that’s “just right” — large enough to explore, small enough to get accepted. The theoretical optimum for 1D is an acceptance rate around 44% (Roberts et al., 1997).",
    "crumbs": [
      "Beyond Conjugates",
      "MCMC"
    ]
  },
  {
    "objectID": "mcmc.html#burn-in-and-convergence",
    "href": "mcmc.html#burn-in-and-convergence",
    "title": "Markov Chain Monte Carlo",
    "section": "Burn-in and convergence",
    "text": "Burn-in and convergence\nA practical concern: the chain starts at an arbitrary value (\\(\\theta_0 = 0\\) above). The early samples reflect the starting point, not the posterior. You need to discard these initial samples — the “burn-in” period.\nHow do you know the chain has converged? Run multiple chains from different starting points. If they all end up exploring the same region, you have evidence of convergence. If they’re stuck in different places, the chains haven’t converged and you need more iterations.\n\nSimulation 2: Multiple chains and convergence\n#| standalone: true\n#| viewerHeight: 580\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_iter2\", \"Iterations:\",\n                  min = 200, max = 3000, value = 1000, step = 200),\n\n      sliderInput(\"burnin\", \"Burn-in (discard first %):\",\n                  min = 0, max = 50, value = 20, step = 5),\n\n      sliderInput(\"prop_sd2\", \"Proposal width:\",\n                  min = 0.1, max = 3, value = 0.5, step = 0.1),\n\n      actionButton(\"go2\", \"Run chains\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"multi_trace\", height = \"420px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat2 &lt;- reactive({\n    input$go2\n    n_iter  &lt;- input$n_iter2\n    burnin  &lt;- input$burnin / 100\n    prop_sd &lt;- input$prop_sd2\n    sigma   &lt;- 2\n    true_mu &lt;- 1.5\n\n    # Generate data once\n    y &lt;- rnorm(30, mean = true_mu, sd = sigma)\n\n    log_post &lt;- function(mu) {\n      sum(dnorm(y, mean = mu, sd = sigma, log = TRUE))\n    }\n\n    # Run 4 chains from different starting points\n    starts &lt;- c(-5, -2, 4, 7)\n    chain_cols &lt;- c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"#f39c12\")\n    chains &lt;- matrix(0, nrow = n_iter, ncol = 4)\n\n    for (ch in 1:4) {\n      chains[1, ch] &lt;- starts[ch]\n      for (t in 2:n_iter) {\n        proposal &lt;- rnorm(1, chains[t - 1, ch], prop_sd)\n        log_r &lt;- log_post(proposal) - log_post(chains[t - 1, ch])\n        if (log(runif(1)) &lt; log_r) {\n          chains[t, ch] &lt;- proposal\n        } else {\n          chains[t, ch] &lt;- chains[t - 1, ch]\n        }\n      }\n    }\n\n    # Posterior (analytic)\n    post_mean &lt;- mean(y)\n\n    list(chains = chains, starts = starts, chain_cols = chain_cols,\n         n_iter = n_iter, burnin = burnin, true_mu = true_mu,\n         post_mean = post_mean)\n  })\n\n  output$multi_trace &lt;- renderPlot({\n    d &lt;- dat2()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    burnin_line &lt;- floor(d$n_iter * d$burnin)\n\n    ylim &lt;- range(d$chains)\n    plot(NULL, xlim = c(1, d$n_iter), ylim = ylim,\n         xlab = \"Iteration\", ylab = expression(mu),\n         main = \"Four Chains from Different Starting Points\")\n\n    # Shade burn-in region\n    if (burnin_line &gt; 0) {\n      rect(0, ylim[1] - 1, burnin_line, ylim[2] + 1,\n           col = \"#f0f0f080\", border = NA)\n      abline(v = burnin_line, lty = 2, col = \"gray40\", lwd = 1.5)\n      text(burnin_line, ylim[2], \"burn-in\", pos = 2,\n           cex = 0.8, col = \"gray40\")\n    }\n\n    for (ch in 1:4) {\n      lines(d$chains[, ch], col = paste0(d$chain_cols[ch], \"90\"),\n            lwd = 0.8)\n    }\n\n    abline(h = d$true_mu, lty = 2, col = \"#2c3e50\", lwd = 2)\n    text(d$n_iter * 0.98, d$true_mu, expression(\"True \" * mu),\n         pos = 3, cex = 0.85, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.75,\n           legend = paste0(\"Chain \", 1:4, \" (start = \", d$starts, \")\"),\n           col = d$chain_cols, lwd = 2)\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- dat2()\n    burnin_n &lt;- floor(d$n_iter * d$burnin)\n\n    # Post-burnin means per chain\n    if (burnin_n &lt; d$n_iter) {\n      post_samples &lt;- d$chains[(burnin_n + 1):d$n_iter, ]\n      chain_means &lt;- round(colMeans(post_samples), 3)\n      spread &lt;- round(max(chain_means) - min(chain_means), 3)\n    } else {\n      chain_means &lt;- rep(NA, 4)\n      spread &lt;- NA\n    }\n\n    converged &lt;- !is.na(spread) && spread &lt; 0.3\n    conv_class &lt;- if (converged) \"good\" else \"bad\"\n    conv_label &lt;- if (converged) \"Chains agree\" else \"Chains disagree\"\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Post-burn-in means:&lt;/b&gt;&lt;br&gt;\",\n        paste0(\"Chain \", 1:4, \": \", chain_means, collapse = \"&lt;br&gt;\"), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Spread:&lt;/b&gt; &lt;span class='\", conv_class, \"'&gt;\",\n        spread, \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;span class='\", conv_class, \"'&gt;\", conv_label, \"&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nDefault settings (1000 iterations, burn-in = 20%): all four chains start at different values (-5, -2, 4, 7) but converge to the same region within ~100 iterations. After burn-in, all chain means agree. This is convergence.\nBurn-in = 0%: the early samples (from the starting points) contaminate the posterior. The chain means diverge because each chain’s average is pulled toward its start.\nProposal width = 0.1: very slow exploration. The chains take longer to converge — you can see them creeping slowly toward the true value. With only 1000 iterations, they might not fully converge. Increase iterations to fix this.\nProposal width = 3: the chains converge quickly but the trace plot shows many flat stretches (rejected proposals). The posterior is explored but inefficiently.",
    "crumbs": [
      "Beyond Conjugates",
      "MCMC"
    ]
  },
  {
    "objectID": "mcmc.html#the-key-message",
    "href": "mcmc.html#the-key-message",
    "title": "Markov Chain Monte Carlo",
    "section": "The key message",
    "text": "The key message\nMCMC lets you compute posteriors for any model — not just conjugate ones. Specify the likelihood and the prior, and the algorithm samples from the posterior. This is what makes Bayesian inference practical for real-world problems like hierarchical models, where closed-form posteriors don’t exist.\nModern tools like Stan, JAGS, and PyMC automate this — you specify the model and they handle the sampling. But understanding the basics (proposal tuning, burn-in, convergence checks) helps you diagnose problems when things go wrong.",
    "crumbs": [
      "Beyond Conjugates",
      "MCMC"
    ]
  },
  {
    "objectID": "mcmc.html#did-you-know",
    "href": "mcmc.html#did-you-know",
    "title": "Markov Chain Monte Carlo",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe Metropolis algorithm was developed at Los Alamos National Laboratory by Nicholas Metropolis, Arianna Rosenbluth, Marshall Rosenbluth, Augusta Teller, and Edward Teller (1953) — originally for simulating equations of state in statistical mechanics, not statistics. W.K. Hastings (1970) generalized it to asymmetric proposal distributions.\nMCMC was named one of the top 10 algorithms of the 20th century by Computing in Science & Engineering (2000). It’s used across physics, chemistry, biology, statistics, and machine learning.\nModern Bayesian computation has largely moved beyond basic Metropolis-Hastings to Hamiltonian Monte Carlo (HMC) and its adaptive variant NUTS (Hoffman & Gelman, 2014). HMC uses gradient information to make smarter proposals, dramatically improving efficiency in high-dimensional problems. This is what Stan uses under the hood.",
    "crumbs": [
      "Beyond Conjugates",
      "MCMC"
    ]
  },
  {
    "objectID": "bayes-vs-freq.html",
    "href": "bayes-vs-freq.html",
    "title": "Bayesian vs Frequentist",
    "section": "",
    "text": "Bayesian and frequentist statistics look at the same data but ask different questions:\n\n\n\n\n\n\n\n\n\nFrequentist\nBayesian\n\n\n\n\nParameters are…\nFixed but unknown\nRandom variables with distributions\n\n\nProbability means…\nLong-run frequency\nDegree of belief\n\n\nResult\nPoint estimate + confidence interval\nFull posterior distribution\n\n\n“There’s a 95% chance…”\n…that this procedure captures the true value\n…that the true value is in this interval\n\n\n\nThe frequentist says: “If I repeated this experiment forever, 95% of my CIs would contain the true value.” The Bayesian says: “Given what I’ve seen, I’m 95% sure the true value is in this range.”\nMost people actually think like Bayesians (“what’s the probability the parameter is between A and B?”) but compute like frequentists (p-values, CIs).\n\n\nThe simulation below runs the same experiment and shows both the frequentist confidence interval and the Bayesian credible interval. Watch how they differ — especially with small samples and informative priors.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_mu\", HTML(\"True &mu;:\"),\n                  min = -3, max = 3, value = 1, step = 0.5),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 2, max = 200, value = 10, step = 1),\n\n      sliderInput(\"prior_mu\", \"Bayesian prior mean:\",\n                  min = -3, max = 3, value = 0, step = 0.5),\n\n      sliderInput(\"prior_sd\", \"Prior SD:\",\n                  min = 0.5, max = 10, value = 2, step = 0.5),\n\n      actionButton(\"go\", \"New experiment\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"interval_plot\", height = \"420px\")),\n        column(6, plotOutput(\"repeat_plot\",   height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    true_mu  &lt;- input$true_mu\n    n        &lt;- input$n\n    prior_mu &lt;- input$prior_mu\n    prior_sd &lt;- input$prior_sd\n    sigma    &lt;- 2\n\n    y &lt;- rnorm(n, mean = true_mu, sd = sigma)\n    y_bar &lt;- mean(y)\n    se &lt;- sigma / sqrt(n)\n\n    # Frequentist 95% CI\n    freq_lo &lt;- y_bar - 1.96 * se\n    freq_hi &lt;- y_bar + 1.96 * se\n\n    # Bayesian posterior\n    prior_prec &lt;- 1 / prior_sd^2\n    data_prec  &lt;- n / sigma^2\n    post_prec  &lt;- prior_prec + data_prec\n    post_sd    &lt;- 1 / sqrt(post_prec)\n    post_mu    &lt;- (prior_prec * prior_mu + data_prec * y_bar) / post_prec\n\n    bayes_lo &lt;- qnorm(0.025, post_mu, post_sd)\n    bayes_hi &lt;- qnorm(0.975, post_mu, post_sd)\n\n    # Repeated experiments for right panel\n    k &lt;- 50\n    reps &lt;- t(replicate(k, {\n      yy &lt;- rnorm(n, mean = true_mu, sd = sigma)\n      yy_bar &lt;- mean(yy)\n      f_lo &lt;- yy_bar - 1.96 * se\n      f_hi &lt;- yy_bar + 1.96 * se\n\n      d_prec &lt;- n / sigma^2\n      p_prec &lt;- prior_prec + d_prec\n      p_sd &lt;- 1 / sqrt(p_prec)\n      p_mu &lt;- (prior_prec * prior_mu + d_prec * yy_bar) / p_prec\n      b_lo &lt;- qnorm(0.025, p_mu, p_sd)\n      b_hi &lt;- qnorm(0.975, p_mu, p_sd)\n\n      c(yy_bar, f_lo, f_hi, p_mu, b_lo, b_hi)\n    }))\n\n    list(true_mu = true_mu, y_bar = y_bar,\n         freq_lo = freq_lo, freq_hi = freq_hi,\n         post_mu = post_mu, post_sd = post_sd,\n         bayes_lo = bayes_lo, bayes_hi = bayes_hi,\n         reps = reps, prior_mu = prior_mu)\n  })\n\n  output$interval_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 8, 3, 1))\n    xlim &lt;- range(c(d$freq_lo, d$freq_hi, d$bayes_lo, d$bayes_hi, d$true_mu)) +\n            c(-0.5, 0.5)\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, 2.5),\n         yaxt = \"n\", ylab = \"\", xlab = expression(mu),\n         main = \"This Experiment\")\n    axis(2, at = 1:2, labels = c(\"Frequentist\\n95% CI\", \"Bayesian\\n95% CrI\"),\n         las = 1, cex.axis = 0.85)\n\n    # Frequentist\n    segments(d$freq_lo, 1, d$freq_hi, 1, lwd = 4, col = \"#e74c3c\")\n    points(d$y_bar, 1, pch = 19, cex = 1.5, col = \"#e74c3c\")\n\n    # Bayesian\n    segments(d$bayes_lo, 2, d$bayes_hi, 2, lwd = 4, col = \"#3498db\")\n    points(d$post_mu, 2, pch = 19, cex = 1.5, col = \"#3498db\")\n\n    # True value\n    abline(v = d$true_mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n    text(d$true_mu, 2.4, expression(\"True \" * mu), cex = 0.9, col = \"#2c3e50\")\n  })\n\n  output$repeat_plot &lt;- renderPlot({\n    d &lt;- dat()\n    k &lt;- nrow(d$reps)\n\n    par(mar = c(4.5, 4, 3, 1))\n\n    freq_covers &lt;- d$reps[, 2] &lt;= d$true_mu & d$reps[, 3] &gt;= d$true_mu\n    bayes_covers &lt;- d$reps[, 5] &lt;= d$true_mu & d$reps[, 6] &gt;= d$true_mu\n\n    xlim &lt;- range(d$reps[, 2:6], d$true_mu) + c(-0.5, 0.5)\n\n    plot(NULL, xlim = xlim, ylim = c(1, k),\n         xlab = expression(mu), ylab = \"Experiment #\",\n         main = paste0(k, \" repeated experiments\"))\n\n    for (i in seq_len(k)) {\n      # Frequentist (left-shifted slightly)\n      clr_f &lt;- if (freq_covers[i]) \"#e74c3c\" else \"#e74c3c40\"\n      segments(d$reps[i, 2], i - 0.15, d$reps[i, 3], i - 0.15,\n               lwd = 1.5, col = clr_f)\n\n      # Bayesian (right-shifted slightly)\n      clr_b &lt;- if (bayes_covers[i]) \"#3498db\" else \"#3498db40\"\n      segments(d$reps[i, 5], i + 0.15, d$reps[i, 6], i + 0.15,\n               lwd = 1.5, col = clr_b)\n    }\n\n    abline(v = d$true_mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\n             paste0(\"Freq CI (\", sum(freq_covers), \"/\", k, \" cover)\"),\n             paste0(\"Bayes CrI (\", sum(bayes_covers), \"/\", k, \" cover)\")\n           ),\n           col = c(\"#e74c3c\", \"#3498db\"), lwd = 3)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Frequentist:&lt;/b&gt;&lt;br&gt;\",\n        \"Estimate: \", round(d$y_bar, 3), \"&lt;br&gt;\",\n        \"95% CI: [\", round(d$freq_lo, 3), \", \", round(d$freq_hi, 3), \"]&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Bayesian:&lt;/b&gt;&lt;br&gt;\",\n        \"Posterior mean: \", round(d$post_mu, 3), \"&lt;br&gt;\",\n        \"95% CrI: [\", round(d$bayes_lo, 3), \", \", round(d$bayes_hi, 3), \"]&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;CrI is narrower because the prior adds information.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nn = 5, prior centered at 0, true mu = 1: the Bayesian CrI is narrower but pulled toward 0 (shrinkage). The frequentist CI is wider but centered on the data.\nn = 200: both intervals are nearly identical. With lots of data, the prior washes out and Bayesian = frequentist.\nSet a wrong prior (prior mean = -3, true mu = 2, n = 5): the Bayesian interval gets pulled toward -3. A bad prior hurts with small samples. Slide n up — the data corrects it.\nRight panel: the frequentist CI is designed so that ~95% of the red intervals cover the truth across repetitions. The Bayesian CrI coverage depends on how good the prior is.\n\n\n\n\n\n\n\n\n\n\n\nUse frequentist when…\nUse Bayesian when…\n\n\n\n\nYou want procedure guarantees (coverage)\nYou want direct probability statements\n\n\nYou have no prior information\nYou have real prior knowledge\n\n\nRegulatory/peer review expects it\nSmall samples, need to borrow strength\n\n\nSimple problems\nComplex hierarchical models\n\n\n\nIn practice, most applied researchers use frequentist methods but interpret them like Bayesians. Understanding both helps you know what your numbers actually mean.",
    "crumbs": [
      "Bayesian vs Frequentist"
    ]
  },
  {
    "objectID": "bayes-vs-freq.html#same-data-different-questions",
    "href": "bayes-vs-freq.html#same-data-different-questions",
    "title": "Bayesian vs Frequentist",
    "section": "",
    "text": "Bayesian and frequentist statistics look at the same data but ask different questions:\n\n\n\n\n\n\n\n\n\nFrequentist\nBayesian\n\n\n\n\nParameters are…\nFixed but unknown\nRandom variables with distributions\n\n\nProbability means…\nLong-run frequency\nDegree of belief\n\n\nResult\nPoint estimate + confidence interval\nFull posterior distribution\n\n\n“There’s a 95% chance…”\n…that this procedure captures the true value\n…that the true value is in this interval\n\n\n\nThe frequentist says: “If I repeated this experiment forever, 95% of my CIs would contain the true value.” The Bayesian says: “Given what I’ve seen, I’m 95% sure the true value is in this range.”\nMost people actually think like Bayesians (“what’s the probability the parameter is between A and B?”) but compute like frequentists (p-values, CIs).\n\n\nThe simulation below runs the same experiment and shows both the frequentist confidence interval and the Bayesian credible interval. Watch how they differ — especially with small samples and informative priors.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_mu\", HTML(\"True &mu;:\"),\n                  min = -3, max = 3, value = 1, step = 0.5),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 2, max = 200, value = 10, step = 1),\n\n      sliderInput(\"prior_mu\", \"Bayesian prior mean:\",\n                  min = -3, max = 3, value = 0, step = 0.5),\n\n      sliderInput(\"prior_sd\", \"Prior SD:\",\n                  min = 0.5, max = 10, value = 2, step = 0.5),\n\n      actionButton(\"go\", \"New experiment\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"interval_plot\", height = \"420px\")),\n        column(6, plotOutput(\"repeat_plot\",   height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    true_mu  &lt;- input$true_mu\n    n        &lt;- input$n\n    prior_mu &lt;- input$prior_mu\n    prior_sd &lt;- input$prior_sd\n    sigma    &lt;- 2\n\n    y &lt;- rnorm(n, mean = true_mu, sd = sigma)\n    y_bar &lt;- mean(y)\n    se &lt;- sigma / sqrt(n)\n\n    # Frequentist 95% CI\n    freq_lo &lt;- y_bar - 1.96 * se\n    freq_hi &lt;- y_bar + 1.96 * se\n\n    # Bayesian posterior\n    prior_prec &lt;- 1 / prior_sd^2\n    data_prec  &lt;- n / sigma^2\n    post_prec  &lt;- prior_prec + data_prec\n    post_sd    &lt;- 1 / sqrt(post_prec)\n    post_mu    &lt;- (prior_prec * prior_mu + data_prec * y_bar) / post_prec\n\n    bayes_lo &lt;- qnorm(0.025, post_mu, post_sd)\n    bayes_hi &lt;- qnorm(0.975, post_mu, post_sd)\n\n    # Repeated experiments for right panel\n    k &lt;- 50\n    reps &lt;- t(replicate(k, {\n      yy &lt;- rnorm(n, mean = true_mu, sd = sigma)\n      yy_bar &lt;- mean(yy)\n      f_lo &lt;- yy_bar - 1.96 * se\n      f_hi &lt;- yy_bar + 1.96 * se\n\n      d_prec &lt;- n / sigma^2\n      p_prec &lt;- prior_prec + d_prec\n      p_sd &lt;- 1 / sqrt(p_prec)\n      p_mu &lt;- (prior_prec * prior_mu + d_prec * yy_bar) / p_prec\n      b_lo &lt;- qnorm(0.025, p_mu, p_sd)\n      b_hi &lt;- qnorm(0.975, p_mu, p_sd)\n\n      c(yy_bar, f_lo, f_hi, p_mu, b_lo, b_hi)\n    }))\n\n    list(true_mu = true_mu, y_bar = y_bar,\n         freq_lo = freq_lo, freq_hi = freq_hi,\n         post_mu = post_mu, post_sd = post_sd,\n         bayes_lo = bayes_lo, bayes_hi = bayes_hi,\n         reps = reps, prior_mu = prior_mu)\n  })\n\n  output$interval_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 8, 3, 1))\n    xlim &lt;- range(c(d$freq_lo, d$freq_hi, d$bayes_lo, d$bayes_hi, d$true_mu)) +\n            c(-0.5, 0.5)\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, 2.5),\n         yaxt = \"n\", ylab = \"\", xlab = expression(mu),\n         main = \"This Experiment\")\n    axis(2, at = 1:2, labels = c(\"Frequentist\\n95% CI\", \"Bayesian\\n95% CrI\"),\n         las = 1, cex.axis = 0.85)\n\n    # Frequentist\n    segments(d$freq_lo, 1, d$freq_hi, 1, lwd = 4, col = \"#e74c3c\")\n    points(d$y_bar, 1, pch = 19, cex = 1.5, col = \"#e74c3c\")\n\n    # Bayesian\n    segments(d$bayes_lo, 2, d$bayes_hi, 2, lwd = 4, col = \"#3498db\")\n    points(d$post_mu, 2, pch = 19, cex = 1.5, col = \"#3498db\")\n\n    # True value\n    abline(v = d$true_mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n    text(d$true_mu, 2.4, expression(\"True \" * mu), cex = 0.9, col = \"#2c3e50\")\n  })\n\n  output$repeat_plot &lt;- renderPlot({\n    d &lt;- dat()\n    k &lt;- nrow(d$reps)\n\n    par(mar = c(4.5, 4, 3, 1))\n\n    freq_covers &lt;- d$reps[, 2] &lt;= d$true_mu & d$reps[, 3] &gt;= d$true_mu\n    bayes_covers &lt;- d$reps[, 5] &lt;= d$true_mu & d$reps[, 6] &gt;= d$true_mu\n\n    xlim &lt;- range(d$reps[, 2:6], d$true_mu) + c(-0.5, 0.5)\n\n    plot(NULL, xlim = xlim, ylim = c(1, k),\n         xlab = expression(mu), ylab = \"Experiment #\",\n         main = paste0(k, \" repeated experiments\"))\n\n    for (i in seq_len(k)) {\n      # Frequentist (left-shifted slightly)\n      clr_f &lt;- if (freq_covers[i]) \"#e74c3c\" else \"#e74c3c40\"\n      segments(d$reps[i, 2], i - 0.15, d$reps[i, 3], i - 0.15,\n               lwd = 1.5, col = clr_f)\n\n      # Bayesian (right-shifted slightly)\n      clr_b &lt;- if (bayes_covers[i]) \"#3498db\" else \"#3498db40\"\n      segments(d$reps[i, 5], i + 0.15, d$reps[i, 6], i + 0.15,\n               lwd = 1.5, col = clr_b)\n    }\n\n    abline(v = d$true_mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\n             paste0(\"Freq CI (\", sum(freq_covers), \"/\", k, \" cover)\"),\n             paste0(\"Bayes CrI (\", sum(bayes_covers), \"/\", k, \" cover)\")\n           ),\n           col = c(\"#e74c3c\", \"#3498db\"), lwd = 3)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Frequentist:&lt;/b&gt;&lt;br&gt;\",\n        \"Estimate: \", round(d$y_bar, 3), \"&lt;br&gt;\",\n        \"95% CI: [\", round(d$freq_lo, 3), \", \", round(d$freq_hi, 3), \"]&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Bayesian:&lt;/b&gt;&lt;br&gt;\",\n        \"Posterior mean: \", round(d$post_mu, 3), \"&lt;br&gt;\",\n        \"95% CrI: [\", round(d$bayes_lo, 3), \", \", round(d$bayes_hi, 3), \"]&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;CrI is narrower because the prior adds information.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nn = 5, prior centered at 0, true mu = 1: the Bayesian CrI is narrower but pulled toward 0 (shrinkage). The frequentist CI is wider but centered on the data.\nn = 200: both intervals are nearly identical. With lots of data, the prior washes out and Bayesian = frequentist.\nSet a wrong prior (prior mean = -3, true mu = 2, n = 5): the Bayesian interval gets pulled toward -3. A bad prior hurts with small samples. Slide n up — the data corrects it.\nRight panel: the frequentist CI is designed so that ~95% of the red intervals cover the truth across repetitions. The Bayesian CrI coverage depends on how good the prior is.\n\n\n\n\n\n\n\n\n\n\n\nUse frequentist when…\nUse Bayesian when…\n\n\n\n\nYou want procedure guarantees (coverage)\nYou want direct probability statements\n\n\nYou have no prior information\nYou have real prior knowledge\n\n\nRegulatory/peer review expects it\nSmall samples, need to borrow strength\n\n\nSimple problems\nComplex hierarchical models\n\n\n\nIn practice, most applied researchers use frequentist methods but interpret them like Bayesians. Understanding both helps you know what your numbers actually mean.",
    "crumbs": [
      "Bayesian vs Frequentist"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Thinking",
    "section": "",
    "text": "Bayesian inference — updating beliefs with data. Start with a prior, observe data, get a posterior. Just the core logic, with simulations.\nBuilds on: Statistical Inference",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "Bayesian Thinking",
    "section": "Topics",
    "text": "Topics\n\nBayes’ Theorem — The engine behind everything: how evidence updates beliefs\nPriors & Posteriors — Watch your prior get overwhelmed by data\nShrinkage — Why pulling estimates toward the mean beats taking them at face value\nBayesian vs Frequentist — Same question, two philosophies, different answers",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#beyond-conjugates",
    "href": "index.html#beyond-conjugates",
    "title": "Bayesian Thinking",
    "section": "Beyond Conjugates",
    "text": "Beyond Conjugates\n\nMCMC — Sampling from posteriors when closed-form solutions don’t exist\nHierarchical Models — Partial pooling: the killer app of Bayesian inference",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#how-does-bayesian-inference-relate-to-causal-inference",
    "href": "index.html#how-does-bayesian-inference-relate-to-causal-inference",
    "title": "Bayesian Thinking",
    "section": "How does Bayesian inference relate to causal inference?",
    "text": "How does Bayesian inference relate to causal inference?\nThey’re different questions:\n\n\n\n\n\n\n\n\n\nBayesian inference\nCausal inference\n\n\n\n\nQuestion\nWhat should I believe given the data?\nDoes X cause Y?\n\n\nFramework\nPrior + likelihood = posterior\nPotential outcomes, DAGs\n\n\nKey concept\nUpdating beliefs\nCounterfactuals\n\n\n\nYou can combine them — Bayesian causal inference uses Bayesian methods to estimate causal effects — but each stands on its own.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "shrinkage.html",
    "href": "shrinkage.html",
    "title": "Bayesian Shrinkage",
    "section": "",
    "text": "Imagine you’re a baseball scout. It’s early in the season and you need to estimate the true batting average for 50 players, each with only 20 at-bats.\nOne player went 10-for-20 (.500). Another went 1-for-20 (.050). Are those their true abilities? Probably not — with only 20 at-bats, there’s a ton of noise. The .500 hitter probably got lucky. The .050 hitter probably got unlucky.\nShrinkage says: don’t take the raw numbers at face value. Pull (“shrink”) every estimate toward the overall average. The more uncertain you are about an individual estimate, the more you pull.\n\\[\\hat{\\theta}_i^{shrunk} = w_i \\cdot \\bar{\\theta}_{overall} + (1 - w_i) \\cdot \\hat{\\theta}_i^{raw}\\]\nThis is the core of empirical Bayes and James-Stein estimation. It sounds like you’re adding bias — and you are — but you’re reducing variance by more than enough to compensate. The result: better predictions overall.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_players\", \"Number of players:\",\n                  min = 10, max = 100, value = 40, step = 5),\n\n      sliderInput(\"at_bats\", \"At-bats per player:\",\n                  min = 5, max = 200, value = 20, step = 5),\n\n      sliderInput(\"true_spread\", \"True talent spread (SD):\",\n                  min = 0.01, max = 0.08, value = 0.03, step = 0.005),\n\n      actionButton(\"go\", \"New season\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"shrinkage_plot\", height = \"420px\")),\n        column(6, plotOutput(\"mse_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    k   &lt;- input$n_players\n    n   &lt;- input$at_bats\n    tau &lt;- input$true_spread\n\n    # True batting averages (centered around .260)\n    true_avg &lt;- rnorm(k, mean = 0.260, sd = tau)\n    true_avg &lt;- pmin(pmax(true_avg, 0.100), 0.400)\n\n    # Observed: hits in n at-bats\n    hits &lt;- rbinom(k, size = n, prob = true_avg)\n    obs_avg &lt;- hits / n\n\n    # Grand mean\n    grand_mean &lt;- mean(obs_avg)\n\n    # Empirical Bayes shrinkage\n    # Estimate prior variance from data\n    obs_var &lt;- var(obs_avg)\n    sampling_var &lt;- mean(obs_avg * (1 - obs_avg) / n)\n    prior_var &lt;- max(obs_var - sampling_var, 0.0001)\n\n    # Shrinkage weight (toward grand mean)\n    w &lt;- sampling_var / (sampling_var + prior_var)\n    shrunk_avg &lt;- w * grand_mean + (1 - w) * obs_avg\n\n    # MSE\n    mse_raw   &lt;- mean((obs_avg - true_avg)^2)\n    mse_shrunk &lt;- mean((shrunk_avg - true_avg)^2)\n\n    # Future performance (another n at-bats from true ability)\n    future_hits &lt;- rbinom(k, size = n, prob = true_avg)\n    future_avg  &lt;- future_hits / n\n\n    pred_err_raw   &lt;- mean((obs_avg - future_avg)^2)\n    pred_err_shrunk &lt;- mean((shrunk_avg - future_avg)^2)\n\n    list(true_avg = true_avg, obs_avg = obs_avg, shrunk_avg = shrunk_avg,\n         grand_mean = grand_mean, w = w,\n         mse_raw = mse_raw, mse_shrunk = mse_shrunk,\n         pred_err_raw = pred_err_raw, pred_err_shrunk = pred_err_shrunk,\n         k = k, n = n)\n  })\n\n  output$shrinkage_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ord &lt;- order(d$obs_avg)\n\n    plot(d$obs_avg[ord], seq_along(ord), pch = 16, col = \"#e74c3c\",\n         xlab = \"Batting average\", ylab = \"Player (sorted by raw avg)\",\n         main = \"Shrinkage in Action\",\n         xlim = range(c(d$obs_avg, d$shrunk_avg, d$true_avg)))\n\n    points(d$shrunk_avg[ord], seq_along(ord), pch = 17, col = \"#3498db\")\n    points(d$true_avg[ord], seq_along(ord), pch = 4, col = \"#27ae60\", cex = 0.8)\n\n    # Draw arrows from raw to shrunk\n    arrows(d$obs_avg[ord], seq_along(ord),\n           d$shrunk_avg[ord], seq_along(ord),\n           length = 0.05, col = \"#bdc3c780\", lwd = 1)\n\n    # Grand mean\n    abline(v = d$grand_mean, lty = 2, col = \"gray50\", lwd = 1.5)\n\n    legend(\"bottomright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Raw average\", \"Shrunk estimate\",\n                      \"True ability\", \"Grand mean\"),\n           col = c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"gray50\"),\n           pch = c(16, 17, 4, NA),\n           lty = c(NA, NA, NA, 2), lwd = c(NA, NA, NA, 1.5))\n  })\n\n  output$mse_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 6, 3, 1))\n\n    vals &lt;- c(d$mse_raw, d$mse_shrunk, d$pred_err_raw, d$pred_err_shrunk)\n    cols &lt;- c(\"#e74c3c\", \"#3498db\", \"#e74c3c80\", \"#3498db80\")\n    labels &lt;- c(\"Raw\\nvs truth\", \"Shrunk\\nvs truth\",\n                \"Raw\\nvs future\", \"Shrunk\\nvs future\")\n\n    bp &lt;- barplot(vals, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.8,\n                  main = \"Mean Squared Error\",\n                  ylab = \"MSE\", las = 1)\n    text(bp, vals + max(vals) * 0.03, round(vals, 5), cex = 0.8)\n\n    pct1 &lt;- round((1 - d$mse_shrunk / d$mse_raw) * 100, 0)\n    pct2 &lt;- round((1 - d$pred_err_shrunk / d$pred_err_raw) * 100, 0)\n\n    mtext(paste0(\"Shrinkage reduces estimation error by ~\", pct1, \"%\"),\n          side = 1, line = 3.5, cex = 0.85, col = \"#2c3e50\")\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    pct_est &lt;- round((1 - d$mse_shrunk / d$mse_raw) * 100, 1)\n    pct_pred &lt;- round((1 - d$pred_err_shrunk / d$pred_err_raw) * 100, 1)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Shrinkage weight:&lt;/b&gt; \", round(d$w * 100, 1),\n        \"% toward grand mean&lt;br&gt;\",\n        \"&lt;b&gt;Grand mean:&lt;/b&gt; \", round(d$grand_mean, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;MSE (raw):&lt;/b&gt; \", round(d$mse_raw, 5), \"&lt;br&gt;\",\n        \"&lt;b&gt;MSE (shrunk):&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$mse_shrunk, 5), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Improvement:&lt;/b&gt; &lt;span class='good'&gt;\", pct_est, \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Prediction error (raw):&lt;/b&gt; \", round(d$pred_err_raw, 5), \"&lt;br&gt;\",\n        \"&lt;b&gt;Prediction error (shrunk):&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$pred_err_shrunk, 5), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Improvement:&lt;/b&gt; &lt;span class='good'&gt;\", pct_pred, \"%&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nAt-bats = 5: extreme noise. Raw averages are all over the place (some players show .000 or .600). Shrinkage pulls them heavily toward the mean — and the green crosses (true ability) confirm the shrunk estimates are closer.\nAt-bats = 200: lots of data per player. Shrinkage is minimal because the raw averages are already precise. With enough data, shrinkage vanishes.\nLook at the MSE bars: shrinkage almost always wins, especially with small samples. It also predicts future performance better.\nTrue talent spread = 0.01 (everyone is similar): shrinkage is aggressive because individual differences are small relative to noise.\nTrue talent spread = 0.08 (wide range of talent): shrinkage is lighter because individual differences are real, not noise.",
    "crumbs": [
      "Shrinkage"
    ]
  },
  {
    "objectID": "shrinkage.html#what-is-shrinkage",
    "href": "shrinkage.html#what-is-shrinkage",
    "title": "Bayesian Shrinkage",
    "section": "",
    "text": "Imagine you’re a baseball scout. It’s early in the season and you need to estimate the true batting average for 50 players, each with only 20 at-bats.\nOne player went 10-for-20 (.500). Another went 1-for-20 (.050). Are those their true abilities? Probably not — with only 20 at-bats, there’s a ton of noise. The .500 hitter probably got lucky. The .050 hitter probably got unlucky.\nShrinkage says: don’t take the raw numbers at face value. Pull (“shrink”) every estimate toward the overall average. The more uncertain you are about an individual estimate, the more you pull.\n\\[\\hat{\\theta}_i^{shrunk} = w_i \\cdot \\bar{\\theta}_{overall} + (1 - w_i) \\cdot \\hat{\\theta}_i^{raw}\\]\nThis is the core of empirical Bayes and James-Stein estimation. It sounds like you’re adding bias — and you are — but you’re reducing variance by more than enough to compensate. The result: better predictions overall.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_players\", \"Number of players:\",\n                  min = 10, max = 100, value = 40, step = 5),\n\n      sliderInput(\"at_bats\", \"At-bats per player:\",\n                  min = 5, max = 200, value = 20, step = 5),\n\n      sliderInput(\"true_spread\", \"True talent spread (SD):\",\n                  min = 0.01, max = 0.08, value = 0.03, step = 0.005),\n\n      actionButton(\"go\", \"New season\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"shrinkage_plot\", height = \"420px\")),\n        column(6, plotOutput(\"mse_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    k   &lt;- input$n_players\n    n   &lt;- input$at_bats\n    tau &lt;- input$true_spread\n\n    # True batting averages (centered around .260)\n    true_avg &lt;- rnorm(k, mean = 0.260, sd = tau)\n    true_avg &lt;- pmin(pmax(true_avg, 0.100), 0.400)\n\n    # Observed: hits in n at-bats\n    hits &lt;- rbinom(k, size = n, prob = true_avg)\n    obs_avg &lt;- hits / n\n\n    # Grand mean\n    grand_mean &lt;- mean(obs_avg)\n\n    # Empirical Bayes shrinkage\n    # Estimate prior variance from data\n    obs_var &lt;- var(obs_avg)\n    sampling_var &lt;- mean(obs_avg * (1 - obs_avg) / n)\n    prior_var &lt;- max(obs_var - sampling_var, 0.0001)\n\n    # Shrinkage weight (toward grand mean)\n    w &lt;- sampling_var / (sampling_var + prior_var)\n    shrunk_avg &lt;- w * grand_mean + (1 - w) * obs_avg\n\n    # MSE\n    mse_raw   &lt;- mean((obs_avg - true_avg)^2)\n    mse_shrunk &lt;- mean((shrunk_avg - true_avg)^2)\n\n    # Future performance (another n at-bats from true ability)\n    future_hits &lt;- rbinom(k, size = n, prob = true_avg)\n    future_avg  &lt;- future_hits / n\n\n    pred_err_raw   &lt;- mean((obs_avg - future_avg)^2)\n    pred_err_shrunk &lt;- mean((shrunk_avg - future_avg)^2)\n\n    list(true_avg = true_avg, obs_avg = obs_avg, shrunk_avg = shrunk_avg,\n         grand_mean = grand_mean, w = w,\n         mse_raw = mse_raw, mse_shrunk = mse_shrunk,\n         pred_err_raw = pred_err_raw, pred_err_shrunk = pred_err_shrunk,\n         k = k, n = n)\n  })\n\n  output$shrinkage_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ord &lt;- order(d$obs_avg)\n\n    plot(d$obs_avg[ord], seq_along(ord), pch = 16, col = \"#e74c3c\",\n         xlab = \"Batting average\", ylab = \"Player (sorted by raw avg)\",\n         main = \"Shrinkage in Action\",\n         xlim = range(c(d$obs_avg, d$shrunk_avg, d$true_avg)))\n\n    points(d$shrunk_avg[ord], seq_along(ord), pch = 17, col = \"#3498db\")\n    points(d$true_avg[ord], seq_along(ord), pch = 4, col = \"#27ae60\", cex = 0.8)\n\n    # Draw arrows from raw to shrunk\n    arrows(d$obs_avg[ord], seq_along(ord),\n           d$shrunk_avg[ord], seq_along(ord),\n           length = 0.05, col = \"#bdc3c780\", lwd = 1)\n\n    # Grand mean\n    abline(v = d$grand_mean, lty = 2, col = \"gray50\", lwd = 1.5)\n\n    legend(\"bottomright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Raw average\", \"Shrunk estimate\",\n                      \"True ability\", \"Grand mean\"),\n           col = c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"gray50\"),\n           pch = c(16, 17, 4, NA),\n           lty = c(NA, NA, NA, 2), lwd = c(NA, NA, NA, 1.5))\n  })\n\n  output$mse_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 6, 3, 1))\n\n    vals &lt;- c(d$mse_raw, d$mse_shrunk, d$pred_err_raw, d$pred_err_shrunk)\n    cols &lt;- c(\"#e74c3c\", \"#3498db\", \"#e74c3c80\", \"#3498db80\")\n    labels &lt;- c(\"Raw\\nvs truth\", \"Shrunk\\nvs truth\",\n                \"Raw\\nvs future\", \"Shrunk\\nvs future\")\n\n    bp &lt;- barplot(vals, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.8,\n                  main = \"Mean Squared Error\",\n                  ylab = \"MSE\", las = 1)\n    text(bp, vals + max(vals) * 0.03, round(vals, 5), cex = 0.8)\n\n    pct1 &lt;- round((1 - d$mse_shrunk / d$mse_raw) * 100, 0)\n    pct2 &lt;- round((1 - d$pred_err_shrunk / d$pred_err_raw) * 100, 0)\n\n    mtext(paste0(\"Shrinkage reduces estimation error by ~\", pct1, \"%\"),\n          side = 1, line = 3.5, cex = 0.85, col = \"#2c3e50\")\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    pct_est &lt;- round((1 - d$mse_shrunk / d$mse_raw) * 100, 1)\n    pct_pred &lt;- round((1 - d$pred_err_shrunk / d$pred_err_raw) * 100, 1)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Shrinkage weight:&lt;/b&gt; \", round(d$w * 100, 1),\n        \"% toward grand mean&lt;br&gt;\",\n        \"&lt;b&gt;Grand mean:&lt;/b&gt; \", round(d$grand_mean, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;MSE (raw):&lt;/b&gt; \", round(d$mse_raw, 5), \"&lt;br&gt;\",\n        \"&lt;b&gt;MSE (shrunk):&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$mse_shrunk, 5), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Improvement:&lt;/b&gt; &lt;span class='good'&gt;\", pct_est, \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Prediction error (raw):&lt;/b&gt; \", round(d$pred_err_raw, 5), \"&lt;br&gt;\",\n        \"&lt;b&gt;Prediction error (shrunk):&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$pred_err_shrunk, 5), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Improvement:&lt;/b&gt; &lt;span class='good'&gt;\", pct_pred, \"%&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nAt-bats = 5: extreme noise. Raw averages are all over the place (some players show .000 or .600). Shrinkage pulls them heavily toward the mean — and the green crosses (true ability) confirm the shrunk estimates are closer.\nAt-bats = 200: lots of data per player. Shrinkage is minimal because the raw averages are already precise. With enough data, shrinkage vanishes.\nLook at the MSE bars: shrinkage almost always wins, especially with small samples. It also predicts future performance better.\nTrue talent spread = 0.01 (everyone is similar): shrinkage is aggressive because individual differences are small relative to noise.\nTrue talent spread = 0.08 (wide range of talent): shrinkage is lighter because individual differences are real, not noise.",
    "crumbs": [
      "Shrinkage"
    ]
  },
  {
    "objectID": "shrinkage.html#why-does-this-work",
    "href": "shrinkage.html#why-does-this-work",
    "title": "Bayesian Shrinkage",
    "section": "Why does this work?",
    "text": "Why does this work?\nIt seems wrong to move estimates away from the data. But consider what happens without shrinkage:\n\nPlayers who got lucky are overestimated\nPlayers who got unlucky are underestimated\nThese errors don’t cancel — they inflate the overall MSE\n\nShrinkage dampens both overestimates and underestimates simultaneously. The small bias it introduces (pulling everyone toward the mean) is more than offset by the massive reduction in variance. This is the bias-variance tradeoff in action.\n\nWhere you see this in practice\n\n\n\nMethod\nWhat gets shrunk\n\n\n\n\nRidge regression\nCoefficients toward zero\n\n\nLASSO\nCoefficients toward zero (with selection)\n\n\nRandom effects models\nGroup means toward grand mean\n\n\nEmpirical Bayes\nIndividual estimates toward overall mean\n\n\nBayesian priors\nPosteriors toward prior mean\n\n\n\nThey all share the same logic: when you have many noisy estimates, borrowing strength across them improves every single one.",
    "crumbs": [
      "Shrinkage"
    ]
  },
  {
    "objectID": "mcmc.html#what-is-mcmc",
    "href": "mcmc.html#what-is-mcmc",
    "title": "Markov Chain Monte Carlo",
    "section": "",
    "text": "The name breaks down into two parts:\n\nMonte Carlo: using random sampling to approximate something you can’t compute exactly. Instead of solving an integral analytically, you draw random samples and use their average as an approximation. (Named after the Monte Carlo casino — it’s fundamentally about randomness.)\nMarkov Chain: a sequence of random values where each value depends only on the previous one — not on the full history. The “chain” is a random walk through parameter space, where each step proposes a new value based on where you currently are.\n\nPut them together: MCMC constructs a Markov chain whose long-run distribution equals the posterior. Run it long enough, and the samples you collect are (approximately) draws from \\(p(\\theta \\mid y)\\) — even though you never computed that distribution directly.",
    "crumbs": [
      "Beyond Conjugates",
      "MCMC"
    ]
  },
  {
    "objectID": "mcmc.html#why-do-we-need-it",
    "href": "mcmc.html#why-do-we-need-it",
    "title": "Markov Chain Monte Carlo",
    "section": "Why do we need it?",
    "text": "Why do we need it?\nOn the Priors & Posteriors page, we used conjugate priors — special prior-likelihood pairs where the posterior has a closed-form solution. That’s elegant but limiting. Most real models don’t have conjugate posteriors:\n\nLogistic regression with a prior on coefficients\nHierarchical models with multiple levels of parameters\nAny model where the posterior \\(p(\\theta \\mid y) \\propto p(y \\mid \\theta) \\, p(\\theta)\\) doesn’t simplify to a known distribution\n\nFor these models, we can’t write down the posterior analytically. We need to sample from it numerically. That’s what MCMC does.",
    "crumbs": [
      "Beyond Conjugates",
      "MCMC"
    ]
  }
]