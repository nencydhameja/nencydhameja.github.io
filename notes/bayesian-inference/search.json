[
  {
    "objectID": "bayes-theorem.html",
    "href": "bayes-theorem.html",
    "title": "Bayes’ Theorem",
    "section": "",
    "text": "\\[P(H \\mid D) = \\frac{P(D \\mid H) \\cdot P(H)}{P(D)}\\]\nIn words:\n\\[\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Evidence}}\\]\nThat looks abstract. Let’s make it concrete.",
    "crumbs": [
      "Bayes' Theorem"
    ]
  },
  {
    "objectID": "bayes-theorem.html#the-formula",
    "href": "bayes-theorem.html#the-formula",
    "title": "Bayes’ Theorem",
    "section": "",
    "text": "\\[P(H \\mid D) = \\frac{P(D \\mid H) \\cdot P(H)}{P(D)}\\]\nIn words:\n\\[\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Evidence}}\\]\nThat looks abstract. Let’s make it concrete.",
    "crumbs": [
      "Bayes' Theorem"
    ]
  },
  {
    "objectID": "bayes-theorem.html#the-medical-test-example",
    "href": "bayes-theorem.html#the-medical-test-example",
    "title": "Bayes’ Theorem",
    "section": "The medical test example",
    "text": "The medical test example\nImagine a disease that affects 1 in 1,000 people. A test for it is 99% accurate — if you have the disease it says positive 99% of the time, and if you don’t have it, it says negative 99% of the time.\nYou test positive. What’s the probability you actually have the disease?\nMost people say 99%. The real answer is about 9%. This is not a trick — it’s Bayes’ theorem. The disease is so rare that even a good test produces more false positives than true positives.\nThe simulator below lets you adjust the base rate and test accuracy and watch how the posterior probability changes.\n#| standalone: true\n#| viewerHeight: 580\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .result-box {\n      background: #f0f4f8; border-radius: 6px; padding: 16px;\n      margin-top: 14px; font-size: 15px; line-height: 2;\n      text-align: center;\n    }\n    .result-box .big {\n      font-size: 32px; color: #e74c3c; font-weight: bold;\n    }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"prev\", \"Base rate (prevalence):\",\n                  min = 0.001, max = 0.20, value = 0.001, step = 0.001),\n\n      sliderInput(\"sens\", \"Sensitivity (true positive rate):\",\n                  min = 0.50, max = 1.00, value = 0.99, step = 0.01),\n\n      sliderInput(\"spec\", \"Specificity (true negative rate):\",\n                  min = 0.50, max = 1.00, value = 0.99, step = 0.01),\n\n      uiOutput(\"result_box\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"tree_plot\", height = \"420px\")),\n        column(6, plotOutput(\"icon_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  vals &lt;- reactive({\n    prev &lt;- input$prev\n    sens &lt;- input$sens\n    spec &lt;- input$spec\n\n    # Out of 10,000 people\n    N &lt;- 10000\n    sick &lt;- round(N * prev)\n    healthy &lt;- N - sick\n\n    true_pos  &lt;- round(sick * sens)\n    false_neg &lt;- sick - true_pos\n    false_pos &lt;- round(healthy * (1 - spec))\n    true_neg  &lt;- healthy - false_pos\n\n    total_pos &lt;- true_pos + false_pos\n    ppv &lt;- if (total_pos &gt; 0) true_pos / total_pos else 0\n\n    list(N = N, sick = sick, healthy = healthy,\n         true_pos = true_pos, false_neg = false_neg,\n         false_pos = false_pos, true_neg = true_neg,\n         total_pos = total_pos, ppv = ppv)\n  })\n\n  output$tree_plot &lt;- renderPlot({\n    v &lt;- vals()\n    par(mar = c(1, 1, 3, 1))\n\n    plot(NULL, xlim = c(0, 10), ylim = c(0, 10), axes = FALSE,\n         xlab = \"\", ylab = \"\", main = \"What happens to 10,000 people?\")\n\n    # Population\n    text(5, 9.5, paste0(\"Population: \", v$N), cex = 1.2, font = 2)\n\n    # Sick vs Healthy\n    text(2.5, 7.5, paste0(\"Sick: \", v$sick), cex = 1.1, col = \"#e74c3c\")\n    text(7.5, 7.5, paste0(\"Healthy: \", v$healthy), cex = 1.1, col = \"#3498db\")\n    segments(5, 9, 2.5, 8, lwd = 2)\n    segments(5, 9, 7.5, 8, lwd = 2)\n\n    # Test results for sick\n    text(1.2, 5.2, paste0(\"Test +\\n\", v$true_pos), cex = 1, col = \"#27ae60\", font = 2)\n    text(3.8, 5.2, paste0(\"Test -\\n\", v$false_neg), cex = 1, col = \"#7f8c8d\")\n    segments(2.5, 7, 1.2, 5.8, lwd = 1.5)\n    segments(2.5, 7, 3.8, 5.8, lwd = 1.5)\n\n    # Test results for healthy\n    text(6.2, 5.2, paste0(\"Test +\\n\", v$false_pos), cex = 1, col = \"#e74c3c\", font = 2)\n    text(8.8, 5.2, paste0(\"Test -\\n\", v$true_neg), cex = 1, col = \"#7f8c8d\")\n    segments(7.5, 7, 6.2, 5.8, lwd = 1.5)\n    segments(7.5, 7, 8.8, 5.8, lwd = 1.5)\n\n    # Total positives\n    text(3.7, 3, paste0(\"Total positive tests: \", v$total_pos), cex = 1.1, font = 2)\n    text(3.7, 2, paste0(\"Of these, truly sick: \", v$true_pos), cex = 1.1,\n         col = \"#27ae60\", font = 2)\n    text(3.7, 1, paste0(\"P(sick | test+) = \",\n         v$true_pos, \"/\", v$total_pos, \" = \",\n         round(v$ppv * 100, 1), \"%\"), cex = 1.2, font = 2, col = \"#e74c3c\")\n  })\n\n  output$icon_plot &lt;- renderPlot({\n    v &lt;- vals()\n    par(mar = c(1, 1, 3, 1))\n\n    # Show total positive tests as dots\n    n_show &lt;- min(v$total_pos, 200)\n    n_true &lt;- round(n_show * v$ppv)\n    n_false &lt;- n_show - n_true\n\n    cols &lt;- c(rep(\"#27ae60\", n_true), rep(\"#e74c3c\", n_false))\n    cols &lt;- sample(cols)\n\n    ncol &lt;- ceiling(sqrt(n_show))\n    nrow &lt;- ceiling(n_show / ncol)\n\n    plot(NULL, xlim = c(0, ncol + 1), ylim = c(0, nrow + 1),\n         axes = FALSE, xlab = \"\", ylab = \"\",\n         main = paste0(\"All \", v$total_pos, \" positive tests\"))\n\n    if (n_show &gt; 0) {\n      x &lt;- rep(seq_len(ncol), times = nrow)[seq_len(n_show)]\n      y &lt;- rep(seq(nrow, 1), each = ncol)[seq_len(n_show)]\n      points(x, y, pch = 15, cex = max(0.5, 3 - n_show / 50), col = cols)\n    }\n\n    legend(\"bottom\", bty = \"n\", horiz = TRUE, cex = 0.95,\n           legend = c(paste0(\"Truly sick (\", n_true, \")\"),\n                      paste0(\"False alarm (\", n_false, \")\")),\n           col = c(\"#27ae60\", \"#e74c3c\"), pch = 15, pt.cex = 1.5)\n  })\n\n  output$result_box &lt;- renderUI({\n    v &lt;- vals()\n    tags$div(class = \"result-box\",\n      HTML(paste0(\n        \"If you test positive,&lt;br&gt;\",\n        \"the probability you're sick is:&lt;br&gt;\",\n        \"&lt;span class='big'&gt;\", round(v$ppv * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;small&gt;not \", round(input$sens * 100), \"%!&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nDefault settings (prevalence 0.1%, test 99% accurate): only ~9% of positive tests are truly sick. The base rate dominates.\nSlide prevalence up to 5%: now ~84% of positives are real. The prior matters!\nSlide prevalence to 50%: the posterior is ~99%. When the disease is common, a positive test is very informative.\nLower specificity to 90%: false positives explode. Watch the right plot fill with red dots.\n\n\n\nThe lesson\nBayes’ theorem tells you: don’t just look at the test accuracy — look at how common the thing is. A 99% accurate test is nearly useless for a 1-in-1,000 disease because most positives are false alarms. This is the base rate fallacy, and Bayes’ theorem is the cure.",
    "crumbs": [
      "Bayes' Theorem"
    ]
  },
  {
    "objectID": "priors-posteriors.html",
    "href": "priors-posteriors.html",
    "title": "Priors & Posteriors",
    "section": "",
    "text": "Imagine you’re trying to estimate something — say, the average effect of a tutoring program on test scores. In frequentist statistics, you collect data, compute a point estimate, and that’s your answer.\nIn Bayesian statistics, you do something different:\n\nStart with a prior — what you believed before seeing data. Maybe from past studies, expert opinion, or just “I have no idea” (a flat prior).\nObserve data — the likelihood tells you how probable the data is for each possible value of the parameter.\nCombine them — Bayes’ theorem multiplies the prior by the likelihood to give you the posterior: your updated belief after seeing the data.\n\n\\[\\underbrace{P(\\theta \\mid \\text{data})}_{\\text{posterior}} \\propto \\underbrace{P(\\text{data} \\mid \\theta)}_{\\text{likelihood}} \\times \\underbrace{P(\\theta)}_{\\text{prior}}\\]\n\n\n\n\n\n\nExample: diagnosing a headache. A doctor sees a patient with a headache. Before any tests, her prior is: 99.9% chance it’s a tension headache, 0.1% chance it’s a brain tumor (base rates from experience). Then an MRI shows something unusual — that’s the data. The posterior updates: maybe now it’s 95% tension headache, 5% tumor. The prior didn’t disappear — it got updated by the evidence. A second MRI (more data) updates it further. With enough evidence, even a strong prior gets overwhelmed. That’s Bayesian inference: start with what you know, update with what you see.\n\n\n\nThe simulation below makes this tangible. You’re estimating the true mean of a normal distribution. Set a prior, generate data, and watch the posterior form.\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_mu\", HTML(\"True &mu; (unknown to you):\"),\n                  min = -5, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"prior_mu\", \"Prior mean:\",\n                  min = -5, max = 5, value = 0, step = 0.5),\n\n      sliderInput(\"prior_sd\", \"Prior SD (certainty):\",\n                  min = 0.5, max = 10, value = 3, step = 0.5),\n\n      sliderInput(\"n\", \"Sample size (data):\",\n                  min = 1, max = 200, value = 5, step = 1),\n\n      sliderInput(\"sigma\", HTML(\"Data noise (&sigma;):\"),\n                  min = 0.5, max = 5, value = 2, step = 0.5),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"posterior_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n\n    true_mu  &lt;- input$true_mu\n    prior_mu &lt;- input$prior_mu\n    prior_sd &lt;- input$prior_sd\n    n        &lt;- input$n\n    sigma    &lt;- input$sigma\n\n    # Generate data\n    y &lt;- rnorm(n, mean = true_mu, sd = sigma)\n    y_bar &lt;- mean(y)\n\n    # Posterior (conjugate normal-normal)\n    prior_prec &lt;- 1 / prior_sd^2\n    data_prec  &lt;- n / sigma^2\n    post_prec  &lt;- prior_prec + data_prec\n    post_sd    &lt;- 1 / sqrt(post_prec)\n    post_mu    &lt;- (prior_prec * prior_mu + data_prec * y_bar) / post_prec\n\n    # Shrinkage weight on prior\n    w_prior &lt;- prior_prec / post_prec\n\n    list(true_mu = true_mu, prior_mu = prior_mu, prior_sd = prior_sd,\n         y_bar = y_bar, sigma = sigma, n = n,\n         post_mu = post_mu, post_sd = post_sd, w_prior = w_prior)\n  })\n\n  output$posterior_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    xmin &lt;- min(d$prior_mu - 3.5 * d$prior_sd, d$post_mu - 4 * d$post_sd, d$true_mu - 2)\n    xmax &lt;- max(d$prior_mu + 3.5 * d$prior_sd, d$post_mu + 4 * d$post_sd, d$true_mu + 2)\n    x &lt;- seq(xmin, xmax, length.out = 500)\n\n    y_prior &lt;- dnorm(x, d$prior_mu, d$prior_sd)\n    y_like  &lt;- dnorm(x, d$y_bar, d$sigma / sqrt(d$n))\n    y_post  &lt;- dnorm(x, d$post_mu, d$post_sd)\n\n    ylim &lt;- c(0, max(y_prior, y_like, y_post) * 1.15)\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(x, y_prior, type = \"l\", lwd = 2.5, col = \"#e74c3c\",\n         xlab = expression(mu), ylab = \"Density\",\n         main = \"Prior + Likelihood = Posterior\",\n         ylim = ylim)\n    lines(x, y_like, lwd = 2.5, col = \"#3498db\")\n    lines(x, y_post, lwd = 3, col = \"#27ae60\")\n\n    # Shade posterior\n    polygon(c(x, rev(x)),\n            c(y_post, rep(0, length(x))),\n            col = adjustcolor(\"#27ae60\", 0.2), border = NA)\n\n    # True value\n    abline(v = d$true_mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.9,\n           legend = c(\"Prior (your belief before data)\",\n                      \"Likelihood (what the data says)\",\n                      \"Posterior (updated belief)\",\n                      expression(\"True \" * mu)),\n           col = c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"#2c3e50\"),\n           lwd = c(2.5, 2.5, 3, 2),\n           lty = c(1, 1, 1, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Prior mean:&lt;/b&gt; \", d$prior_mu, \"&lt;br&gt;\",\n        \"&lt;b&gt;Data mean:&lt;/b&gt; \", round(d$y_bar, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Posterior mean:&lt;/b&gt; \", round(d$post_mu, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Posterior SD:&lt;/b&gt; \", round(d$post_sd, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Weight on prior:&lt;/b&gt; \", round(d$w_prior * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;b&gt;Weight on data:&lt;/b&gt; \", round((1 - d$w_prior) * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;small&gt;Posterior = weighted average of prior & data&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nn = 1: the posterior is mostly the prior (red). You barely have data.\nSlide n to 100: the posterior (green) collapses onto the data mean (blue). Data overwhelms the prior. With enough data, the prior doesn’t matter.\nSet prior SD = 0.5 (strong prior) with n = 5: the posterior is pulled toward the prior. This is shrinkage — the prior is “shrinking” your estimate away from the data and toward your prior belief.\nSet prior SD = 10 (vague prior): the posterior tracks the data almost exactly. A flat prior says “I have no opinion” and lets the data speak.\nWatch the weight on prior in the sidebar — it shows exactly how much the posterior is a compromise between prior and data.",
    "crumbs": [
      "Priors & Posteriors"
    ]
  },
  {
    "objectID": "priors-posteriors.html#what-is-bayesian-inference-really",
    "href": "priors-posteriors.html#what-is-bayesian-inference-really",
    "title": "Priors & Posteriors",
    "section": "",
    "text": "Imagine you’re trying to estimate something — say, the average effect of a tutoring program on test scores. In frequentist statistics, you collect data, compute a point estimate, and that’s your answer.\nIn Bayesian statistics, you do something different:\n\nStart with a prior — what you believed before seeing data. Maybe from past studies, expert opinion, or just “I have no idea” (a flat prior).\nObserve data — the likelihood tells you how probable the data is for each possible value of the parameter.\nCombine them — Bayes’ theorem multiplies the prior by the likelihood to give you the posterior: your updated belief after seeing the data.\n\n\\[\\underbrace{P(\\theta \\mid \\text{data})}_{\\text{posterior}} \\propto \\underbrace{P(\\text{data} \\mid \\theta)}_{\\text{likelihood}} \\times \\underbrace{P(\\theta)}_{\\text{prior}}\\]\n\n\n\n\n\n\nExample: diagnosing a headache. A doctor sees a patient with a headache. Before any tests, her prior is: 99.9% chance it’s a tension headache, 0.1% chance it’s a brain tumor (base rates from experience). Then an MRI shows something unusual — that’s the data. The posterior updates: maybe now it’s 95% tension headache, 5% tumor. The prior didn’t disappear — it got updated by the evidence. A second MRI (more data) updates it further. With enough evidence, even a strong prior gets overwhelmed. That’s Bayesian inference: start with what you know, update with what you see.\n\n\n\nThe simulation below makes this tangible. You’re estimating the true mean of a normal distribution. Set a prior, generate data, and watch the posterior form.\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_mu\", HTML(\"True &mu; (unknown to you):\"),\n                  min = -5, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"prior_mu\", \"Prior mean:\",\n                  min = -5, max = 5, value = 0, step = 0.5),\n\n      sliderInput(\"prior_sd\", \"Prior SD (certainty):\",\n                  min = 0.5, max = 10, value = 3, step = 0.5),\n\n      sliderInput(\"n\", \"Sample size (data):\",\n                  min = 1, max = 200, value = 5, step = 1),\n\n      sliderInput(\"sigma\", HTML(\"Data noise (&sigma;):\"),\n                  min = 0.5, max = 5, value = 2, step = 0.5),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"posterior_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n\n    true_mu  &lt;- input$true_mu\n    prior_mu &lt;- input$prior_mu\n    prior_sd &lt;- input$prior_sd\n    n        &lt;- input$n\n    sigma    &lt;- input$sigma\n\n    # Generate data\n    y &lt;- rnorm(n, mean = true_mu, sd = sigma)\n    y_bar &lt;- mean(y)\n\n    # Posterior (conjugate normal-normal)\n    prior_prec &lt;- 1 / prior_sd^2\n    data_prec  &lt;- n / sigma^2\n    post_prec  &lt;- prior_prec + data_prec\n    post_sd    &lt;- 1 / sqrt(post_prec)\n    post_mu    &lt;- (prior_prec * prior_mu + data_prec * y_bar) / post_prec\n\n    # Shrinkage weight on prior\n    w_prior &lt;- prior_prec / post_prec\n\n    list(true_mu = true_mu, prior_mu = prior_mu, prior_sd = prior_sd,\n         y_bar = y_bar, sigma = sigma, n = n,\n         post_mu = post_mu, post_sd = post_sd, w_prior = w_prior)\n  })\n\n  output$posterior_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    xmin &lt;- min(d$prior_mu - 3.5 * d$prior_sd, d$post_mu - 4 * d$post_sd, d$true_mu - 2)\n    xmax &lt;- max(d$prior_mu + 3.5 * d$prior_sd, d$post_mu + 4 * d$post_sd, d$true_mu + 2)\n    x &lt;- seq(xmin, xmax, length.out = 500)\n\n    y_prior &lt;- dnorm(x, d$prior_mu, d$prior_sd)\n    y_like  &lt;- dnorm(x, d$y_bar, d$sigma / sqrt(d$n))\n    y_post  &lt;- dnorm(x, d$post_mu, d$post_sd)\n\n    ylim &lt;- c(0, max(y_prior, y_like, y_post) * 1.15)\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(x, y_prior, type = \"l\", lwd = 2.5, col = \"#e74c3c\",\n         xlab = expression(mu), ylab = \"Density\",\n         main = \"Prior + Likelihood = Posterior\",\n         ylim = ylim)\n    lines(x, y_like, lwd = 2.5, col = \"#3498db\")\n    lines(x, y_post, lwd = 3, col = \"#27ae60\")\n\n    # Shade posterior\n    polygon(c(x, rev(x)),\n            c(y_post, rep(0, length(x))),\n            col = adjustcolor(\"#27ae60\", 0.2), border = NA)\n\n    # True value\n    abline(v = d$true_mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.9,\n           legend = c(\"Prior (your belief before data)\",\n                      \"Likelihood (what the data says)\",\n                      \"Posterior (updated belief)\",\n                      expression(\"True \" * mu)),\n           col = c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"#2c3e50\"),\n           lwd = c(2.5, 2.5, 3, 2),\n           lty = c(1, 1, 1, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Prior mean:&lt;/b&gt; \", d$prior_mu, \"&lt;br&gt;\",\n        \"&lt;b&gt;Data mean:&lt;/b&gt; \", round(d$y_bar, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Posterior mean:&lt;/b&gt; \", round(d$post_mu, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Posterior SD:&lt;/b&gt; \", round(d$post_sd, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Weight on prior:&lt;/b&gt; \", round(d$w_prior * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;b&gt;Weight on data:&lt;/b&gt; \", round((1 - d$w_prior) * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;small&gt;Posterior = weighted average of prior & data&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nn = 1: the posterior is mostly the prior (red). You barely have data.\nSlide n to 100: the posterior (green) collapses onto the data mean (blue). Data overwhelms the prior. With enough data, the prior doesn’t matter.\nSet prior SD = 0.5 (strong prior) with n = 5: the posterior is pulled toward the prior. This is shrinkage — the prior is “shrinking” your estimate away from the data and toward your prior belief.\nSet prior SD = 10 (vague prior): the posterior tracks the data almost exactly. A flat prior says “I have no opinion” and lets the data speak.\nWatch the weight on prior in the sidebar — it shows exactly how much the posterior is a compromise between prior and data.",
    "crumbs": [
      "Priors & Posteriors"
    ]
  },
  {
    "objectID": "priors-posteriors.html#shrinkage-the-bayesian-superpower",
    "href": "priors-posteriors.html#shrinkage-the-bayesian-superpower",
    "title": "Priors & Posteriors",
    "section": "Shrinkage: the Bayesian superpower",
    "text": "Shrinkage: the Bayesian superpower\nLook at the “weight on prior” number in the sidebar. The posterior mean is literally a weighted average:\n\\[\\mu_{post} = w \\cdot \\mu_{prior} + (1 - w) \\cdot \\bar{y}\\]\nwhere \\(w\\) depends on how confident your prior is relative to how much data you have.\nThis is shrinkage: the posterior “shrinks” the data estimate toward the prior. When is this useful?\n\nSmall samples: noisy data gets regularized toward a sensible default.\nMany groups: estimating batting averages for 500 baseball players? Shrink extreme estimates toward the league average. A player who went 3-for-3 on opening day probably isn’t a .1000 hitter.\nHierarchical models: borrow strength across groups by shrinking toward a common mean.\n\nShrinkage isn’t bias — it’s a bias-variance tradeoff. You add a little bias but reduce variance a lot, often improving overall accuracy.",
    "crumbs": [
      "Priors & Posteriors"
    ]
  },
  {
    "objectID": "priors-posteriors.html#why-did-the-math-work-out-so-cleanly",
    "href": "priors-posteriors.html#why-did-the-math-work-out-so-cleanly",
    "title": "Priors & Posteriors",
    "section": "Why did the math work out so cleanly?",
    "text": "Why did the math work out so cleanly?\nThe simulation above computes the posterior instantly — no sampling, no iteration, just a formula. That’s because we used a conjugate prior: a special prior-likelihood pair where the posterior has the same distributional form as the prior.\nHere, the prior is Normal, the likelihood is Normal, and the posterior is Normal too. You just update the mean and variance:\n\\[\\mu_{post} = \\frac{\\frac{\\mu_0}{\\sigma_0^2} + \\frac{n\\bar{y}}{\\sigma^2}}{\\frac{1}{\\sigma_0^2} + \\frac{n}{\\sigma^2}}\\]\nPlug in numbers, get the answer. No algorithm required.\n\nCommon conjugate pairs\n\n\n\nLikelihood\nConjugate prior\nPosterior\nExample\n\n\n\n\nNormal (known \\(\\sigma\\))\nNormal\nNormal\nEstimating a mean (this page)\n\n\nBinomial\nBeta\nBeta\nEstimating a proportion (Bayes’ Theorem)\n\n\nPoisson\nGamma\nGamma\nEstimating a rate\n\n\n\n\n\nThe problem: most real models aren’t conjugate\nConjugacy is elegant but limited. It only works for these specific combinations. The moment your model gets realistic — logistic regression with priors on coefficients, hierarchical models with multiple levels, non-standard likelihoods — there’s no conjugate solution. The posterior is some high-dimensional surface with no closed-form expression.\n\n\n\n\n\n\nWhat does “closed-form” mean? A closed-form solution is one you can write as a finite formula using standard operations (addition, multiplication, exponents, etc.) and evaluate directly.\nClosed-form (conjugate case): the posterior mean above — plug in \\(\\mu_0\\), \\(\\sigma_0\\), \\(n\\), \\(\\bar{y}\\), \\(\\sigma\\), do arithmetic, get the exact answer. Done.\nNot closed-form (non-conjugate case): say you want the posterior for a logistic regression coefficient \\(\\theta\\) with a normal prior:\n\\[p(\\theta \\mid y) = \\frac{\\prod_{i=1}^n \\frac{1}{1 + e^{-\\theta x_i}} \\cdot e^{-\\theta^2/2}}{\\int_{-\\infty}^{\\infty} \\prod_{i=1}^n \\frac{1}{1 + e^{-\\theta x_i}} \\cdot e^{-\\theta^2/2} \\, d\\theta}\\]\nThat integral in the denominator? There’s no formula for it. You can’t simplify it to “plug in numbers.” You’d have to numerically approximate it — which is exactly what MCMC does (it sidesteps the integral entirely by sampling).\n\n\n\nThat’s why MCMC exists: when you can’t write down the posterior, you sample from it instead. The progression in this course:\n\nThis page: conjugate priors — exact, instant posteriors (the special case)\nMCMC: numerical sampling — posteriors for any model (the general case)\nHierarchical Models: the reason you need MCMC in practice",
    "crumbs": [
      "Priors & Posteriors"
    ]
  },
  {
    "objectID": "hierarchical.html",
    "href": "hierarchical.html",
    "title": "Hierarchical Models",
    "section": "",
    "text": "You have data from \\(K\\) groups — schools, hospitals, factories, regions — and you want to estimate a parameter (say, the mean) for each group. Two extreme approaches:\n\nNo pooling: estimate each group separately using only its own data. With small groups, the estimates are noisy and unreliable.\nComplete pooling: ignore groups entirely and estimate a single grand mean. This throws away real group differences.\n\nNeither is satisfying. No pooling overfits to noise. Complete pooling underfits by ignoring structure. You want something in between.\n\n\n\n\n\n\nExample: Uber driver ratings. Uber has millions of drivers. A new driver with 3 rides and a perfect 5.0 rating — is she really the best driver on the platform? A veteran with 2,000 rides and a 4.7 — is he worse? Common sense says no: the 5.0 is mostly luck (small sample), and the 4.7 is a reliable signal (large sample). Partial pooling formalizes this: shrink the new driver’s 5.0 toward the platform average (say, 4.8), but barely touch the veteran’s 4.7. Every driver’s estimate improves — especially the ones with few rides.",
    "crumbs": [
      "Beyond Conjugates",
      "Hierarchical Models"
    ]
  },
  {
    "objectID": "hierarchical.html#the-problem",
    "href": "hierarchical.html#the-problem",
    "title": "Hierarchical Models",
    "section": "",
    "text": "You have data from \\(K\\) groups — schools, hospitals, factories, regions — and you want to estimate a parameter (say, the mean) for each group. Two extreme approaches:\n\nNo pooling: estimate each group separately using only its own data. With small groups, the estimates are noisy and unreliable.\nComplete pooling: ignore groups entirely and estimate a single grand mean. This throws away real group differences.\n\nNeither is satisfying. No pooling overfits to noise. Complete pooling underfits by ignoring structure. You want something in between.\n\n\n\n\n\n\nExample: Uber driver ratings. Uber has millions of drivers. A new driver with 3 rides and a perfect 5.0 rating — is she really the best driver on the platform? A veteran with 2,000 rides and a 4.7 — is he worse? Common sense says no: the 5.0 is mostly luck (small sample), and the 4.7 is a reliable signal (large sample). Partial pooling formalizes this: shrink the new driver’s 5.0 toward the platform average (say, 4.8), but barely touch the veteran’s 4.7. Every driver’s estimate improves — especially the ones with few rides.",
    "crumbs": [
      "Beyond Conjugates",
      "Hierarchical Models"
    ]
  },
  {
    "objectID": "hierarchical.html#hierarchical-models-partial-pooling",
    "href": "hierarchical.html#hierarchical-models-partial-pooling",
    "title": "Hierarchical Models",
    "section": "Hierarchical models: partial pooling",
    "text": "Hierarchical models: partial pooling\nA hierarchical (multilevel) model learns the group-level variation from the data and uses it to shrink each group’s estimate toward the grand mean — more for small groups, less for large groups.\nThe model:\n\\[y_{ij} \\sim N(\\theta_j, \\sigma^2) \\quad \\text{(data within group } j\\text{)}\\] \\[\\theta_j \\sim N(\\mu, \\tau^2) \\quad \\text{(group means come from a population)}\\]\n\n\\(\\theta_j\\) is the true mean for group \\(j\\)\n\\(\\mu\\) is the overall population mean\n\\(\\tau^2\\) is the between-group variance (how different groups really are)\n\\(\\sigma^2\\) is the within-group variance (noise)\n\nThe posterior for each \\(\\theta_j\\) is a weighted average of the group’s own data and the grand mean:\n\\[\\hat{\\theta}_j^{partial} = w_j \\cdot \\bar{y}_j + (1 - w_j) \\cdot \\hat{\\mu}\\]\nwhere the weight \\(w_j = \\frac{n_j / \\sigma^2}{n_j / \\sigma^2 + 1/\\tau^2}\\) depends on the group sample size \\(n_j\\). Small groups shrink more because their data is less informative relative to the population prior.\nThis is the same shrinkage logic from the shrinkage page, but now applied within a formal model that estimates \\(\\tau^2\\) from the data.\n\nSimulation: School test scores\n\\(K\\) schools, each with \\(n_k\\) students. Compare three estimates for each school’s true mean:\n\nNo pooling (red): each school’s raw sample mean\nComplete pooling (gray): the grand mean for all schools\nPartial pooling (blue): the hierarchical Bayes estimate\n\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"K\", \"Number of schools:\",\n                  min = 5, max = 30, value = 12, step = 1),\n\n      sliderInput(\"n_per\", \"Students per school:\",\n                  min = 5, max = 100, value = 15, step = 5),\n\n      sliderInput(\"tau\", \"Between-school SD (\\u03C4):\",\n                  min = 1, max = 15, value = 5, step = 1),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"school_plot\", height = \"450px\")),\n        column(6, plotOutput(\"mse_plot\",    height = \"450px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    K     &lt;- input$K\n    n_per &lt;- input$n_per\n    tau   &lt;- input$tau\n    sigma &lt;- 10  # within-school SD (fixed)\n\n    # True school means\n    mu &lt;- 70  # grand mean\n    theta_true &lt;- rnorm(K, mean = mu, sd = tau)\n\n    # Generate student scores\n    y_bar &lt;- numeric(K)\n    for (j in 1:K) {\n      scores &lt;- rnorm(n_per, mean = theta_true[j], sd = sigma)\n      y_bar[j] &lt;- mean(scores)\n    }\n\n    # No pooling: raw means\n    no_pool &lt;- y_bar\n\n    # Complete pooling: grand mean\n    grand_mean &lt;- mean(y_bar)\n    complete_pool &lt;- rep(grand_mean, K)\n\n    # Partial pooling (empirical Bayes)\n    # Estimate tau from data\n    between_var &lt;- var(y_bar)\n    within_var  &lt;- sigma^2 / n_per\n    tau_hat_sq  &lt;- max(between_var - within_var, 0.01)\n\n    w &lt;- (n_per / sigma^2) / (n_per / sigma^2 + 1 / tau_hat_sq)\n    partial_pool &lt;- w * y_bar + (1 - w) * grand_mean\n\n    # MSE\n    mse_no   &lt;- mean((no_pool - theta_true)^2)\n    mse_comp &lt;- mean((complete_pool - theta_true)^2)\n    mse_part &lt;- mean((partial_pool - theta_true)^2)\n\n    list(theta_true = theta_true, no_pool = no_pool,\n         complete_pool = complete_pool, partial_pool = partial_pool,\n         grand_mean = grand_mean, w = w,\n         mse_no = mse_no, mse_comp = mse_comp, mse_part = mse_part,\n         K = K, n_per = n_per, tau = tau)\n  })\n\n  output$school_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 5.5, 3, 1))\n\n    K &lt;- d$K\n    ord &lt;- order(d$no_pool)\n\n    xlim &lt;- range(c(d$no_pool, d$partial_pool, d$theta_true, d$grand_mean))\n    xlim &lt;- xlim + c(-2, 2)\n\n    plot(NULL, xlim = xlim, ylim = c(1, K),\n         yaxt = \"n\", xlab = \"Score\",\n         ylab = \"\", main = \"School Mean Estimates\")\n    axis(2, at = 1:K, labels = paste0(\"School \", ord), las = 1, cex.axis = 0.7)\n\n    # Grand mean line\n    abline(v = d$grand_mean, lty = 2, col = \"gray50\", lwd = 1.5)\n\n    for (i in 1:K) {\n      j &lt;- ord[i]\n\n      # Arrow from no-pooling to partial-pooling\n      arrows(d$no_pool[j], i, d$partial_pool[j], i,\n             length = 0.04, col = \"#bdc3c780\", lwd = 1)\n\n      # Points\n      points(d$no_pool[j], i, pch = 16, col = \"#e74c3c\", cex = 1.2)\n      points(d$partial_pool[j], i, pch = 17, col = \"#3498db\", cex = 1.2)\n      points(d$theta_true[j], i, pch = 4, col = \"#27ae60\", cex = 1, lwd = 2)\n    }\n\n    legend(\"bottomright\", bty = \"n\", cex = 0.8,\n           legend = c(\"No pooling (raw mean)\", \"Partial pooling\",\n                      \"True mean\", \"Grand mean\"),\n           col = c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"gray50\"),\n           pch = c(16, 17, 4, NA),\n           lty = c(NA, NA, NA, 2), lwd = c(NA, NA, 2, 1.5))\n  })\n\n  output$mse_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4, 3, 1))\n\n    vals &lt;- c(d$mse_no, d$mse_comp, d$mse_part)\n    cols &lt;- c(\"#e74c3c\", \"gray60\", \"#3498db\")\n    labels &lt;- c(\"No pooling\", \"Complete\\npooling\", \"Partial\\npooling\")\n\n    bp &lt;- barplot(vals, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.85,\n                  main = \"Mean Squared Error\",\n                  ylab = \"MSE\")\n    text(bp, vals + max(vals) * 0.03, round(vals, 1), cex = 0.85, font = 2)\n\n    pct &lt;- round((1 - d$mse_part / d$mse_no) * 100, 0)\n    mtext(paste0(\"Partial pooling reduces MSE by ~\", pct, \"% vs no pooling\"),\n          side = 1, line = 3.5, cex = 0.85, col = \"#2c3e50\")\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    pct_np &lt;- round((1 - d$mse_part / d$mse_no) * 100, 1)\n    pct_cp &lt;- round((1 - d$mse_part / d$mse_comp) * 100, 1)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Shrinkage weight:&lt;/b&gt; \", round(d$w * 100, 1),\n        \"% on group data&lt;br&gt;\",\n        \"&lt;small&gt;(\", round((1 - d$w) * 100, 1), \"% on grand mean)&lt;/small&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;MSE no pooling:&lt;/b&gt; &lt;span class='bad'&gt;\",\n        round(d$mse_no, 1), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;MSE complete pooling:&lt;/b&gt; \",\n        round(d$mse_comp, 1), \"&lt;br&gt;\",\n        \"&lt;b&gt;MSE partial pooling:&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$mse_part, 1), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;vs no pooling:&lt;/b&gt; &lt;span class='good'&gt;\",\n        pct_np, \"% lower&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;vs complete pooling:&lt;/b&gt; &lt;span class='good'&gt;\",\n        pct_cp, \"% lower&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nStudents per school = 5, between-school SD = 5: small samples, real group differences. The no-pooling estimates (red) are scattered — some far from the truth. Partial pooling (blue triangles) shrinks them toward the grand mean, landing closer to the true values (green crosses). MSE drops substantially.\nStudents per school = 100: with lots of data per group, the raw means are already precise. Shrinkage is minimal — blue and red nearly overlap. With enough data, partial pooling = no pooling.\nBetween-school SD = 1: schools are very similar. Complete pooling is nearly optimal because the group differences are tiny. Partial pooling agrees — it shrinks almost entirely to the grand mean.\nBetween-school SD = 15: schools differ a lot. Complete pooling is terrible because it ignores real differences. No pooling is better, and partial pooling is best — it respects both the data and the group structure.\nLook at the left plot: the arrows show shrinkage. Extreme schools (raw means far from the grand mean) shrink the most. Schools near the middle barely move. This is adaptive — the model learns how much shrinkage is appropriate.",
    "crumbs": [
      "Beyond Conjugates",
      "Hierarchical Models"
    ]
  },
  {
    "objectID": "hierarchical.html#why-partial-pooling-wins",
    "href": "hierarchical.html#why-partial-pooling-wins",
    "title": "Hierarchical Models",
    "section": "Why partial pooling wins",
    "text": "Why partial pooling wins\nThe logic is identical to the shrinkage page, but in a structured model:\n\nNo pooling has zero bias but high variance (each estimate uses only local data).\nComplete pooling has high bias but zero variance across groups.\nPartial pooling trades a little bias for a large reduction in variance. The bias-variance tradeoff is optimized by the model.\n\nThe more groups you have, the better the model estimates \\(\\tau^2\\) (the between-group variance), and the more precisely it calibrates the amount of shrinkage.\n\nWhere hierarchical models show up\n\n\n\nApplication\nGroups\nWhat gets shrunk\n\n\n\n\nEducation\nSchools / districts\nTest score means\n\n\nSports analytics\nPlayers / teams\nBatting averages, win rates\n\n\nClinical trials\nStudy sites / subgroups\nTreatment effects\n\n\nMarketing\nRegions / customer segments\nResponse rates\n\n\nEcology\nSpecies / habitats\nPopulation parameters\n\n\n\nIn each case, the hierarchical structure lets you borrow strength across groups — improving estimates for every group, especially the small ones.\n\n\nWhen is your data hierarchical?\nWhenever units are nested inside groups, and you expect units in the same group to be more similar to each other than to units in other groups.\n\n\n\n\n\n\nExample: dollar stores and obesity. Say you’re studying whether dollar store presence affects obesity rates. Your data has census tracts nested within counties. That’s hierarchical — tracts in the same county share the local food environment, demographics, and policy context.\n\nLevel 1 (tracts): each tract has its own obesity rate, influenced by dollar store presence + tract-level covariates\nLevel 2 (counties): tract-level effects come from a county-level distribution — tracts in the same county are more similar to each other than to tracts in other counties\n\nPartial pooling matters for small counties with few tracts. If a county has only 2 tracts, the raw county average is noisy. The hierarchical model shrinks it toward the overall pattern — borrowing strength from larger counties.\n\n\n\nWithout a hierarchical model, you’d have to choose:\n\n\n\n\n\n\n\nApproach\nProblem\n\n\n\n\nIgnore counties, pool all tracts\nMisses that tracts in the same county share unobserved factors\n\n\nCounty fixed effects\nEats degrees of freedom; can’t estimate county-level predictors; small counties get noisy estimates\n\n\nHierarchical model\nBest of both — partial pooling, can include county-level predictors, small counties get regularized\n\n\n\nThe rule of thumb: if your research question involves variation across groups (does the effect differ by county? do rural vs urban counties respond differently?), a hierarchical model is the natural fit. If you’re just estimating one overall effect and groups are a nuisance, fixed effects or clustered SEs might be enough.",
    "crumbs": [
      "Beyond Conjugates",
      "Hierarchical Models"
    ]
  },
  {
    "objectID": "hierarchical.html#did-you-know",
    "href": "hierarchical.html#did-you-know",
    "title": "Hierarchical Models",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe “8 schools” example from Rubin (1981) and Gelman et al. (2013, Bayesian Data Analysis) is the canonical textbook example of hierarchical modeling. Eight schools ran a coaching program; the hierarchical model showed that the most impressive result (School A, +28 points) was largely noise, and the true effects were more modest and similar across schools.\nHierarchical models are the Bayesian counterpart of random effects models in frequentist statistics. The key difference: Bayesian hierarchical models provide full posterior distributions for each group parameter, while frequentist random effects models typically give only point estimates (BLUPs — Best Linear Unbiased Predictors).\nStein’s paradox (1956) proved that when estimating 3 or more means simultaneously, the sample means are inadmissible — there always exists an estimator with lower total MSE. The James-Stein estimator and hierarchical models both exploit this: by shrinking toward a common mean, they beat the “obvious” estimator that uses each group’s data alone.",
    "crumbs": [
      "Beyond Conjugates",
      "Hierarchical Models"
    ]
  },
  {
    "objectID": "bayesian-regression.html",
    "href": "bayesian-regression.html",
    "title": "Bayesian Regression",
    "section": "",
    "text": "On the MCMC page, you saw how to sample from any posterior — even when no closed-form solution exists. Now we apply that machinery to something familiar: regression.\nIn Stata, you type reg y x and get a point estimate \\(\\hat{\\beta}\\), a standard error, and a 95% confidence interval. That’s OLS — it picks the single line that minimizes the sum of squared residuals.\nBayesian regression starts from the same place (the same likelihood) but adds one ingredient: a prior distribution on \\(\\beta\\). Instead of a single best-fit line, you get a full posterior distribution — a curve showing how plausible each value of \\(\\beta\\) is, given both the data and your prior beliefs.\n\n\n\n\n\n\nExample: You’re estimating the effect of an extra year of education on hourly wages. Before seeing your data, you’ve read the literature — most estimates land between $0.50 and $1.50 per year. A Bayesian approach lets you encode this: set a prior of \\(\\beta \\sim N(1.0, 0.5^2)\\), centered on $1.00 with moderate uncertainty. OLS ignores this information entirely. The Bayesian estimate blends it with your data.\n\n\n\n\n\n\n\n\n\nWhat “a prior on \\(\\beta\\)” means: it’s a probability distribution expressing your beliefs about the slope before seeing the data. A prior of \\(N(0, 10^2)\\) says “I think \\(\\beta\\) is probably near 0 but I’m very uncertain” (vague). A prior of \\(N(1, 0.3^2)\\) says “I’m fairly confident \\(\\beta\\) is close to 1” (informative). After seeing data, the prior gets updated to a posterior.\n\n\n\nThe math side-by-side:\n\n\n\n\n\n\n\n\n\nOLS\nBayesian\n\n\n\n\nModel\n\\(y = X\\beta + \\varepsilon, \\;\\; \\varepsilon \\sim N(0, \\sigma^2 I)\\)\nSame likelihood + prior \\(\\beta \\sim N(\\mu_0, \\sigma_0^2)\\)\n\n\nEstimate\n\\(\\hat{\\beta}_{OLS} = (X'X)^{-1}X'y\\)\nPosterior mean \\(= w \\cdot \\hat{\\beta}_{OLS} + (1-w) \\cdot \\mu_0\\)\n\n\nUncertainty\nSE, confidence interval\nFull posterior distribution, credible interval\n\n\nResult\nOne number + interval\nEntire distribution\n\n\n\nThe posterior mean is a precision-weighted average of the prior mean and the OLS estimate, where \\(w = \\frac{\\text{data precision}}{\\text{data precision} + \\text{prior precision}}\\). The more data you have, the more the posterior looks like OLS. The stronger the prior (smaller \\(\\sigma_0^2\\)), the more the posterior is pulled toward the prior mean.\n\n\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_beta\", HTML(\"True &beta;:\"),\n                  min = -3, max = 3, value = 1.5, step = 0.5),\n\n      sliderInput(\"prior_mean\", \"Prior mean:\",\n                  min = -3, max = 3, value = 0, step = 0.5),\n\n      sliderInput(\"prior_sd\", \"Prior SD:\",\n                  min = 0.1, max = 5, value = 1, step = 0.1),\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 5, max = 200, value = 20, step = 5),\n\n      sliderInput(\"sigma\", HTML(\"Noise (&sigma;):\"),\n                  min = 0.5, max = 5, value = 2, step = 0.5),\n\n      actionButton(\"go\", \"New data\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"scatter_plot\", height = \"420px\")),\n        column(6, plotOutput(\"density_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    true_beta  &lt;- input$true_beta\n    prior_mean &lt;- input$prior_mean\n    prior_sd   &lt;- input$prior_sd\n    n          &lt;- input$n\n    sigma      &lt;- input$sigma\n\n    # Generate data: y = true_beta * x + noise (centered x, no intercept)\n    x &lt;- rnorm(n, 0, 1)\n    y &lt;- true_beta * x + rnorm(n, 0, sigma)\n\n    # OLS estimate\n    beta_ols &lt;- sum(x * y) / sum(x^2)\n    se_ols   &lt;- sigma / sqrt(sum(x^2))\n\n    # Bayesian posterior (conjugate normal-normal, known sigma)\n    prior_prec &lt;- 1 / prior_sd^2\n    data_prec  &lt;- sum(x^2) / sigma^2\n    post_prec  &lt;- prior_prec + data_prec\n    post_sd    &lt;- 1 / sqrt(post_prec)\n    post_mean  &lt;- (prior_mean * prior_prec + beta_ols * data_prec) / post_prec\n\n    # Weights\n    w_data  &lt;- data_prec / post_prec\n    w_prior &lt;- prior_prec / post_prec\n\n    list(x = x, y = y, true_beta = true_beta,\n         beta_ols = beta_ols, se_ols = se_ols,\n         prior_mean = prior_mean, prior_sd = prior_sd,\n         post_mean = post_mean, post_sd = post_sd,\n         w_data = w_data, w_prior = w_prior, sigma = sigma)\n  })\n\n  output$scatter_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$y, pch = 16, col = \"#2c3e5060\", cex = 1.2,\n         xlab = \"x\", ylab = \"y\", main = \"Data + Regression Lines\")\n\n    # True line (green, dashed)\n    abline(0, d$true_beta, col = \"#27ae60\", lwd = 2.5, lty = 2)\n    # OLS line (red)\n    abline(0, d$beta_ols, col = \"#e74c3c\", lwd = 2.5)\n    # Bayesian line (blue)\n    abline(0, d$post_mean, col = \"#3498db\", lwd = 2.5)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\n             paste0(\"True: \", d$true_beta),\n             paste0(\"OLS: \", round(d$beta_ols, 3)),\n             paste0(\"Bayesian: \", round(d$post_mean, 3))\n           ),\n           col = c(\"#27ae60\", \"#e74c3c\", \"#3498db\"),\n           lwd = 2.5, lty = c(2, 1, 1))\n  })\n\n  output$density_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Range for plotting\n    all_means &lt;- c(d$prior_mean, d$beta_ols, d$post_mean)\n    all_sds   &lt;- c(d$prior_sd, d$se_ols, d$post_sd)\n    xlim &lt;- range(c(all_means - 3.5 * all_sds, all_means + 3.5 * all_sds))\n    x_seq &lt;- seq(xlim[1], xlim[2], length.out = 300)\n\n    # Densities\n    prior_y &lt;- dnorm(x_seq, d$prior_mean, d$prior_sd)\n    lik_y   &lt;- dnorm(x_seq, d$beta_ols, d$se_ols)\n    post_y  &lt;- dnorm(x_seq, d$post_mean, d$post_sd)\n\n    ylim &lt;- c(0, max(c(prior_y, lik_y, post_y)) * 1.15)\n\n    plot(NULL, xlim = xlim, ylim = ylim,\n         xlab = expression(beta), ylab = \"Density\",\n         main = expression(\"Prior, Likelihood, & Posterior for \" * beta))\n\n    # Prior (red, dashed)\n    lines(x_seq, prior_y, col = \"#e74c3c\", lwd = 2, lty = 2)\n\n    # Likelihood (gray, dotted)\n    lines(x_seq, lik_y, col = \"gray50\", lwd = 2, lty = 3)\n\n    # Posterior (blue, shaded)\n    polygon(c(x_seq, rev(x_seq)), c(post_y, rep(0, length(x_seq))),\n            col = \"#3498db30\", border = NA)\n    lines(x_seq, post_y, col = \"#3498db\", lwd = 2.5)\n\n    # True beta\n    abline(v = d$true_beta, lty = 2, col = \"#27ae60\", lwd = 1.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Prior\", \"Likelihood (data)\",\n                      \"Posterior\", expression(\"True \" * beta)),\n           col = c(\"#e74c3c\", \"gray50\", \"#3498db\", \"#27ae60\"),\n           lwd = c(2, 2, 2.5, 1.5),\n           lty = c(2, 3, 1, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;OLS estimate:&lt;/b&gt; \", round(d$beta_ols, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Posterior mean:&lt;/b&gt; \", round(d$post_mean, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;True &beta;:&lt;/b&gt; \", d$true_beta, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Weight on data:&lt;/b&gt; \", round(d$w_data * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;b&gt;Weight on prior:&lt;/b&gt; \", round(d$w_prior * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;OLS SE:&lt;/b&gt; \", round(d$se_ols, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Posterior SD:&lt;/b&gt; \", round(d$post_sd, 3)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nVague prior (prior SD = 5): the posterior almost exactly matches OLS. With a wide prior, the data dominates — the weight on prior drops near 0%. A vague Bayesian is a frequentist.\nStrong prior (prior SD = 0.3): the posterior is pulled heavily toward the prior mean. Even if the data says otherwise, the posterior barely moves. This is shrinkage in action.\nLarge n (n = 200): regardless of the prior, the posterior converges to the OLS estimate. Data overwhelms the prior. Watch the weight on data approach 100%.\nWrong prior (prior mean = -2, true beta = 1.5, n = 10): the Bayesian estimate is biased toward -2. Now increase n — the data corrects the bad prior. This is why Bayesian inference is “self-correcting” with enough data.\nCompare the right panel: the posterior (blue) always sits between the prior (red) and the likelihood (gray). It’s literally a compromise between what you believed and what you saw.",
    "crumbs": [
      "Beyond Conjugates",
      "Bayesian Regression"
    ]
  },
  {
    "objectID": "bayesian-regression.html#from-reg-y-x-to-bayesian-regression",
    "href": "bayesian-regression.html#from-reg-y-x-to-bayesian-regression",
    "title": "Bayesian Regression",
    "section": "",
    "text": "On the MCMC page, you saw how to sample from any posterior — even when no closed-form solution exists. Now we apply that machinery to something familiar: regression.\nIn Stata, you type reg y x and get a point estimate \\(\\hat{\\beta}\\), a standard error, and a 95% confidence interval. That’s OLS — it picks the single line that minimizes the sum of squared residuals.\nBayesian regression starts from the same place (the same likelihood) but adds one ingredient: a prior distribution on \\(\\beta\\). Instead of a single best-fit line, you get a full posterior distribution — a curve showing how plausible each value of \\(\\beta\\) is, given both the data and your prior beliefs.\n\n\n\n\n\n\nExample: You’re estimating the effect of an extra year of education on hourly wages. Before seeing your data, you’ve read the literature — most estimates land between $0.50 and $1.50 per year. A Bayesian approach lets you encode this: set a prior of \\(\\beta \\sim N(1.0, 0.5^2)\\), centered on $1.00 with moderate uncertainty. OLS ignores this information entirely. The Bayesian estimate blends it with your data.\n\n\n\n\n\n\n\n\n\nWhat “a prior on \\(\\beta\\)” means: it’s a probability distribution expressing your beliefs about the slope before seeing the data. A prior of \\(N(0, 10^2)\\) says “I think \\(\\beta\\) is probably near 0 but I’m very uncertain” (vague). A prior of \\(N(1, 0.3^2)\\) says “I’m fairly confident \\(\\beta\\) is close to 1” (informative). After seeing data, the prior gets updated to a posterior.\n\n\n\nThe math side-by-side:\n\n\n\n\n\n\n\n\n\nOLS\nBayesian\n\n\n\n\nModel\n\\(y = X\\beta + \\varepsilon, \\;\\; \\varepsilon \\sim N(0, \\sigma^2 I)\\)\nSame likelihood + prior \\(\\beta \\sim N(\\mu_0, \\sigma_0^2)\\)\n\n\nEstimate\n\\(\\hat{\\beta}_{OLS} = (X'X)^{-1}X'y\\)\nPosterior mean \\(= w \\cdot \\hat{\\beta}_{OLS} + (1-w) \\cdot \\mu_0\\)\n\n\nUncertainty\nSE, confidence interval\nFull posterior distribution, credible interval\n\n\nResult\nOne number + interval\nEntire distribution\n\n\n\nThe posterior mean is a precision-weighted average of the prior mean and the OLS estimate, where \\(w = \\frac{\\text{data precision}}{\\text{data precision} + \\text{prior precision}}\\). The more data you have, the more the posterior looks like OLS. The stronger the prior (smaller \\(\\sigma_0^2\\)), the more the posterior is pulled toward the prior mean.\n\n\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_beta\", HTML(\"True &beta;:\"),\n                  min = -3, max = 3, value = 1.5, step = 0.5),\n\n      sliderInput(\"prior_mean\", \"Prior mean:\",\n                  min = -3, max = 3, value = 0, step = 0.5),\n\n      sliderInput(\"prior_sd\", \"Prior SD:\",\n                  min = 0.1, max = 5, value = 1, step = 0.1),\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 5, max = 200, value = 20, step = 5),\n\n      sliderInput(\"sigma\", HTML(\"Noise (&sigma;):\"),\n                  min = 0.5, max = 5, value = 2, step = 0.5),\n\n      actionButton(\"go\", \"New data\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"scatter_plot\", height = \"420px\")),\n        column(6, plotOutput(\"density_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    true_beta  &lt;- input$true_beta\n    prior_mean &lt;- input$prior_mean\n    prior_sd   &lt;- input$prior_sd\n    n          &lt;- input$n\n    sigma      &lt;- input$sigma\n\n    # Generate data: y = true_beta * x + noise (centered x, no intercept)\n    x &lt;- rnorm(n, 0, 1)\n    y &lt;- true_beta * x + rnorm(n, 0, sigma)\n\n    # OLS estimate\n    beta_ols &lt;- sum(x * y) / sum(x^2)\n    se_ols   &lt;- sigma / sqrt(sum(x^2))\n\n    # Bayesian posterior (conjugate normal-normal, known sigma)\n    prior_prec &lt;- 1 / prior_sd^2\n    data_prec  &lt;- sum(x^2) / sigma^2\n    post_prec  &lt;- prior_prec + data_prec\n    post_sd    &lt;- 1 / sqrt(post_prec)\n    post_mean  &lt;- (prior_mean * prior_prec + beta_ols * data_prec) / post_prec\n\n    # Weights\n    w_data  &lt;- data_prec / post_prec\n    w_prior &lt;- prior_prec / post_prec\n\n    list(x = x, y = y, true_beta = true_beta,\n         beta_ols = beta_ols, se_ols = se_ols,\n         prior_mean = prior_mean, prior_sd = prior_sd,\n         post_mean = post_mean, post_sd = post_sd,\n         w_data = w_data, w_prior = w_prior, sigma = sigma)\n  })\n\n  output$scatter_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$y, pch = 16, col = \"#2c3e5060\", cex = 1.2,\n         xlab = \"x\", ylab = \"y\", main = \"Data + Regression Lines\")\n\n    # True line (green, dashed)\n    abline(0, d$true_beta, col = \"#27ae60\", lwd = 2.5, lty = 2)\n    # OLS line (red)\n    abline(0, d$beta_ols, col = \"#e74c3c\", lwd = 2.5)\n    # Bayesian line (blue)\n    abline(0, d$post_mean, col = \"#3498db\", lwd = 2.5)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\n             paste0(\"True: \", d$true_beta),\n             paste0(\"OLS: \", round(d$beta_ols, 3)),\n             paste0(\"Bayesian: \", round(d$post_mean, 3))\n           ),\n           col = c(\"#27ae60\", \"#e74c3c\", \"#3498db\"),\n           lwd = 2.5, lty = c(2, 1, 1))\n  })\n\n  output$density_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Range for plotting\n    all_means &lt;- c(d$prior_mean, d$beta_ols, d$post_mean)\n    all_sds   &lt;- c(d$prior_sd, d$se_ols, d$post_sd)\n    xlim &lt;- range(c(all_means - 3.5 * all_sds, all_means + 3.5 * all_sds))\n    x_seq &lt;- seq(xlim[1], xlim[2], length.out = 300)\n\n    # Densities\n    prior_y &lt;- dnorm(x_seq, d$prior_mean, d$prior_sd)\n    lik_y   &lt;- dnorm(x_seq, d$beta_ols, d$se_ols)\n    post_y  &lt;- dnorm(x_seq, d$post_mean, d$post_sd)\n\n    ylim &lt;- c(0, max(c(prior_y, lik_y, post_y)) * 1.15)\n\n    plot(NULL, xlim = xlim, ylim = ylim,\n         xlab = expression(beta), ylab = \"Density\",\n         main = expression(\"Prior, Likelihood, & Posterior for \" * beta))\n\n    # Prior (red, dashed)\n    lines(x_seq, prior_y, col = \"#e74c3c\", lwd = 2, lty = 2)\n\n    # Likelihood (gray, dotted)\n    lines(x_seq, lik_y, col = \"gray50\", lwd = 2, lty = 3)\n\n    # Posterior (blue, shaded)\n    polygon(c(x_seq, rev(x_seq)), c(post_y, rep(0, length(x_seq))),\n            col = \"#3498db30\", border = NA)\n    lines(x_seq, post_y, col = \"#3498db\", lwd = 2.5)\n\n    # True beta\n    abline(v = d$true_beta, lty = 2, col = \"#27ae60\", lwd = 1.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Prior\", \"Likelihood (data)\",\n                      \"Posterior\", expression(\"True \" * beta)),\n           col = c(\"#e74c3c\", \"gray50\", \"#3498db\", \"#27ae60\"),\n           lwd = c(2, 2, 2.5, 1.5),\n           lty = c(2, 3, 1, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;OLS estimate:&lt;/b&gt; \", round(d$beta_ols, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Posterior mean:&lt;/b&gt; \", round(d$post_mean, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;True &beta;:&lt;/b&gt; \", d$true_beta, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Weight on data:&lt;/b&gt; \", round(d$w_data * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;b&gt;Weight on prior:&lt;/b&gt; \", round(d$w_prior * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;OLS SE:&lt;/b&gt; \", round(d$se_ols, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Posterior SD:&lt;/b&gt; \", round(d$post_sd, 3)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nVague prior (prior SD = 5): the posterior almost exactly matches OLS. With a wide prior, the data dominates — the weight on prior drops near 0%. A vague Bayesian is a frequentist.\nStrong prior (prior SD = 0.3): the posterior is pulled heavily toward the prior mean. Even if the data says otherwise, the posterior barely moves. This is shrinkage in action.\nLarge n (n = 200): regardless of the prior, the posterior converges to the OLS estimate. Data overwhelms the prior. Watch the weight on data approach 100%.\nWrong prior (prior mean = -2, true beta = 1.5, n = 10): the Bayesian estimate is biased toward -2. Now increase n — the data corrects the bad prior. This is why Bayesian inference is “self-correcting” with enough data.\nCompare the right panel: the posterior (blue) always sits between the prior (red) and the likelihood (gray). It’s literally a compromise between what you believed and what you saw.",
    "crumbs": [
      "Beyond Conjugates",
      "Bayesian Regression"
    ]
  },
  {
    "objectID": "bayesian-regression.html#credible-intervals-vs-confidence-intervals",
    "href": "bayesian-regression.html#credible-intervals-vs-confidence-intervals",
    "title": "Bayesian Regression",
    "section": "Credible intervals vs confidence intervals",
    "text": "Credible intervals vs confidence intervals\nBoth give you a range of plausible values for \\(\\beta\\). But they answer different questions:\n\n\n\n\n\n\n\n\n\nConfidence interval (frequentist)\nCredible interval (Bayesian)\n\n\n\n\nStatement\n“If I repeated this experiment many times, 95% of my CIs would contain the true \\(\\beta\\)”\n“Given the data and my prior, there’s a 95% probability that \\(\\beta\\) is in this interval”\n\n\nAbout\nThe procedure\nThe parameter\n\n\nDepends on prior?\nNo\nYes\n\n\nInterpretation\nFrequency guarantee across experiments\nDirect probability statement for this experiment\n\n\n\nThe credible interval says what most people think the confidence interval says. See Bayesian vs Frequentist for more on this distinction.",
    "crumbs": [
      "Beyond Conjugates",
      "Bayesian Regression"
    ]
  },
  {
    "objectID": "bayesian-regression.html#the-prior-as-regularization",
    "href": "bayesian-regression.html#the-prior-as-regularization",
    "title": "Bayesian Regression",
    "section": "The prior as regularization",
    "text": "The prior as regularization\nA prior isn’t just a philosophical stance — it has a direct mathematical connection to regularization:\n\n\n\n\n\n\n\n\nPrior on \\(\\beta\\)\nEquivalent to\nPenalty\n\n\n\n\n\\(N(0, \\sigma_0^2)\\)\nRidge regression\n\\(\\lambda = \\sigma^2 / \\sigma_0^2\\)\n\n\nLaplace\\((0, b)\\)\nLASSO\n\\(\\lambda = \\sigma^2 / b\\)\n\n\nFlat (improper)\nOLS\nNo penalty\n\n\n\n\n\n\n\n\n\nMAP vs full Bayesian: The maximum a posteriori (MAP) estimate — the peak of the posterior — equals the ridge/LASSO solution exactly. But full Bayesian inference gives you the entire posterior, not just the peak. You get uncertainty for free.\n\n\n\nA tighter prior (smaller \\(\\sigma_0^2\\)) implies stronger regularization (larger \\(\\lambda\\)). This is why Bayesian regression with an informative prior produces shrinkage — it pulls estimates toward the prior mean, just like ridge regression pulls coefficients toward zero.\n\nSimulation: Repeated experiments — CI vs CrI coverage\nRun the same experiment many times. Each repetition generates new data from the true model, computes both a frequentist CI and a Bayesian CrI, and checks whether each contains the true \\(\\beta\\). The prior is centered at 0 (like ridge regression).\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_beta2\", HTML(\"True &beta;:\"),\n                  min = -3, max = 3, value = 1.5, step = 0.5),\n\n      sliderInput(\"prior_sd2\", \"Prior SD:\",\n                  min = 0.1, max = 5, value = 1, step = 0.1),\n\n      sliderInput(\"n2\", \"Sample size (n):\",\n                  min = 5, max = 200, value = 20, step = 5),\n\n      sliderInput(\"n_reps\", \"Repetitions:\",\n                  min = 10, max = 100, value = 40, step = 10),\n\n      actionButton(\"go2\", \"Run experiments\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"interval_plot\", height = \"420px\")),\n        column(6, plotOutput(\"dot_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat2 &lt;- reactive({\n    input$go2\n    true_beta  &lt;- input$true_beta2\n    prior_sd   &lt;- input$prior_sd2\n    n          &lt;- input$n2\n    n_reps     &lt;- input$n_reps\n    sigma      &lt;- 2\n    prior_mean &lt;- 0  # centered at 0, like ridge\n\n    prior_prec &lt;- 1 / prior_sd^2\n\n    # Storage: ols_est, ols_lo, ols_hi, bayes_est, bayes_lo, bayes_hi\n    results &lt;- matrix(NA, nrow = n_reps, ncol = 6)\n\n    for (r in 1:n_reps) {\n      x &lt;- rnorm(n, 0, 1)\n      y &lt;- true_beta * x + rnorm(n, 0, sigma)\n\n      beta_ols &lt;- sum(x * y) / sum(x^2)\n      se_ols   &lt;- sigma / sqrt(sum(x^2))\n\n      data_prec &lt;- sum(x^2) / sigma^2\n      post_prec &lt;- prior_prec + data_prec\n      post_sd   &lt;- 1 / sqrt(post_prec)\n      post_mean &lt;- (prior_mean * prior_prec + beta_ols * data_prec) / post_prec\n\n      results[r, ] &lt;- c(\n        beta_ols,\n        beta_ols - 1.96 * se_ols,\n        beta_ols + 1.96 * se_ols,\n        post_mean,\n        qnorm(0.025, post_mean, post_sd),\n        qnorm(0.975, post_mean, post_sd)\n      )\n    }\n\n    # Coverage\n    ci_covers  &lt;- results[, 2] &lt;= true_beta & results[, 3] &gt;= true_beta\n    cri_covers &lt;- results[, 5] &lt;= true_beta & results[, 6] &gt;= true_beta\n\n    # Average widths\n    ci_width  &lt;- mean(results[, 3] - results[, 2])\n    cri_width &lt;- mean(results[, 6] - results[, 5])\n\n    # MSE\n    mse_ols   &lt;- mean((results[, 1] - true_beta)^2)\n    mse_bayes &lt;- mean((results[, 4] - true_beta)^2)\n\n    # Implied ridge lambda\n    lambda &lt;- sigma^2 / prior_sd^2\n\n    list(results = results, true_beta = true_beta, n_reps = n_reps,\n         ci_covers = ci_covers, cri_covers = cri_covers,\n         ci_width = ci_width, cri_width = cri_width,\n         mse_ols = mse_ols, mse_bayes = mse_bayes, lambda = lambda)\n  })\n\n  output$interval_plot &lt;- renderPlot({\n    d &lt;- dat2()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    n_show &lt;- min(d$n_reps, 40)\n    xlim &lt;- range(c(d$results[1:n_show, 2:3],\n                    d$results[1:n_show, 5:6], d$true_beta)) + c(-0.3, 0.3)\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, n_show + 0.5),\n         xlab = expression(beta), ylab = \"Experiment #\",\n         main = \"95% Intervals Across Experiments\")\n\n    for (i in 1:n_show) {\n      # CI (red)\n      ci_col &lt;- if (d$ci_covers[i]) \"#e74c3c\" else \"#e74c3c40\"\n      segments(d$results[i, 2], i - 0.15, d$results[i, 3], i - 0.15,\n               lwd = 2, col = ci_col)\n\n      # CrI (blue)\n      cri_col &lt;- if (d$cri_covers[i]) \"#3498db\" else \"#3498db40\"\n      segments(d$results[i, 5], i + 0.15, d$results[i, 6], i + 0.15,\n               lwd = 2, col = cri_col)\n    }\n\n    abline(v = d$true_beta, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\n             paste0(\"CI (\", sum(d$ci_covers[1:n_show]), \"/\", n_show, \" cover)\"),\n             paste0(\"CrI (\", sum(d$cri_covers[1:n_show]), \"/\", n_show, \" cover)\")\n           ),\n           col = c(\"#e74c3c\", \"#3498db\"), lwd = 3)\n  })\n\n  output$dot_plot &lt;- renderPlot({\n    d &lt;- dat2()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    n_show &lt;- min(d$n_reps, 40)\n    xlim &lt;- range(c(d$results[1:n_show, 1],\n                    d$results[1:n_show, 4], d$true_beta)) + c(-0.3, 0.3)\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, n_show + 0.5),\n         xlab = expression(hat(beta)), ylab = \"Experiment #\",\n         main = \"Point Estimates Across Experiments\")\n\n    for (i in 1:n_show) {\n      points(d$results[i, 1], i - 0.12, pch = 16, col = \"#e74c3c\", cex = 0.9)\n      points(d$results[i, 4], i + 0.12, pch = 17, col = \"#3498db\", cex = 0.9)\n    }\n\n    abline(v = d$true_beta, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\"OLS estimate\", \"Posterior mean\"),\n           col = c(\"#e74c3c\", \"#3498db\"),\n           pch = c(16, 17))\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- dat2()\n\n    ci_rate  &lt;- round(mean(d$ci_covers) * 100, 1)\n    cri_rate &lt;- round(mean(d$cri_covers) * 100, 1)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;CI coverage:&lt;/b&gt; \", ci_rate, \"%&lt;br&gt;\",\n        \"&lt;b&gt;CrI coverage:&lt;/b&gt; \", cri_rate, \"%&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Avg CI width:&lt;/b&gt; \", round(d$ci_width, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Avg CrI width:&lt;/b&gt; \", round(d$cri_width, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;MSE (OLS):&lt;/b&gt; \", round(d$mse_ols, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;MSE (Bayes):&lt;/b&gt; \", round(d$mse_bayes, 4), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Implied ridge &lambda;:&lt;/b&gt; \", round(d$lambda, 3)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nLarge n (n = 200): both intervals are nearly identical. CI and CrI coverage, width, and MSE all converge. With lots of data, Bayesian = frequentist.\nStrong prior (prior SD = 0.3), true beta = 1.5: the CrI is narrower but the prior pulls estimates toward 0. Some CrIs miss the true value — the coverage drops below 95%. A strong wrong prior hurts.\nVague prior (prior SD = 5): the CrI and CI are virtually the same. The implied ridge \\(\\lambda\\) is tiny — almost no regularization.\nTrue beta = 0, prior SD = 1: the prior is correct! The CrI has excellent coverage and is narrower than the CI. The Bayesian MSE beats OLS because the prior genuinely helps.\nRight panel: notice how the blue dots (Bayesian) cluster tighter around the true value than the red dots (OLS) when the prior is reasonable. That’s the bias-variance tradeoff — a little bias from the prior reduces variance enough to lower overall MSE.",
    "crumbs": [
      "Beyond Conjugates",
      "Bayesian Regression"
    ]
  },
  {
    "objectID": "bayesian-regression.html#in-stata-bayes-reg-y-x",
    "href": "bayesian-regression.html#in-stata-bayes-reg-y-x",
    "title": "Bayesian Regression",
    "section": "In Stata: bayes: reg y x",
    "text": "In Stata: bayes: reg y x\nStata makes Bayesian regression straightforward. Just prepend bayes: to your regression command:\n* Frequentist OLS\nreg wage education experience\n\n* Bayesian regression (default priors)\nbayes: reg wage education experience\n\n* With custom priors\nbayes, prior({wage:education}, normal(1, 0.25)) ///\n      prior({wage:experience}, normal(0.5, 1))  ///\n      : reg wage education experience\nThe output looks different from OLS — instead of a coefficient table with p-values, you get posterior means, standard deviations, and credible intervals. Under the hood, Stata uses the Metropolis-Hastings algorithm (see MCMC) to sample from the posterior.\n\n\n\n\n\n\nTry it: run reg y x and bayes: reg y x on the same dataset. With Stata’s default (vague) priors and moderate sample sizes, the results will be nearly identical — confirming that the prior washes out with enough data.",
    "crumbs": [
      "Beyond Conjugates",
      "Bayesian Regression"
    ]
  },
  {
    "objectID": "bayesian-regression.html#did-you-know",
    "href": "bayesian-regression.html#did-you-know",
    "title": "Bayesian Regression",
    "section": "Did you know?",
    "text": "Did you know?\n\nLindley & Smith (1972) formalized the conjugate Bayesian linear model, showing how hierarchical priors on regression coefficients lead to shrinkage estimators. Their work unified the Bayesian and empirical Bayes approaches to regression.\nHoerl & Kennard (1970) introduced ridge regression as a purely frequentist technique for handling multicollinearity. The Bayesian interpretation — that ridge is equivalent to a normal prior — came later, connecting two literatures that developed independently.\nModern frontiers: Bayesian regression ideas power some of today’s most flexible methods. BART (Bayesian Additive Regression Trees) uses priors on tree structures for nonparametric regression. Bayesian model averaging puts priors on competing models themselves, not just parameters, to account for model uncertainty.",
    "crumbs": [
      "Beyond Conjugates",
      "Bayesian Regression"
    ]
  },
  {
    "objectID": "mcmc.html",
    "href": "mcmc.html",
    "title": "Markov Chain Monte Carlo",
    "section": "",
    "text": "The name breaks down into two parts:\n\nMonte Carlo: using random sampling to approximate something you can’t compute exactly. Instead of solving an integral analytically, you draw random samples and use their average as an approximation. (Named after the Monte Carlo casino — it’s fundamentally about randomness.)\nMarkov Chain: a sequence of random values where each value depends only on the previous one — not on the full history. The “chain” is a random walk through parameter space, where each step proposes a new value based on where you currently are.\n\nPut them together: MCMC constructs a Markov chain whose long-run distribution equals the posterior. Run it long enough, and the samples you collect are (approximately) draws from \\(p(\\theta \\mid y)\\) — even though you never computed that distribution directly.\n\n\n\n\n\n\nThe blindfolded hiker. Imagine you’re blindfolded on a hilly landscape and want to map out the shape of a mountain. You can’t see the whole thing, but at each step you can feel whether the ground goes up or down. You take a step — if it’s uphill (higher probability), you go. If it’s downhill, you might go (sometimes you need to cross valleys to find higher peaks). After thousands of steps, plot everywhere you’ve been on a map — the places you visited most often are the peaks. You’ve mapped the mountain without ever seeing it. That’s MCMC.\n\n\n\n\n\n\n\n\n\nIsn’t this just optimization? It looks like it — both climb hills. But an optimizer (like maximum likelihood in Stata’s reg) only goes uphill, finds the peak, and stops. That gives you one number: the best \\(\\hat\\beta\\). MCMC deliberately goes downhill sometimes (step 4 above: accepting worse proposals with some probability). That’s what lets it explore the full shape of the posterior — not just where the peak is, but how wide, how skewed, and whether there are multiple peaks. Turn off the downhill moves and you’d have a hill-climbing optimizer. Keep them on and you have a sampler.\n\n\n\n\n\nThe chain wanders through parameter space. Early on, the histogram of visited values looks nothing like the target. But as samples accumulate, the histogram converges to the true posterior — the chain has “learned” the distribution just by random walking.\n#| standalone: true\n#| viewerHeight: 480\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 8px; font-size: 13px; line-height: 1.7;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"show_n\", \"Samples to show:\",\n                  min = 10, max = 5000, value = 50, step = 10,\n                  animate = animationOptions(interval = 80, loop = FALSE)),\n\n      actionButton(\"go_intro\", \"New chain\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_intro\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"trace_intro\", height = \"340px\")),\n        column(6, plotOutput(\"hist_intro\", height = \"340px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  chain_data &lt;- reactive({\n    input$go_intro\n\n    # Target: posterior is N(2, 0.5^2) — we pretend we can't compute this\n    target_mu &lt;- 2\n    target_sd &lt;- 0.5\n    log_target &lt;- function(x) dnorm(x, target_mu, target_sd, log = TRUE)\n\n    # Run a long chain once\n    n_total &lt;- 5000\n    chain &lt;- numeric(n_total)\n    chain[1] &lt;- -1  # start far from the target\n    prop_sd &lt;- 0.4\n\n    for (t in 2:n_total) {\n      proposal &lt;- rnorm(1, chain[t - 1], prop_sd)\n      log_r &lt;- log_target(proposal) - log_target(chain[t - 1])\n      if (log(runif(1)) &lt; log_r) {\n        chain[t] &lt;- proposal\n      } else {\n        chain[t] &lt;- chain[t - 1]\n      }\n    }\n\n    list(chain = chain, target_mu = target_mu, target_sd = target_sd)\n  })\n\n  output$trace_intro &lt;- renderPlot({\n    d &lt;- chain_data()\n    n_show &lt;- min(input$show_n, length(d$chain))\n    par(mar = c(4, 4.5, 3, 1))\n\n    plot(1:n_show, d$chain[1:n_show], type = \"l\",\n         col = \"#3498db80\", lwd = 0.6,\n         xlim = c(1, max(200, n_show)),\n         ylim = range(d$chain),\n         xlab = \"Step\", ylab = expression(theta),\n         main = \"The chain explores\")\n\n    # Show current position\n    points(n_show, d$chain[n_show], pch = 19, col = \"#e74c3c\", cex = 1.5)\n\n    abline(h = d$target_mu, lty = 2, col = \"#27ae60\", lwd = 1.5)\n    text(max(200, n_show) * 0.95, d$target_mu,\n         expression(\"Target \" * mu), pos = 3, cex = 0.8, col = \"#27ae60\")\n  })\n\n  output$hist_intro &lt;- renderPlot({\n    d &lt;- chain_data()\n    n_show &lt;- min(input$show_n, length(d$chain))\n    samples &lt;- d$chain[1:n_show]\n    par(mar = c(4, 4.5, 3, 1))\n\n    xlim &lt;- c(d$target_mu - 3 * d$target_sd - 1,\n              d$target_mu + 3 * d$target_sd + 1)\n\n    hist(samples, breaks = seq(xlim[1], xlim[2], length.out = 40),\n         freq = FALSE, col = \"#3498db40\", border = \"#3498db\",\n         xlim = xlim, ylim = c(0, 1.2),\n         xlab = expression(theta), main = \"Histogram vs true posterior\")\n\n    # True target\n    x_seq &lt;- seq(xlim[1], xlim[2], length.out = 300)\n    lines(x_seq, dnorm(x_seq, d$target_mu, d$target_sd),\n          col = \"#27ae60\", lwd = 2.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(paste0(\"MCMC samples (n=\", n_show, \")\"),\n                      \"True posterior\"),\n           col = c(\"#3498db\", \"#27ae60\"),\n           lwd = c(NA, 2.5), pch = c(15, NA), pt.cex = c(1.5, NA))\n  })\n\n  output$results_intro &lt;- renderUI({\n    d &lt;- chain_data()\n    n_show &lt;- min(input$show_n, length(d$chain))\n    samples &lt;- d$chain[1:n_show]\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Samples:&lt;/b&gt; \", n_show, \"&lt;br&gt;\",\n        \"&lt;b&gt;MCMC mean:&lt;/b&gt; \", round(mean(samples), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;True mean:&lt;/b&gt; \", d$target_mu, \"&lt;br&gt;\",\n        \"&lt;b&gt;Error:&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(abs(mean(samples) - d$target_mu), 3), \"&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\nDrag the slider (or hit the play button) and watch:\n\n10 samples: the histogram is jagged, nothing like the green curve. The chain hasn’t explored enough.\n100 samples: the shape starts to emerge. The chain has found the high-probability region.\n1000+ samples: the histogram matches the true posterior almost exactly. The chain has “learned” the distribution through random walking alone.\n\nThe chain never knew the formula for the green curve. It only knew how to evaluate the posterior at any single point and compare “is this new spot better or worse?” That’s enough.",
    "crumbs": [
      "Beyond Conjugates",
      "MCMC"
    ]
  },
  {
    "objectID": "mcmc.html#what-is-mcmc",
    "href": "mcmc.html#what-is-mcmc",
    "title": "Markov Chain Monte Carlo",
    "section": "",
    "text": "The name breaks down into two parts:\n\nMonte Carlo: using random sampling to approximate something you can’t compute exactly. Instead of solving an integral analytically, you draw random samples and use their average as an approximation. (Named after the Monte Carlo casino — it’s fundamentally about randomness.)\nMarkov Chain: a sequence of random values where each value depends only on the previous one — not on the full history. The “chain” is a random walk through parameter space, where each step proposes a new value based on where you currently are.\n\nPut them together: MCMC constructs a Markov chain whose long-run distribution equals the posterior. Run it long enough, and the samples you collect are (approximately) draws from \\(p(\\theta \\mid y)\\) — even though you never computed that distribution directly.\n\n\n\n\n\n\nThe blindfolded hiker. Imagine you’re blindfolded on a hilly landscape and want to map out the shape of a mountain. You can’t see the whole thing, but at each step you can feel whether the ground goes up or down. You take a step — if it’s uphill (higher probability), you go. If it’s downhill, you might go (sometimes you need to cross valleys to find higher peaks). After thousands of steps, plot everywhere you’ve been on a map — the places you visited most often are the peaks. You’ve mapped the mountain without ever seeing it. That’s MCMC.\n\n\n\n\n\n\n\n\n\nIsn’t this just optimization? It looks like it — both climb hills. But an optimizer (like maximum likelihood in Stata’s reg) only goes uphill, finds the peak, and stops. That gives you one number: the best \\(\\hat\\beta\\). MCMC deliberately goes downhill sometimes (step 4 above: accepting worse proposals with some probability). That’s what lets it explore the full shape of the posterior — not just where the peak is, but how wide, how skewed, and whether there are multiple peaks. Turn off the downhill moves and you’d have a hill-climbing optimizer. Keep them on and you have a sampler.\n\n\n\n\n\nThe chain wanders through parameter space. Early on, the histogram of visited values looks nothing like the target. But as samples accumulate, the histogram converges to the true posterior — the chain has “learned” the distribution just by random walking.\n#| standalone: true\n#| viewerHeight: 480\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 8px; font-size: 13px; line-height: 1.7;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"show_n\", \"Samples to show:\",\n                  min = 10, max = 5000, value = 50, step = 10,\n                  animate = animationOptions(interval = 80, loop = FALSE)),\n\n      actionButton(\"go_intro\", \"New chain\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_intro\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"trace_intro\", height = \"340px\")),\n        column(6, plotOutput(\"hist_intro\", height = \"340px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  chain_data &lt;- reactive({\n    input$go_intro\n\n    # Target: posterior is N(2, 0.5^2) — we pretend we can't compute this\n    target_mu &lt;- 2\n    target_sd &lt;- 0.5\n    log_target &lt;- function(x) dnorm(x, target_mu, target_sd, log = TRUE)\n\n    # Run a long chain once\n    n_total &lt;- 5000\n    chain &lt;- numeric(n_total)\n    chain[1] &lt;- -1  # start far from the target\n    prop_sd &lt;- 0.4\n\n    for (t in 2:n_total) {\n      proposal &lt;- rnorm(1, chain[t - 1], prop_sd)\n      log_r &lt;- log_target(proposal) - log_target(chain[t - 1])\n      if (log(runif(1)) &lt; log_r) {\n        chain[t] &lt;- proposal\n      } else {\n        chain[t] &lt;- chain[t - 1]\n      }\n    }\n\n    list(chain = chain, target_mu = target_mu, target_sd = target_sd)\n  })\n\n  output$trace_intro &lt;- renderPlot({\n    d &lt;- chain_data()\n    n_show &lt;- min(input$show_n, length(d$chain))\n    par(mar = c(4, 4.5, 3, 1))\n\n    plot(1:n_show, d$chain[1:n_show], type = \"l\",\n         col = \"#3498db80\", lwd = 0.6,\n         xlim = c(1, max(200, n_show)),\n         ylim = range(d$chain),\n         xlab = \"Step\", ylab = expression(theta),\n         main = \"The chain explores\")\n\n    # Show current position\n    points(n_show, d$chain[n_show], pch = 19, col = \"#e74c3c\", cex = 1.5)\n\n    abline(h = d$target_mu, lty = 2, col = \"#27ae60\", lwd = 1.5)\n    text(max(200, n_show) * 0.95, d$target_mu,\n         expression(\"Target \" * mu), pos = 3, cex = 0.8, col = \"#27ae60\")\n  })\n\n  output$hist_intro &lt;- renderPlot({\n    d &lt;- chain_data()\n    n_show &lt;- min(input$show_n, length(d$chain))\n    samples &lt;- d$chain[1:n_show]\n    par(mar = c(4, 4.5, 3, 1))\n\n    xlim &lt;- c(d$target_mu - 3 * d$target_sd - 1,\n              d$target_mu + 3 * d$target_sd + 1)\n\n    hist(samples, breaks = seq(xlim[1], xlim[2], length.out = 40),\n         freq = FALSE, col = \"#3498db40\", border = \"#3498db\",\n         xlim = xlim, ylim = c(0, 1.2),\n         xlab = expression(theta), main = \"Histogram vs true posterior\")\n\n    # True target\n    x_seq &lt;- seq(xlim[1], xlim[2], length.out = 300)\n    lines(x_seq, dnorm(x_seq, d$target_mu, d$target_sd),\n          col = \"#27ae60\", lwd = 2.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(paste0(\"MCMC samples (n=\", n_show, \")\"),\n                      \"True posterior\"),\n           col = c(\"#3498db\", \"#27ae60\"),\n           lwd = c(NA, 2.5), pch = c(15, NA), pt.cex = c(1.5, NA))\n  })\n\n  output$results_intro &lt;- renderUI({\n    d &lt;- chain_data()\n    n_show &lt;- min(input$show_n, length(d$chain))\n    samples &lt;- d$chain[1:n_show]\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Samples:&lt;/b&gt; \", n_show, \"&lt;br&gt;\",\n        \"&lt;b&gt;MCMC mean:&lt;/b&gt; \", round(mean(samples), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;True mean:&lt;/b&gt; \", d$target_mu, \"&lt;br&gt;\",\n        \"&lt;b&gt;Error:&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(abs(mean(samples) - d$target_mu), 3), \"&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\nDrag the slider (or hit the play button) and watch:\n\n10 samples: the histogram is jagged, nothing like the green curve. The chain hasn’t explored enough.\n100 samples: the shape starts to emerge. The chain has found the high-probability region.\n1000+ samples: the histogram matches the true posterior almost exactly. The chain has “learned” the distribution through random walking alone.\n\nThe chain never knew the formula for the green curve. It only knew how to evaluate the posterior at any single point and compare “is this new spot better or worse?” That’s enough.",
    "crumbs": [
      "Beyond Conjugates",
      "MCMC"
    ]
  },
  {
    "objectID": "mcmc.html#why-do-we-need-it",
    "href": "mcmc.html#why-do-we-need-it",
    "title": "Markov Chain Monte Carlo",
    "section": "Why do we need it?",
    "text": "Why do we need it?\nOn the Priors & Posteriors page, we used conjugate priors — special prior-likelihood pairs where the posterior has a closed-form solution. That’s elegant but limiting. Most real models don’t have conjugate posteriors:\n\nLogistic regression with a prior on coefficients\nHierarchical models with multiple levels of parameters\nAny model where the posterior \\(p(\\theta \\mid y) \\propto p(y \\mid \\theta) \\, p(\\theta)\\) doesn’t simplify to a known distribution\n\nFor these models, we can’t write down the posterior analytically. We need to sample from it numerically. That’s what MCMC does.",
    "crumbs": [
      "Beyond Conjugates",
      "MCMC"
    ]
  },
  {
    "objectID": "mcmc.html#the-metropolis-hastings-algorithm",
    "href": "mcmc.html#the-metropolis-hastings-algorithm",
    "title": "Markov Chain Monte Carlo",
    "section": "The Metropolis-Hastings algorithm",
    "text": "The Metropolis-Hastings algorithm\nThe most foundational MCMC algorithm. The core idea is beautifully simple:\n\nStart at some value \\(\\theta_0\\).\nPropose a new value \\(\\theta^*\\) from a proposal distribution \\(q(\\theta^* \\mid \\theta_t)\\) — typically \\(\\theta^* \\sim N(\\theta_t, \\sigma_{prop}^2)\\).\nCompute the acceptance ratio: \\[\\alpha = \\min\\left(1, \\, \\frac{p(\\theta^* \\mid y) \\; q(\\theta_t \\mid \\theta^*)}{p(\\theta_t \\mid y) \\; q(\\theta^* \\mid \\theta_t)}\\right)\\]\nAccept \\(\\theta^*\\) with probability \\(\\alpha\\) (set \\(\\theta_{t+1} = \\theta^*\\)). Otherwise stay: \\(\\theta_{t+1} = \\theta_t\\).\nRepeat.\n\nWhen the proposal is symmetric — e.g. \\(q(\\theta^* \\mid \\theta_t) = N(\\theta_t, \\sigma_{prop}^2)\\) — the proposal ratio \\(q(\\theta_t \\mid \\theta^*) / q(\\theta^* \\mid \\theta_t) = 1\\) and the acceptance ratio simplifies to \\(\\alpha = \\min\\!\\left(1,\\; p(\\theta^* \\mid y) \\,/\\, p(\\theta_t \\mid y)\\right)\\). This is the original Metropolis algorithm. The Hastings (1970) generalization adds the proposal ratio so that asymmetric proposals can be used.\nKey insight: you never need to compute the normalizing constant \\(p(y) = \\int p(y \\mid \\theta) \\, p(\\theta) \\, d\\theta\\). The ratio \\(p(\\theta^* \\mid y) / p(\\theta_t \\mid y)\\) cancels it out. You only need to evaluate the unnormalized posterior — the numerator of Bayes’ theorem.\nAfter enough iterations, the chain converges to the posterior distribution. The samples \\(\\theta_1, \\theta_2, \\ldots\\) are (correlated) draws from \\(p(\\theta \\mid y)\\).\n\nSimulation 1: Metropolis-Hastings in action\nEstimate the mean \\(\\mu\\) of normally distributed data with unknown true value. The proposal width controls exploration: too narrow and the chain moves slowly; too wide and most proposals get rejected.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_mu\", HTML(\"True &mu;:\"),\n                  min = -3, max = 3, value = 1, step = 0.5),\n\n      sliderInput(\"n_data\", \"Data points:\",\n                  min = 5, max = 100, value = 20, step = 5),\n\n      sliderInput(\"prop_sd\", \"Proposal width (SD):\",\n                  min = 0.05, max = 5, value = 0.5, step = 0.05),\n\n      sliderInput(\"n_iter\", \"Iterations:\",\n                  min = 500, max = 5000, value = 2000, step = 500),\n\n      actionButton(\"go\", \"Run chain\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"trace_plot\", height = \"420px\")),\n        column(6, plotOutput(\"hist_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    true_mu  &lt;- input$true_mu\n    n_data   &lt;- input$n_data\n    prop_sd  &lt;- input$prop_sd\n    n_iter   &lt;- input$n_iter\n    sigma    &lt;- 2  # known SD\n\n    # Generate data\n    y &lt;- rnorm(n_data, mean = true_mu, sd = sigma)\n\n    # Log posterior (unnormalized): normal likelihood + flat prior\n    log_post &lt;- function(mu) {\n      sum(dnorm(y, mean = mu, sd = sigma, log = TRUE))\n    }\n\n    # Metropolis-Hastings\n    chain &lt;- numeric(n_iter)\n    chain[1] &lt;- 0  # start at 0\n    accepted &lt;- 0\n\n    for (t in 2:n_iter) {\n      proposal &lt;- rnorm(1, mean = chain[t - 1], sd = prop_sd)\n      log_ratio &lt;- log_post(proposal) - log_post(chain[t - 1])\n\n      if (log(runif(1)) &lt; log_ratio) {\n        chain[t] &lt;- proposal\n        accepted &lt;- accepted + 1\n      } else {\n        chain[t] &lt;- chain[t - 1]\n      }\n    }\n\n    accept_rate &lt;- accepted / (n_iter - 1)\n\n    # Analytic posterior for comparison (conjugate: flat prior + normal)\n    post_mean &lt;- mean(y)\n    post_sd   &lt;- sigma / sqrt(n_data)\n\n    list(chain = chain, accept_rate = accept_rate,\n         true_mu = true_mu, post_mean = post_mean, post_sd = post_sd,\n         n_iter = n_iter, prop_sd = prop_sd)\n  })\n\n  output$trace_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$chain, type = \"l\", col = \"#3498db80\", lwd = 0.5,\n         xlab = \"Iteration\", ylab = expression(mu),\n         main = \"Trace Plot\")\n\n    abline(h = d$true_mu, lty = 2, col = \"#e74c3c\", lwd = 2)\n    abline(h = d$post_mean, lty = 3, col = \"#27ae60\", lwd = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(expression(\"True \" * mu),\n                      \"Posterior mean (analytic)\"),\n           col = c(\"#e74c3c\", \"#27ae60\"),\n           lty = c(2, 3), lwd = 2)\n  })\n\n  output$hist_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Discard first 20% as burn-in\n    burnin &lt;- floor(d$n_iter * 0.2)\n    samples &lt;- d$chain[(burnin + 1):d$n_iter]\n\n    hist(samples, breaks = 40, freq = FALSE,\n         col = \"#3498db40\", border = \"#3498db\",\n         xlab = expression(mu), main = \"Posterior Distribution\",\n         xlim = range(c(samples, d$true_mu - 0.5, d$true_mu + 0.5)))\n\n    # Analytic posterior\n    x_seq &lt;- seq(min(samples) - 0.5, max(samples) + 0.5, length.out = 200)\n    lines(x_seq, dnorm(x_seq, d$post_mean, d$post_sd),\n          col = \"#27ae60\", lwd = 2.5)\n\n    abline(v = d$true_mu, lty = 2, col = \"#e74c3c\", lwd = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\"MCMC samples\", \"Analytic posterior\",\n                      expression(\"True \" * mu)),\n           col = c(\"#3498db\", \"#27ae60\", \"#e74c3c\"),\n           lwd = c(NA, 2.5, 2), lty = c(NA, 1, 2),\n           pch = c(15, NA, NA), pt.cex = c(1.5, NA, NA))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    burnin &lt;- floor(d$n_iter * 0.2)\n    samples &lt;- d$chain[(burnin + 1):d$n_iter]\n\n    rate_class &lt;- if (d$accept_rate &gt; 0.15 && d$accept_rate &lt; 0.5) \"good\" else \"bad\"\n    rate_note &lt;- if (d$accept_rate &lt; 0.15) {\n      \"Too low — proposal too wide\"\n    } else if (d$accept_rate &gt; 0.5) {\n      \"Too high — proposal too narrow\"\n    } else {\n      \"Good range (0.15-0.50)\"\n    }\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Acceptance rate:&lt;/b&gt; &lt;span class='\", rate_class, \"'&gt;\",\n        round(d$accept_rate * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;small&gt;\", rate_note, \"&lt;/small&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;MCMC posterior mean:&lt;/b&gt; \", round(mean(samples), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Analytic posterior mean:&lt;/b&gt; \", round(d$post_mean, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;True &mu;:&lt;/b&gt; \", d$true_mu, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Proposal width:&lt;/b&gt; \", d$prop_sd\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nProposal width = 0.5: a well-tuned chain. The trace plot shows good mixing (bouncing around the posterior), and the histogram matches the analytic posterior (green curve). Acceptance rate is in the sweet spot (20–40%).\nProposal width = 0.05: too narrow. The chain takes tiny steps — the trace plot shows slow, random-walk behavior. Acceptance rate is near 100% (almost every proposal is accepted because it’s barely different). The chain explores the posterior very slowly.\nProposal width = 5: too wide. Most proposals jump far from the current value and land in low-probability regions — they get rejected. The trace plot shows long flat stretches (the chain is stuck). Acceptance rate is very low.\nThe goldilocks principle: you want a proposal width that’s “just right” — large enough to explore, small enough to get accepted. The theoretical optimum for 1D is an acceptance rate around 44% (Roberts et al., 1997).",
    "crumbs": [
      "Beyond Conjugates",
      "MCMC"
    ]
  },
  {
    "objectID": "mcmc.html#burn-in-and-convergence",
    "href": "mcmc.html#burn-in-and-convergence",
    "title": "Markov Chain Monte Carlo",
    "section": "Burn-in and convergence",
    "text": "Burn-in and convergence\nA practical concern: the chain starts at an arbitrary value (\\(\\theta_0 = 0\\) above). The early samples reflect the starting point, not the posterior. You need to discard these initial samples — the “burn-in” period.\nHow do you know the chain has converged? Run multiple chains from different starting points. If they all end up exploring the same region, you have evidence of convergence. If they’re stuck in different places, the chains haven’t converged and you need more iterations.\n\nSimulation 2: Multiple chains and convergence\n#| standalone: true\n#| viewerHeight: 580\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_iter2\", \"Iterations:\",\n                  min = 200, max = 3000, value = 1000, step = 200),\n\n      sliderInput(\"burnin\", \"Burn-in (discard first %):\",\n                  min = 0, max = 50, value = 20, step = 5),\n\n      sliderInput(\"prop_sd2\", \"Proposal width:\",\n                  min = 0.1, max = 3, value = 0.5, step = 0.1),\n\n      actionButton(\"go2\", \"Run chains\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"multi_trace\", height = \"420px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat2 &lt;- reactive({\n    input$go2\n    n_iter  &lt;- input$n_iter2\n    burnin  &lt;- input$burnin / 100\n    prop_sd &lt;- input$prop_sd2\n    sigma   &lt;- 2\n    true_mu &lt;- 1.5\n\n    # Generate data once\n    y &lt;- rnorm(30, mean = true_mu, sd = sigma)\n\n    log_post &lt;- function(mu) {\n      sum(dnorm(y, mean = mu, sd = sigma, log = TRUE))\n    }\n\n    # Run 4 chains from different starting points\n    starts &lt;- c(-5, -2, 4, 7)\n    chain_cols &lt;- c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"#f39c12\")\n    chains &lt;- matrix(0, nrow = n_iter, ncol = 4)\n\n    for (ch in 1:4) {\n      chains[1, ch] &lt;- starts[ch]\n      for (t in 2:n_iter) {\n        proposal &lt;- rnorm(1, chains[t - 1, ch], prop_sd)\n        log_r &lt;- log_post(proposal) - log_post(chains[t - 1, ch])\n        if (log(runif(1)) &lt; log_r) {\n          chains[t, ch] &lt;- proposal\n        } else {\n          chains[t, ch] &lt;- chains[t - 1, ch]\n        }\n      }\n    }\n\n    # Posterior (analytic)\n    post_mean &lt;- mean(y)\n\n    list(chains = chains, starts = starts, chain_cols = chain_cols,\n         n_iter = n_iter, burnin = burnin, true_mu = true_mu,\n         post_mean = post_mean)\n  })\n\n  output$multi_trace &lt;- renderPlot({\n    d &lt;- dat2()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    burnin_line &lt;- floor(d$n_iter * d$burnin)\n\n    ylim &lt;- range(d$chains)\n    plot(NULL, xlim = c(1, d$n_iter), ylim = ylim,\n         xlab = \"Iteration\", ylab = expression(mu),\n         main = \"Four Chains from Different Starting Points\")\n\n    # Shade burn-in region\n    if (burnin_line &gt; 0) {\n      rect(0, ylim[1] - 1, burnin_line, ylim[2] + 1,\n           col = \"#f0f0f080\", border = NA)\n      abline(v = burnin_line, lty = 2, col = \"gray40\", lwd = 1.5)\n      text(burnin_line, ylim[2], \"burn-in\", pos = 2,\n           cex = 0.8, col = \"gray40\")\n    }\n\n    for (ch in 1:4) {\n      lines(d$chains[, ch], col = paste0(d$chain_cols[ch], \"90\"),\n            lwd = 0.8)\n    }\n\n    abline(h = d$true_mu, lty = 2, col = \"#2c3e50\", lwd = 2)\n    text(d$n_iter * 0.98, d$true_mu, expression(\"True \" * mu),\n         pos = 3, cex = 0.85, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.75,\n           legend = paste0(\"Chain \", 1:4, \" (start = \", d$starts, \")\"),\n           col = d$chain_cols, lwd = 2)\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- dat2()\n    burnin_n &lt;- floor(d$n_iter * d$burnin)\n\n    # Post-burnin means per chain\n    if (burnin_n &lt; d$n_iter) {\n      post_samples &lt;- d$chains[(burnin_n + 1):d$n_iter, ]\n      chain_means &lt;- round(colMeans(post_samples), 3)\n      spread &lt;- round(max(chain_means) - min(chain_means), 3)\n    } else {\n      chain_means &lt;- rep(NA, 4)\n      spread &lt;- NA\n    }\n\n    converged &lt;- !is.na(spread) && spread &lt; 0.3\n    conv_class &lt;- if (converged) \"good\" else \"bad\"\n    conv_label &lt;- if (converged) \"Chains agree\" else \"Chains disagree\"\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Post-burn-in means:&lt;/b&gt;&lt;br&gt;\",\n        paste0(\"Chain \", 1:4, \": \", chain_means, collapse = \"&lt;br&gt;\"), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Spread:&lt;/b&gt; &lt;span class='\", conv_class, \"'&gt;\",\n        spread, \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;span class='\", conv_class, \"'&gt;\", conv_label, \"&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nDefault settings (1000 iterations, burn-in = 20%): all four chains start at different values (-5, -2, 4, 7) but converge to the same region within ~100 iterations. After burn-in, all chain means agree. This is convergence.\nBurn-in = 0%: the early samples (from the starting points) contaminate the posterior. The chain means diverge because each chain’s average is pulled toward its start.\nProposal width = 0.1: very slow exploration. The chains take longer to converge — you can see them creeping slowly toward the true value. With only 1000 iterations, they might not fully converge. Increase iterations to fix this.\nProposal width = 3: the chains converge quickly but the trace plot shows many flat stretches (rejected proposals). The posterior is explored but inefficiently.",
    "crumbs": [
      "Beyond Conjugates",
      "MCMC"
    ]
  },
  {
    "objectID": "mcmc.html#the-key-message",
    "href": "mcmc.html#the-key-message",
    "title": "Markov Chain Monte Carlo",
    "section": "The key message",
    "text": "The key message\nMCMC lets you compute posteriors for any model — not just conjugate ones. Specify the likelihood and the prior, and the algorithm samples from the posterior. This is what makes Bayesian inference practical for real-world problems like hierarchical models, where closed-form posteriors don’t exist.\nThis is also the engine behind Bayesian regression. When you type bayes: reg y x in Stata, it runs Metropolis-Hastings to sample from the posterior over regression coefficients — the same algorithm you just watched above, applied to a model you already know.\nModern tools like Stan, JAGS, and PyMC automate this — you specify the model and they handle the sampling. But understanding the basics (proposal tuning, burn-in, convergence checks) helps you diagnose problems when things go wrong.",
    "crumbs": [
      "Beyond Conjugates",
      "MCMC"
    ]
  },
  {
    "objectID": "mcmc.html#did-you-know",
    "href": "mcmc.html#did-you-know",
    "title": "Markov Chain Monte Carlo",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe Metropolis algorithm was developed at Los Alamos National Laboratory by Nicholas Metropolis, Arianna Rosenbluth, Marshall Rosenbluth, Augusta Teller, and Edward Teller (1953) — originally for simulating equations of state in statistical mechanics, not statistics. W.K. Hastings (1970) generalized it to asymmetric proposal distributions.\nMCMC was named one of the top 10 algorithms of the 20th century by Computing in Science & Engineering (2000). It’s used across physics, chemistry, biology, statistics, and machine learning.\nModern Bayesian computation has largely moved beyond basic Metropolis-Hastings to Hamiltonian Monte Carlo (HMC) and its adaptive variant NUTS (Hoffman & Gelman, 2014). HMC uses gradient information to make smarter proposals, dramatically improving efficiency in high-dimensional problems. This is what Stan uses under the hood.",
    "crumbs": [
      "Beyond Conjugates",
      "MCMC"
    ]
  },
  {
    "objectID": "bayes-vs-freq.html",
    "href": "bayes-vs-freq.html",
    "title": "Bayesian vs Frequentist",
    "section": "",
    "text": "Bayesian and frequentist statistics look at the same data but ask different questions:\n\n\n\n\n\n\n\n\n\nFrequentist\nBayesian\n\n\n\n\nParameters are…\nFixed but unknown\nRandom variables with distributions\n\n\nProbability means…\nLong-run frequency\nDegree of belief\n\n\nResult\nPoint estimate + confidence interval\nFull posterior distribution\n\n\n“There’s a 95% chance…”\n…that this procedure captures the true value\n…that the true value is in this interval\n\n\n\nThe frequentist says: “If I repeated this experiment forever, 95% of my CIs would contain the true value.” The Bayesian says: “Given what I’ve seen, I’m 95% sure the true value is in this range.”\nMost people actually think like Bayesians (“what’s the probability the parameter is between A and B?”) but compute like frequentists (p-values, CIs).\n\n\nThe simulation below runs the same experiment and shows both the frequentist confidence interval and the Bayesian credible interval. Watch how they differ — especially with small samples and informative priors.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_mu\", HTML(\"True &mu;:\"),\n                  min = -3, max = 3, value = 1, step = 0.5),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 2, max = 200, value = 10, step = 1),\n\n      sliderInput(\"prior_mu\", \"Bayesian prior mean:\",\n                  min = -3, max = 3, value = 0, step = 0.5),\n\n      sliderInput(\"prior_sd\", \"Prior SD:\",\n                  min = 0.5, max = 10, value = 2, step = 0.5),\n\n      actionButton(\"go\", \"New experiment\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"interval_plot\", height = \"420px\")),\n        column(6, plotOutput(\"repeat_plot\",   height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    true_mu  &lt;- input$true_mu\n    n        &lt;- input$n\n    prior_mu &lt;- input$prior_mu\n    prior_sd &lt;- input$prior_sd\n    sigma    &lt;- 2\n\n    y &lt;- rnorm(n, mean = true_mu, sd = sigma)\n    y_bar &lt;- mean(y)\n    se &lt;- sigma / sqrt(n)\n\n    # Frequentist 95% CI\n    freq_lo &lt;- y_bar - 1.96 * se\n    freq_hi &lt;- y_bar + 1.96 * se\n\n    # Bayesian posterior\n    prior_prec &lt;- 1 / prior_sd^2\n    data_prec  &lt;- n / sigma^2\n    post_prec  &lt;- prior_prec + data_prec\n    post_sd    &lt;- 1 / sqrt(post_prec)\n    post_mu    &lt;- (prior_prec * prior_mu + data_prec * y_bar) / post_prec\n\n    bayes_lo &lt;- qnorm(0.025, post_mu, post_sd)\n    bayes_hi &lt;- qnorm(0.975, post_mu, post_sd)\n\n    # Repeated experiments for right panel\n    k &lt;- 50\n    reps &lt;- t(replicate(k, {\n      yy &lt;- rnorm(n, mean = true_mu, sd = sigma)\n      yy_bar &lt;- mean(yy)\n      f_lo &lt;- yy_bar - 1.96 * se\n      f_hi &lt;- yy_bar + 1.96 * se\n\n      d_prec &lt;- n / sigma^2\n      p_prec &lt;- prior_prec + d_prec\n      p_sd &lt;- 1 / sqrt(p_prec)\n      p_mu &lt;- (prior_prec * prior_mu + d_prec * yy_bar) / p_prec\n      b_lo &lt;- qnorm(0.025, p_mu, p_sd)\n      b_hi &lt;- qnorm(0.975, p_mu, p_sd)\n\n      c(yy_bar, f_lo, f_hi, p_mu, b_lo, b_hi)\n    }))\n\n    list(true_mu = true_mu, y_bar = y_bar,\n         freq_lo = freq_lo, freq_hi = freq_hi,\n         post_mu = post_mu, post_sd = post_sd,\n         bayes_lo = bayes_lo, bayes_hi = bayes_hi,\n         reps = reps, prior_mu = prior_mu)\n  })\n\n  output$interval_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 8, 3, 1))\n    xlim &lt;- range(c(d$freq_lo, d$freq_hi, d$bayes_lo, d$bayes_hi, d$true_mu)) +\n            c(-0.5, 0.5)\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, 2.5),\n         yaxt = \"n\", ylab = \"\", xlab = expression(mu),\n         main = \"This Experiment\")\n    axis(2, at = 1:2, labels = c(\"Frequentist\\n95% CI\", \"Bayesian\\n95% CrI\"),\n         las = 1, cex.axis = 0.85)\n\n    # Frequentist\n    segments(d$freq_lo, 1, d$freq_hi, 1, lwd = 4, col = \"#e74c3c\")\n    points(d$y_bar, 1, pch = 19, cex = 1.5, col = \"#e74c3c\")\n\n    # Bayesian\n    segments(d$bayes_lo, 2, d$bayes_hi, 2, lwd = 4, col = \"#3498db\")\n    points(d$post_mu, 2, pch = 19, cex = 1.5, col = \"#3498db\")\n\n    # True value\n    abline(v = d$true_mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n    text(d$true_mu, 2.4, expression(\"True \" * mu), cex = 0.9, col = \"#2c3e50\")\n  })\n\n  output$repeat_plot &lt;- renderPlot({\n    d &lt;- dat()\n    k &lt;- nrow(d$reps)\n\n    par(mar = c(4.5, 4, 3, 1))\n\n    freq_covers &lt;- d$reps[, 2] &lt;= d$true_mu & d$reps[, 3] &gt;= d$true_mu\n    bayes_covers &lt;- d$reps[, 5] &lt;= d$true_mu & d$reps[, 6] &gt;= d$true_mu\n\n    xlim &lt;- range(d$reps[, 2:6], d$true_mu) + c(-0.5, 0.5)\n\n    plot(NULL, xlim = xlim, ylim = c(1, k),\n         xlab = expression(mu), ylab = \"Experiment #\",\n         main = paste0(k, \" repeated experiments\"))\n\n    for (i in seq_len(k)) {\n      # Frequentist (left-shifted slightly)\n      clr_f &lt;- if (freq_covers[i]) \"#e74c3c\" else \"#e74c3c40\"\n      segments(d$reps[i, 2], i - 0.15, d$reps[i, 3], i - 0.15,\n               lwd = 1.5, col = clr_f)\n\n      # Bayesian (right-shifted slightly)\n      clr_b &lt;- if (bayes_covers[i]) \"#3498db\" else \"#3498db40\"\n      segments(d$reps[i, 5], i + 0.15, d$reps[i, 6], i + 0.15,\n               lwd = 1.5, col = clr_b)\n    }\n\n    abline(v = d$true_mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\n             paste0(\"Freq CI (\", sum(freq_covers), \"/\", k, \" cover)\"),\n             paste0(\"Bayes CrI (\", sum(bayes_covers), \"/\", k, \" cover)\")\n           ),\n           col = c(\"#e74c3c\", \"#3498db\"), lwd = 3)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Frequentist:&lt;/b&gt;&lt;br&gt;\",\n        \"Estimate: \", round(d$y_bar, 3), \"&lt;br&gt;\",\n        \"95% CI: [\", round(d$freq_lo, 3), \", \", round(d$freq_hi, 3), \"]&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Bayesian:&lt;/b&gt;&lt;br&gt;\",\n        \"Posterior mean: \", round(d$post_mu, 3), \"&lt;br&gt;\",\n        \"95% CrI: [\", round(d$bayes_lo, 3), \", \", round(d$bayes_hi, 3), \"]&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;CrI is narrower because the prior adds information.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nn = 5, prior centered at 0, true mu = 1: the Bayesian CrI is narrower but pulled toward 0 (shrinkage). The frequentist CI is wider but centered on the data.\nn = 200: both intervals are nearly identical. With lots of data, the prior washes out and Bayesian = frequentist.\nSet a wrong prior (prior mean = -3, true mu = 2, n = 5): the Bayesian interval gets pulled toward -3. A bad prior hurts with small samples. Slide n up — the data corrects it.\nRight panel: the frequentist CI is designed so that ~95% of the red intervals cover the truth across repetitions. The Bayesian CrI coverage depends on how good the prior is.\n\n\n\n\n\n\n\n\n\n\n\nUse frequentist when…\nUse Bayesian when…\n\n\n\n\nYou want procedure guarantees (coverage)\nYou want direct probability statements\n\n\nYou have no prior information\nYou have real prior knowledge\n\n\nRegulatory/peer review expects it\nSmall samples, need to borrow strength\n\n\nSimple problems\nComplex hierarchical models\n\n\n\nIn practice, most applied researchers use frequentist methods but interpret them like Bayesians. Understanding both helps you know what your numbers actually mean.",
    "crumbs": [
      "Bayesian vs Frequentist"
    ]
  },
  {
    "objectID": "bayes-vs-freq.html#same-data-different-questions",
    "href": "bayes-vs-freq.html#same-data-different-questions",
    "title": "Bayesian vs Frequentist",
    "section": "",
    "text": "Bayesian and frequentist statistics look at the same data but ask different questions:\n\n\n\n\n\n\n\n\n\nFrequentist\nBayesian\n\n\n\n\nParameters are…\nFixed but unknown\nRandom variables with distributions\n\n\nProbability means…\nLong-run frequency\nDegree of belief\n\n\nResult\nPoint estimate + confidence interval\nFull posterior distribution\n\n\n“There’s a 95% chance…”\n…that this procedure captures the true value\n…that the true value is in this interval\n\n\n\nThe frequentist says: “If I repeated this experiment forever, 95% of my CIs would contain the true value.” The Bayesian says: “Given what I’ve seen, I’m 95% sure the true value is in this range.”\nMost people actually think like Bayesians (“what’s the probability the parameter is between A and B?”) but compute like frequentists (p-values, CIs).\n\n\nThe simulation below runs the same experiment and shows both the frequentist confidence interval and the Bayesian credible interval. Watch how they differ — especially with small samples and informative priors.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_mu\", HTML(\"True &mu;:\"),\n                  min = -3, max = 3, value = 1, step = 0.5),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 2, max = 200, value = 10, step = 1),\n\n      sliderInput(\"prior_mu\", \"Bayesian prior mean:\",\n                  min = -3, max = 3, value = 0, step = 0.5),\n\n      sliderInput(\"prior_sd\", \"Prior SD:\",\n                  min = 0.5, max = 10, value = 2, step = 0.5),\n\n      actionButton(\"go\", \"New experiment\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"interval_plot\", height = \"420px\")),\n        column(6, plotOutput(\"repeat_plot\",   height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    true_mu  &lt;- input$true_mu\n    n        &lt;- input$n\n    prior_mu &lt;- input$prior_mu\n    prior_sd &lt;- input$prior_sd\n    sigma    &lt;- 2\n\n    y &lt;- rnorm(n, mean = true_mu, sd = sigma)\n    y_bar &lt;- mean(y)\n    se &lt;- sigma / sqrt(n)\n\n    # Frequentist 95% CI\n    freq_lo &lt;- y_bar - 1.96 * se\n    freq_hi &lt;- y_bar + 1.96 * se\n\n    # Bayesian posterior\n    prior_prec &lt;- 1 / prior_sd^2\n    data_prec  &lt;- n / sigma^2\n    post_prec  &lt;- prior_prec + data_prec\n    post_sd    &lt;- 1 / sqrt(post_prec)\n    post_mu    &lt;- (prior_prec * prior_mu + data_prec * y_bar) / post_prec\n\n    bayes_lo &lt;- qnorm(0.025, post_mu, post_sd)\n    bayes_hi &lt;- qnorm(0.975, post_mu, post_sd)\n\n    # Repeated experiments for right panel\n    k &lt;- 50\n    reps &lt;- t(replicate(k, {\n      yy &lt;- rnorm(n, mean = true_mu, sd = sigma)\n      yy_bar &lt;- mean(yy)\n      f_lo &lt;- yy_bar - 1.96 * se\n      f_hi &lt;- yy_bar + 1.96 * se\n\n      d_prec &lt;- n / sigma^2\n      p_prec &lt;- prior_prec + d_prec\n      p_sd &lt;- 1 / sqrt(p_prec)\n      p_mu &lt;- (prior_prec * prior_mu + d_prec * yy_bar) / p_prec\n      b_lo &lt;- qnorm(0.025, p_mu, p_sd)\n      b_hi &lt;- qnorm(0.975, p_mu, p_sd)\n\n      c(yy_bar, f_lo, f_hi, p_mu, b_lo, b_hi)\n    }))\n\n    list(true_mu = true_mu, y_bar = y_bar,\n         freq_lo = freq_lo, freq_hi = freq_hi,\n         post_mu = post_mu, post_sd = post_sd,\n         bayes_lo = bayes_lo, bayes_hi = bayes_hi,\n         reps = reps, prior_mu = prior_mu)\n  })\n\n  output$interval_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 8, 3, 1))\n    xlim &lt;- range(c(d$freq_lo, d$freq_hi, d$bayes_lo, d$bayes_hi, d$true_mu)) +\n            c(-0.5, 0.5)\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, 2.5),\n         yaxt = \"n\", ylab = \"\", xlab = expression(mu),\n         main = \"This Experiment\")\n    axis(2, at = 1:2, labels = c(\"Frequentist\\n95% CI\", \"Bayesian\\n95% CrI\"),\n         las = 1, cex.axis = 0.85)\n\n    # Frequentist\n    segments(d$freq_lo, 1, d$freq_hi, 1, lwd = 4, col = \"#e74c3c\")\n    points(d$y_bar, 1, pch = 19, cex = 1.5, col = \"#e74c3c\")\n\n    # Bayesian\n    segments(d$bayes_lo, 2, d$bayes_hi, 2, lwd = 4, col = \"#3498db\")\n    points(d$post_mu, 2, pch = 19, cex = 1.5, col = \"#3498db\")\n\n    # True value\n    abline(v = d$true_mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n    text(d$true_mu, 2.4, expression(\"True \" * mu), cex = 0.9, col = \"#2c3e50\")\n  })\n\n  output$repeat_plot &lt;- renderPlot({\n    d &lt;- dat()\n    k &lt;- nrow(d$reps)\n\n    par(mar = c(4.5, 4, 3, 1))\n\n    freq_covers &lt;- d$reps[, 2] &lt;= d$true_mu & d$reps[, 3] &gt;= d$true_mu\n    bayes_covers &lt;- d$reps[, 5] &lt;= d$true_mu & d$reps[, 6] &gt;= d$true_mu\n\n    xlim &lt;- range(d$reps[, 2:6], d$true_mu) + c(-0.5, 0.5)\n\n    plot(NULL, xlim = xlim, ylim = c(1, k),\n         xlab = expression(mu), ylab = \"Experiment #\",\n         main = paste0(k, \" repeated experiments\"))\n\n    for (i in seq_len(k)) {\n      # Frequentist (left-shifted slightly)\n      clr_f &lt;- if (freq_covers[i]) \"#e74c3c\" else \"#e74c3c40\"\n      segments(d$reps[i, 2], i - 0.15, d$reps[i, 3], i - 0.15,\n               lwd = 1.5, col = clr_f)\n\n      # Bayesian (right-shifted slightly)\n      clr_b &lt;- if (bayes_covers[i]) \"#3498db\" else \"#3498db40\"\n      segments(d$reps[i, 5], i + 0.15, d$reps[i, 6], i + 0.15,\n               lwd = 1.5, col = clr_b)\n    }\n\n    abline(v = d$true_mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\n             paste0(\"Freq CI (\", sum(freq_covers), \"/\", k, \" cover)\"),\n             paste0(\"Bayes CrI (\", sum(bayes_covers), \"/\", k, \" cover)\")\n           ),\n           col = c(\"#e74c3c\", \"#3498db\"), lwd = 3)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Frequentist:&lt;/b&gt;&lt;br&gt;\",\n        \"Estimate: \", round(d$y_bar, 3), \"&lt;br&gt;\",\n        \"95% CI: [\", round(d$freq_lo, 3), \", \", round(d$freq_hi, 3), \"]&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Bayesian:&lt;/b&gt;&lt;br&gt;\",\n        \"Posterior mean: \", round(d$post_mu, 3), \"&lt;br&gt;\",\n        \"95% CrI: [\", round(d$bayes_lo, 3), \", \", round(d$bayes_hi, 3), \"]&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;CrI is narrower because the prior adds information.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nn = 5, prior centered at 0, true mu = 1: the Bayesian CrI is narrower but pulled toward 0 (shrinkage). The frequentist CI is wider but centered on the data.\nn = 200: both intervals are nearly identical. With lots of data, the prior washes out and Bayesian = frequentist.\nSet a wrong prior (prior mean = -3, true mu = 2, n = 5): the Bayesian interval gets pulled toward -3. A bad prior hurts with small samples. Slide n up — the data corrects it.\nRight panel: the frequentist CI is designed so that ~95% of the red intervals cover the truth across repetitions. The Bayesian CrI coverage depends on how good the prior is.\n\n\n\n\n\n\n\n\n\n\n\nUse frequentist when…\nUse Bayesian when…\n\n\n\n\nYou want procedure guarantees (coverage)\nYou want direct probability statements\n\n\nYou have no prior information\nYou have real prior knowledge\n\n\nRegulatory/peer review expects it\nSmall samples, need to borrow strength\n\n\nSimple problems\nComplex hierarchical models\n\n\n\nIn practice, most applied researchers use frequentist methods but interpret them like Bayesians. Understanding both helps you know what your numbers actually mean.",
    "crumbs": [
      "Bayesian vs Frequentist"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Thinking",
    "section": "",
    "text": "Bayesian inference — updating beliefs with data. Start with a prior, observe data, get a posterior. Just the core logic, with simulations.\nBuilds on: Statistical Inference",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#the-workflow",
    "href": "index.html#the-workflow",
    "title": "Bayesian Thinking",
    "section": "The workflow",
    "text": "The workflow\nEvery Bayesian analysis follows the same loop:\n\n\n\nStep\nWhat you do\nWhere to learn it\n\n\n\n\n1. Write down the likelihood\nWhat’s your data model? Normal, binomial, Poisson?\nBayes’ Theorem — shows how the likelihood plugs into the updating formula\n\n\n2. Choose a prior\nWhat do you believe before seeing data? Vague or informative?\nPriors & Posteriors — watch different priors get updated by data\n\n\n3. Compute the posterior\nConjugate pair → formula. Otherwise → MCMC.\nPriors & Posteriors for closed-form; MCMC for sampling when no formula exists\n\n\n4. Summarize & interpret\nPosterior mean, credible intervals, posterior probabilities\nBayesian Regression — credible intervals and how to read Bayesian output\n\n\n5. Check sensitivity\nHow much do results change with different priors?\nShrinkage — why the prior pulls estimates and when that helps vs hurts\n\n\n6. Scale up\nMultiple groups? Partial pooling via hierarchical models.\nHierarchical Models — let groups borrow strength from each other\n\n\n\nThe core loop: likelihood + prior → posterior → interpret → check.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "Bayesian Thinking",
    "section": "Topics",
    "text": "Topics\n\nBayes’ Theorem — The engine behind everything: how evidence updates beliefs\nPriors & Posteriors — Watch your prior get overwhelmed by data\nShrinkage — Why pulling estimates toward the mean beats taking them at face value\nBayesian vs Frequentist — Same question, two philosophies, different answers",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#beyond-conjugates",
    "href": "index.html#beyond-conjugates",
    "title": "Bayesian Thinking",
    "section": "Beyond Conjugates",
    "text": "Beyond Conjugates\n\nMCMC — Sampling from posteriors when closed-form solutions don’t exist\nHierarchical Models — Partial pooling: the killer app of Bayesian inference\nBayesian Regression — From Stata’s reg y x to full posterior inference on coefficients",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#how-does-bayesian-inference-relate-to-causal-inference",
    "href": "index.html#how-does-bayesian-inference-relate-to-causal-inference",
    "title": "Bayesian Thinking",
    "section": "How does Bayesian inference relate to causal inference?",
    "text": "How does Bayesian inference relate to causal inference?\nThey’re different questions:\n\n\n\n\n\n\n\n\n\nBayesian inference\nCausal inference\n\n\n\n\nQuestion\nWhat should I believe given the data?\nDoes X cause Y?\n\n\nFramework\nPrior + likelihood = posterior\nPotential outcomes, DAGs\n\n\nKey concept\nUpdating beliefs\nCounterfactuals\n\n\n\nYou can combine them — Bayesian causal inference uses Bayesian methods to estimate causal effects — but each stands on its own.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "shrinkage.html",
    "href": "shrinkage.html",
    "title": "Bayesian Shrinkage",
    "section": "",
    "text": "Imagine you’re a baseball scout. It’s early in the season and you need to estimate the true batting average for 50 players, each with only 20 at-bats.\nOne player went 10-for-20 (.500). Another went 1-for-20 (.050). Are those their true abilities? Probably not — with only 20 at-bats, there’s a ton of noise. The .500 hitter probably got lucky. The .050 hitter probably got unlucky.\nShrinkage says: don’t take the raw numbers at face value. Pull (“shrink”) every estimate toward the overall average. The more uncertain you are about an individual estimate, the more you pull.\n\\[\\hat{\\theta}_i^{shrunk} = w_i \\cdot \\bar{\\theta}_{overall} + (1 - w_i) \\cdot \\hat{\\theta}_i^{raw}\\]\nThis is the core of empirical Bayes and James-Stein estimation. It sounds like you’re adding bias — and you are — but you’re reducing variance by more than enough to compensate. The result: better predictions overall.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_players\", \"Number of players:\",\n                  min = 10, max = 100, value = 40, step = 5),\n\n      sliderInput(\"at_bats\", \"At-bats per player:\",\n                  min = 5, max = 200, value = 20, step = 5),\n\n      sliderInput(\"true_spread\", \"True talent spread (SD):\",\n                  min = 0.01, max = 0.08, value = 0.03, step = 0.005),\n\n      actionButton(\"go\", \"New season\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"shrinkage_plot\", height = \"420px\")),\n        column(6, plotOutput(\"mse_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    k   &lt;- input$n_players\n    n   &lt;- input$at_bats\n    tau &lt;- input$true_spread\n\n    # True batting averages (centered around .260)\n    true_avg &lt;- rnorm(k, mean = 0.260, sd = tau)\n    true_avg &lt;- pmin(pmax(true_avg, 0.100), 0.400)\n\n    # Observed: hits in n at-bats\n    hits &lt;- rbinom(k, size = n, prob = true_avg)\n    obs_avg &lt;- hits / n\n\n    # Grand mean\n    grand_mean &lt;- mean(obs_avg)\n\n    # Empirical Bayes shrinkage\n    # Estimate prior variance from data\n    obs_var &lt;- var(obs_avg)\n    sampling_var &lt;- mean(obs_avg * (1 - obs_avg) / n)\n    prior_var &lt;- max(obs_var - sampling_var, 0.0001)\n\n    # Shrinkage weight (toward grand mean)\n    w &lt;- sampling_var / (sampling_var + prior_var)\n    shrunk_avg &lt;- w * grand_mean + (1 - w) * obs_avg\n\n    # MSE\n    mse_raw   &lt;- mean((obs_avg - true_avg)^2)\n    mse_shrunk &lt;- mean((shrunk_avg - true_avg)^2)\n\n    # Future performance (another n at-bats from true ability)\n    future_hits &lt;- rbinom(k, size = n, prob = true_avg)\n    future_avg  &lt;- future_hits / n\n\n    pred_err_raw   &lt;- mean((obs_avg - future_avg)^2)\n    pred_err_shrunk &lt;- mean((shrunk_avg - future_avg)^2)\n\n    list(true_avg = true_avg, obs_avg = obs_avg, shrunk_avg = shrunk_avg,\n         grand_mean = grand_mean, w = w,\n         mse_raw = mse_raw, mse_shrunk = mse_shrunk,\n         pred_err_raw = pred_err_raw, pred_err_shrunk = pred_err_shrunk,\n         k = k, n = n)\n  })\n\n  output$shrinkage_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ord &lt;- order(d$obs_avg)\n\n    plot(d$obs_avg[ord], seq_along(ord), pch = 16, col = \"#e74c3c\",\n         xlab = \"Batting average\", ylab = \"Player (sorted by raw avg)\",\n         main = \"Shrinkage in Action\",\n         xlim = range(c(d$obs_avg, d$shrunk_avg, d$true_avg)))\n\n    points(d$shrunk_avg[ord], seq_along(ord), pch = 17, col = \"#3498db\")\n    points(d$true_avg[ord], seq_along(ord), pch = 4, col = \"#27ae60\", cex = 0.8)\n\n    # Draw arrows from raw to shrunk\n    arrows(d$obs_avg[ord], seq_along(ord),\n           d$shrunk_avg[ord], seq_along(ord),\n           length = 0.05, col = \"#bdc3c780\", lwd = 1)\n\n    # Grand mean\n    abline(v = d$grand_mean, lty = 2, col = \"gray50\", lwd = 1.5)\n\n    legend(\"bottomright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Raw average\", \"Shrunk estimate\",\n                      \"True ability\", \"Grand mean\"),\n           col = c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"gray50\"),\n           pch = c(16, 17, 4, NA),\n           lty = c(NA, NA, NA, 2), lwd = c(NA, NA, NA, 1.5))\n  })\n\n  output$mse_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 6, 3, 1))\n\n    vals &lt;- c(d$mse_raw, d$mse_shrunk, d$pred_err_raw, d$pred_err_shrunk)\n    cols &lt;- c(\"#e74c3c\", \"#3498db\", \"#e74c3c80\", \"#3498db80\")\n    labels &lt;- c(\"Raw\\nvs truth\", \"Shrunk\\nvs truth\",\n                \"Raw\\nvs future\", \"Shrunk\\nvs future\")\n\n    bp &lt;- barplot(vals, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.8,\n                  main = \"Mean Squared Error\",\n                  ylab = \"MSE\", las = 1)\n    text(bp, vals + max(vals) * 0.03, round(vals, 5), cex = 0.8)\n\n    pct1 &lt;- round((1 - d$mse_shrunk / d$mse_raw) * 100, 0)\n    pct2 &lt;- round((1 - d$pred_err_shrunk / d$pred_err_raw) * 100, 0)\n\n    mtext(paste0(\"Shrinkage reduces estimation error by ~\", pct1, \"%\"),\n          side = 1, line = 3.5, cex = 0.85, col = \"#2c3e50\")\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    pct_est &lt;- round((1 - d$mse_shrunk / d$mse_raw) * 100, 1)\n    pct_pred &lt;- round((1 - d$pred_err_shrunk / d$pred_err_raw) * 100, 1)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Shrinkage weight:&lt;/b&gt; \", round(d$w * 100, 1),\n        \"% toward grand mean&lt;br&gt;\",\n        \"&lt;b&gt;Grand mean:&lt;/b&gt; \", round(d$grand_mean, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;MSE (raw):&lt;/b&gt; \", round(d$mse_raw, 5), \"&lt;br&gt;\",\n        \"&lt;b&gt;MSE (shrunk):&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$mse_shrunk, 5), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Improvement:&lt;/b&gt; &lt;span class='good'&gt;\", pct_est, \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Prediction error (raw):&lt;/b&gt; \", round(d$pred_err_raw, 5), \"&lt;br&gt;\",\n        \"&lt;b&gt;Prediction error (shrunk):&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$pred_err_shrunk, 5), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Improvement:&lt;/b&gt; &lt;span class='good'&gt;\", pct_pred, \"%&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nAt-bats = 5: extreme noise. Raw averages are all over the place (some players show .000 or .600). Shrinkage pulls them heavily toward the mean — and the green crosses (true ability) confirm the shrunk estimates are closer.\nAt-bats = 200: lots of data per player. Shrinkage is minimal because the raw averages are already precise. With enough data, shrinkage vanishes.\nLook at the MSE bars: shrinkage almost always wins, especially with small samples. It also predicts future performance better.\nTrue talent spread = 0.01 (everyone is similar): shrinkage is aggressive because individual differences are small relative to noise.\nTrue talent spread = 0.08 (wide range of talent): shrinkage is lighter because individual differences are real, not noise.",
    "crumbs": [
      "Shrinkage"
    ]
  },
  {
    "objectID": "shrinkage.html#what-is-shrinkage",
    "href": "shrinkage.html#what-is-shrinkage",
    "title": "Bayesian Shrinkage",
    "section": "",
    "text": "Imagine you’re a baseball scout. It’s early in the season and you need to estimate the true batting average for 50 players, each with only 20 at-bats.\nOne player went 10-for-20 (.500). Another went 1-for-20 (.050). Are those their true abilities? Probably not — with only 20 at-bats, there’s a ton of noise. The .500 hitter probably got lucky. The .050 hitter probably got unlucky.\nShrinkage says: don’t take the raw numbers at face value. Pull (“shrink”) every estimate toward the overall average. The more uncertain you are about an individual estimate, the more you pull.\n\\[\\hat{\\theta}_i^{shrunk} = w_i \\cdot \\bar{\\theta}_{overall} + (1 - w_i) \\cdot \\hat{\\theta}_i^{raw}\\]\nThis is the core of empirical Bayes and James-Stein estimation. It sounds like you’re adding bias — and you are — but you’re reducing variance by more than enough to compensate. The result: better predictions overall.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_players\", \"Number of players:\",\n                  min = 10, max = 100, value = 40, step = 5),\n\n      sliderInput(\"at_bats\", \"At-bats per player:\",\n                  min = 5, max = 200, value = 20, step = 5),\n\n      sliderInput(\"true_spread\", \"True talent spread (SD):\",\n                  min = 0.01, max = 0.08, value = 0.03, step = 0.005),\n\n      actionButton(\"go\", \"New season\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"shrinkage_plot\", height = \"420px\")),\n        column(6, plotOutput(\"mse_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    k   &lt;- input$n_players\n    n   &lt;- input$at_bats\n    tau &lt;- input$true_spread\n\n    # True batting averages (centered around .260)\n    true_avg &lt;- rnorm(k, mean = 0.260, sd = tau)\n    true_avg &lt;- pmin(pmax(true_avg, 0.100), 0.400)\n\n    # Observed: hits in n at-bats\n    hits &lt;- rbinom(k, size = n, prob = true_avg)\n    obs_avg &lt;- hits / n\n\n    # Grand mean\n    grand_mean &lt;- mean(obs_avg)\n\n    # Empirical Bayes shrinkage\n    # Estimate prior variance from data\n    obs_var &lt;- var(obs_avg)\n    sampling_var &lt;- mean(obs_avg * (1 - obs_avg) / n)\n    prior_var &lt;- max(obs_var - sampling_var, 0.0001)\n\n    # Shrinkage weight (toward grand mean)\n    w &lt;- sampling_var / (sampling_var + prior_var)\n    shrunk_avg &lt;- w * grand_mean + (1 - w) * obs_avg\n\n    # MSE\n    mse_raw   &lt;- mean((obs_avg - true_avg)^2)\n    mse_shrunk &lt;- mean((shrunk_avg - true_avg)^2)\n\n    # Future performance (another n at-bats from true ability)\n    future_hits &lt;- rbinom(k, size = n, prob = true_avg)\n    future_avg  &lt;- future_hits / n\n\n    pred_err_raw   &lt;- mean((obs_avg - future_avg)^2)\n    pred_err_shrunk &lt;- mean((shrunk_avg - future_avg)^2)\n\n    list(true_avg = true_avg, obs_avg = obs_avg, shrunk_avg = shrunk_avg,\n         grand_mean = grand_mean, w = w,\n         mse_raw = mse_raw, mse_shrunk = mse_shrunk,\n         pred_err_raw = pred_err_raw, pred_err_shrunk = pred_err_shrunk,\n         k = k, n = n)\n  })\n\n  output$shrinkage_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ord &lt;- order(d$obs_avg)\n\n    plot(d$obs_avg[ord], seq_along(ord), pch = 16, col = \"#e74c3c\",\n         xlab = \"Batting average\", ylab = \"Player (sorted by raw avg)\",\n         main = \"Shrinkage in Action\",\n         xlim = range(c(d$obs_avg, d$shrunk_avg, d$true_avg)))\n\n    points(d$shrunk_avg[ord], seq_along(ord), pch = 17, col = \"#3498db\")\n    points(d$true_avg[ord], seq_along(ord), pch = 4, col = \"#27ae60\", cex = 0.8)\n\n    # Draw arrows from raw to shrunk\n    arrows(d$obs_avg[ord], seq_along(ord),\n           d$shrunk_avg[ord], seq_along(ord),\n           length = 0.05, col = \"#bdc3c780\", lwd = 1)\n\n    # Grand mean\n    abline(v = d$grand_mean, lty = 2, col = \"gray50\", lwd = 1.5)\n\n    legend(\"bottomright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Raw average\", \"Shrunk estimate\",\n                      \"True ability\", \"Grand mean\"),\n           col = c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"gray50\"),\n           pch = c(16, 17, 4, NA),\n           lty = c(NA, NA, NA, 2), lwd = c(NA, NA, NA, 1.5))\n  })\n\n  output$mse_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 6, 3, 1))\n\n    vals &lt;- c(d$mse_raw, d$mse_shrunk, d$pred_err_raw, d$pred_err_shrunk)\n    cols &lt;- c(\"#e74c3c\", \"#3498db\", \"#e74c3c80\", \"#3498db80\")\n    labels &lt;- c(\"Raw\\nvs truth\", \"Shrunk\\nvs truth\",\n                \"Raw\\nvs future\", \"Shrunk\\nvs future\")\n\n    bp &lt;- barplot(vals, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.8,\n                  main = \"Mean Squared Error\",\n                  ylab = \"MSE\", las = 1)\n    text(bp, vals + max(vals) * 0.03, round(vals, 5), cex = 0.8)\n\n    pct1 &lt;- round((1 - d$mse_shrunk / d$mse_raw) * 100, 0)\n    pct2 &lt;- round((1 - d$pred_err_shrunk / d$pred_err_raw) * 100, 0)\n\n    mtext(paste0(\"Shrinkage reduces estimation error by ~\", pct1, \"%\"),\n          side = 1, line = 3.5, cex = 0.85, col = \"#2c3e50\")\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    pct_est &lt;- round((1 - d$mse_shrunk / d$mse_raw) * 100, 1)\n    pct_pred &lt;- round((1 - d$pred_err_shrunk / d$pred_err_raw) * 100, 1)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Shrinkage weight:&lt;/b&gt; \", round(d$w * 100, 1),\n        \"% toward grand mean&lt;br&gt;\",\n        \"&lt;b&gt;Grand mean:&lt;/b&gt; \", round(d$grand_mean, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;MSE (raw):&lt;/b&gt; \", round(d$mse_raw, 5), \"&lt;br&gt;\",\n        \"&lt;b&gt;MSE (shrunk):&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$mse_shrunk, 5), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Improvement:&lt;/b&gt; &lt;span class='good'&gt;\", pct_est, \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Prediction error (raw):&lt;/b&gt; \", round(d$pred_err_raw, 5), \"&lt;br&gt;\",\n        \"&lt;b&gt;Prediction error (shrunk):&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$pred_err_shrunk, 5), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Improvement:&lt;/b&gt; &lt;span class='good'&gt;\", pct_pred, \"%&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nAt-bats = 5: extreme noise. Raw averages are all over the place (some players show .000 or .600). Shrinkage pulls them heavily toward the mean — and the green crosses (true ability) confirm the shrunk estimates are closer.\nAt-bats = 200: lots of data per player. Shrinkage is minimal because the raw averages are already precise. With enough data, shrinkage vanishes.\nLook at the MSE bars: shrinkage almost always wins, especially with small samples. It also predicts future performance better.\nTrue talent spread = 0.01 (everyone is similar): shrinkage is aggressive because individual differences are small relative to noise.\nTrue talent spread = 0.08 (wide range of talent): shrinkage is lighter because individual differences are real, not noise.",
    "crumbs": [
      "Shrinkage"
    ]
  },
  {
    "objectID": "shrinkage.html#why-does-this-work",
    "href": "shrinkage.html#why-does-this-work",
    "title": "Bayesian Shrinkage",
    "section": "Why does this work?",
    "text": "Why does this work?\nIt seems wrong to move estimates away from the data. But consider what happens without shrinkage:\n\nPlayers who got lucky are overestimated\nPlayers who got unlucky are underestimated\nThese errors don’t cancel — they inflate the overall MSE\n\nShrinkage dampens both overestimates and underestimates simultaneously. The small bias it introduces (pulling everyone toward the mean) is more than offset by the massive reduction in variance. This is the bias-variance tradeoff in action.\n\nWhere you see this in practice\n\n\n\nMethod\nWhat gets shrunk\n\n\n\n\nRidge regression\nCoefficients toward zero\n\n\nLASSO\nCoefficients toward zero (with selection)\n\n\nRandom effects models\nGroup means toward grand mean\n\n\nEmpirical Bayes\nIndividual estimates toward overall mean\n\n\nBayesian priors\nPosteriors toward prior mean\n\n\n\nThey all share the same logic: when you have many noisy estimates, borrowing strength across them improves every single one.",
    "crumbs": [
      "Shrinkage"
    ]
  }
]