[
  {
    "objectID": "iv.html",
    "href": "iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "You want the causal effect of \\(X\\) on \\(Y\\), but \\(X\\) is endogenous — correlated with the error term because of confounding, reverse causality, or measurement error. OLS is biased.\nThe fix: find a variable \\(Z\\) (the instrument) that:\n\nRelevance: \\(Z\\) is correlated with \\(X\\) — it actually moves \\(X\\)\nExclusion restriction: \\(Z\\) affects \\(Y\\) only through \\(X\\) — no back doors\n\n\\[Z \\to X \\to Y\\]\nIf both conditions hold, you can use \\(Z\\) to isolate the part of \\(X\\) that’s “as good as random” and estimate the causal effect.\n\n\nThe mechanics are simple:\nFirst stage: regress \\(X\\) on \\(Z\\) to get predicted values \\(\\hat{X}\\)\n\\[X = \\pi_0 + \\pi_1 Z + v\\]\nSecond stage: regress \\(Y\\) on \\(\\hat{X}\\) instead of \\(X\\)\n\\[Y = \\beta_0 + \\beta_1 \\hat{X} + \\varepsilon\\]\nWhy does this work? \\(\\hat{X}\\) contains only the variation in \\(X\\) that comes from \\(Z\\). Since \\(Z\\) is exogenous (by assumption), \\(\\hat{X}\\) is uncorrelated with the error term. The confounding is gone.\n\n\n\nReturns to education. You want to know if more schooling causes higher earnings. But ability confounds: smarter people get more education and earn more. OLS overstates the return.\nAngrist & Krueger (1991) used quarter of birth as an instrument. Because of compulsory schooling laws, people born in Q1 can drop out slightly earlier than Q4 births — so quarter of birth affects education (relevance) but presumably doesn’t affect earnings directly (exclusion).\n\n\n\n\nWeak instruments: if \\(Z\\) barely moves \\(X\\), the first stage is weak and the IV estimate becomes wildly noisy and biased. Rule of thumb: first-stage F-statistic &gt; 10.\nExclusion restriction violated: if \\(Z\\) affects \\(Y\\) through channels other than \\(X\\), the estimate is biased. This assumption is untestable — you argue it, you don’t prove it.\n\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 100, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"true_b\", \"True causal effect of X on Y:\",\n                  min = 0, max = 5, value = 2, step = 0.25),\n\n      sliderInput(\"confound\", \"Confounding strength:\",\n                  min = 0, max = 5, value = 3, step = 0.25),\n\n      sliderInput(\"inst_str\", \"Instrument strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.1),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"iv_plot\", height = \"470px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    b   &lt;- input$true_b\n    cf  &lt;- input$confound\n    pi1 &lt;- input$inst_str\n\n    # Confounder (unobserved ability)\n    u &lt;- rnorm(n)\n\n    # Instrument\n    z &lt;- rnorm(n)\n\n    # Endogenous regressor: driven by instrument + confounder\n    x &lt;- pi1 * z + cf * u + rnorm(n)\n\n    # Outcome: causal effect of x + confounder\n    y &lt;- b * x + cf * u + rnorm(n)\n\n    # OLS (biased)\n    ols &lt;- lm(y ~ x)\n\n    # 2SLS by hand\n    first &lt;- lm(x ~ z)\n    x_hat &lt;- fitted(first)\n    second &lt;- lm(y ~ x_hat)\n\n    # First-stage F\n    f_stat &lt;- summary(first)$fstatistic[1]\n\n    list(x = x, y = y, z = z, x_hat = x_hat,\n         b_ols = coef(ols)[2],\n         b_iv = coef(second)[2],\n         first_coef = coef(first)[2],\n         f_stat = f_stat,\n         true_b = b, confound = cf, inst_str = pi1)\n  })\n\n  output$iv_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mfrow = c(1, 2), mar = c(4.5, 4.5, 3, 1))\n\n    # Left: OLS scatter (X vs Y)\n    plot(d$x, d$y, pch = 16, cex = 0.4,\n         col = adjustcolor(\"#3498db\", 0.3),\n         xlab = \"X (endogenous)\", ylab = \"Y\",\n         main = \"OLS: Y on X\")\n    abline(lm(d$y ~ d$x), col = \"#e74c3c\", lwd = 3)\n    abline(a = 0, b = d$true_b, col = \"#27ae60\", lwd = 2, lty = 2)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(paste0(\"OLS = \", round(d$b_ols, 2)),\n                      paste0(\"True = \", d$true_b)),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = c(3, 2), lty = c(1, 2))\n\n    # Right: IV scatter (X-hat vs Y)\n    plot(d$x_hat, d$y, pch = 16, cex = 0.4,\n         col = adjustcolor(\"#9b59b6\", 0.3),\n         xlab = expression(hat(X) ~ \"(from first stage)\"), ylab = \"Y\",\n         main = expression(\"2SLS: Y on \" * hat(X)))\n    abline(lm(d$y ~ d$x_hat), col = \"#e74c3c\", lwd = 3)\n    abline(a = 0, b = d$true_b, col = \"#27ae60\", lwd = 2, lty = 2)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(paste0(\"IV = \", round(d$b_iv, 2)),\n                      paste0(\"True = \", d$true_b)),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = c(3, 2), lty = c(1, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    ols_bias &lt;- d$b_ols - d$true_b\n    iv_bias  &lt;- d$b_iv - d$true_b\n    weak &lt;- d$f_stat &lt; 10\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$true_b, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;OLS:&lt;/b&gt; \", round(d$b_ols, 3),\n        \" &nbsp; Bias: &lt;span class='bad'&gt;\", round(ols_bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;IV (2SLS):&lt;/b&gt; \", round(d$b_iv, 3),\n        \" &nbsp; Bias: &lt;span class='\", ifelse(abs(iv_bias) &lt; abs(ols_bias), \"good\", \"bad\"), \"'&gt;\",\n        round(iv_bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;First-stage F:&lt;/b&gt; \", round(d$f_stat, 1),\n        if (weak) \" &lt;span class='bad'&gt;&lt; 10 (weak!)&lt;/span&gt;\"\n        else \" &lt;span class='good'&gt;&ge; 10&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nConfounding = 3, instrument = 1.5: OLS is badly biased. IV recovers the true effect. This is the whole point.\nSet confounding = 0: OLS and IV agree — when there’s no endogeneity, you don’t need an instrument.\nSlide instrument strength toward 0: the first-stage F drops below 10. The IV estimate becomes erratic — sometimes worse than OLS. That’s the weak instrument problem.\nIncrease sample size with a weak instrument: it doesn’t help much. Weak instruments bias IV toward OLS, and more data doesn’t fix that.\nTrue effect = 0, confounding = 3: OLS “finds” a large effect. IV correctly shows ~0.",
    "crumbs": [
      "Instrumental Variables"
    ]
  },
  {
    "objectID": "iv.html#the-idea",
    "href": "iv.html#the-idea",
    "title": "Instrumental Variables",
    "section": "",
    "text": "You want the causal effect of \\(X\\) on \\(Y\\), but \\(X\\) is endogenous — correlated with the error term because of confounding, reverse causality, or measurement error. OLS is biased.\nThe fix: find a variable \\(Z\\) (the instrument) that:\n\nRelevance: \\(Z\\) is correlated with \\(X\\) — it actually moves \\(X\\)\nExclusion restriction: \\(Z\\) affects \\(Y\\) only through \\(X\\) — no back doors\n\n\\[Z \\to X \\to Y\\]\nIf both conditions hold, you can use \\(Z\\) to isolate the part of \\(X\\) that’s “as good as random” and estimate the causal effect.\n\n\nThe mechanics are simple:\nFirst stage: regress \\(X\\) on \\(Z\\) to get predicted values \\(\\hat{X}\\)\n\\[X = \\pi_0 + \\pi_1 Z + v\\]\nSecond stage: regress \\(Y\\) on \\(\\hat{X}\\) instead of \\(X\\)\n\\[Y = \\beta_0 + \\beta_1 \\hat{X} + \\varepsilon\\]\nWhy does this work? \\(\\hat{X}\\) contains only the variation in \\(X\\) that comes from \\(Z\\). Since \\(Z\\) is exogenous (by assumption), \\(\\hat{X}\\) is uncorrelated with the error term. The confounding is gone.\n\n\n\nReturns to education. You want to know if more schooling causes higher earnings. But ability confounds: smarter people get more education and earn more. OLS overstates the return.\nAngrist & Krueger (1991) used quarter of birth as an instrument. Because of compulsory schooling laws, people born in Q1 can drop out slightly earlier than Q4 births — so quarter of birth affects education (relevance) but presumably doesn’t affect earnings directly (exclusion).\n\n\n\n\nWeak instruments: if \\(Z\\) barely moves \\(X\\), the first stage is weak and the IV estimate becomes wildly noisy and biased. Rule of thumb: first-stage F-statistic &gt; 10.\nExclusion restriction violated: if \\(Z\\) affects \\(Y\\) through channels other than \\(X\\), the estimate is biased. This assumption is untestable — you argue it, you don’t prove it.\n\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 100, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"true_b\", \"True causal effect of X on Y:\",\n                  min = 0, max = 5, value = 2, step = 0.25),\n\n      sliderInput(\"confound\", \"Confounding strength:\",\n                  min = 0, max = 5, value = 3, step = 0.25),\n\n      sliderInput(\"inst_str\", \"Instrument strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.1),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"iv_plot\", height = \"470px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    b   &lt;- input$true_b\n    cf  &lt;- input$confound\n    pi1 &lt;- input$inst_str\n\n    # Confounder (unobserved ability)\n    u &lt;- rnorm(n)\n\n    # Instrument\n    z &lt;- rnorm(n)\n\n    # Endogenous regressor: driven by instrument + confounder\n    x &lt;- pi1 * z + cf * u + rnorm(n)\n\n    # Outcome: causal effect of x + confounder\n    y &lt;- b * x + cf * u + rnorm(n)\n\n    # OLS (biased)\n    ols &lt;- lm(y ~ x)\n\n    # 2SLS by hand\n    first &lt;- lm(x ~ z)\n    x_hat &lt;- fitted(first)\n    second &lt;- lm(y ~ x_hat)\n\n    # First-stage F\n    f_stat &lt;- summary(first)$fstatistic[1]\n\n    list(x = x, y = y, z = z, x_hat = x_hat,\n         b_ols = coef(ols)[2],\n         b_iv = coef(second)[2],\n         first_coef = coef(first)[2],\n         f_stat = f_stat,\n         true_b = b, confound = cf, inst_str = pi1)\n  })\n\n  output$iv_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mfrow = c(1, 2), mar = c(4.5, 4.5, 3, 1))\n\n    # Left: OLS scatter (X vs Y)\n    plot(d$x, d$y, pch = 16, cex = 0.4,\n         col = adjustcolor(\"#3498db\", 0.3),\n         xlab = \"X (endogenous)\", ylab = \"Y\",\n         main = \"OLS: Y on X\")\n    abline(lm(d$y ~ d$x), col = \"#e74c3c\", lwd = 3)\n    abline(a = 0, b = d$true_b, col = \"#27ae60\", lwd = 2, lty = 2)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(paste0(\"OLS = \", round(d$b_ols, 2)),\n                      paste0(\"True = \", d$true_b)),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = c(3, 2), lty = c(1, 2))\n\n    # Right: IV scatter (X-hat vs Y)\n    plot(d$x_hat, d$y, pch = 16, cex = 0.4,\n         col = adjustcolor(\"#9b59b6\", 0.3),\n         xlab = expression(hat(X) ~ \"(from first stage)\"), ylab = \"Y\",\n         main = expression(\"2SLS: Y on \" * hat(X)))\n    abline(lm(d$y ~ d$x_hat), col = \"#e74c3c\", lwd = 3)\n    abline(a = 0, b = d$true_b, col = \"#27ae60\", lwd = 2, lty = 2)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(paste0(\"IV = \", round(d$b_iv, 2)),\n                      paste0(\"True = \", d$true_b)),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = c(3, 2), lty = c(1, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    ols_bias &lt;- d$b_ols - d$true_b\n    iv_bias  &lt;- d$b_iv - d$true_b\n    weak &lt;- d$f_stat &lt; 10\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$true_b, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;OLS:&lt;/b&gt; \", round(d$b_ols, 3),\n        \" &nbsp; Bias: &lt;span class='bad'&gt;\", round(ols_bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;IV (2SLS):&lt;/b&gt; \", round(d$b_iv, 3),\n        \" &nbsp; Bias: &lt;span class='\", ifelse(abs(iv_bias) &lt; abs(ols_bias), \"good\", \"bad\"), \"'&gt;\",\n        round(iv_bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;First-stage F:&lt;/b&gt; \", round(d$f_stat, 1),\n        if (weak) \" &lt;span class='bad'&gt;&lt; 10 (weak!)&lt;/span&gt;\"\n        else \" &lt;span class='good'&gt;&ge; 10&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nConfounding = 3, instrument = 1.5: OLS is badly biased. IV recovers the true effect. This is the whole point.\nSet confounding = 0: OLS and IV agree — when there’s no endogeneity, you don’t need an instrument.\nSlide instrument strength toward 0: the first-stage F drops below 10. The IV estimate becomes erratic — sometimes worse than OLS. That’s the weak instrument problem.\nIncrease sample size with a weak instrument: it doesn’t help much. Weak instruments bias IV toward OLS, and more data doesn’t fix that.\nTrue effect = 0, confounding = 3: OLS “finds” a large effect. IV correctly shows ~0.",
    "crumbs": [
      "Instrumental Variables"
    ]
  },
  {
    "objectID": "iv.html#what-does-iv-actually-estimate",
    "href": "iv.html#what-does-iv-actually-estimate",
    "title": "Instrumental Variables",
    "section": "What does IV actually estimate?",
    "text": "What does IV actually estimate?\nA subtle point: IV doesn’t estimate the effect for everyone. It estimates the Local Average Treatment Effect (LATE) — the effect for compliers, people whose treatment status is actually changed by the instrument.\nIn the Angrist & Krueger example: IV estimates the return to education for people who would have dropped out if born in a different quarter. It says nothing about people who would have stayed in school regardless.\nThis means two different valid instruments can give you two different IV estimates — not because one is wrong, but because they’re identifying effects for different subpopulations.",
    "crumbs": [
      "Instrumental Variables"
    ]
  },
  {
    "objectID": "iv.html#did-you-know",
    "href": "iv.html#did-you-know",
    "title": "Instrumental Variables",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe instrumental variables method dates back to Philip Wright (1928), who used it to estimate supply and demand curves for butter and flax seed. Some historians credit his son, Sewall Wright, with the actual derivation.\nThe “weak instruments” problem was formalized by Staiger & Stock (1997). They showed that when the first-stage F is below 10, IV can be more biased than OLS — the cure becomes worse than the disease.\nJoshua Angrist, one of the 2021 Nobel laureates, built much of his career on clever instruments: quarter of birth for schooling, draft lottery numbers for military service, religious composition for family size. The art is finding instruments that are both relevant and excludable.",
    "crumbs": [
      "Instrumental Variables"
    ]
  },
  {
    "objectID": "rdd.html",
    "href": "rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "Some treatments are assigned by a cutoff rule: you get a scholarship if your test score is above 80, you qualify for a program if your income is below $30k, you win an election if your vote share exceeds 50%.\nRight around the cutoff, people just above and just below are nearly identical — they differ by a fraction of a point. The cutoff creates near-random assignment in a narrow window. That’s the identifying variation.\n\\[\\tau_{RDD} = \\lim_{x \\downarrow c} E[Y \\mid X = x] - \\lim_{x \\uparrow c} E[Y \\mid X = x]\\]\nThe treatment effect is the jump in the outcome at the cutoff. If the outcome is smooth everywhere except at the cutoff, that discontinuity is the causal effect.\n\n\n\nSharp RDD: treatment is a deterministic function of the running variable. Score \\(\\geq\\) 80 → treated, period. Everyone complies.\nFuzzy RDD: the cutoff changes the probability of treatment, but not perfectly. Some people above the cutoff don’t take treatment, some below do. This is essentially an IV problem — the cutoff is the instrument.\n\n\n\n\n\nManipulation: if people can precisely control their score to land just above or below the cutoff, the “as good as random” logic breaks. Check for bunching at the cutoff (McCrary test).\nWrong functional form: if you fit a straight line but the true relationship is curved, you might mistake curvature for a jump. Use local polynomials and check sensitivity to bandwidth.\n\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"tau\", \"True treatment effect:\",\n                  min = 0, max = 5, value = 2, step = 0.25),\n\n      sliderInput(\"sigma\", \"Noise (SD):\",\n                  min = 0.5, max = 4, value = 1.5, step = 0.25),\n\n      sliderInput(\"bw\", \"Bandwidth around cutoff:\",\n                  min = 0.05, max = 0.5, value = 0.2, step = 0.05),\n\n      selectInput(\"curve\", \"True relationship:\",\n                  choices = c(\"Linear\" = \"linear\",\n                              \"Quadratic\" = \"quad\",\n                              \"Flat\" = \"flat\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"rdd_plot\", height = \"480px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n\n    tau   &lt;- input$tau\n    sigma &lt;- input$sigma\n    bw    &lt;- input$bw\n    curve &lt;- input$curve\n\n    # Running variable: uniform on [0, 1], cutoff at 0.5\n    x &lt;- runif(n)\n    cutoff &lt;- 0.5\n    treat &lt;- as.numeric(x &gt;= cutoff)\n\n    # Potential outcome (smooth function of x)\n    if (curve == \"linear\") {\n      mu &lt;- 2 + 1.5 * x\n    } else if (curve == \"quad\") {\n      mu &lt;- 2 + 3 * (x - 0.5)^2\n    } else {\n      mu &lt;- rep(3, n)\n    }\n\n    y &lt;- mu + tau * treat + rnorm(n, sd = sigma)\n\n    # Local linear regression within bandwidth\n    in_bw &lt;- abs(x - cutoff) &lt;= bw\n    x_bw &lt;- x[in_bw]\n    y_bw &lt;- y[in_bw]\n    t_bw &lt;- treat[in_bw]\n\n    # Separate regressions left and right\n    left  &lt;- x_bw &lt; cutoff\n    right &lt;- x_bw &gt;= cutoff\n\n    if (sum(left) &gt; 2 && sum(right) &gt; 2) {\n      fit_l &lt;- lm(y_bw[left] ~ x_bw[left])\n      fit_r &lt;- lm(y_bw[right] ~ x_bw[right])\n\n      # Predicted values at cutoff\n      pred_l &lt;- coef(fit_l)[1] + coef(fit_l)[2] * cutoff\n      pred_r &lt;- coef(fit_r)[1] + coef(fit_r)[2] * cutoff\n\n      rdd_est &lt;- pred_r - pred_l\n\n      # Fitted lines for plotting\n      xseq_l &lt;- seq(cutoff - bw, cutoff, length.out = 100)\n      xseq_r &lt;- seq(cutoff, cutoff + bw, length.out = 100)\n      yhat_l &lt;- coef(fit_l)[1] + coef(fit_l)[2] * xseq_l\n      yhat_r &lt;- coef(fit_r)[1] + coef(fit_r)[2] * xseq_r\n    } else {\n      rdd_est &lt;- NA\n      xseq_l &lt;- xseq_r &lt;- yhat_l &lt;- yhat_r &lt;- NULL\n      pred_l &lt;- pred_r &lt;- NA\n    }\n\n    list(x = x, y = y, treat = treat, cutoff = cutoff,\n         bw = bw, in_bw = in_bw, rdd_est = rdd_est,\n         tau = tau, sigma = sigma,\n         xseq_l = xseq_l, xseq_r = xseq_r,\n         yhat_l = yhat_l, yhat_r = yhat_r,\n         pred_l = pred_l, pred_r = pred_r)\n  })\n\n  output$rdd_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Color by treatment\n    cols &lt;- ifelse(d$treat == 1, adjustcolor(\"#3498db\", 0.25),\n                   adjustcolor(\"#e74c3c\", 0.25))\n\n    # Dim points outside bandwidth\n    cols[!d$in_bw] &lt;- adjustcolor(\"gray70\", 0.15)\n\n    plot(d$x, d$y, pch = 16, cex = 0.5, col = cols,\n         xlab = \"Running variable (X)\", ylab = \"Outcome (Y)\",\n         main = \"Regression Discontinuity Design\")\n\n    # Cutoff line\n    abline(v = d$cutoff, lty = 2, col = \"gray40\", lwd = 1.5)\n\n    # Bandwidth shading\n    rect(d$cutoff - d$bw, par(\"usr\")[3],\n         d$cutoff + d$bw, par(\"usr\")[4],\n         col = adjustcolor(\"#f39c12\", 0.08), border = NA)\n\n    # Local linear fits\n    if (!is.null(d$xseq_l)) {\n      lines(d$xseq_l, d$yhat_l, col = \"#e74c3c\", lwd = 3)\n      lines(d$xseq_r, d$yhat_r, col = \"#3498db\", lwd = 3)\n\n      # Jump arrow\n      arrows(d$cutoff + 0.01, d$pred_l, d$cutoff + 0.01, d$pred_r,\n             code = 3, lwd = 2.5, col = \"#27ae60\", length = 0.1)\n\n      text(d$cutoff + 0.03,\n           (d$pred_l + d$pred_r) / 2,\n           paste0(\"Jump = \", round(d$rdd_est, 2)),\n           col = \"#27ae60\", cex = 0.95, adj = 0, font = 2)\n    }\n\n    text(d$cutoff, par(\"usr\")[4] * 0.98, \"Cutoff\",\n         col = \"gray40\", cex = 0.8, pos = 4)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Control (below cutoff)\", \"Treated (above cutoff)\",\n                      \"Estimation window\"),\n           pch = c(16, 16, 15),\n           col = c(\"#e74c3c\", \"#3498db\", adjustcolor(\"#f39c12\", 0.3)))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    if (is.na(d$rdd_est)) {\n      return(tags$div(class = \"stats-box\",\n        HTML(\"&lt;b&gt;Not enough observations in bandwidth.&lt;/b&gt; Widen it.\")))\n    }\n\n    bias &lt;- d$rdd_est - d$tau\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;RDD estimate:&lt;/b&gt; \", round(d$rdd_est, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(abs(bias) &lt; 0.3, \"good\", \"bad\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;small&gt;Bandwidth: &plusmn;\", d$bw, \" around cutoff&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDefault settings: the jump at the cutoff is clear. The RDD estimate is close to the true effect.\nNarrow the bandwidth (0.05): fewer observations, noisier estimate — but less bias from misspecification. Widen it (0.5): more data, more precise, but you’re using observations far from the cutoff.\nSwitch to quadratic: with a narrow bandwidth, the local linear fit still works fine. But widen the bandwidth with a quadratic DGP and the estimate gets biased — the line can’t capture the curve.\nSet true effect = 0: there should be no visible jump. If you see one, it’s noise (or a bad bandwidth choice).\nCrank up noise: the jump gets harder to see, and you need more data or a wider bandwidth to detect it. This is the bias-variance tradeoff of bandwidth selection.",
    "crumbs": [
      "Regression Discontinuity"
    ]
  },
  {
    "objectID": "rdd.html#the-idea",
    "href": "rdd.html#the-idea",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "Some treatments are assigned by a cutoff rule: you get a scholarship if your test score is above 80, you qualify for a program if your income is below $30k, you win an election if your vote share exceeds 50%.\nRight around the cutoff, people just above and just below are nearly identical — they differ by a fraction of a point. The cutoff creates near-random assignment in a narrow window. That’s the identifying variation.\n\\[\\tau_{RDD} = \\lim_{x \\downarrow c} E[Y \\mid X = x] - \\lim_{x \\uparrow c} E[Y \\mid X = x]\\]\nThe treatment effect is the jump in the outcome at the cutoff. If the outcome is smooth everywhere except at the cutoff, that discontinuity is the causal effect.\n\n\n\nSharp RDD: treatment is a deterministic function of the running variable. Score \\(\\geq\\) 80 → treated, period. Everyone complies.\nFuzzy RDD: the cutoff changes the probability of treatment, but not perfectly. Some people above the cutoff don’t take treatment, some below do. This is essentially an IV problem — the cutoff is the instrument.\n\n\n\n\n\nManipulation: if people can precisely control their score to land just above or below the cutoff, the “as good as random” logic breaks. Check for bunching at the cutoff (McCrary test).\nWrong functional form: if you fit a straight line but the true relationship is curved, you might mistake curvature for a jump. Use local polynomials and check sensitivity to bandwidth.\n\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"tau\", \"True treatment effect:\",\n                  min = 0, max = 5, value = 2, step = 0.25),\n\n      sliderInput(\"sigma\", \"Noise (SD):\",\n                  min = 0.5, max = 4, value = 1.5, step = 0.25),\n\n      sliderInput(\"bw\", \"Bandwidth around cutoff:\",\n                  min = 0.05, max = 0.5, value = 0.2, step = 0.05),\n\n      selectInput(\"curve\", \"True relationship:\",\n                  choices = c(\"Linear\" = \"linear\",\n                              \"Quadratic\" = \"quad\",\n                              \"Flat\" = \"flat\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"rdd_plot\", height = \"480px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n\n    tau   &lt;- input$tau\n    sigma &lt;- input$sigma\n    bw    &lt;- input$bw\n    curve &lt;- input$curve\n\n    # Running variable: uniform on [0, 1], cutoff at 0.5\n    x &lt;- runif(n)\n    cutoff &lt;- 0.5\n    treat &lt;- as.numeric(x &gt;= cutoff)\n\n    # Potential outcome (smooth function of x)\n    if (curve == \"linear\") {\n      mu &lt;- 2 + 1.5 * x\n    } else if (curve == \"quad\") {\n      mu &lt;- 2 + 3 * (x - 0.5)^2\n    } else {\n      mu &lt;- rep(3, n)\n    }\n\n    y &lt;- mu + tau * treat + rnorm(n, sd = sigma)\n\n    # Local linear regression within bandwidth\n    in_bw &lt;- abs(x - cutoff) &lt;= bw\n    x_bw &lt;- x[in_bw]\n    y_bw &lt;- y[in_bw]\n    t_bw &lt;- treat[in_bw]\n\n    # Separate regressions left and right\n    left  &lt;- x_bw &lt; cutoff\n    right &lt;- x_bw &gt;= cutoff\n\n    if (sum(left) &gt; 2 && sum(right) &gt; 2) {\n      fit_l &lt;- lm(y_bw[left] ~ x_bw[left])\n      fit_r &lt;- lm(y_bw[right] ~ x_bw[right])\n\n      # Predicted values at cutoff\n      pred_l &lt;- coef(fit_l)[1] + coef(fit_l)[2] * cutoff\n      pred_r &lt;- coef(fit_r)[1] + coef(fit_r)[2] * cutoff\n\n      rdd_est &lt;- pred_r - pred_l\n\n      # Fitted lines for plotting\n      xseq_l &lt;- seq(cutoff - bw, cutoff, length.out = 100)\n      xseq_r &lt;- seq(cutoff, cutoff + bw, length.out = 100)\n      yhat_l &lt;- coef(fit_l)[1] + coef(fit_l)[2] * xseq_l\n      yhat_r &lt;- coef(fit_r)[1] + coef(fit_r)[2] * xseq_r\n    } else {\n      rdd_est &lt;- NA\n      xseq_l &lt;- xseq_r &lt;- yhat_l &lt;- yhat_r &lt;- NULL\n      pred_l &lt;- pred_r &lt;- NA\n    }\n\n    list(x = x, y = y, treat = treat, cutoff = cutoff,\n         bw = bw, in_bw = in_bw, rdd_est = rdd_est,\n         tau = tau, sigma = sigma,\n         xseq_l = xseq_l, xseq_r = xseq_r,\n         yhat_l = yhat_l, yhat_r = yhat_r,\n         pred_l = pred_l, pred_r = pred_r)\n  })\n\n  output$rdd_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Color by treatment\n    cols &lt;- ifelse(d$treat == 1, adjustcolor(\"#3498db\", 0.25),\n                   adjustcolor(\"#e74c3c\", 0.25))\n\n    # Dim points outside bandwidth\n    cols[!d$in_bw] &lt;- adjustcolor(\"gray70\", 0.15)\n\n    plot(d$x, d$y, pch = 16, cex = 0.5, col = cols,\n         xlab = \"Running variable (X)\", ylab = \"Outcome (Y)\",\n         main = \"Regression Discontinuity Design\")\n\n    # Cutoff line\n    abline(v = d$cutoff, lty = 2, col = \"gray40\", lwd = 1.5)\n\n    # Bandwidth shading\n    rect(d$cutoff - d$bw, par(\"usr\")[3],\n         d$cutoff + d$bw, par(\"usr\")[4],\n         col = adjustcolor(\"#f39c12\", 0.08), border = NA)\n\n    # Local linear fits\n    if (!is.null(d$xseq_l)) {\n      lines(d$xseq_l, d$yhat_l, col = \"#e74c3c\", lwd = 3)\n      lines(d$xseq_r, d$yhat_r, col = \"#3498db\", lwd = 3)\n\n      # Jump arrow\n      arrows(d$cutoff + 0.01, d$pred_l, d$cutoff + 0.01, d$pred_r,\n             code = 3, lwd = 2.5, col = \"#27ae60\", length = 0.1)\n\n      text(d$cutoff + 0.03,\n           (d$pred_l + d$pred_r) / 2,\n           paste0(\"Jump = \", round(d$rdd_est, 2)),\n           col = \"#27ae60\", cex = 0.95, adj = 0, font = 2)\n    }\n\n    text(d$cutoff, par(\"usr\")[4] * 0.98, \"Cutoff\",\n         col = \"gray40\", cex = 0.8, pos = 4)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Control (below cutoff)\", \"Treated (above cutoff)\",\n                      \"Estimation window\"),\n           pch = c(16, 16, 15),\n           col = c(\"#e74c3c\", \"#3498db\", adjustcolor(\"#f39c12\", 0.3)))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    if (is.na(d$rdd_est)) {\n      return(tags$div(class = \"stats-box\",\n        HTML(\"&lt;b&gt;Not enough observations in bandwidth.&lt;/b&gt; Widen it.\")))\n    }\n\n    bias &lt;- d$rdd_est - d$tau\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;RDD estimate:&lt;/b&gt; \", round(d$rdd_est, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(abs(bias) &lt; 0.3, \"good\", \"bad\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;small&gt;Bandwidth: &plusmn;\", d$bw, \" around cutoff&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDefault settings: the jump at the cutoff is clear. The RDD estimate is close to the true effect.\nNarrow the bandwidth (0.05): fewer observations, noisier estimate — but less bias from misspecification. Widen it (0.5): more data, more precise, but you’re using observations far from the cutoff.\nSwitch to quadratic: with a narrow bandwidth, the local linear fit still works fine. But widen the bandwidth with a quadratic DGP and the estimate gets biased — the line can’t capture the curve.\nSet true effect = 0: there should be no visible jump. If you see one, it’s noise (or a bad bandwidth choice).\nCrank up noise: the jump gets harder to see, and you need more data or a wider bandwidth to detect it. This is the bias-variance tradeoff of bandwidth selection.",
    "crumbs": [
      "Regression Discontinuity"
    ]
  },
  {
    "objectID": "rdd.html#the-bandwidth-tradeoff",
    "href": "rdd.html#the-bandwidth-tradeoff",
    "title": "Regression Discontinuity",
    "section": "The bandwidth tradeoff",
    "text": "The bandwidth tradeoff\nBandwidth is the central tuning parameter in RDD:\n\n\n\n\n\n\n\n\n\nNarrow bandwidth\nWide bandwidth\n\n\n\n\nBias\nLow — only using near-identical units\nHigh — far-away units may differ systematically\n\n\nVariance\nHigh — few observations\nLow — more data\n\n\nRisk\nNoisy, imprecise estimate\nPrecise but potentially wrong\n\n\n\nIn practice, researchers use data-driven bandwidth selectors (Imbens & Kalyanaraman 2012, Calonico, Cattaneo & Titiunik 2014) that optimize this tradeoff. You should always check that your results are robust to different bandwidth choices.",
    "crumbs": [
      "Regression Discontinuity"
    ]
  },
  {
    "objectID": "rdd.html#did-you-know",
    "href": "rdd.html#did-you-know",
    "title": "Regression Discontinuity",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe RDD idea goes back to Thistlethwaite & Campbell (1960), who studied the effect of merit scholarships on career outcomes using the score cutoff. It was largely ignored for decades until economists rediscovered it in the late 1990s.\nLee (2008) used RDD to study the incumbency advantage in US elections — candidates who barely win vs barely lose. This paper helped establish RDD as a workhorse method in political economy.\nCattaneo, Idrobo & Titiunik wrote an excellent practical guide: A Practical Introduction to Regression Discontinuity Designs. It’s freely available and covers everything from basic plots to formal inference.",
    "crumbs": [
      "Regression Discontinuity"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Causal Inference",
    "section": "",
    "text": "Causal inference methods — each topic pairs explanation with an interactive simulation you can run in the browser.\nBuilds on: Statistical Inference",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "Causal Inference",
    "section": "Topics",
    "text": "Topics\n\nPotential Outcomes & ATE — The framework behind all causal questions\nDifference-in-Differences — Parallel trends, treatment effects & event studies\nInverse Probability Weighting — Reweighting to balance treated and control\nEntropy Balancing — Exact moment balancing without a propensity score model\nInstrumental Variables — When X is endogenous, use an instrument to isolate exogenous variation\nRegression Discontinuity — Cutoff rules as natural experiments",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "potential-outcomes.html",
    "href": "potential-outcomes.html",
    "title": "Potential Outcomes & ATE",
    "section": "",
    "text": "For every person, there are two potential outcomes:\n\n\\(Y_i(1)\\): what happens if they get the treatment\n\\(Y_i(0)\\): what happens if they don’t\n\nThe individual treatment effect is \\(\\tau_i = Y_i(1) - Y_i(0)\\). The problem? We only ever observe one of these. A person is either treated or not — never both. The unobserved outcome is the counterfactual.\nThe Average Treatment Effect (ATE) is:\n\\[\\text{ATE} = E[Y(1) - Y(0)]\\]\nSince we can’t observe both for anyone, we need assumptions (like random assignment) to estimate it.\n\n\nIf treatment is assigned randomly, the treated and control groups are comparable on average. The difference in group means is an unbiased estimator of the ATE. But if treatment isn’t random — if sicker people seek treatment — the naive comparison is biased. That’s selection bias.\nThe simulation below lets you see both potential outcomes (which you never get in real life), watch selection bias appear, and see how randomization fixes it.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Population size:\",\n                  min = 100, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = -2, max = 5, value = 2, step = 0.5),\n\n      selectInput(\"assign\", \"Treatment assignment:\",\n                  choices = c(\"Random (coin flip)\",\n                              \"Self-selection (high Y0 seek treatment)\",\n                              \"Self-selection (low Y0 seek treatment)\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"po_plot\", height = \"400px\")),\n        column(6, plotOutput(\"obs_plot\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    ate &lt;- input$ate\n\n    # Potential outcomes\n    y0 &lt;- rnorm(n, mean = 5, sd = 2)\n    y1 &lt;- y0 + ate + rnorm(n, sd = 0.5)\n\n    # Assignment\n    if (input$assign == \"Random (coin flip)\") {\n      treat &lt;- rbinom(n, 1, 0.5)\n    } else if (input$assign == \"Self-selection (high Y0 seek treatment)\") {\n      prob &lt;- pnorm(y0, mean = mean(y0), sd = sd(y0))\n      treat &lt;- rbinom(n, 1, prob)\n    } else {\n      prob &lt;- 1 - pnorm(y0, mean = mean(y0), sd = sd(y0))\n      treat &lt;- rbinom(n, 1, prob)\n    }\n\n    # Observed outcome\n    y_obs &lt;- ifelse(treat == 1, y1, y0)\n\n    # Estimates\n    naive &lt;- mean(y_obs[treat == 1]) - mean(y_obs[treat == 0])\n    true_ate &lt;- mean(y1 - y0)\n\n    list(y0 = y0, y1 = y1, treat = treat, y_obs = y_obs,\n         naive = naive, true_ate = true_ate, ate = ate)\n  })\n\n  output$po_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$y0, d$y1, pch = 16, cex = 0.6,\n         col = ifelse(d$treat == 1, \"#3498db80\", \"#e74c3c80\"),\n         xlab = \"Y(0) — outcome without treatment\",\n         ylab = \"Y(1) — outcome with treatment\",\n         main = \"Both Potential Outcomes (God's view)\")\n    abline(0, 1, lty = 2, col = \"gray40\", lwd = 1.5)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\", \"45° line (no effect)\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray40\"),\n           pch = c(16, 16, NA), lty = c(NA, NA, 2), lwd = c(NA, NA, 1.5))\n  })\n\n  output$obs_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    grp &lt;- factor(d$treat, labels = c(\"Control\", \"Treated\"))\n    boxplot(d$y_obs ~ grp,\n            col = c(\"#e74c3c40\", \"#3498db40\"),\n            border = c(\"#e74c3c\", \"#3498db\"),\n            main = \"What we actually observe\",\n            ylab = \"Observed Y\", xlab = \"\")\n\n    m0 &lt;- mean(d$y_obs[d$treat == 0])\n    m1 &lt;- mean(d$y_obs[d$treat == 1])\n    points(1:2, c(m0, m1), pch = 18, cex = 2.5, col = c(\"#e74c3c\", \"#3498db\"))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$naive - d$true_ate\n    biased &lt;- abs(bias) &gt; 0.3\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", round(d$true_ate, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive estimate:&lt;/b&gt; \", round(d$naive, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(biased, \"bad\", \"good\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        if (biased) \"&lt;br&gt;&lt;small&gt;Selection bias: treated & control groups aren't comparable.&lt;/small&gt;\"\n        else \"&lt;br&gt;&lt;small&gt;Random assignment makes groups comparable.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nStart with random assignment: the naive estimate is close to the true ATE.\nSwitch to self-selection (high Y₀ seek treatment): people who would have done well anyway are the ones getting treated. The naive estimate is too high — that’s positive selection bias.\nSwitch to self-selection (low Y₀ seek treatment): now the opposite. Sicker people seek treatment, making it look less effective than it is.\nThe left plot shows both potential outcomes — something you never see in real data. That’s the fundamental problem.",
    "crumbs": [
      "Potential Outcomes & ATE"
    ]
  },
  {
    "objectID": "potential-outcomes.html#the-fundamental-problem-of-causal-inference",
    "href": "potential-outcomes.html#the-fundamental-problem-of-causal-inference",
    "title": "Potential Outcomes & ATE",
    "section": "",
    "text": "For every person, there are two potential outcomes:\n\n\\(Y_i(1)\\): what happens if they get the treatment\n\\(Y_i(0)\\): what happens if they don’t\n\nThe individual treatment effect is \\(\\tau_i = Y_i(1) - Y_i(0)\\). The problem? We only ever observe one of these. A person is either treated or not — never both. The unobserved outcome is the counterfactual.\nThe Average Treatment Effect (ATE) is:\n\\[\\text{ATE} = E[Y(1) - Y(0)]\\]\nSince we can’t observe both for anyone, we need assumptions (like random assignment) to estimate it.\n\n\nIf treatment is assigned randomly, the treated and control groups are comparable on average. The difference in group means is an unbiased estimator of the ATE. But if treatment isn’t random — if sicker people seek treatment — the naive comparison is biased. That’s selection bias.\nThe simulation below lets you see both potential outcomes (which you never get in real life), watch selection bias appear, and see how randomization fixes it.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Population size:\",\n                  min = 100, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = -2, max = 5, value = 2, step = 0.5),\n\n      selectInput(\"assign\", \"Treatment assignment:\",\n                  choices = c(\"Random (coin flip)\",\n                              \"Self-selection (high Y0 seek treatment)\",\n                              \"Self-selection (low Y0 seek treatment)\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"po_plot\", height = \"400px\")),\n        column(6, plotOutput(\"obs_plot\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    ate &lt;- input$ate\n\n    # Potential outcomes\n    y0 &lt;- rnorm(n, mean = 5, sd = 2)\n    y1 &lt;- y0 + ate + rnorm(n, sd = 0.5)\n\n    # Assignment\n    if (input$assign == \"Random (coin flip)\") {\n      treat &lt;- rbinom(n, 1, 0.5)\n    } else if (input$assign == \"Self-selection (high Y0 seek treatment)\") {\n      prob &lt;- pnorm(y0, mean = mean(y0), sd = sd(y0))\n      treat &lt;- rbinom(n, 1, prob)\n    } else {\n      prob &lt;- 1 - pnorm(y0, mean = mean(y0), sd = sd(y0))\n      treat &lt;- rbinom(n, 1, prob)\n    }\n\n    # Observed outcome\n    y_obs &lt;- ifelse(treat == 1, y1, y0)\n\n    # Estimates\n    naive &lt;- mean(y_obs[treat == 1]) - mean(y_obs[treat == 0])\n    true_ate &lt;- mean(y1 - y0)\n\n    list(y0 = y0, y1 = y1, treat = treat, y_obs = y_obs,\n         naive = naive, true_ate = true_ate, ate = ate)\n  })\n\n  output$po_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$y0, d$y1, pch = 16, cex = 0.6,\n         col = ifelse(d$treat == 1, \"#3498db80\", \"#e74c3c80\"),\n         xlab = \"Y(0) — outcome without treatment\",\n         ylab = \"Y(1) — outcome with treatment\",\n         main = \"Both Potential Outcomes (God's view)\")\n    abline(0, 1, lty = 2, col = \"gray40\", lwd = 1.5)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\", \"45° line (no effect)\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray40\"),\n           pch = c(16, 16, NA), lty = c(NA, NA, 2), lwd = c(NA, NA, 1.5))\n  })\n\n  output$obs_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    grp &lt;- factor(d$treat, labels = c(\"Control\", \"Treated\"))\n    boxplot(d$y_obs ~ grp,\n            col = c(\"#e74c3c40\", \"#3498db40\"),\n            border = c(\"#e74c3c\", \"#3498db\"),\n            main = \"What we actually observe\",\n            ylab = \"Observed Y\", xlab = \"\")\n\n    m0 &lt;- mean(d$y_obs[d$treat == 0])\n    m1 &lt;- mean(d$y_obs[d$treat == 1])\n    points(1:2, c(m0, m1), pch = 18, cex = 2.5, col = c(\"#e74c3c\", \"#3498db\"))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$naive - d$true_ate\n    biased &lt;- abs(bias) &gt; 0.3\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", round(d$true_ate, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive estimate:&lt;/b&gt; \", round(d$naive, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(biased, \"bad\", \"good\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        if (biased) \"&lt;br&gt;&lt;small&gt;Selection bias: treated & control groups aren't comparable.&lt;/small&gt;\"\n        else \"&lt;br&gt;&lt;small&gt;Random assignment makes groups comparable.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nStart with random assignment: the naive estimate is close to the true ATE.\nSwitch to self-selection (high Y₀ seek treatment): people who would have done well anyway are the ones getting treated. The naive estimate is too high — that’s positive selection bias.\nSwitch to self-selection (low Y₀ seek treatment): now the opposite. Sicker people seek treatment, making it look less effective than it is.\nThe left plot shows both potential outcomes — something you never see in real data. That’s the fundamental problem.",
    "crumbs": [
      "Potential Outcomes & ATE"
    ]
  },
  {
    "objectID": "potential-outcomes.html#what-if-you-cant-randomize",
    "href": "potential-outcomes.html#what-if-you-cant-randomize",
    "title": "Potential Outcomes & ATE",
    "section": "What if you can’t randomize?",
    "text": "What if you can’t randomize?\nIn a true experiment (RCT):\n\nYou randomly assign people to treatment vs control\nBecause it’s random, the two groups are identical on average — so any difference in outcomes must be caused by the treatment\n\nBut most questions in economics, policy, and social science can’t be answered with an RCT. You can’t randomly assign poverty, or force some cities to build highways and others not to. So how do you estimate causal effects?\n\nNatural experiments\nA natural experiment is when something in the real world — a policy change, a rule, a geographic boundary, a disaster — creates treatment and control groups that are as-if randomly assigned. Nobody designed it as an experiment, but the logic is the same.\nExample: Say the government announced “all tracts in counties with food desert score &gt; X get a healthy food program.” Tracts at X+1 vs X−1 didn’t choose to be on different sides of that line — the cutoff did it for them. So comparing those tracts is like comparing treatment and control in an experiment.\n\nThe word “natural” = it happened in the real world, not in a lab.\nThe word “experiment” = it created as-if random variation in who got treated.\n\nThe whole point is to get around the selection bias problem the simulation above shows. If people (or firms, or cities) choose their treatment status, the naive comparison is biased. A natural experiment gives you variation that the units didn’t choose.\n\n\nThe rest of this course\nEvery method in this course is a strategy for exploiting natural experiments or otherwise correcting for selection bias:\n\n\n\nMethod\nThe idea\n\n\n\n\nDifference-in-Differences\nCompare changes over time between treated and control groups\n\n\nIPW\nReweight observations so treated and control look similar on observables\n\n\nEntropy Balancing\nDirectly balance covariates between groups without modeling the propensity score\n\n\n\nEach one makes a different assumption about why the comparison is valid. The art of causal inference is choosing the right method for your setting — and being honest about when the assumptions fail.",
    "crumbs": [
      "Potential Outcomes & ATE"
    ]
  },
  {
    "objectID": "did.html",
    "href": "did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "You have two groups: one gets treated at some point, the other never does. You observe both before and after treatment. The key assumption: absent treatment, both groups would have followed parallel trends.\n\\[\\hat{\\tau}_{DID} = (\\bar{Y}_{treat,post} - \\bar{Y}_{treat,pre}) - (\\bar{Y}_{ctrl,post} - \\bar{Y}_{ctrl,pre})\\]\nThe first difference removes time-invariant group characteristics. The second difference removes common time trends. What’s left is the treatment effect.\n\n\nWhen the parallel trends assumption is violated — if the treated group was already on a different trajectory before treatment. The simulation below lets you break this assumption and see the bias that results.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_units\", \"Units per group:\",\n                  min = 20, max = 200, value = 50, step = 10),\n\n      sliderInput(\"true_effect\", \"True treatment effect:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"trend_diff\", \"Differential pre-trend\\n(violation of parallel trends):\",\n                  min = -1, max = 1, value = 0, step = 0.1),\n\n      sliderInput(\"sigma\", \"Noise (SD):\",\n                  min = 0.5, max = 3, value = 1, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"did_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n_units\n    tau   &lt;- input$true_effect\n    delta &lt;- input$trend_diff\n    sigma &lt;- input$sigma\n\n    periods &lt;- -4:4\n    treat_time &lt;- 1  # treatment at t = 1\n\n    # Group means over time\n    ctrl_mean &lt;- 3 + 0.3 * periods\n    treat_mean &lt;- 5 + (0.3 + delta) * periods\n\n    # Add treatment effect post\n    treat_mean[periods &gt;= treat_time] &lt;- treat_mean[periods &gt;= treat_time] + tau\n\n    # Generate unit-level data\n    ctrl_data &lt;- sapply(ctrl_mean, function(m) m + rnorm(n, sd = sigma))\n    treat_data &lt;- sapply(treat_mean, function(m) m + rnorm(n, sd = sigma))\n\n    ctrl_means_obs &lt;- colMeans(ctrl_data)\n    treat_means_obs &lt;- colMeans(treat_data)\n\n    # DID estimate (using t=0 as pre, t=1 as post)\n    pre_idx  &lt;- which(periods == 0)\n    post_idx &lt;- which(periods == 1)\n\n    did_est &lt;- (treat_means_obs[post_idx] - treat_means_obs[pre_idx]) -\n               (ctrl_means_obs[post_idx] - ctrl_means_obs[pre_idx])\n\n    # Counterfactual for treated (parallel to control from t=0)\n    cf &lt;- treat_means_obs[pre_idx] + (ctrl_means_obs - ctrl_means_obs[pre_idx])\n\n    list(periods = periods, ctrl = ctrl_means_obs, treat = treat_means_obs,\n         cf = cf, did_est = did_est, tau = tau, delta = delta,\n         treat_time = treat_time)\n  })\n\n  output$did_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    ylim &lt;- range(c(d$ctrl, d$treat, d$cf)) + c(-0.5, 0.5)\n\n    plot(d$periods, d$treat, type = \"b\", pch = 19, lwd = 2.5, col = \"#3498db\",\n         xlab = \"Time period\", ylab = \"Mean outcome\",\n         main = \"Difference-in-Differences\",\n         ylim = ylim, xaxt = \"n\")\n    axis(1, at = d$periods)\n\n    lines(d$periods, d$ctrl, type = \"b\", pch = 19, lwd = 2.5, col = \"#e74c3c\")\n\n    # Counterfactual (dashed, post only)\n    post &lt;- d$periods &gt;= d$treat_time\n    lines(d$periods[post], d$cf[post], type = \"b\", pch = 1, lwd = 2, lty = 2,\n          col = \"#3498db80\")\n\n    # Treatment onset\n    abline(v = d$treat_time - 0.5, lty = 3, col = \"gray50\", lwd = 1.5)\n    text(d$treat_time - 0.5, ylim[2], \"Treatment\", pos = 4, cex = 0.85, col = \"gray40\")\n\n    # DID bracket\n    pre_idx  &lt;- which(d$periods == 0)\n    post_idx &lt;- which(d$periods == 1)\n    arrows(max(d$periods) - 0.3, d$cf[post_idx],\n           max(d$periods) - 0.3, d$treat[post_idx],\n           code = 3, lwd = 2, col = \"#27ae60\", length = 0.1)\n    text(max(d$periods) - 0.1, (d$cf[post_idx] + d$treat[post_idx]) / 2,\n         paste0(\"DID = \", round(d$did_est, 2)),\n         col = \"#27ae60\", cex = 0.9, adj = 0)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\", \"Counterfactual (parallel trends)\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"#3498db80\"),\n           pch = c(19, 19, 1), lty = c(1, 1, 2), lwd = c(2.5, 2.5, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$did_est - d$tau\n    biased &lt;- abs(d$delta) &gt; 0.05\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;DID estimate:&lt;/b&gt; \", round(d$did_est, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(biased, \"bad\", \"good\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        if (biased) \"&lt;br&gt;&lt;small&gt;Parallel trends violated &mdash; DID is biased.&lt;/small&gt;\"\n        else \"&lt;br&gt;&lt;small&gt;Parallel trends hold &mdash; DID is unbiased.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDifferential pre-trend = 0: parallel trends hold, DID nails the true effect.\nSlide the differential pre-trend to +0.5: the treated group was already rising faster. DID attributes some of that trend to the treatment — the estimate is biased upward.\nSet true effect = 0 with a differential trend: DID “finds” an effect that doesn’t exist. That’s how pre-trend violations create false positives.\nLook at the pre-treatment periods — if the lines aren’t parallel before treatment, you should worry.",
    "crumbs": [
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "did.html#the-idea",
    "href": "did.html#the-idea",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "You have two groups: one gets treated at some point, the other never does. You observe both before and after treatment. The key assumption: absent treatment, both groups would have followed parallel trends.\n\\[\\hat{\\tau}_{DID} = (\\bar{Y}_{treat,post} - \\bar{Y}_{treat,pre}) - (\\bar{Y}_{ctrl,post} - \\bar{Y}_{ctrl,pre})\\]\nThe first difference removes time-invariant group characteristics. The second difference removes common time trends. What’s left is the treatment effect.\n\n\nWhen the parallel trends assumption is violated — if the treated group was already on a different trajectory before treatment. The simulation below lets you break this assumption and see the bias that results.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_units\", \"Units per group:\",\n                  min = 20, max = 200, value = 50, step = 10),\n\n      sliderInput(\"true_effect\", \"True treatment effect:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"trend_diff\", \"Differential pre-trend\\n(violation of parallel trends):\",\n                  min = -1, max = 1, value = 0, step = 0.1),\n\n      sliderInput(\"sigma\", \"Noise (SD):\",\n                  min = 0.5, max = 3, value = 1, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"did_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n_units\n    tau   &lt;- input$true_effect\n    delta &lt;- input$trend_diff\n    sigma &lt;- input$sigma\n\n    periods &lt;- -4:4\n    treat_time &lt;- 1  # treatment at t = 1\n\n    # Group means over time\n    ctrl_mean &lt;- 3 + 0.3 * periods\n    treat_mean &lt;- 5 + (0.3 + delta) * periods\n\n    # Add treatment effect post\n    treat_mean[periods &gt;= treat_time] &lt;- treat_mean[periods &gt;= treat_time] + tau\n\n    # Generate unit-level data\n    ctrl_data &lt;- sapply(ctrl_mean, function(m) m + rnorm(n, sd = sigma))\n    treat_data &lt;- sapply(treat_mean, function(m) m + rnorm(n, sd = sigma))\n\n    ctrl_means_obs &lt;- colMeans(ctrl_data)\n    treat_means_obs &lt;- colMeans(treat_data)\n\n    # DID estimate (using t=0 as pre, t=1 as post)\n    pre_idx  &lt;- which(periods == 0)\n    post_idx &lt;- which(periods == 1)\n\n    did_est &lt;- (treat_means_obs[post_idx] - treat_means_obs[pre_idx]) -\n               (ctrl_means_obs[post_idx] - ctrl_means_obs[pre_idx])\n\n    # Counterfactual for treated (parallel to control from t=0)\n    cf &lt;- treat_means_obs[pre_idx] + (ctrl_means_obs - ctrl_means_obs[pre_idx])\n\n    list(periods = periods, ctrl = ctrl_means_obs, treat = treat_means_obs,\n         cf = cf, did_est = did_est, tau = tau, delta = delta,\n         treat_time = treat_time)\n  })\n\n  output$did_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    ylim &lt;- range(c(d$ctrl, d$treat, d$cf)) + c(-0.5, 0.5)\n\n    plot(d$periods, d$treat, type = \"b\", pch = 19, lwd = 2.5, col = \"#3498db\",\n         xlab = \"Time period\", ylab = \"Mean outcome\",\n         main = \"Difference-in-Differences\",\n         ylim = ylim, xaxt = \"n\")\n    axis(1, at = d$periods)\n\n    lines(d$periods, d$ctrl, type = \"b\", pch = 19, lwd = 2.5, col = \"#e74c3c\")\n\n    # Counterfactual (dashed, post only)\n    post &lt;- d$periods &gt;= d$treat_time\n    lines(d$periods[post], d$cf[post], type = \"b\", pch = 1, lwd = 2, lty = 2,\n          col = \"#3498db80\")\n\n    # Treatment onset\n    abline(v = d$treat_time - 0.5, lty = 3, col = \"gray50\", lwd = 1.5)\n    text(d$treat_time - 0.5, ylim[2], \"Treatment\", pos = 4, cex = 0.85, col = \"gray40\")\n\n    # DID bracket\n    pre_idx  &lt;- which(d$periods == 0)\n    post_idx &lt;- which(d$periods == 1)\n    arrows(max(d$periods) - 0.3, d$cf[post_idx],\n           max(d$periods) - 0.3, d$treat[post_idx],\n           code = 3, lwd = 2, col = \"#27ae60\", length = 0.1)\n    text(max(d$periods) - 0.1, (d$cf[post_idx] + d$treat[post_idx]) / 2,\n         paste0(\"DID = \", round(d$did_est, 2)),\n         col = \"#27ae60\", cex = 0.9, adj = 0)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\", \"Counterfactual (parallel trends)\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"#3498db80\"),\n           pch = c(19, 19, 1), lty = c(1, 1, 2), lwd = c(2.5, 2.5, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$did_est - d$tau\n    biased &lt;- abs(d$delta) &gt; 0.05\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;DID estimate:&lt;/b&gt; \", round(d$did_est, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(biased, \"bad\", \"good\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        if (biased) \"&lt;br&gt;&lt;small&gt;Parallel trends violated &mdash; DID is biased.&lt;/small&gt;\"\n        else \"&lt;br&gt;&lt;small&gt;Parallel trends hold &mdash; DID is unbiased.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDifferential pre-trend = 0: parallel trends hold, DID nails the true effect.\nSlide the differential pre-trend to +0.5: the treated group was already rising faster. DID attributes some of that trend to the treatment — the estimate is biased upward.\nSet true effect = 0 with a differential trend: DID “finds” an effect that doesn’t exist. That’s how pre-trend violations create false positives.\nLook at the pre-treatment periods — if the lines aren’t parallel before treatment, you should worry.",
    "crumbs": [
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "ipw.html",
    "href": "ipw.html",
    "title": "Inverse Probability Weighting",
    "section": "",
    "text": "In observational data, treatment isn’t random. People who get treated differ from those who don’t — they may be older, sicker, richer, etc. A naive comparison of outcomes is biased by these confounders.",
    "crumbs": [
      "IPW"
    ]
  },
  {
    "objectID": "ipw.html#the-problem",
    "href": "ipw.html#the-problem",
    "title": "Inverse Probability Weighting",
    "section": "",
    "text": "In observational data, treatment isn’t random. People who get treated differ from those who don’t — they may be older, sicker, richer, etc. A naive comparison of outcomes is biased by these confounders.",
    "crumbs": [
      "IPW"
    ]
  },
  {
    "objectID": "ipw.html#the-idea",
    "href": "ipw.html#the-idea",
    "title": "Inverse Probability Weighting",
    "section": "The idea",
    "text": "The idea\nInverse Probability Weighting (IPW) reweights observations so that the treated and control groups look alike on observed covariates. The steps:\n\nEstimate the propensity score \\(e(X) = P(\\text{treated} \\mid X)\\) — the probability of treatment given covariates.\nWeight each observation inversely by its probability of receiving the treatment it actually got:\n\nTreated units get weight \\(1 / e(X)\\)\nControl units get weight \\(1 / (1 - e(X))\\)\n\nCompute the weighted difference in means.\n\nIntuition: if a treated person had only a 20% chance of being treated (based on their X), they’re “surprising” — they represent 5 similar people who weren’t treated. So they get upweighted. This creates a pseudo-population where treatment is independent of X.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"confounding\", \"Confounding strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"balance_plot\", height = \"380px\")),\n        column(6, plotOutput(\"ps_plot\", height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    ate  &lt;- input$ate\n    conf &lt;- input$confounding\n\n    # Confounder\n    x &lt;- rnorm(n)\n\n    # Treatment depends on x (confounding)\n    p_true &lt;- pnorm(conf * x)\n    treat &lt;- rbinom(n, 1, p_true)\n\n    # Outcome depends on x and treatment\n    y &lt;- 1 + 2 * x + ate * treat + rnorm(n)\n\n    # Naive estimate\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # IPW estimate\n    ps &lt;- fitted(glm(treat ~ x, family = binomial))\n    w &lt;- ifelse(treat == 1, 1 / ps, 1 / (1 - ps))\n    ipw_est &lt;- weighted.mean(y[treat == 1], w[treat == 1]) -\n               weighted.mean(y[treat == 0], w[treat == 0])\n\n    list(x = x, treat = treat, y = y, ps = ps, w = w,\n         naive = naive, ipw_est = ipw_est, ate = ate)\n  })\n\n  output$balance_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Unweighted densities\n    x_t &lt;- d$x[d$treat == 1]\n    x_c &lt;- d$x[d$treat == 0]\n\n    rng &lt;- range(d$x)\n    dens_t &lt;- density(x_t, from = rng[1], to = rng[2])\n    dens_c &lt;- density(x_c, from = rng[1], to = rng[2])\n\n    ylim &lt;- c(0, max(dens_t$y, dens_c$y) * 1.2)\n\n    plot(dens_t, col = \"#3498db\", lwd = 2.5, main = \"Covariate Balance (X)\",\n         xlab = \"X (confounder)\", ylab = \"Density\", ylim = ylim)\n    lines(dens_c, col = \"#e74c3c\", lwd = 2.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\"),\n           col = c(\"#3498db\", \"#e74c3c\"), lwd = 2.5)\n  })\n\n  output$ps_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$ps, pch = 16, cex = 0.5,\n         col = ifelse(d$treat == 1, \"#3498db80\", \"#e74c3c80\"),\n         xlab = \"X (confounder)\", ylab = \"Propensity score e(X)\",\n         main = \"Propensity Score vs Confounder\")\n    abline(h = 0.5, lty = 2, col = \"gray50\")\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\"),\n           col = c(\"#3498db\", \"#e74c3c\"), pch = 16)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive estimate:&lt;/b&gt; &lt;span class='bad'&gt;\", round(d$naive, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;IPW estimate:&lt;/b&gt; &lt;span class='good'&gt;\", round(d$ipw_est, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Naive bias:&lt;/b&gt; \", round(d$naive - d$ate, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;IPW bias:&lt;/b&gt; \", round(d$ipw_est - d$ate, 3)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nConfounding = 0: treatment is random. Naive and IPW give the same answer.\nConfounding = 1.5: the covariate distributions for treated and control diverge (left plot). Naive is biased, IPW corrects it.\nConfounding = 3: extreme selection. The propensity scores are near 0 or 1 (right plot), meaning some units get huge weights. IPW becomes noisy — this is the extreme weights problem.",
    "crumbs": [
      "IPW"
    ]
  },
  {
    "objectID": "entropy-balancing.html",
    "href": "entropy-balancing.html",
    "title": "Entropy Balancing",
    "section": "",
    "text": "IPW relies on correctly specifying the propensity score model. If you get the model wrong, the weights are wrong, and the estimate is biased. Even if the model is right, extreme propensity scores create extreme weights and noisy estimates.",
    "crumbs": [
      "Entropy Balancing"
    ]
  },
  {
    "objectID": "entropy-balancing.html#the-problem-with-ipw",
    "href": "entropy-balancing.html#the-problem-with-ipw",
    "title": "Entropy Balancing",
    "section": "",
    "text": "IPW relies on correctly specifying the propensity score model. If you get the model wrong, the weights are wrong, and the estimate is biased. Even if the model is right, extreme propensity scores create extreme weights and noisy estimates.",
    "crumbs": [
      "Entropy Balancing"
    ]
  },
  {
    "objectID": "entropy-balancing.html#entropy-balancing-a-different-approach",
    "href": "entropy-balancing.html#entropy-balancing-a-different-approach",
    "title": "Entropy Balancing",
    "section": "Entropy balancing: a different approach",
    "text": "Entropy balancing: a different approach\nEntropy balancing (Hainmueller, 2012) skips the propensity score entirely. Instead, it directly finds weights for the control group that make the covariate distributions exactly match the treated group on specified moments (mean, variance, skewness).\nThe weights are chosen to be as close to uniform as possible (maximum entropy) subject to the balance constraints. This guarantees:\n\nExact balance on the moments you specify\nSmooth weights (no extreme values like IPW can produce)\n\n\nIPW vs Entropy Balancing\n\n\n\n\nIPW\nEntropy Balancing\n\n\n\n\nRequires a propensity score model\nYes\nNo\n\n\nBalance is…\nApproximate (check after)\nExact (by construction)\n\n\nExtreme weights?\nCan be severe\nControlled\n\n\nSensitive to misspecification?\nYes\nLess so\n\n\n\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"confounding\", \"Confounding strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"balance_plot\", height = \"380px\")),\n        column(6, plotOutput(\"weight_plot\",  height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  # Simple entropy balancing: find weights for control group\n  # that match treated group mean of X\n  ebal &lt;- function(x_ctrl, target_mean, max_iter = 200, tol = 1e-6) {\n    n &lt;- length(x_ctrl)\n    lambda &lt;- 0\n    for (i in seq_len(max_iter)) {\n      w &lt;- exp(lambda * x_ctrl)\n      w &lt;- w / sum(w) * n\n      current &lt;- weighted.mean(x_ctrl, w)\n      grad &lt;- current - target_mean\n      if (abs(grad) &lt; tol) break\n      lambda &lt;- lambda - 0.5 * grad\n    }\n    w / sum(w) * n\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    ate  &lt;- input$ate\n    conf &lt;- input$confounding\n\n    x &lt;- rnorm(n)\n    p_true &lt;- pnorm(conf * x)\n    treat &lt;- rbinom(n, 1, p_true)\n\n    y &lt;- 1 + 2 * x + ate * treat + rnorm(n)\n\n    # Naive\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # IPW\n    ps &lt;- fitted(glm(treat ~ x, family = binomial))\n    w_ipw &lt;- ifelse(treat == 1, 1 / ps, 1 / (1 - ps))\n    ipw_est &lt;- weighted.mean(y[treat == 1], w_ipw[treat == 1]) -\n               weighted.mean(y[treat == 0], w_ipw[treat == 0])\n\n    # Entropy balancing (balance control to match treated mean of X)\n    x_ctrl &lt;- x[treat == 0]\n    x_treat_mean &lt;- mean(x[treat == 1])\n    w_eb &lt;- ebal(x_ctrl, x_treat_mean)\n\n    eb_est &lt;- mean(y[treat == 1]) - weighted.mean(y[treat == 0], w_eb)\n\n    # Balance diagnostics\n    ctrl_mean_raw &lt;- mean(x[treat == 0])\n    ctrl_mean_eb  &lt;- weighted.mean(x[treat == 0], w_eb)\n    treat_mean_x  &lt;- x_treat_mean\n\n    list(x = x, treat = treat, y = y,\n         w_ipw = w_ipw, w_eb = w_eb,\n         naive = naive, ipw_est = ipw_est, eb_est = eb_est,\n         ate = ate,\n         ctrl_mean_raw = ctrl_mean_raw,\n         ctrl_mean_eb = ctrl_mean_eb,\n         treat_mean_x = treat_mean_x)\n  })\n\n  output$balance_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 1, 3, 1))\n\n    means &lt;- c(d$treat_mean_x, d$ctrl_mean_raw, d$ctrl_mean_eb)\n    cols &lt;- c(\"#3498db\", \"#e74c3c\", \"#27ae60\")\n    labels &lt;- c(\"Treated\", \"Control\\n(unweighted)\", \"Control\\n(EB weighted)\")\n\n    bp &lt;- barplot(means, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.85,\n                  main = \"Mean of X: Balance Check\",\n                  ylab = \"\", ylim = range(means) * c(0.8, 1.3))\n    text(bp, means + 0.05, round(means, 3), cex = 0.9)\n  })\n\n  output$weight_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ctrl_idx &lt;- which(d$treat == 0)\n\n    w_ipw_ctrl &lt;- d$w_ipw[ctrl_idx]\n    w_eb_ctrl  &lt;- d$w_eb\n\n    xlim &lt;- c(0, max(c(w_ipw_ctrl, w_eb_ctrl)) * 1.1)\n\n    d_ipw &lt;- density(w_ipw_ctrl, from = 0)\n    d_eb  &lt;- density(w_eb_ctrl, from = 0)\n    ylim &lt;- c(0, max(d_ipw$y, d_eb$y) * 1.2)\n\n    plot(d_ipw, col = \"#e74c3c\", lwd = 2.5,\n         main = \"Weight Distributions (Control Units)\",\n         xlab = \"Weight\", ylab = \"Density\",\n         xlim = xlim, ylim = ylim)\n    lines(d_eb, col = \"#27ae60\", lwd = 2.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"IPW weights\", \"Entropy balancing weights\"),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = 2.5)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; &lt;span class='bad'&gt;\", round(d$naive, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(d$naive - d$ate, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;IPW:&lt;/b&gt; \", round(d$ipw_est, 3),\n        \" (bias: \", round(d$ipw_est - d$ate, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;Entropy Bal:&lt;/b&gt; &lt;span class='good'&gt;\", round(d$eb_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(d$eb_est - d$ate, 3), \")\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nConfounding = 1.5: look at the right plot. IPW weights have a long tail (some control units get huge weight). Entropy balancing weights are much smoother.\nConfounding = 3: IPW weights become extreme. EB stays stable.\nLeft plot: the green bar (EB-weighted control mean) exactly matches the blue bar (treated mean). That’s the guarantee — exact balance by construction.\nCompare the bias numbers in the sidebar: EB is typically closer to the true ATE, especially under strong confounding.",
    "crumbs": [
      "Entropy Balancing"
    ]
  }
]