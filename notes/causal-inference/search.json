[
  {
    "objectID": "iv.html",
    "href": "iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "You want the causal effect of \\(X\\) on \\(Y\\), but \\(X\\) is endogenous — correlated with the error term because of confounding, reverse causality, or measurement error. OLS is biased.\nThe fix: find a variable \\(Z\\) (the instrument) that:\n\nRelevance: \\(Z\\) is correlated with \\(X\\) — it actually moves \\(X\\)\nExclusion restriction: \\(Z\\) affects \\(Y\\) only through \\(X\\) — no back doors\n\n\\[Z \\to X \\to Y\\]\nIf both conditions hold, you can use \\(Z\\) to isolate the part of \\(X\\) that’s “as good as random” and estimate the causal effect.\n\n\nThe mechanics are simple:\nFirst stage: regress \\(X\\) on \\(Z\\) to get predicted values \\(\\hat{X}\\)\n\\[X = \\pi_0 + \\pi_1 Z + v\\]\nSecond stage: regress \\(Y\\) on \\(\\hat{X}\\) instead of \\(X\\)\n\\[Y = \\beta_0 + \\beta_1 \\hat{X} + \\varepsilon\\]\nWhy does this work? \\(\\hat{X}\\) contains only the variation in \\(X\\) that comes from \\(Z\\). Since \\(Z\\) is exogenous (by assumption), \\(\\hat{X}\\) is uncorrelated with the error term. The confounding is gone.\n\n\n\n\nRelevance: \\(Z\\) is correlated with \\(X\\) — the instrument actually moves the endogenous variable. Testable: check the first-stage F-statistic.\nExclusion restriction: \\(Z\\) affects \\(Y\\) only through \\(X\\) — no direct effect and no back-door paths. Not testable — you argue it.\nIndependence: \\(Z\\) is as good as randomly assigned — uncorrelated with the error term in the outcome equation\nMonotonicity (for LATE): the instrument moves everyone in the same direction — no “defiers” who do the opposite of what the instrument encourages\n\n\n\n\nReturns to education. You want to know if more schooling causes higher earnings. But ability confounds: smarter people get more education and earn more. OLS overstates the return.\nAngrist & Krueger (1991) used quarter of birth as an instrument. Because of compulsory schooling laws, people born in Q1 can drop out slightly earlier than Q4 births — so quarter of birth affects education (relevance) but presumably doesn’t affect earnings directly (exclusion).\n\n\n\n\nWeak instruments: if \\(Z\\) barely moves \\(X\\), the first stage is weak and the IV estimate becomes wildly noisy and biased. Rule of thumb: first-stage F-statistic &gt; 10.\nExclusion restriction violated: if \\(Z\\) affects \\(Y\\) through channels other than \\(X\\), the estimate is biased. This assumption is untestable — you argue it, you don’t prove it.\n\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 100, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"true_b\", \"True causal effect of X on Y:\",\n                  min = 0, max = 5, value = 2, step = 0.25),\n\n      sliderInput(\"confound\", \"Confounding strength:\",\n                  min = 0, max = 5, value = 3, step = 0.25),\n\n      sliderInput(\"inst_str\", \"Instrument strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.1),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"iv_plot\", height = \"470px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    b   &lt;- input$true_b\n    cf  &lt;- input$confound\n    pi1 &lt;- input$inst_str\n\n    # Confounder (unobserved ability)\n    u &lt;- rnorm(n)\n\n    # Instrument\n    z &lt;- rnorm(n)\n\n    # Endogenous regressor: driven by instrument + confounder\n    x &lt;- pi1 * z + cf * u + rnorm(n)\n\n    # Outcome: causal effect of x + confounder\n    y &lt;- b * x + cf * u + rnorm(n)\n\n    # OLS (biased)\n    ols &lt;- lm(y ~ x)\n\n    # 2SLS by hand\n    first &lt;- lm(x ~ z)\n    x_hat &lt;- fitted(first)\n    second &lt;- lm(y ~ x_hat)\n\n    # First-stage F\n    f_stat &lt;- summary(first)$fstatistic[1]\n\n    list(x = x, y = y, z = z, x_hat = x_hat,\n         b_ols = coef(ols)[2],\n         b_iv = coef(second)[2],\n         first_coef = coef(first)[2],\n         f_stat = f_stat,\n         true_b = b, confound = cf, inst_str = pi1)\n  })\n\n  output$iv_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mfrow = c(1, 2), mar = c(4.5, 4.5, 3, 1))\n\n    # Left: OLS scatter (X vs Y)\n    plot(d$x, d$y, pch = 16, cex = 0.4,\n         col = adjustcolor(\"#3498db\", 0.3),\n         xlab = \"X (endogenous)\", ylab = \"Y\",\n         main = \"OLS: Y on X\")\n    abline(lm(d$y ~ d$x), col = \"#e74c3c\", lwd = 3)\n    abline(a = 0, b = d$true_b, col = \"#27ae60\", lwd = 2, lty = 2)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(paste0(\"OLS = \", round(d$b_ols, 2)),\n                      paste0(\"True = \", d$true_b)),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = c(3, 2), lty = c(1, 2))\n\n    # Right: IV scatter (X-hat vs Y)\n    plot(d$x_hat, d$y, pch = 16, cex = 0.4,\n         col = adjustcolor(\"#9b59b6\", 0.3),\n         xlab = expression(hat(X) ~ \"(from first stage)\"), ylab = \"Y\",\n         main = expression(\"2SLS: Y on \" * hat(X)))\n    abline(lm(d$y ~ d$x_hat), col = \"#e74c3c\", lwd = 3)\n    abline(a = 0, b = d$true_b, col = \"#27ae60\", lwd = 2, lty = 2)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(paste0(\"IV = \", round(d$b_iv, 2)),\n                      paste0(\"True = \", d$true_b)),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = c(3, 2), lty = c(1, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    ols_bias &lt;- d$b_ols - d$true_b\n    iv_bias  &lt;- d$b_iv - d$true_b\n    weak &lt;- d$f_stat &lt; 10\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$true_b, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;OLS:&lt;/b&gt; \", round(d$b_ols, 3),\n        \" &nbsp; Bias: &lt;span class='bad'&gt;\", round(ols_bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;IV (2SLS):&lt;/b&gt; \", round(d$b_iv, 3),\n        \" &nbsp; Bias: &lt;span class='\", ifelse(abs(iv_bias) &lt; abs(ols_bias), \"good\", \"bad\"), \"'&gt;\",\n        round(iv_bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;First-stage F:&lt;/b&gt; \", round(d$f_stat, 1),\n        if (weak) \" &lt;span class='bad'&gt;&lt; 10 (weak!)&lt;/span&gt;\"\n        else \" &lt;span class='good'&gt;&ge; 10&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nConfounding = 3, instrument = 1.5: OLS is badly biased. IV recovers the true effect. This is the whole point.\nSet confounding = 0: OLS and IV agree — when there’s no endogeneity, you don’t need an instrument.\nSlide instrument strength toward 0: the first-stage F drops below 10. The IV estimate becomes erratic — sometimes worse than OLS. That’s the weak instrument problem.\nIncrease sample size with a weak instrument: it doesn’t help much. Weak instruments bias IV toward OLS, and more data doesn’t fix that.\nTrue effect = 0, confounding = 3: OLS “finds” a large effect. IV correctly shows ~0.",
    "crumbs": [
      "Methods",
      "Instrumental Variables"
    ]
  },
  {
    "objectID": "iv.html#the-idea",
    "href": "iv.html#the-idea",
    "title": "Instrumental Variables",
    "section": "",
    "text": "You want the causal effect of \\(X\\) on \\(Y\\), but \\(X\\) is endogenous — correlated with the error term because of confounding, reverse causality, or measurement error. OLS is biased.\nThe fix: find a variable \\(Z\\) (the instrument) that:\n\nRelevance: \\(Z\\) is correlated with \\(X\\) — it actually moves \\(X\\)\nExclusion restriction: \\(Z\\) affects \\(Y\\) only through \\(X\\) — no back doors\n\n\\[Z \\to X \\to Y\\]\nIf both conditions hold, you can use \\(Z\\) to isolate the part of \\(X\\) that’s “as good as random” and estimate the causal effect.\n\n\nThe mechanics are simple:\nFirst stage: regress \\(X\\) on \\(Z\\) to get predicted values \\(\\hat{X}\\)\n\\[X = \\pi_0 + \\pi_1 Z + v\\]\nSecond stage: regress \\(Y\\) on \\(\\hat{X}\\) instead of \\(X\\)\n\\[Y = \\beta_0 + \\beta_1 \\hat{X} + \\varepsilon\\]\nWhy does this work? \\(\\hat{X}\\) contains only the variation in \\(X\\) that comes from \\(Z\\). Since \\(Z\\) is exogenous (by assumption), \\(\\hat{X}\\) is uncorrelated with the error term. The confounding is gone.\n\n\n\n\nRelevance: \\(Z\\) is correlated with \\(X\\) — the instrument actually moves the endogenous variable. Testable: check the first-stage F-statistic.\nExclusion restriction: \\(Z\\) affects \\(Y\\) only through \\(X\\) — no direct effect and no back-door paths. Not testable — you argue it.\nIndependence: \\(Z\\) is as good as randomly assigned — uncorrelated with the error term in the outcome equation\nMonotonicity (for LATE): the instrument moves everyone in the same direction — no “defiers” who do the opposite of what the instrument encourages\n\n\n\n\nReturns to education. You want to know if more schooling causes higher earnings. But ability confounds: smarter people get more education and earn more. OLS overstates the return.\nAngrist & Krueger (1991) used quarter of birth as an instrument. Because of compulsory schooling laws, people born in Q1 can drop out slightly earlier than Q4 births — so quarter of birth affects education (relevance) but presumably doesn’t affect earnings directly (exclusion).\n\n\n\n\nWeak instruments: if \\(Z\\) barely moves \\(X\\), the first stage is weak and the IV estimate becomes wildly noisy and biased. Rule of thumb: first-stage F-statistic &gt; 10.\nExclusion restriction violated: if \\(Z\\) affects \\(Y\\) through channels other than \\(X\\), the estimate is biased. This assumption is untestable — you argue it, you don’t prove it.\n\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 100, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"true_b\", \"True causal effect of X on Y:\",\n                  min = 0, max = 5, value = 2, step = 0.25),\n\n      sliderInput(\"confound\", \"Confounding strength:\",\n                  min = 0, max = 5, value = 3, step = 0.25),\n\n      sliderInput(\"inst_str\", \"Instrument strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.1),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"iv_plot\", height = \"470px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    b   &lt;- input$true_b\n    cf  &lt;- input$confound\n    pi1 &lt;- input$inst_str\n\n    # Confounder (unobserved ability)\n    u &lt;- rnorm(n)\n\n    # Instrument\n    z &lt;- rnorm(n)\n\n    # Endogenous regressor: driven by instrument + confounder\n    x &lt;- pi1 * z + cf * u + rnorm(n)\n\n    # Outcome: causal effect of x + confounder\n    y &lt;- b * x + cf * u + rnorm(n)\n\n    # OLS (biased)\n    ols &lt;- lm(y ~ x)\n\n    # 2SLS by hand\n    first &lt;- lm(x ~ z)\n    x_hat &lt;- fitted(first)\n    second &lt;- lm(y ~ x_hat)\n\n    # First-stage F\n    f_stat &lt;- summary(first)$fstatistic[1]\n\n    list(x = x, y = y, z = z, x_hat = x_hat,\n         b_ols = coef(ols)[2],\n         b_iv = coef(second)[2],\n         first_coef = coef(first)[2],\n         f_stat = f_stat,\n         true_b = b, confound = cf, inst_str = pi1)\n  })\n\n  output$iv_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mfrow = c(1, 2), mar = c(4.5, 4.5, 3, 1))\n\n    # Left: OLS scatter (X vs Y)\n    plot(d$x, d$y, pch = 16, cex = 0.4,\n         col = adjustcolor(\"#3498db\", 0.3),\n         xlab = \"X (endogenous)\", ylab = \"Y\",\n         main = \"OLS: Y on X\")\n    abline(lm(d$y ~ d$x), col = \"#e74c3c\", lwd = 3)\n    abline(a = 0, b = d$true_b, col = \"#27ae60\", lwd = 2, lty = 2)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(paste0(\"OLS = \", round(d$b_ols, 2)),\n                      paste0(\"True = \", d$true_b)),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = c(3, 2), lty = c(1, 2))\n\n    # Right: IV scatter (X-hat vs Y)\n    plot(d$x_hat, d$y, pch = 16, cex = 0.4,\n         col = adjustcolor(\"#9b59b6\", 0.3),\n         xlab = expression(hat(X) ~ \"(from first stage)\"), ylab = \"Y\",\n         main = expression(\"2SLS: Y on \" * hat(X)))\n    abline(lm(d$y ~ d$x_hat), col = \"#e74c3c\", lwd = 3)\n    abline(a = 0, b = d$true_b, col = \"#27ae60\", lwd = 2, lty = 2)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(paste0(\"IV = \", round(d$b_iv, 2)),\n                      paste0(\"True = \", d$true_b)),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = c(3, 2), lty = c(1, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    ols_bias &lt;- d$b_ols - d$true_b\n    iv_bias  &lt;- d$b_iv - d$true_b\n    weak &lt;- d$f_stat &lt; 10\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$true_b, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;OLS:&lt;/b&gt; \", round(d$b_ols, 3),\n        \" &nbsp; Bias: &lt;span class='bad'&gt;\", round(ols_bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;IV (2SLS):&lt;/b&gt; \", round(d$b_iv, 3),\n        \" &nbsp; Bias: &lt;span class='\", ifelse(abs(iv_bias) &lt; abs(ols_bias), \"good\", \"bad\"), \"'&gt;\",\n        round(iv_bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;First-stage F:&lt;/b&gt; \", round(d$f_stat, 1),\n        if (weak) \" &lt;span class='bad'&gt;&lt; 10 (weak!)&lt;/span&gt;\"\n        else \" &lt;span class='good'&gt;&ge; 10&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nConfounding = 3, instrument = 1.5: OLS is badly biased. IV recovers the true effect. This is the whole point.\nSet confounding = 0: OLS and IV agree — when there’s no endogeneity, you don’t need an instrument.\nSlide instrument strength toward 0: the first-stage F drops below 10. The IV estimate becomes erratic — sometimes worse than OLS. That’s the weak instrument problem.\nIncrease sample size with a weak instrument: it doesn’t help much. Weak instruments bias IV toward OLS, and more data doesn’t fix that.\nTrue effect = 0, confounding = 3: OLS “finds” a large effect. IV correctly shows ~0.",
    "crumbs": [
      "Methods",
      "Instrumental Variables"
    ]
  },
  {
    "objectID": "iv.html#what-does-iv-actually-estimate",
    "href": "iv.html#what-does-iv-actually-estimate",
    "title": "Instrumental Variables",
    "section": "What does IV actually estimate?",
    "text": "What does IV actually estimate?\nA subtle point: IV doesn’t estimate the effect for everyone. It estimates the Local Average Treatment Effect (LATE) — the effect for compliers, people whose treatment status is actually changed by the instrument.\nIn the Angrist & Krueger example: IV estimates the return to education for people who would have dropped out if born in a different quarter. It says nothing about people who would have stayed in school regardless.\nThis means two different valid instruments can give you two different IV estimates — not because one is wrong, but because they’re identifying effects for different subpopulations.",
    "crumbs": [
      "Methods",
      "Instrumental Variables"
    ]
  },
  {
    "objectID": "iv.html#did-you-know",
    "href": "iv.html#did-you-know",
    "title": "Instrumental Variables",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe instrumental variables method dates back to Philip Wright (1928), who used it to estimate supply and demand curves for butter and flax seed. Some historians credit his son, Sewall Wright, with the actual derivation.\nThe “weak instruments” problem was formalized by Staiger & Stock (1997). They showed that when the first-stage F is below 10, IV can be more biased than OLS — the cure becomes worse than the disease.\nJoshua Angrist, one of the 2021 Nobel laureates, built much of his career on clever instruments: quarter of birth for schooling, draft lottery numbers for military service, religious composition for family size. The art is finding instruments that are both relevant and excludable.",
    "crumbs": [
      "Methods",
      "Instrumental Variables"
    ]
  },
  {
    "objectID": "identification-estimation.html",
    "href": "identification-estimation.html",
    "title": "Identification vs Estimation",
    "section": "",
    "text": "Every causal inference study answers two questions:\n\nIdentification: Why is this comparison causal? What assumption makes your estimand equal the causal parameter?\nEstimation: How do you compute the number from data? What statistical procedure do you use?\n\nThese are independent. You can have perfect identification with a bad estimator (noisy but unbiased). You can have a brilliant estimator with no identification (precise but wrong). Identification comes first. If your assumptions don’t hold, no estimator can save you.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#two-separate-questions",
    "href": "identification-estimation.html#two-separate-questions",
    "title": "Identification vs Estimation",
    "section": "",
    "text": "Every causal inference study answers two questions:\n\nIdentification: Why is this comparison causal? What assumption makes your estimand equal the causal parameter?\nEstimation: How do you compute the number from data? What statistical procedure do you use?\n\nThese are independent. You can have perfect identification with a bad estimator (noisy but unbiased). You can have a brilliant estimator with no identification (precise but wrong). Identification comes first. If your assumptions don’t hold, no estimator can save you.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#identification-the-assumption",
    "href": "identification-estimation.html#identification-the-assumption",
    "title": "Identification vs Estimation",
    "section": "Identification: the assumption",
    "text": "Identification: the assumption\nIdentification is about the source of exogenous variation — why the variation in treatment you’re using is “as good as random” for estimating a causal effect.\n\n\n\n\n\n\n\n\nIdentification strategy\nThe assumption\nIn words\n\n\n\n\nSelection on observables\n\\(Y(0), Y(1) \\perp D \\mid X\\)\nConditional on X, treatment is as good as random\n\n\nParallel trends\n\\(E[Y(0)_t - Y(0)_{t-1} \\mid D=1] = E[Y(0)_t - Y(0)_{t-1} \\mid D=0]\\)\nAbsent treatment, both groups would have trended the same\n\n\nExclusion restriction\n\\(Z\\) affects \\(Y\\) only through \\(X\\)\nThe instrument has no direct effect on the outcome\n\n\nContinuity\n\\(E[Y(0) \\mid X=x]\\) is continuous at the cutoff\nNo other jump happens at the cutoff\n\n\n\nEach assumption is a claim about the world — not something you compute. You argue it using institutional knowledge, theory, and indirect evidence. Some are partially testable (you can check pre-trends for DID, run a McCrary test for RDD), but none can be fully proven from data.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#estimation-the-computation",
    "href": "identification-estimation.html#estimation-the-computation",
    "title": "Identification vs Estimation",
    "section": "Estimation: the computation",
    "text": "Estimation: the computation\nEstimation is about how you turn data into a number, given that you believe your identification assumption holds.\n\n\n\n\n\n\n\nEstimator\nWhat it does\n\n\n\n\nOLS regression\nFits a linear model, uses coefficients\n\n\nMatching\nPairs treated/control units with similar covariates\n\n\nIPW\nReweights observations by inverse propensity scores\n\n\nEntropy balancing\nFinds weights that exactly balance covariate moments\n\n\nDoubly robust\nCombines regression and weighting\n\n\n2SLS\nTwo-stage regression using predicted values from the first stage\n\n\nLocal polynomial\nFits flexible curves on each side of a cutoff\n\n\nSynthetic control weights\nConstrained optimization to match pre-treatment trends\n\n\nTWFE\nTwo-way fixed effects regression\n\n\n\nThese are tools — they can often be combined with different identification strategies. IPW can implement selection on observables or be used in a DID design (IPW-DID). Regression can adjust for covariates in an RDD or in a cross-sectional study. The estimator doesn’t determine identification; the assumption does.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#research-designs-bundle-both",
    "href": "identification-estimation.html#research-designs-bundle-both",
    "title": "Identification vs Estimation",
    "section": "Research designs bundle both",
    "text": "Research designs bundle both\nWhat we usually call “methods” in applied work — DID, IV, RDD — are really research designs that bundle an identification strategy with a default estimator:\n\n\n\n\n\n\n\n\nResearch design\nIdentification\nCommon estimators\n\n\n\n\nSOO study\nConditional independence\nRegression, matching, IPW, EB, doubly robust\n\n\nDID\nParallel trends\n2×2 difference, TWFE, IPW-DID (Abadie 2005), DR-DID (Sant’Anna & Zhao 2020)\n\n\nIV\nExclusion restriction + relevance\n2SLS, LIML, GMM\n\n\nRDD\nContinuity at cutoff\nLocal polynomial, local randomization\n\n\nSynthetic control\nPre-treatment fit → valid counterfactual\nConstrained weight optimization, augmented SCM\n\n\n\nThis is why the same estimation tool shows up in multiple designs. IPW appears in the SOO column and the DID column — because it’s a tool, not a strategy.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#simulation-identification-matters-estimation-is-secondary",
    "href": "identification-estimation.html#simulation-identification-matters-estimation-is-secondary",
    "title": "Identification vs Estimation",
    "section": "Simulation: identification matters, estimation is secondary",
    "text": "Simulation: identification matters, estimation is secondary\nSame data, same identification assumption, three different estimators. When the assumption holds, they all work. When it doesn’t, they all fail.\n#| standalone: true\n#| viewerHeight: 580\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 13px; line-height: 1.8;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_ie\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate_ie\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"obs_ie\", \"Observed confounding (X):\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      sliderInput(\"unobs_ie\", \"Unobserved confounding (U):\",\n                  min = 0, max = 3, value = 0, step = 0.25),\n\n      actionButton(\"go_ie\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_ie\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"ie_plot\", height = \"420px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go_ie\n    n   &lt;- input$n_ie\n    ate &lt;- input$ate_ie\n    gx  &lt;- input$obs_ie\n    gu  &lt;- input$unobs_ie\n\n    x &lt;- rnorm(n)\n    u &lt;- rnorm(n)\n\n    p &lt;- pnorm(gx * x + gu * u)\n    treat &lt;- rbinom(n, 1, p)\n\n    y &lt;- 1 + 2 * x + 1.5 * u + ate * treat + rnorm(n)\n\n    # Estimator 1: OLS regression controlling for X\n    est_reg &lt;- coef(lm(y ~ treat + x))[2]\n\n    # Estimator 2: IPW\n    ps &lt;- fitted(glm(treat ~ x, family = binomial))\n    ps &lt;- pmin(pmax(ps, 0.01), 0.99)\n    w &lt;- ifelse(treat == 1, 1 / ps, 1 / (1 - ps))\n    est_ipw &lt;- weighted.mean(y[treat == 1], w[treat == 1]) -\n               weighted.mean(y[treat == 0], w[treat == 0])\n\n    # Estimator 3: Matching (simple: nearest neighbor on X)\n    matched_y &lt;- numeric(sum(treat == 1))\n    x_t &lt;- x[treat == 1]\n    y_t &lt;- y[treat == 1]\n    x_c &lt;- x[treat == 0]\n    y_c &lt;- y[treat == 0]\n    for (i in seq_along(x_t)) {\n      nearest &lt;- which.min(abs(x_c - x_t[i]))\n      matched_y[i] &lt;- y_c[nearest]\n    }\n    est_match &lt;- mean(y_t) - mean(matched_y)\n\n    list(est_reg = est_reg, est_ipw = est_ipw, est_match = est_match,\n         ate = ate, gu = gu)\n  })\n\n  output$ie_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(5, 4.5, 3, 1))\n\n    estimates &lt;- c(d$est_reg, d$est_ipw, d$est_match)\n    biases &lt;- estimates - d$ate\n    labels &lt;- c(\"OLS\\nregression\", \"IPW\", \"Nearest-neighbor\\nmatching\")\n\n    cia_holds &lt;- d$gu == 0\n    cols &lt;- ifelse(abs(biases) &lt; 0.5, \"#27ae60\", \"#e74c3c\")\n\n    bp &lt;- barplot(estimates, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.85,\n                  main = ifelse(cia_holds,\n                    \"CIA holds: all estimators work\",\n                    \"CIA violated: all estimators fail\"),\n                  ylab = \"Estimate\",\n                  ylim = c(0, max(estimates, d$ate) * 1.5))\n\n    abline(h = d$ate, lty = 2, col = \"gray40\", lwd = 2)\n    text(0.2, d$ate + 0.15, paste0(\"True ATE = \", d$ate),\n         col = \"gray40\", cex = 0.85, adj = 0)\n\n    text(bp, estimates + 0.2,\n         paste0(round(estimates, 2)),\n         cex = 0.9, font = 2)\n  })\n\n  output$results_ie &lt;- renderUI({\n    d &lt;- dat()\n    cia_holds &lt;- d$gu == 0\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Regression:&lt;/b&gt; \", round(d$est_reg, 2),\n        \" (bias: \", round(d$est_reg - d$ate, 2), \")&lt;br&gt;\",\n        \"&lt;b&gt;IPW:&lt;/b&gt; \", round(d$est_ipw, 2),\n        \" (bias: \", round(d$est_ipw - d$ate, 2), \")&lt;br&gt;\",\n        \"&lt;b&gt;Matching:&lt;/b&gt; \", round(d$est_match, 2),\n        \" (bias: \", round(d$est_match - d$ate, 2), \")&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        if (cia_holds)\n          \"&lt;span class='good'&gt;CIA holds.&lt;/span&gt; All three estimators give similar, roughly unbiased answers. The choice of estimator is secondary.\"\n        else\n          \"&lt;span class='bad'&gt;CIA violated.&lt;/span&gt; All three estimators are biased. Switching estimators doesn't help — you need a different identification strategy.\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nUnobserved confounding = 0: the CIA holds. All three estimators — regression, IPW, matching — give roughly the same answer, close to the true ATE. The choice between them is about efficiency, not bias.\nUnobserved confounding = 2: the CIA is violated. All three estimators are biased in the same direction. Switching from regression to IPW to matching doesn’t help — the problem is identification, not estimation.\nIncrease sample size with unobserved confounding: all three get more precise but stay biased. More data doesn’t fix a broken assumption.\n\nThe lesson: spend your energy on identification, not on the fanciest estimator.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#did-you-know",
    "href": "identification-estimation.html#did-you-know",
    "title": "Identification vs Estimation",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe distinction between identification and estimation was articulated clearly by Charles Manski in his 1995 book Identification Problems in the Social Sciences. He argued that most debates in empirical work are really about identification, not estimation.\nAngrist & Pischke (Mostly Harmless Econometrics, 2009) organized their entire textbook around identification strategies — regression, IV, DID, RDD — rather than estimators. This framing reshaped how a generation of economists thinks about empirical work.\nA common mistake in applied papers: spending pages discussing the estimator (clustered SEs, bootstrap, semiparametric methods) while spending one paragraph on identification. The estimator is the easy part. The hard part is arguing that your comparison is causal.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "rdd.html",
    "href": "rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "Some treatments are assigned by a cutoff rule: you get a scholarship if your test score is above 80, you qualify for a program if your income is below $30k, you win an election if your vote share exceeds 50%.\nRight around the cutoff, people just above and just below are nearly identical — they differ by a fraction of a point. The cutoff creates near-random assignment in a narrow window. That’s the identifying variation.\n\\[\\tau_{RDD} = \\lim_{x \\downarrow c} E[Y \\mid X = x] - \\lim_{x \\uparrow c} E[Y \\mid X = x]\\]\nThe treatment effect is the jump in the outcome at the cutoff. If the outcome is smooth everywhere except at the cutoff, that discontinuity is the causal effect.\n\n\n\nSharp RDD: treatment is a deterministic function of the running variable. Score \\(\\geq\\) 80 → treated, period. Everyone complies.\nFuzzy RDD: the cutoff changes the probability of treatment, but not perfectly. Some people above the cutoff don’t take treatment, some below do. This is essentially an IV problem — the cutoff is the instrument.\n\n\n\n\n\nContinuity: the potential outcomes \\(E[Y(0) \\mid X = x]\\) and \\(E[Y(1) \\mid X = x]\\) are continuous at the cutoff — no other jump happens at exactly that point\nNo manipulation: units cannot precisely control their running variable to sort across the cutoff. If they can, the “as good as random” logic breaks.\nLocal randomization: units just above and just below the cutoff are comparable on all observed and unobserved characteristics\nCorrect functional form: the relationship between the running variable and outcome is correctly modeled within the bandwidth window\n\n\n\n\n\nManipulation: if people can precisely control their score to land just above or below the cutoff, the “as good as random” logic breaks. Check for bunching at the cutoff (McCrary test).\nWrong functional form: if you fit a straight line but the true relationship is curved, you might mistake curvature for a jump. Use local polynomials and check sensitivity to bandwidth.\n\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"tau\", \"True treatment effect:\",\n                  min = 0, max = 5, value = 2, step = 0.25),\n\n      sliderInput(\"sigma\", \"Noise (SD):\",\n                  min = 0.5, max = 4, value = 1.5, step = 0.25),\n\n      sliderInput(\"bw\", \"Bandwidth around cutoff:\",\n                  min = 0.05, max = 0.5, value = 0.2, step = 0.05),\n\n      selectInput(\"curve\", \"True relationship:\",\n                  choices = c(\"Linear\" = \"linear\",\n                              \"Quadratic\" = \"quad\",\n                              \"Flat\" = \"flat\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"rdd_plot\", height = \"480px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n\n    tau   &lt;- input$tau\n    sigma &lt;- input$sigma\n    bw    &lt;- input$bw\n    curve &lt;- input$curve\n\n    # Running variable: uniform on [0, 1], cutoff at 0.5\n    x &lt;- runif(n)\n    cutoff &lt;- 0.5\n    treat &lt;- as.numeric(x &gt;= cutoff)\n\n    # Potential outcome (smooth function of x)\n    if (curve == \"linear\") {\n      mu &lt;- 2 + 1.5 * x\n    } else if (curve == \"quad\") {\n      mu &lt;- 2 + 3 * (x - 0.5)^2\n    } else {\n      mu &lt;- rep(3, n)\n    }\n\n    y &lt;- mu + tau * treat + rnorm(n, sd = sigma)\n\n    # Local linear regression within bandwidth\n    in_bw &lt;- abs(x - cutoff) &lt;= bw\n    x_bw &lt;- x[in_bw]\n    y_bw &lt;- y[in_bw]\n    t_bw &lt;- treat[in_bw]\n\n    # Separate regressions left and right\n    left  &lt;- x_bw &lt; cutoff\n    right &lt;- x_bw &gt;= cutoff\n\n    if (sum(left) &gt; 2 && sum(right) &gt; 2) {\n      fit_l &lt;- lm(y_bw[left] ~ x_bw[left])\n      fit_r &lt;- lm(y_bw[right] ~ x_bw[right])\n\n      # Predicted values at cutoff\n      pred_l &lt;- coef(fit_l)[1] + coef(fit_l)[2] * cutoff\n      pred_r &lt;- coef(fit_r)[1] + coef(fit_r)[2] * cutoff\n\n      rdd_est &lt;- pred_r - pred_l\n\n      # Fitted lines for plotting\n      xseq_l &lt;- seq(cutoff - bw, cutoff, length.out = 100)\n      xseq_r &lt;- seq(cutoff, cutoff + bw, length.out = 100)\n      yhat_l &lt;- coef(fit_l)[1] + coef(fit_l)[2] * xseq_l\n      yhat_r &lt;- coef(fit_r)[1] + coef(fit_r)[2] * xseq_r\n    } else {\n      rdd_est &lt;- NA\n      xseq_l &lt;- xseq_r &lt;- yhat_l &lt;- yhat_r &lt;- NULL\n      pred_l &lt;- pred_r &lt;- NA\n    }\n\n    list(x = x, y = y, treat = treat, cutoff = cutoff,\n         bw = bw, in_bw = in_bw, rdd_est = rdd_est,\n         tau = tau, sigma = sigma,\n         xseq_l = xseq_l, xseq_r = xseq_r,\n         yhat_l = yhat_l, yhat_r = yhat_r,\n         pred_l = pred_l, pred_r = pred_r)\n  })\n\n  output$rdd_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Color by treatment\n    cols &lt;- ifelse(d$treat == 1, adjustcolor(\"#3498db\", 0.25),\n                   adjustcolor(\"#e74c3c\", 0.25))\n\n    # Dim points outside bandwidth\n    cols[!d$in_bw] &lt;- adjustcolor(\"gray70\", 0.15)\n\n    plot(d$x, d$y, pch = 16, cex = 0.5, col = cols,\n         xlab = \"Running variable (X)\", ylab = \"Outcome (Y)\",\n         main = \"Regression Discontinuity Design\")\n\n    # Cutoff line\n    abline(v = d$cutoff, lty = 2, col = \"gray40\", lwd = 1.5)\n\n    # Bandwidth shading\n    rect(d$cutoff - d$bw, par(\"usr\")[3],\n         d$cutoff + d$bw, par(\"usr\")[4],\n         col = adjustcolor(\"#f39c12\", 0.08), border = NA)\n\n    # Local linear fits\n    if (!is.null(d$xseq_l)) {\n      lines(d$xseq_l, d$yhat_l, col = \"#e74c3c\", lwd = 3)\n      lines(d$xseq_r, d$yhat_r, col = \"#3498db\", lwd = 3)\n\n      # Jump arrow\n      arrows(d$cutoff + 0.01, d$pred_l, d$cutoff + 0.01, d$pred_r,\n             code = 3, lwd = 2.5, col = \"#27ae60\", length = 0.1)\n\n      text(d$cutoff + 0.03,\n           (d$pred_l + d$pred_r) / 2,\n           paste0(\"Jump = \", round(d$rdd_est, 2)),\n           col = \"#27ae60\", cex = 0.95, adj = 0, font = 2)\n    }\n\n    text(d$cutoff, par(\"usr\")[4] * 0.98, \"Cutoff\",\n         col = \"gray40\", cex = 0.8, pos = 4)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Control (below cutoff)\", \"Treated (above cutoff)\",\n                      \"Estimation window\"),\n           pch = c(16, 16, 15),\n           col = c(\"#e74c3c\", \"#3498db\", adjustcolor(\"#f39c12\", 0.3)))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    if (is.na(d$rdd_est)) {\n      return(tags$div(class = \"stats-box\",\n        HTML(\"&lt;b&gt;Not enough observations in bandwidth.&lt;/b&gt; Widen it.\")))\n    }\n\n    bias &lt;- d$rdd_est - d$tau\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;RDD estimate:&lt;/b&gt; \", round(d$rdd_est, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(abs(bias) &lt; 0.3, \"good\", \"bad\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;small&gt;Bandwidth: &plusmn;\", d$bw, \" around cutoff&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDefault settings: the jump at the cutoff is clear. The RDD estimate is close to the true effect.\nNarrow the bandwidth (0.05): fewer observations, noisier estimate — but less bias from misspecification. Widen it (0.5): more data, more precise, but you’re using observations far from the cutoff.\nSwitch to quadratic: with a narrow bandwidth, the local linear fit still works fine. But widen the bandwidth with a quadratic DGP and the estimate gets biased — the line can’t capture the curve.\nSet true effect = 0: there should be no visible jump. If you see one, it’s noise (or a bad bandwidth choice).\nCrank up noise: the jump gets harder to see, and you need more data or a wider bandwidth to detect it. This is the bias-variance tradeoff of bandwidth selection.",
    "crumbs": [
      "Methods",
      "Regression Discontinuity"
    ]
  },
  {
    "objectID": "rdd.html#the-idea",
    "href": "rdd.html#the-idea",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "Some treatments are assigned by a cutoff rule: you get a scholarship if your test score is above 80, you qualify for a program if your income is below $30k, you win an election if your vote share exceeds 50%.\nRight around the cutoff, people just above and just below are nearly identical — they differ by a fraction of a point. The cutoff creates near-random assignment in a narrow window. That’s the identifying variation.\n\\[\\tau_{RDD} = \\lim_{x \\downarrow c} E[Y \\mid X = x] - \\lim_{x \\uparrow c} E[Y \\mid X = x]\\]\nThe treatment effect is the jump in the outcome at the cutoff. If the outcome is smooth everywhere except at the cutoff, that discontinuity is the causal effect.\n\n\n\nSharp RDD: treatment is a deterministic function of the running variable. Score \\(\\geq\\) 80 → treated, period. Everyone complies.\nFuzzy RDD: the cutoff changes the probability of treatment, but not perfectly. Some people above the cutoff don’t take treatment, some below do. This is essentially an IV problem — the cutoff is the instrument.\n\n\n\n\n\nContinuity: the potential outcomes \\(E[Y(0) \\mid X = x]\\) and \\(E[Y(1) \\mid X = x]\\) are continuous at the cutoff — no other jump happens at exactly that point\nNo manipulation: units cannot precisely control their running variable to sort across the cutoff. If they can, the “as good as random” logic breaks.\nLocal randomization: units just above and just below the cutoff are comparable on all observed and unobserved characteristics\nCorrect functional form: the relationship between the running variable and outcome is correctly modeled within the bandwidth window\n\n\n\n\n\nManipulation: if people can precisely control their score to land just above or below the cutoff, the “as good as random” logic breaks. Check for bunching at the cutoff (McCrary test).\nWrong functional form: if you fit a straight line but the true relationship is curved, you might mistake curvature for a jump. Use local polynomials and check sensitivity to bandwidth.\n\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"tau\", \"True treatment effect:\",\n                  min = 0, max = 5, value = 2, step = 0.25),\n\n      sliderInput(\"sigma\", \"Noise (SD):\",\n                  min = 0.5, max = 4, value = 1.5, step = 0.25),\n\n      sliderInput(\"bw\", \"Bandwidth around cutoff:\",\n                  min = 0.05, max = 0.5, value = 0.2, step = 0.05),\n\n      selectInput(\"curve\", \"True relationship:\",\n                  choices = c(\"Linear\" = \"linear\",\n                              \"Quadratic\" = \"quad\",\n                              \"Flat\" = \"flat\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"rdd_plot\", height = \"480px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n\n    tau   &lt;- input$tau\n    sigma &lt;- input$sigma\n    bw    &lt;- input$bw\n    curve &lt;- input$curve\n\n    # Running variable: uniform on [0, 1], cutoff at 0.5\n    x &lt;- runif(n)\n    cutoff &lt;- 0.5\n    treat &lt;- as.numeric(x &gt;= cutoff)\n\n    # Potential outcome (smooth function of x)\n    if (curve == \"linear\") {\n      mu &lt;- 2 + 1.5 * x\n    } else if (curve == \"quad\") {\n      mu &lt;- 2 + 3 * (x - 0.5)^2\n    } else {\n      mu &lt;- rep(3, n)\n    }\n\n    y &lt;- mu + tau * treat + rnorm(n, sd = sigma)\n\n    # Local linear regression within bandwidth\n    in_bw &lt;- abs(x - cutoff) &lt;= bw\n    x_bw &lt;- x[in_bw]\n    y_bw &lt;- y[in_bw]\n    t_bw &lt;- treat[in_bw]\n\n    # Separate regressions left and right\n    left  &lt;- x_bw &lt; cutoff\n    right &lt;- x_bw &gt;= cutoff\n\n    if (sum(left) &gt; 2 && sum(right) &gt; 2) {\n      fit_l &lt;- lm(y_bw[left] ~ x_bw[left])\n      fit_r &lt;- lm(y_bw[right] ~ x_bw[right])\n\n      # Predicted values at cutoff\n      pred_l &lt;- coef(fit_l)[1] + coef(fit_l)[2] * cutoff\n      pred_r &lt;- coef(fit_r)[1] + coef(fit_r)[2] * cutoff\n\n      rdd_est &lt;- pred_r - pred_l\n\n      # Fitted lines for plotting\n      xseq_l &lt;- seq(cutoff - bw, cutoff, length.out = 100)\n      xseq_r &lt;- seq(cutoff, cutoff + bw, length.out = 100)\n      yhat_l &lt;- coef(fit_l)[1] + coef(fit_l)[2] * xseq_l\n      yhat_r &lt;- coef(fit_r)[1] + coef(fit_r)[2] * xseq_r\n    } else {\n      rdd_est &lt;- NA\n      xseq_l &lt;- xseq_r &lt;- yhat_l &lt;- yhat_r &lt;- NULL\n      pred_l &lt;- pred_r &lt;- NA\n    }\n\n    list(x = x, y = y, treat = treat, cutoff = cutoff,\n         bw = bw, in_bw = in_bw, rdd_est = rdd_est,\n         tau = tau, sigma = sigma,\n         xseq_l = xseq_l, xseq_r = xseq_r,\n         yhat_l = yhat_l, yhat_r = yhat_r,\n         pred_l = pred_l, pred_r = pred_r)\n  })\n\n  output$rdd_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Color by treatment\n    cols &lt;- ifelse(d$treat == 1, adjustcolor(\"#3498db\", 0.25),\n                   adjustcolor(\"#e74c3c\", 0.25))\n\n    # Dim points outside bandwidth\n    cols[!d$in_bw] &lt;- adjustcolor(\"gray70\", 0.15)\n\n    plot(d$x, d$y, pch = 16, cex = 0.5, col = cols,\n         xlab = \"Running variable (X)\", ylab = \"Outcome (Y)\",\n         main = \"Regression Discontinuity Design\")\n\n    # Cutoff line\n    abline(v = d$cutoff, lty = 2, col = \"gray40\", lwd = 1.5)\n\n    # Bandwidth shading\n    rect(d$cutoff - d$bw, par(\"usr\")[3],\n         d$cutoff + d$bw, par(\"usr\")[4],\n         col = adjustcolor(\"#f39c12\", 0.08), border = NA)\n\n    # Local linear fits\n    if (!is.null(d$xseq_l)) {\n      lines(d$xseq_l, d$yhat_l, col = \"#e74c3c\", lwd = 3)\n      lines(d$xseq_r, d$yhat_r, col = \"#3498db\", lwd = 3)\n\n      # Jump arrow\n      arrows(d$cutoff + 0.01, d$pred_l, d$cutoff + 0.01, d$pred_r,\n             code = 3, lwd = 2.5, col = \"#27ae60\", length = 0.1)\n\n      text(d$cutoff + 0.03,\n           (d$pred_l + d$pred_r) / 2,\n           paste0(\"Jump = \", round(d$rdd_est, 2)),\n           col = \"#27ae60\", cex = 0.95, adj = 0, font = 2)\n    }\n\n    text(d$cutoff, par(\"usr\")[4] * 0.98, \"Cutoff\",\n         col = \"gray40\", cex = 0.8, pos = 4)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Control (below cutoff)\", \"Treated (above cutoff)\",\n                      \"Estimation window\"),\n           pch = c(16, 16, 15),\n           col = c(\"#e74c3c\", \"#3498db\", adjustcolor(\"#f39c12\", 0.3)))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    if (is.na(d$rdd_est)) {\n      return(tags$div(class = \"stats-box\",\n        HTML(\"&lt;b&gt;Not enough observations in bandwidth.&lt;/b&gt; Widen it.\")))\n    }\n\n    bias &lt;- d$rdd_est - d$tau\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;RDD estimate:&lt;/b&gt; \", round(d$rdd_est, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(abs(bias) &lt; 0.3, \"good\", \"bad\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;small&gt;Bandwidth: &plusmn;\", d$bw, \" around cutoff&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDefault settings: the jump at the cutoff is clear. The RDD estimate is close to the true effect.\nNarrow the bandwidth (0.05): fewer observations, noisier estimate — but less bias from misspecification. Widen it (0.5): more data, more precise, but you’re using observations far from the cutoff.\nSwitch to quadratic: with a narrow bandwidth, the local linear fit still works fine. But widen the bandwidth with a quadratic DGP and the estimate gets biased — the line can’t capture the curve.\nSet true effect = 0: there should be no visible jump. If you see one, it’s noise (or a bad bandwidth choice).\nCrank up noise: the jump gets harder to see, and you need more data or a wider bandwidth to detect it. This is the bias-variance tradeoff of bandwidth selection.",
    "crumbs": [
      "Methods",
      "Regression Discontinuity"
    ]
  },
  {
    "objectID": "rdd.html#the-bandwidth-tradeoff",
    "href": "rdd.html#the-bandwidth-tradeoff",
    "title": "Regression Discontinuity",
    "section": "The bandwidth tradeoff",
    "text": "The bandwidth tradeoff\nBandwidth is the central tuning parameter in RDD:\n\n\n\n\n\n\n\n\n\nNarrow bandwidth\nWide bandwidth\n\n\n\n\nBias\nLow — only using near-identical units\nHigh — far-away units may differ systematically\n\n\nVariance\nHigh — few observations\nLow — more data\n\n\nRisk\nNoisy, imprecise estimate\nPrecise but potentially wrong\n\n\n\nIn practice, researchers use data-driven bandwidth selectors (Imbens & Kalyanaraman 2012, Calonico, Cattaneo & Titiunik 2014) that optimize this tradeoff. You should always check that your results are robust to different bandwidth choices.",
    "crumbs": [
      "Methods",
      "Regression Discontinuity"
    ]
  },
  {
    "objectID": "rdd.html#did-you-know",
    "href": "rdd.html#did-you-know",
    "title": "Regression Discontinuity",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe RDD idea goes back to Thistlethwaite & Campbell (1960), who studied the effect of merit scholarships on career outcomes using the score cutoff. It was largely ignored for decades until economists rediscovered it in the late 1990s.\nLee (2008) used RDD to study the incumbency advantage in US elections — candidates who barely win vs barely lose. This paper helped establish RDD as a workhorse method in political economy.\nCattaneo, Idrobo & Titiunik wrote an excellent practical guide: A Practical Introduction to Regression Discontinuity Designs. It’s freely available and covers everything from basic plots to formal inference.",
    "crumbs": [
      "Methods",
      "Regression Discontinuity"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Causal Inference",
    "section": "",
    "text": "Causal inference methods — each topic pairs explanation with an interactive simulation you can run in the browser.\nBuilds on: Statistical Inference",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#framework",
    "href": "index.html#framework",
    "title": "Causal Inference",
    "section": "Framework",
    "text": "Framework\n\nPotential Outcomes & ATE — The framework behind all causal questions\nIdentification vs Estimation — The distinction that organizes everything",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#methods",
    "href": "index.html#methods",
    "title": "Causal Inference",
    "section": "Methods",
    "text": "Methods\n\nSelection on Observables — When conditioning on X is enough\nDifference-in-Differences — Parallel trends & treatment effects\nInstrumental Variables — Exogenous variation to isolate causal effects\nRegression Discontinuity — Cutoff rules as natural experiments\nSynthetic Control — Building a counterfactual from weighted donors",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#estimation-tools",
    "href": "index.html#estimation-tools",
    "title": "Causal Inference",
    "section": "Estimation Tools",
    "text": "Estimation Tools\nTools that can be paired with different research designs.\n\nInverse Probability Weighting — Reweighting to balance treated and control\nEntropy Balancing — Exact moment balancing without a propensity score model",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "potential-outcomes.html",
    "href": "potential-outcomes.html",
    "title": "Potential Outcomes & ATE",
    "section": "",
    "text": "For every person, there are two potential outcomes:\n\n\\(Y_i(1)\\): what happens if they get the treatment\n\\(Y_i(0)\\): what happens if they don’t\n\nThe individual treatment effect is \\(\\tau_i = Y_i(1) - Y_i(0)\\). The problem? We only ever observe one of these. A person is either treated or not — never both. The unobserved outcome is the counterfactual.\nThe Average Treatment Effect (ATE) is:\n\\[\\text{ATE} = E[Y(1) - Y(0)]\\]\nSince we can’t observe both for anyone, we need assumptions (like random assignment) to estimate it.\n\n\n\nSUTVA (Stable Unit Treatment Value Assumption): one person’s treatment doesn’t affect another person’s outcome — no interference between units\nConsistency: the observed outcome for a treated person equals their potential outcome under treatment, \\(Y_i = Y_i(1)\\) if treated\nRandom assignment (for unbiased estimation): treatment is independent of potential outcomes — \\(Y(0), Y(1) \\perp D\\)\n\n\n\n\nIf treatment is assigned randomly, the treated and control groups are comparable on average. The difference in group means is an unbiased estimator of the ATE. But if treatment isn’t random — if sicker people seek treatment — the naive comparison is biased. That’s selection bias.\nThe simulation below lets you see both potential outcomes (which you never get in real life), watch selection bias appear, and see how randomization fixes it.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Population size:\",\n                  min = 100, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = -2, max = 5, value = 2, step = 0.5),\n\n      selectInput(\"assign\", \"Treatment assignment:\",\n                  choices = c(\"Random (coin flip)\",\n                              \"Self-selection (high Y0 seek treatment)\",\n                              \"Self-selection (low Y0 seek treatment)\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"po_plot\", height = \"400px\")),\n        column(6, plotOutput(\"obs_plot\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    ate &lt;- input$ate\n\n    # Potential outcomes\n    y0 &lt;- rnorm(n, mean = 5, sd = 2)\n    y1 &lt;- y0 + ate + rnorm(n, sd = 0.5)\n\n    # Assignment\n    if (input$assign == \"Random (coin flip)\") {\n      treat &lt;- rbinom(n, 1, 0.5)\n    } else if (input$assign == \"Self-selection (high Y0 seek treatment)\") {\n      prob &lt;- pnorm(y0, mean = mean(y0), sd = sd(y0))\n      treat &lt;- rbinom(n, 1, prob)\n    } else {\n      prob &lt;- 1 - pnorm(y0, mean = mean(y0), sd = sd(y0))\n      treat &lt;- rbinom(n, 1, prob)\n    }\n\n    # Observed outcome\n    y_obs &lt;- ifelse(treat == 1, y1, y0)\n\n    # Estimates\n    naive &lt;- mean(y_obs[treat == 1]) - mean(y_obs[treat == 0])\n    true_ate &lt;- mean(y1 - y0)\n\n    list(y0 = y0, y1 = y1, treat = treat, y_obs = y_obs,\n         naive = naive, true_ate = true_ate, ate = ate)\n  })\n\n  output$po_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$y0, d$y1, pch = 16, cex = 0.6,\n         col = ifelse(d$treat == 1, \"#3498db80\", \"#e74c3c80\"),\n         xlab = \"Y(0) — outcome without treatment\",\n         ylab = \"Y(1) — outcome with treatment\",\n         main = \"Both Potential Outcomes (God's view)\")\n    abline(0, 1, lty = 2, col = \"gray40\", lwd = 1.5)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\", \"45° line (no effect)\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray40\"),\n           pch = c(16, 16, NA), lty = c(NA, NA, 2), lwd = c(NA, NA, 1.5))\n  })\n\n  output$obs_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    grp &lt;- factor(d$treat, labels = c(\"Control\", \"Treated\"))\n    boxplot(d$y_obs ~ grp,\n            col = c(\"#e74c3c40\", \"#3498db40\"),\n            border = c(\"#e74c3c\", \"#3498db\"),\n            main = \"What we actually observe\",\n            ylab = \"Observed Y\", xlab = \"\")\n\n    m0 &lt;- mean(d$y_obs[d$treat == 0])\n    m1 &lt;- mean(d$y_obs[d$treat == 1])\n    points(1:2, c(m0, m1), pch = 18, cex = 2.5, col = c(\"#e74c3c\", \"#3498db\"))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$naive - d$true_ate\n    biased &lt;- abs(bias) &gt; 0.3\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", round(d$true_ate, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive estimate:&lt;/b&gt; \", round(d$naive, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(biased, \"bad\", \"good\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        if (biased) \"&lt;br&gt;&lt;small&gt;Selection bias: treated & control groups aren't comparable.&lt;/small&gt;\"\n        else \"&lt;br&gt;&lt;small&gt;Random assignment makes groups comparable.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nStart with random assignment: the naive estimate is close to the true ATE.\nSwitch to self-selection (high Y₀ seek treatment): people who would have done well anyway are the ones getting treated. The naive estimate is too high — that’s positive selection bias.\nSwitch to self-selection (low Y₀ seek treatment): now the opposite. Sicker people seek treatment, making it look less effective than it is.\nThe left plot shows both potential outcomes — something you never see in real data. That’s the fundamental problem.",
    "crumbs": [
      "Framework",
      "Potential Outcomes & ATE"
    ]
  },
  {
    "objectID": "potential-outcomes.html#the-fundamental-problem-of-causal-inference",
    "href": "potential-outcomes.html#the-fundamental-problem-of-causal-inference",
    "title": "Potential Outcomes & ATE",
    "section": "",
    "text": "For every person, there are two potential outcomes:\n\n\\(Y_i(1)\\): what happens if they get the treatment\n\\(Y_i(0)\\): what happens if they don’t\n\nThe individual treatment effect is \\(\\tau_i = Y_i(1) - Y_i(0)\\). The problem? We only ever observe one of these. A person is either treated or not — never both. The unobserved outcome is the counterfactual.\nThe Average Treatment Effect (ATE) is:\n\\[\\text{ATE} = E[Y(1) - Y(0)]\\]\nSince we can’t observe both for anyone, we need assumptions (like random assignment) to estimate it.\n\n\n\nSUTVA (Stable Unit Treatment Value Assumption): one person’s treatment doesn’t affect another person’s outcome — no interference between units\nConsistency: the observed outcome for a treated person equals their potential outcome under treatment, \\(Y_i = Y_i(1)\\) if treated\nRandom assignment (for unbiased estimation): treatment is independent of potential outcomes — \\(Y(0), Y(1) \\perp D\\)\n\n\n\n\nIf treatment is assigned randomly, the treated and control groups are comparable on average. The difference in group means is an unbiased estimator of the ATE. But if treatment isn’t random — if sicker people seek treatment — the naive comparison is biased. That’s selection bias.\nThe simulation below lets you see both potential outcomes (which you never get in real life), watch selection bias appear, and see how randomization fixes it.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Population size:\",\n                  min = 100, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = -2, max = 5, value = 2, step = 0.5),\n\n      selectInput(\"assign\", \"Treatment assignment:\",\n                  choices = c(\"Random (coin flip)\",\n                              \"Self-selection (high Y0 seek treatment)\",\n                              \"Self-selection (low Y0 seek treatment)\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"po_plot\", height = \"400px\")),\n        column(6, plotOutput(\"obs_plot\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    ate &lt;- input$ate\n\n    # Potential outcomes\n    y0 &lt;- rnorm(n, mean = 5, sd = 2)\n    y1 &lt;- y0 + ate + rnorm(n, sd = 0.5)\n\n    # Assignment\n    if (input$assign == \"Random (coin flip)\") {\n      treat &lt;- rbinom(n, 1, 0.5)\n    } else if (input$assign == \"Self-selection (high Y0 seek treatment)\") {\n      prob &lt;- pnorm(y0, mean = mean(y0), sd = sd(y0))\n      treat &lt;- rbinom(n, 1, prob)\n    } else {\n      prob &lt;- 1 - pnorm(y0, mean = mean(y0), sd = sd(y0))\n      treat &lt;- rbinom(n, 1, prob)\n    }\n\n    # Observed outcome\n    y_obs &lt;- ifelse(treat == 1, y1, y0)\n\n    # Estimates\n    naive &lt;- mean(y_obs[treat == 1]) - mean(y_obs[treat == 0])\n    true_ate &lt;- mean(y1 - y0)\n\n    list(y0 = y0, y1 = y1, treat = treat, y_obs = y_obs,\n         naive = naive, true_ate = true_ate, ate = ate)\n  })\n\n  output$po_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$y0, d$y1, pch = 16, cex = 0.6,\n         col = ifelse(d$treat == 1, \"#3498db80\", \"#e74c3c80\"),\n         xlab = \"Y(0) — outcome without treatment\",\n         ylab = \"Y(1) — outcome with treatment\",\n         main = \"Both Potential Outcomes (God's view)\")\n    abline(0, 1, lty = 2, col = \"gray40\", lwd = 1.5)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\", \"45° line (no effect)\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray40\"),\n           pch = c(16, 16, NA), lty = c(NA, NA, 2), lwd = c(NA, NA, 1.5))\n  })\n\n  output$obs_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    grp &lt;- factor(d$treat, labels = c(\"Control\", \"Treated\"))\n    boxplot(d$y_obs ~ grp,\n            col = c(\"#e74c3c40\", \"#3498db40\"),\n            border = c(\"#e74c3c\", \"#3498db\"),\n            main = \"What we actually observe\",\n            ylab = \"Observed Y\", xlab = \"\")\n\n    m0 &lt;- mean(d$y_obs[d$treat == 0])\n    m1 &lt;- mean(d$y_obs[d$treat == 1])\n    points(1:2, c(m0, m1), pch = 18, cex = 2.5, col = c(\"#e74c3c\", \"#3498db\"))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$naive - d$true_ate\n    biased &lt;- abs(bias) &gt; 0.3\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", round(d$true_ate, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive estimate:&lt;/b&gt; \", round(d$naive, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(biased, \"bad\", \"good\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        if (biased) \"&lt;br&gt;&lt;small&gt;Selection bias: treated & control groups aren't comparable.&lt;/small&gt;\"\n        else \"&lt;br&gt;&lt;small&gt;Random assignment makes groups comparable.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nStart with random assignment: the naive estimate is close to the true ATE.\nSwitch to self-selection (high Y₀ seek treatment): people who would have done well anyway are the ones getting treated. The naive estimate is too high — that’s positive selection bias.\nSwitch to self-selection (low Y₀ seek treatment): now the opposite. Sicker people seek treatment, making it look less effective than it is.\nThe left plot shows both potential outcomes — something you never see in real data. That’s the fundamental problem.",
    "crumbs": [
      "Framework",
      "Potential Outcomes & ATE"
    ]
  },
  {
    "objectID": "potential-outcomes.html#what-if-you-cant-randomize",
    "href": "potential-outcomes.html#what-if-you-cant-randomize",
    "title": "Potential Outcomes & ATE",
    "section": "What if you can’t randomize?",
    "text": "What if you can’t randomize?\nIn a true experiment (RCT):\n\nYou randomly assign people to treatment vs control\nBecause it’s random, the two groups are identical on average — so any difference in outcomes must be caused by the treatment\n\nBut most questions in economics, policy, and social science can’t be answered with an RCT. You can’t randomly assign poverty, or force some cities to build highways and others not to. So how do you estimate causal effects?\n\nNatural experiments\nA natural experiment is when something in the real world — a policy change, a rule, a geographic boundary, a disaster — creates treatment and control groups that are as-if randomly assigned. Nobody designed it as an experiment, but the logic is the same.\nExample: Say the government announced “all tracts in counties with food desert score &gt; X get a healthy food program.” Tracts at X+1 vs X−1 didn’t choose to be on different sides of that line — the cutoff did it for them. So comparing those tracts is like comparing treatment and control in an experiment.\n\nThe word “natural” = it happened in the real world, not in a lab.\nThe word “experiment” = it created as-if random variation in who got treated.\n\nThe whole point is to get around the selection bias problem the simulation above shows. If people (or firms, or cities) choose their treatment status, the naive comparison is biased. A natural experiment gives you variation that the units didn’t choose.\n\n\nThe rest of this course\nEvery method in this course is a strategy for exploiting natural experiments or otherwise correcting for selection bias:\n\n\n\nMethod\nThe idea\n\n\n\n\nSelection on Observables\nCondition on everything that drives both treatment and outcome\n\n\nIPW\nReweight observations so treated and control look similar on observables\n\n\nEntropy Balancing\nDirectly balance covariates between groups without modeling the propensity score\n\n\nDifference-in-Differences\nCompare changes over time between treated and control groups\n\n\nInstrumental Variables\nUse exogenous variation to isolate the causal effect\n\n\nRegression Discontinuity\nExploit a cutoff rule as a natural experiment\n\n\nSynthetic Control\nBuild a weighted counterfactual from donor units\n\n\n\nEach one makes a different assumption about why the comparison is valid. The art of causal inference is choosing the right method for your setting — and being honest about when the assumptions fail.",
    "crumbs": [
      "Framework",
      "Potential Outcomes & ATE"
    ]
  },
  {
    "objectID": "synth.html",
    "href": "synth.html",
    "title": "Synthetic Control",
    "section": "",
    "text": "You have one treated unit — a state that passed a law, a country hit by a crisis, a company that changed policy. You want to know what would have happened without the treatment. But there’s no single control unit that’s a good comparison.\nThe synthetic control method builds a weighted combination of untreated units that matches the treated unit’s pre-treatment trajectory. That weighted combination — the “synthetic” version — serves as the counterfactual.\n\\[\\hat{Y}_{1t}^{N} = \\sum_{j=2}^{J+1} w_j \\, Y_{jt}\\]\nwhere \\(w_j \\geq 0\\) and \\(\\sum w_j = 1\\). The weights are chosen so that the synthetic unit tracks the treated unit closely before treatment. After treatment, the gap between the treated unit and its synthetic version is the estimated effect.\n\n\nAbadie, Diamond & Hainmueller (2010): California passed Proposition 99 in 1988, a major tobacco control program. No single state is a good comparison — some are too urban, some too rural, some already had anti-smoking laws. The synthetic California is a weighted mix of states (Utah, Nevada, Colorado, Connecticut, Montana…) that together match California’s pre-1988 smoking trend almost exactly. After 1988, actual California diverges sharply below its synthetic version — that gap is the treatment effect.\n\n\n\n\nNo interference / SUTVA: the treatment of the treated unit doesn’t affect the donor units (no spillovers)\nConvex hull: the treated unit’s pre-treatment outcomes can be expressed as a weighted average of the donors — the treated unit isn’t an outlier that no combination of donors can match\nNo anticipation: the treated unit doesn’t change behavior before the treatment date\nCommon factors: treated and donor units are driven by the same underlying factors, just with different loadings — the weights that work pre-treatment continue to work post-treatment\n\n\n\n\n\nBad pre-treatment fit: if the synthetic unit can’t track the treated unit before treatment, you can’t trust the post-treatment gap. There’s no magic — if no combination of donors resembles the treated unit, the method doesn’t work.\nSpillovers: if the treatment affects the donor units too (e.g., smokers move from California to Nevada), the synthetic control is contaminated.\nToo few donors: with very few comparison units, the weights are forced and the match may be poor.\n\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n    .wt-table { font-size: 12px; margin-top: 6px; }\n    .wt-table td { padding: 1px 6px; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"tau_sc\", \"True treatment effect:\",\n                  min = -5, max = 5, value = -3, step = 0.5),\n\n      sliderInput(\"n_donors\", \"Number of donor units:\",\n                  min = 3, max = 15, value = 8, step = 1),\n\n      sliderInput(\"sigma_sc\", \"Noise (SD):\",\n                  min = 0.1, max = 2, value = 0.5, step = 0.1),\n\n      sliderInput(\"treat_time\", \"Treatment period:\",\n                  min = 8, max = 18, value = 12, step = 1),\n\n      actionButton(\"go_sc\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_sc\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"synth_plot\", height = \"500px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go_sc\n    tau   &lt;- input$tau_sc\n    J     &lt;- input$n_donors\n    sigma &lt;- input$sigma_sc\n    T0    &lt;- input$treat_time\n    TT    &lt;- 25\n\n    # Generate donor unit trajectories\n    # Each donor has its own intercept and slope\n    set.seed(NULL)\n    intercepts &lt;- rnorm(J, mean = 10, sd = 2)\n    slopes     &lt;- rnorm(J, mean = 0.3, sd = 0.15)\n    donors &lt;- matrix(NA, nrow = TT, ncol = J)\n    for (j in 1:J) {\n      donors[, j] &lt;- intercepts[j] + slopes[j] * (1:TT) + rnorm(TT, sd = sigma)\n    }\n\n    # True weights (sparse: pick 3-4 donors that matter)\n    n_active &lt;- min(4, J)\n    active &lt;- sample(1:J, n_active)\n    true_w &lt;- rep(0, J)\n    raw &lt;- runif(n_active, 0.1, 1)\n    true_w[active] &lt;- raw / sum(raw)\n\n    # Treated unit = weighted combo of donors + treatment effect after T0\n    treated &lt;- donors %*% true_w + rnorm(TT, sd = sigma * 0.5)\n    treated[(T0 + 1):TT] &lt;- treated[(T0 + 1):TT] + tau\n\n    # Estimate synthetic control weights (OLS on pre-period, constrained to sum to 1)\n    # Simple approach: non-negative least squares via iterative projection\n    pre &lt;- 1:T0\n    Y1_pre &lt;- treated[pre]\n    Y0_pre &lt;- donors[pre, ]\n\n    # Use a simple regression + normalize approach\n    # Unconstrained OLS, then clip negatives and renormalize\n    if (J &lt;= T0) {\n      fit &lt;- lm(Y1_pre ~ Y0_pre - 1)\n      w_hat &lt;- coef(fit)\n    } else {\n      # More donors than periods: use ridge-like approach\n      lambda &lt;- 0.01\n      w_hat &lt;- solve(t(Y0_pre) %*% Y0_pre + lambda * diag(J),\n                     t(Y0_pre) %*% Y1_pre)\n    }\n    w_hat[w_hat &lt; 0] &lt;- 0\n    if (sum(w_hat) &gt; 0) w_hat &lt;- w_hat / sum(w_hat) else w_hat &lt;- rep(1/J, J)\n\n    # Synthetic control trajectory\n    synth &lt;- donors %*% w_hat\n\n    # Estimated effect (post-treatment gap)\n    post &lt;- (T0 + 1):TT\n    gaps &lt;- treated[post] - synth[post]\n    avg_effect &lt;- mean(gaps)\n\n    # Pre-treatment fit (RMSPE)\n    pre_rmspe &lt;- sqrt(mean((treated[pre] - synth[pre])^2))\n\n    list(time = 1:TT, treated = as.numeric(treated),\n         synth = as.numeric(synth), donors = donors,\n         w_hat = w_hat, T0 = T0, tau = tau,\n         avg_effect = avg_effect, pre_rmspe = pre_rmspe, J = J)\n  })\n\n  output$synth_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ylim &lt;- range(c(d$treated, d$synth, d$donors)) + c(-1, 1)\n\n    # Donor units (gray background)\n    plot(d$time, d$donors[, 1], type = \"n\",\n         xlab = \"Time\", ylab = \"Outcome\",\n         main = \"Synthetic Control Method\",\n         ylim = ylim)\n\n    for (j in 1:d$J) {\n      lines(d$time, d$donors[, j], col = adjustcolor(\"gray70\", 0.4), lwd = 0.8)\n    }\n\n    # Synthetic control\n    lines(d$time, d$synth, col = \"#e74c3c\", lwd = 3, lty = 2)\n\n    # Treated unit\n    lines(d$time, d$treated, col = \"#3498db\", lwd = 3)\n\n    # Treatment line\n    abline(v = d$T0 + 0.5, lty = 3, col = \"gray40\", lwd = 1.5)\n    text(d$T0 + 0.5, ylim[2], \"Treatment\", pos = 4, cex = 0.85, col = \"gray40\")\n\n    # Gap shading in post period\n    post &lt;- (d$T0 + 1):length(d$time)\n    polygon(c(d$time[post], rev(d$time[post])),\n            c(d$treated[post], rev(d$synth[post])),\n            col = adjustcolor(\"#27ae60\", 0.15), border = NA)\n\n    # Gap label\n    mid_post &lt;- d$time[round(median(post))]\n    mid_gap &lt;- (d$treated[round(median(post))] + d$synth[round(median(post))]) / 2\n    text(mid_post, mid_gap,\n         paste0(\"Avg gap = \", round(d$avg_effect, 2)),\n         col = \"#27ae60\", font = 2, cex = 0.9)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated unit\", \"Synthetic control\", \"Donor units\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray70\"),\n           lwd = c(3, 3, 1), lty = c(1, 2, 1))\n  })\n\n  output$results_sc &lt;- renderUI({\n    d &lt;- dat()\n\n    # Top weights\n    ord &lt;- order(d$w_hat, decreasing = TRUE)\n    top &lt;- ord[d$w_hat[ord] &gt; 0.01]\n    wt_rows &lt;- paste0(\n      sapply(top, function(j) {\n        paste0(\"&lt;tr&gt;&lt;td&gt;Donor \", j, \"&lt;/td&gt;&lt;td&gt;&lt;b&gt;\",\n               round(d$w_hat[j] * 100), \"%&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;\")\n      }),\n      collapse = \"\"\n    )\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;Avg post-treatment gap:&lt;/b&gt; \",\n        \"&lt;span class='\", ifelse(abs(d$avg_effect - d$tau) &lt; 1, \"good\", \"bad\"), \"'&gt;\",\n        round(d$avg_effect, 2), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Pre-treatment RMSPE:&lt;/b&gt; \", round(d$pre_rmspe, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Weights:&lt;/b&gt;\",\n        \"&lt;table class='wt-table'&gt;\", wt_rows, \"&lt;/table&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDefault settings (effect = -3): the synthetic control (red dashed) tracks the treated unit closely before treatment, then diverges. The green-shaded gap is the estimated effect.\nSet true effect = 0: the two lines should stay close after treatment too. If you see a big gap, it’s noise — this is why pre-treatment fit matters.\nReduce donors to 3: fewer building blocks means a worse pre-treatment fit. The estimate gets noisier.\nIncrease donors to 15: more building blocks, better fit. But watch the weights — most donors get zero weight. The method is naturally sparse.\nMove treatment period later (18): short post-period, harder to judge whether the gap is real or just noise.\nCrank up noise: the pre-treatment fit deteriorates and the post-treatment gap becomes unreliable.",
    "crumbs": [
      "Methods",
      "Synthetic Control"
    ]
  },
  {
    "objectID": "synth.html#the-idea",
    "href": "synth.html#the-idea",
    "title": "Synthetic Control",
    "section": "",
    "text": "You have one treated unit — a state that passed a law, a country hit by a crisis, a company that changed policy. You want to know what would have happened without the treatment. But there’s no single control unit that’s a good comparison.\nThe synthetic control method builds a weighted combination of untreated units that matches the treated unit’s pre-treatment trajectory. That weighted combination — the “synthetic” version — serves as the counterfactual.\n\\[\\hat{Y}_{1t}^{N} = \\sum_{j=2}^{J+1} w_j \\, Y_{jt}\\]\nwhere \\(w_j \\geq 0\\) and \\(\\sum w_j = 1\\). The weights are chosen so that the synthetic unit tracks the treated unit closely before treatment. After treatment, the gap between the treated unit and its synthetic version is the estimated effect.\n\n\nAbadie, Diamond & Hainmueller (2010): California passed Proposition 99 in 1988, a major tobacco control program. No single state is a good comparison — some are too urban, some too rural, some already had anti-smoking laws. The synthetic California is a weighted mix of states (Utah, Nevada, Colorado, Connecticut, Montana…) that together match California’s pre-1988 smoking trend almost exactly. After 1988, actual California diverges sharply below its synthetic version — that gap is the treatment effect.\n\n\n\n\nNo interference / SUTVA: the treatment of the treated unit doesn’t affect the donor units (no spillovers)\nConvex hull: the treated unit’s pre-treatment outcomes can be expressed as a weighted average of the donors — the treated unit isn’t an outlier that no combination of donors can match\nNo anticipation: the treated unit doesn’t change behavior before the treatment date\nCommon factors: treated and donor units are driven by the same underlying factors, just with different loadings — the weights that work pre-treatment continue to work post-treatment\n\n\n\n\n\nBad pre-treatment fit: if the synthetic unit can’t track the treated unit before treatment, you can’t trust the post-treatment gap. There’s no magic — if no combination of donors resembles the treated unit, the method doesn’t work.\nSpillovers: if the treatment affects the donor units too (e.g., smokers move from California to Nevada), the synthetic control is contaminated.\nToo few donors: with very few comparison units, the weights are forced and the match may be poor.\n\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n    .wt-table { font-size: 12px; margin-top: 6px; }\n    .wt-table td { padding: 1px 6px; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"tau_sc\", \"True treatment effect:\",\n                  min = -5, max = 5, value = -3, step = 0.5),\n\n      sliderInput(\"n_donors\", \"Number of donor units:\",\n                  min = 3, max = 15, value = 8, step = 1),\n\n      sliderInput(\"sigma_sc\", \"Noise (SD):\",\n                  min = 0.1, max = 2, value = 0.5, step = 0.1),\n\n      sliderInput(\"treat_time\", \"Treatment period:\",\n                  min = 8, max = 18, value = 12, step = 1),\n\n      actionButton(\"go_sc\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_sc\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"synth_plot\", height = \"500px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go_sc\n    tau   &lt;- input$tau_sc\n    J     &lt;- input$n_donors\n    sigma &lt;- input$sigma_sc\n    T0    &lt;- input$treat_time\n    TT    &lt;- 25\n\n    # Generate donor unit trajectories\n    # Each donor has its own intercept and slope\n    set.seed(NULL)\n    intercepts &lt;- rnorm(J, mean = 10, sd = 2)\n    slopes     &lt;- rnorm(J, mean = 0.3, sd = 0.15)\n    donors &lt;- matrix(NA, nrow = TT, ncol = J)\n    for (j in 1:J) {\n      donors[, j] &lt;- intercepts[j] + slopes[j] * (1:TT) + rnorm(TT, sd = sigma)\n    }\n\n    # True weights (sparse: pick 3-4 donors that matter)\n    n_active &lt;- min(4, J)\n    active &lt;- sample(1:J, n_active)\n    true_w &lt;- rep(0, J)\n    raw &lt;- runif(n_active, 0.1, 1)\n    true_w[active] &lt;- raw / sum(raw)\n\n    # Treated unit = weighted combo of donors + treatment effect after T0\n    treated &lt;- donors %*% true_w + rnorm(TT, sd = sigma * 0.5)\n    treated[(T0 + 1):TT] &lt;- treated[(T0 + 1):TT] + tau\n\n    # Estimate synthetic control weights (OLS on pre-period, constrained to sum to 1)\n    # Simple approach: non-negative least squares via iterative projection\n    pre &lt;- 1:T0\n    Y1_pre &lt;- treated[pre]\n    Y0_pre &lt;- donors[pre, ]\n\n    # Use a simple regression + normalize approach\n    # Unconstrained OLS, then clip negatives and renormalize\n    if (J &lt;= T0) {\n      fit &lt;- lm(Y1_pre ~ Y0_pre - 1)\n      w_hat &lt;- coef(fit)\n    } else {\n      # More donors than periods: use ridge-like approach\n      lambda &lt;- 0.01\n      w_hat &lt;- solve(t(Y0_pre) %*% Y0_pre + lambda * diag(J),\n                     t(Y0_pre) %*% Y1_pre)\n    }\n    w_hat[w_hat &lt; 0] &lt;- 0\n    if (sum(w_hat) &gt; 0) w_hat &lt;- w_hat / sum(w_hat) else w_hat &lt;- rep(1/J, J)\n\n    # Synthetic control trajectory\n    synth &lt;- donors %*% w_hat\n\n    # Estimated effect (post-treatment gap)\n    post &lt;- (T0 + 1):TT\n    gaps &lt;- treated[post] - synth[post]\n    avg_effect &lt;- mean(gaps)\n\n    # Pre-treatment fit (RMSPE)\n    pre_rmspe &lt;- sqrt(mean((treated[pre] - synth[pre])^2))\n\n    list(time = 1:TT, treated = as.numeric(treated),\n         synth = as.numeric(synth), donors = donors,\n         w_hat = w_hat, T0 = T0, tau = tau,\n         avg_effect = avg_effect, pre_rmspe = pre_rmspe, J = J)\n  })\n\n  output$synth_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ylim &lt;- range(c(d$treated, d$synth, d$donors)) + c(-1, 1)\n\n    # Donor units (gray background)\n    plot(d$time, d$donors[, 1], type = \"n\",\n         xlab = \"Time\", ylab = \"Outcome\",\n         main = \"Synthetic Control Method\",\n         ylim = ylim)\n\n    for (j in 1:d$J) {\n      lines(d$time, d$donors[, j], col = adjustcolor(\"gray70\", 0.4), lwd = 0.8)\n    }\n\n    # Synthetic control\n    lines(d$time, d$synth, col = \"#e74c3c\", lwd = 3, lty = 2)\n\n    # Treated unit\n    lines(d$time, d$treated, col = \"#3498db\", lwd = 3)\n\n    # Treatment line\n    abline(v = d$T0 + 0.5, lty = 3, col = \"gray40\", lwd = 1.5)\n    text(d$T0 + 0.5, ylim[2], \"Treatment\", pos = 4, cex = 0.85, col = \"gray40\")\n\n    # Gap shading in post period\n    post &lt;- (d$T0 + 1):length(d$time)\n    polygon(c(d$time[post], rev(d$time[post])),\n            c(d$treated[post], rev(d$synth[post])),\n            col = adjustcolor(\"#27ae60\", 0.15), border = NA)\n\n    # Gap label\n    mid_post &lt;- d$time[round(median(post))]\n    mid_gap &lt;- (d$treated[round(median(post))] + d$synth[round(median(post))]) / 2\n    text(mid_post, mid_gap,\n         paste0(\"Avg gap = \", round(d$avg_effect, 2)),\n         col = \"#27ae60\", font = 2, cex = 0.9)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated unit\", \"Synthetic control\", \"Donor units\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray70\"),\n           lwd = c(3, 3, 1), lty = c(1, 2, 1))\n  })\n\n  output$results_sc &lt;- renderUI({\n    d &lt;- dat()\n\n    # Top weights\n    ord &lt;- order(d$w_hat, decreasing = TRUE)\n    top &lt;- ord[d$w_hat[ord] &gt; 0.01]\n    wt_rows &lt;- paste0(\n      sapply(top, function(j) {\n        paste0(\"&lt;tr&gt;&lt;td&gt;Donor \", j, \"&lt;/td&gt;&lt;td&gt;&lt;b&gt;\",\n               round(d$w_hat[j] * 100), \"%&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;\")\n      }),\n      collapse = \"\"\n    )\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;Avg post-treatment gap:&lt;/b&gt; \",\n        \"&lt;span class='\", ifelse(abs(d$avg_effect - d$tau) &lt; 1, \"good\", \"bad\"), \"'&gt;\",\n        round(d$avg_effect, 2), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Pre-treatment RMSPE:&lt;/b&gt; \", round(d$pre_rmspe, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Weights:&lt;/b&gt;\",\n        \"&lt;table class='wt-table'&gt;\", wt_rows, \"&lt;/table&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDefault settings (effect = -3): the synthetic control (red dashed) tracks the treated unit closely before treatment, then diverges. The green-shaded gap is the estimated effect.\nSet true effect = 0: the two lines should stay close after treatment too. If you see a big gap, it’s noise — this is why pre-treatment fit matters.\nReduce donors to 3: fewer building blocks means a worse pre-treatment fit. The estimate gets noisier.\nIncrease donors to 15: more building blocks, better fit. But watch the weights — most donors get zero weight. The method is naturally sparse.\nMove treatment period later (18): short post-period, harder to judge whether the gap is real or just noise.\nCrank up noise: the pre-treatment fit deteriorates and the post-treatment gap becomes unreliable.",
    "crumbs": [
      "Methods",
      "Synthetic Control"
    ]
  },
  {
    "objectID": "synth.html#inference-placebo-tests",
    "href": "synth.html#inference-placebo-tests",
    "title": "Synthetic Control",
    "section": "Inference: placebo tests",
    "text": "Inference: placebo tests\nWith one treated unit, you can’t do standard inference. Instead, you run placebo tests:\nIn-space placebos. Apply the synthetic control method to each donor unit — pretend it was treated and build a synthetic version from the remaining donors. If the treated unit’s gap is much larger than the placebo gaps, the effect is likely real.\nIn-time placebos. Move the treatment date earlier (to a period when no treatment occurred). If you find a gap in the placebo period, your method is picking up something other than the treatment.\nThese aren’t formal p-values, but they give you a sense of whether the effect is distinguishable from noise.",
    "crumbs": [
      "Methods",
      "Synthetic Control"
    ]
  },
  {
    "objectID": "synth.html#did-you-know",
    "href": "synth.html#did-you-know",
    "title": "Synthetic Control",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe synthetic control method was developed by Abadie & Gardeazabal (2003) to study the economic impact of terrorism in the Basque Country, and formalized by Abadie, Diamond & Hainmueller (2010) in the California tobacco study. It has since become one of the most widely used methods in policy evaluation.\nAthey & Imbens (2017) called synthetic control “arguably the most important innovation in the policy evaluation literature in the last 15 years.”\nThe method works best when you have long pre-treatment panels (many time periods before treatment) and a moderate number of donor units. It struggles with short panels or when no combination of donors can approximate the treated unit.",
    "crumbs": [
      "Methods",
      "Synthetic Control"
    ]
  },
  {
    "objectID": "did.html",
    "href": "did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "You have two groups: one gets treated at some point, the other never does. You observe both before and after treatment. The key assumption: absent treatment, both groups would have followed parallel trends.\n\\[\\hat{\\tau}_{DID} = (\\bar{Y}_{treat,post} - \\bar{Y}_{treat,pre}) - (\\bar{Y}_{ctrl,post} - \\bar{Y}_{ctrl,pre})\\]\nThe first difference removes time-invariant group characteristics. The second difference removes common time trends. What’s left is the treatment effect.\n\n\n\nParallel trends: absent treatment, the treated and control groups would have followed the same trajectory over time — the key assumption\nNo anticipation: treated units don’t change behavior before the treatment date\nSUTVA: treatment of one group doesn’t spill over to the control group\nStable composition: the groups don’t change membership over time (no differential attrition)\n\n\n\n\nWhen the parallel trends assumption is violated — if the treated group was already on a different trajectory before treatment. The simulation below lets you break this assumption and see the bias that results.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_units\", \"Units per group:\",\n                  min = 20, max = 200, value = 50, step = 10),\n\n      sliderInput(\"true_effect\", \"True treatment effect:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"trend_diff\", \"Differential pre-trend\\n(violation of parallel trends):\",\n                  min = -1, max = 1, value = 0, step = 0.1),\n\n      sliderInput(\"sigma\", \"Noise (SD):\",\n                  min = 0.5, max = 3, value = 1, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"did_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n_units\n    tau   &lt;- input$true_effect\n    delta &lt;- input$trend_diff\n    sigma &lt;- input$sigma\n\n    periods &lt;- -4:4\n    treat_time &lt;- 1  # treatment at t = 1\n\n    # Group means over time\n    ctrl_mean &lt;- 3 + 0.3 * periods\n    treat_mean &lt;- 5 + (0.3 + delta) * periods\n\n    # Add treatment effect post\n    treat_mean[periods &gt;= treat_time] &lt;- treat_mean[periods &gt;= treat_time] + tau\n\n    # Generate unit-level data\n    ctrl_data &lt;- sapply(ctrl_mean, function(m) m + rnorm(n, sd = sigma))\n    treat_data &lt;- sapply(treat_mean, function(m) m + rnorm(n, sd = sigma))\n\n    ctrl_means_obs &lt;- colMeans(ctrl_data)\n    treat_means_obs &lt;- colMeans(treat_data)\n\n    # DID estimate (using t=0 as pre, t=1 as post)\n    pre_idx  &lt;- which(periods == 0)\n    post_idx &lt;- which(periods == 1)\n\n    did_est &lt;- (treat_means_obs[post_idx] - treat_means_obs[pre_idx]) -\n               (ctrl_means_obs[post_idx] - ctrl_means_obs[pre_idx])\n\n    # Counterfactual for treated (parallel to control from t=0)\n    cf &lt;- treat_means_obs[pre_idx] + (ctrl_means_obs - ctrl_means_obs[pre_idx])\n\n    list(periods = periods, ctrl = ctrl_means_obs, treat = treat_means_obs,\n         cf = cf, did_est = did_est, tau = tau, delta = delta,\n         treat_time = treat_time)\n  })\n\n  output$did_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    ylim &lt;- range(c(d$ctrl, d$treat, d$cf)) + c(-0.5, 0.5)\n\n    plot(d$periods, d$treat, type = \"b\", pch = 19, lwd = 2.5, col = \"#3498db\",\n         xlab = \"Time period\", ylab = \"Mean outcome\",\n         main = \"Difference-in-Differences\",\n         ylim = ylim, xaxt = \"n\")\n    axis(1, at = d$periods)\n\n    lines(d$periods, d$ctrl, type = \"b\", pch = 19, lwd = 2.5, col = \"#e74c3c\")\n\n    # Counterfactual (dashed, post only)\n    post &lt;- d$periods &gt;= d$treat_time\n    lines(d$periods[post], d$cf[post], type = \"b\", pch = 1, lwd = 2, lty = 2,\n          col = \"#3498db80\")\n\n    # Treatment onset\n    abline(v = d$treat_time - 0.5, lty = 3, col = \"gray50\", lwd = 1.5)\n    text(d$treat_time - 0.5, ylim[2], \"Treatment\", pos = 4, cex = 0.85, col = \"gray40\")\n\n    # DID bracket\n    pre_idx  &lt;- which(d$periods == 0)\n    post_idx &lt;- which(d$periods == 1)\n    arrows(max(d$periods) - 0.3, d$cf[post_idx],\n           max(d$periods) - 0.3, d$treat[post_idx],\n           code = 3, lwd = 2, col = \"#27ae60\", length = 0.1)\n    text(max(d$periods) - 0.1, (d$cf[post_idx] + d$treat[post_idx]) / 2,\n         paste0(\"DID = \", round(d$did_est, 2)),\n         col = \"#27ae60\", cex = 0.9, adj = 0)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\", \"Counterfactual (parallel trends)\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"#3498db80\"),\n           pch = c(19, 19, 1), lty = c(1, 1, 2), lwd = c(2.5, 2.5, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$did_est - d$tau\n    biased &lt;- abs(d$delta) &gt; 0.05\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;DID estimate:&lt;/b&gt; \", round(d$did_est, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(biased, \"bad\", \"good\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        if (biased) \"&lt;br&gt;&lt;small&gt;Parallel trends violated &mdash; DID is biased.&lt;/small&gt;\"\n        else \"&lt;br&gt;&lt;small&gt;Parallel trends hold &mdash; DID is unbiased.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDifferential pre-trend = 0: parallel trends hold, DID nails the true effect.\nSlide the differential pre-trend to +0.5: the treated group was already rising faster. DID attributes some of that trend to the treatment — the estimate is biased upward.\nSet true effect = 0 with a differential trend: DID “finds” an effect that doesn’t exist. That’s how pre-trend violations create false positives.\nLook at the pre-treatment periods — if the lines aren’t parallel before treatment, you should worry.",
    "crumbs": [
      "Methods",
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "did.html#the-idea",
    "href": "did.html#the-idea",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "You have two groups: one gets treated at some point, the other never does. You observe both before and after treatment. The key assumption: absent treatment, both groups would have followed parallel trends.\n\\[\\hat{\\tau}_{DID} = (\\bar{Y}_{treat,post} - \\bar{Y}_{treat,pre}) - (\\bar{Y}_{ctrl,post} - \\bar{Y}_{ctrl,pre})\\]\nThe first difference removes time-invariant group characteristics. The second difference removes common time trends. What’s left is the treatment effect.\n\n\n\nParallel trends: absent treatment, the treated and control groups would have followed the same trajectory over time — the key assumption\nNo anticipation: treated units don’t change behavior before the treatment date\nSUTVA: treatment of one group doesn’t spill over to the control group\nStable composition: the groups don’t change membership over time (no differential attrition)\n\n\n\n\nWhen the parallel trends assumption is violated — if the treated group was already on a different trajectory before treatment. The simulation below lets you break this assumption and see the bias that results.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_units\", \"Units per group:\",\n                  min = 20, max = 200, value = 50, step = 10),\n\n      sliderInput(\"true_effect\", \"True treatment effect:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"trend_diff\", \"Differential pre-trend\\n(violation of parallel trends):\",\n                  min = -1, max = 1, value = 0, step = 0.1),\n\n      sliderInput(\"sigma\", \"Noise (SD):\",\n                  min = 0.5, max = 3, value = 1, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"did_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n_units\n    tau   &lt;- input$true_effect\n    delta &lt;- input$trend_diff\n    sigma &lt;- input$sigma\n\n    periods &lt;- -4:4\n    treat_time &lt;- 1  # treatment at t = 1\n\n    # Group means over time\n    ctrl_mean &lt;- 3 + 0.3 * periods\n    treat_mean &lt;- 5 + (0.3 + delta) * periods\n\n    # Add treatment effect post\n    treat_mean[periods &gt;= treat_time] &lt;- treat_mean[periods &gt;= treat_time] + tau\n\n    # Generate unit-level data\n    ctrl_data &lt;- sapply(ctrl_mean, function(m) m + rnorm(n, sd = sigma))\n    treat_data &lt;- sapply(treat_mean, function(m) m + rnorm(n, sd = sigma))\n\n    ctrl_means_obs &lt;- colMeans(ctrl_data)\n    treat_means_obs &lt;- colMeans(treat_data)\n\n    # DID estimate (using t=0 as pre, t=1 as post)\n    pre_idx  &lt;- which(periods == 0)\n    post_idx &lt;- which(periods == 1)\n\n    did_est &lt;- (treat_means_obs[post_idx] - treat_means_obs[pre_idx]) -\n               (ctrl_means_obs[post_idx] - ctrl_means_obs[pre_idx])\n\n    # Counterfactual for treated (parallel to control from t=0)\n    cf &lt;- treat_means_obs[pre_idx] + (ctrl_means_obs - ctrl_means_obs[pre_idx])\n\n    list(periods = periods, ctrl = ctrl_means_obs, treat = treat_means_obs,\n         cf = cf, did_est = did_est, tau = tau, delta = delta,\n         treat_time = treat_time)\n  })\n\n  output$did_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    ylim &lt;- range(c(d$ctrl, d$treat, d$cf)) + c(-0.5, 0.5)\n\n    plot(d$periods, d$treat, type = \"b\", pch = 19, lwd = 2.5, col = \"#3498db\",\n         xlab = \"Time period\", ylab = \"Mean outcome\",\n         main = \"Difference-in-Differences\",\n         ylim = ylim, xaxt = \"n\")\n    axis(1, at = d$periods)\n\n    lines(d$periods, d$ctrl, type = \"b\", pch = 19, lwd = 2.5, col = \"#e74c3c\")\n\n    # Counterfactual (dashed, post only)\n    post &lt;- d$periods &gt;= d$treat_time\n    lines(d$periods[post], d$cf[post], type = \"b\", pch = 1, lwd = 2, lty = 2,\n          col = \"#3498db80\")\n\n    # Treatment onset\n    abline(v = d$treat_time - 0.5, lty = 3, col = \"gray50\", lwd = 1.5)\n    text(d$treat_time - 0.5, ylim[2], \"Treatment\", pos = 4, cex = 0.85, col = \"gray40\")\n\n    # DID bracket\n    pre_idx  &lt;- which(d$periods == 0)\n    post_idx &lt;- which(d$periods == 1)\n    arrows(max(d$periods) - 0.3, d$cf[post_idx],\n           max(d$periods) - 0.3, d$treat[post_idx],\n           code = 3, lwd = 2, col = \"#27ae60\", length = 0.1)\n    text(max(d$periods) - 0.1, (d$cf[post_idx] + d$treat[post_idx]) / 2,\n         paste0(\"DID = \", round(d$did_est, 2)),\n         col = \"#27ae60\", cex = 0.9, adj = 0)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\", \"Counterfactual (parallel trends)\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"#3498db80\"),\n           pch = c(19, 19, 1), lty = c(1, 1, 2), lwd = c(2.5, 2.5, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$did_est - d$tau\n    biased &lt;- abs(d$delta) &gt; 0.05\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;DID estimate:&lt;/b&gt; \", round(d$did_est, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(biased, \"bad\", \"good\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        if (biased) \"&lt;br&gt;&lt;small&gt;Parallel trends violated &mdash; DID is biased.&lt;/small&gt;\"\n        else \"&lt;br&gt;&lt;small&gt;Parallel trends hold &mdash; DID is unbiased.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDifferential pre-trend = 0: parallel trends hold, DID nails the true effect.\nSlide the differential pre-trend to +0.5: the treated group was already rising faster. DID attributes some of that trend to the treatment — the estimate is biased upward.\nSet true effect = 0 with a differential trend: DID “finds” an effect that doesn’t exist. That’s how pre-trend violations create false positives.\nLook at the pre-treatment periods — if the lines aren’t parallel before treatment, you should worry.",
    "crumbs": [
      "Methods",
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "ipw.html",
    "href": "ipw.html",
    "title": "Inverse Probability Weighting",
    "section": "",
    "text": "In observational data, treatment isn’t random. People who get treated differ from those who don’t — they may be older, sicker, richer, etc. A naive comparison of outcomes is biased by these confounders.",
    "crumbs": [
      "Estimation Tools",
      "IPW"
    ]
  },
  {
    "objectID": "ipw.html#the-problem",
    "href": "ipw.html#the-problem",
    "title": "Inverse Probability Weighting",
    "section": "",
    "text": "In observational data, treatment isn’t random. People who get treated differ from those who don’t — they may be older, sicker, richer, etc. A naive comparison of outcomes is biased by these confounders.",
    "crumbs": [
      "Estimation Tools",
      "IPW"
    ]
  },
  {
    "objectID": "ipw.html#the-idea",
    "href": "ipw.html#the-idea",
    "title": "Inverse Probability Weighting",
    "section": "The idea",
    "text": "The idea\nInverse Probability Weighting (IPW) reweights observations so that the treated and control groups look alike on observed covariates. The steps:\n\nEstimate the propensity score \\(e(X) = P(\\text{treated} \\mid X)\\) — the probability of treatment given covariates.\nWeight each observation inversely by its probability of receiving the treatment it actually got:\n\nTreated units get weight \\(1 / e(X)\\)\nControl units get weight \\(1 / (1 - e(X))\\)\n\nCompute the weighted difference in means.\n\nIntuition: if a treated person had only a 20% chance of being treated (based on their X), they’re “surprising” — they represent 5 similar people who weren’t treated. So they get upweighted. This creates a pseudo-population where treatment is independent of X.\n\nAssumptions\n\nUnconfoundedness (selection on observables): conditional on observed covariates \\(X\\), treatment is independent of potential outcomes — \\(Y(0), Y(1) \\perp D \\mid X\\). All confounders are observed and included.\nOverlap (positivity): every unit has a non-zero probability of being in either group — \\(0 &lt; e(X) &lt; 1\\) for all \\(X\\). No one is deterministically treated or untreated.\nCorrect propensity score model: the functional form of \\(e(X)\\) is correctly specified. If you use a logit and the true relationship is nonlinear, the weights are wrong.\n\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"confounding\", \"Confounding strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"balance_plot\", height = \"380px\")),\n        column(6, plotOutput(\"ps_plot\", height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    ate  &lt;- input$ate\n    conf &lt;- input$confounding\n\n    # Confounder\n    x &lt;- rnorm(n)\n\n    # Treatment depends on x (confounding)\n    p_true &lt;- pnorm(conf * x)\n    treat &lt;- rbinom(n, 1, p_true)\n\n    # Outcome depends on x and treatment\n    y &lt;- 1 + 2 * x + ate * treat + rnorm(n)\n\n    # Naive estimate\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # IPW estimate\n    ps &lt;- fitted(glm(treat ~ x, family = binomial))\n    w &lt;- ifelse(treat == 1, 1 / ps, 1 / (1 - ps))\n    ipw_est &lt;- weighted.mean(y[treat == 1], w[treat == 1]) -\n               weighted.mean(y[treat == 0], w[treat == 0])\n\n    list(x = x, treat = treat, y = y, ps = ps, w = w,\n         naive = naive, ipw_est = ipw_est, ate = ate)\n  })\n\n  output$balance_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Unweighted densities\n    x_t &lt;- d$x[d$treat == 1]\n    x_c &lt;- d$x[d$treat == 0]\n\n    rng &lt;- range(d$x)\n    dens_t &lt;- density(x_t, from = rng[1], to = rng[2])\n    dens_c &lt;- density(x_c, from = rng[1], to = rng[2])\n\n    ylim &lt;- c(0, max(dens_t$y, dens_c$y) * 1.2)\n\n    plot(dens_t, col = \"#3498db\", lwd = 2.5, main = \"Covariate Balance (X)\",\n         xlab = \"X (confounder)\", ylab = \"Density\", ylim = ylim)\n    lines(dens_c, col = \"#e74c3c\", lwd = 2.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\"),\n           col = c(\"#3498db\", \"#e74c3c\"), lwd = 2.5)\n  })\n\n  output$ps_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$ps, pch = 16, cex = 0.5,\n         col = ifelse(d$treat == 1, \"#3498db80\", \"#e74c3c80\"),\n         xlab = \"X (confounder)\", ylab = \"Propensity score e(X)\",\n         main = \"Propensity Score vs Confounder\")\n    abline(h = 0.5, lty = 2, col = \"gray50\")\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\"),\n           col = c(\"#3498db\", \"#e74c3c\"), pch = 16)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive estimate:&lt;/b&gt; &lt;span class='bad'&gt;\", round(d$naive, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;IPW estimate:&lt;/b&gt; &lt;span class='good'&gt;\", round(d$ipw_est, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Naive bias:&lt;/b&gt; \", round(d$naive - d$ate, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;IPW bias:&lt;/b&gt; \", round(d$ipw_est - d$ate, 3)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nConfounding = 0: treatment is random. Naive and IPW give the same answer.\nConfounding = 1.5: the covariate distributions for treated and control diverge (left plot). Naive is biased, IPW corrects it.\nConfounding = 3: extreme selection. The propensity scores are near 0 or 1 (right plot), meaning some units get huge weights. IPW becomes noisy — this is the extreme weights problem.",
    "crumbs": [
      "Estimation Tools",
      "IPW"
    ]
  },
  {
    "objectID": "selection-observables.html",
    "href": "selection-observables.html",
    "title": "Selection on Observables",
    "section": "",
    "text": "You want the causal effect of a treatment, but people select into treatment based on their characteristics. Sicker patients seek medication, motivated students enroll in programs, richer firms adopt new technology.\nThe selection on observables strategy says: if you can observe everything that drives both treatment and outcome, you can condition on it and recover the causal effect. Once you hold those variables fixed, treatment is as good as random.\n\\[Y(0), Y(1) \\perp D \\mid X\\]\nThis is the conditional independence assumption (CIA), also called unconfoundedness or ignorability. It says: among people with the same \\(X\\), who gets treated is effectively random.\n\n\nIt’s the same logic, made precise. When you run \\(Y = \\alpha + \\tau D + \\beta X + \\varepsilon\\) and claim \\(\\hat{\\tau}\\) is causal, you’re implicitly assuming selection on observables — that \\(X\\) contains all the confounders. The difference is that causal inference makes this assumption explicit and offers multiple ways to implement it, each with different strengths:\n\n\n\nMethod\nHow it adjusts for X\n\n\n\n\nRegression adjustment\nModels the outcome as a function of X and D\n\n\nMatching\nPairs treated and control units with similar X\n\n\nIPW\nReweights units by their probability of treatment given X\n\n\nEntropy Balancing\nDirectly reweights controls to match treated group’s X distribution\n\n\nDoubly robust\nCombines regression and weighting — consistent if either model is correct\n\n\n\nAll of these rely on the same fundamental assumption. They differ in how they use X to make the comparison fair.\n\n\n\n\nConditional independence (CIA): all confounders are observed and included in X. If an unobserved variable affects both treatment and outcome, every method above is biased. This is untestable — you argue it based on institutional knowledge.\nOverlap (common support): for every value of X, there are both treated and untreated units — \\(0 &lt; P(D = 1 \\mid X) &lt; 1\\). If some covariate profiles always get treated, you can’t estimate the counterfactual for them.\nSUTVA: one unit’s treatment doesn’t affect another’s outcome.\n\n\n\n\nWhen there are unobserved confounders — variables that affect both treatment and outcome but aren’t in your data. No amount of regression, matching, or weighting can fix this.\nExamples:\n\nReturns to education: ability is unobserved. More able people get more education and earn more. Controlling for test scores helps but doesn’t fully capture ability. → You need IV.\nEffect of a new policy: states that adopt the policy may differ in unobservable ways (political will, citizen preferences). → You need DID or synthetic control.\nEffect of a drug: patients who take the drug may be sicker in ways the chart doesn’t capture. → You need an RCT.\n\nThe simulation below shows what happens when the CIA holds vs when it doesn’t.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_so\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate_so\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"obs_conf\", \"Observed confounding (X):\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      sliderInput(\"unobs_conf\", \"Unobserved confounding (U):\",\n                  min = 0, max = 3, value = 0, step = 0.25),\n\n      actionButton(\"go_so\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_so\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"so_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go_so\n    n   &lt;- input$n_so\n    ate &lt;- input$ate_so\n    gx  &lt;- input$obs_conf\n    gu  &lt;- input$unobs_conf\n\n    # Observed confounder\n    x &lt;- rnorm(n)\n\n    # Unobserved confounder\n    u &lt;- rnorm(n)\n\n    # Treatment depends on both\n    p &lt;- pnorm(gx * x + gu * u)\n    treat &lt;- rbinom(n, 1, p)\n\n    # Outcome depends on both\n    y &lt;- 1 + 2 * x + 1.5 * u + ate * treat + rnorm(n)\n\n    # Naive (no controls)\n    naive &lt;- coef(lm(y ~ treat))[2]\n\n    # Controlling for X only\n    ctrl_x &lt;- coef(lm(y ~ treat + x))[2]\n\n    # Oracle: controlling for X and U\n    oracle &lt;- coef(lm(y ~ treat + x + u))[2]\n\n    list(x = x, u = u, treat = treat, y = y,\n         naive = naive, ctrl_x = ctrl_x, oracle = oracle,\n         ate = ate, gx = gx, gu = gu)\n  })\n\n  output$so_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    estimates &lt;- c(d$naive, d$ctrl_x, d$oracle)\n    biases &lt;- estimates - d$ate\n    labels &lt;- c(\"Naive\\n(no controls)\", \"Control for X\\n(observed)\", \"Control for X + U\\n(oracle)\")\n    cols &lt;- c(\"#e74c3c\", ifelse(abs(biases[2]) &lt; 0.3, \"#27ae60\", \"#f39c12\"), \"#27ae60\")\n\n    bp &lt;- barplot(estimates, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.85,\n                  main = \"Estimated Treatment Effect by Method\",\n                  ylab = \"Estimate\", ylim = c(0, max(estimates) * 1.4))\n\n    # True ATE line\n    abline(h = d$ate, lty = 2, col = \"gray40\", lwd = 2)\n    text(0.2, d$ate + 0.15, paste0(\"True ATE = \", d$ate),\n         col = \"gray40\", cex = 0.85, adj = 0)\n\n    # Bias labels\n    text(bp, estimates + 0.15,\n         paste0(round(estimates, 2), \"\\n(bias: \", round(biases, 2), \")\"),\n         cex = 0.8)\n  })\n\n  output$results_so &lt;- renderUI({\n    d &lt;- dat()\n    bias_naive &lt;- d$naive - d$ate\n    bias_x &lt;- d$ctrl_x - d$ate\n    bias_oracle &lt;- d$oracle - d$ate\n    cia_holds &lt;- d$gu == 0\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; \", round(d$naive, 2),\n        \" &lt;span class='bad'&gt;(bias: \", round(bias_naive, 2), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Control X:&lt;/b&gt; \", round(d$ctrl_x, 2),\n        \" &lt;span class='\", ifelse(cia_holds, \"good\", \"bad\"), \"'&gt;(bias: \",\n        round(bias_x, 2), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Oracle (X+U):&lt;/b&gt; \", round(d$oracle, 2),\n        \" &lt;span class='good'&gt;(bias: \", round(bias_oracle, 2), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        if (cia_holds)\n          \"&lt;small&gt;CIA holds: controlling for X is enough.&lt;/small&gt;\"\n        else\n          \"&lt;small&gt;CIA violated: U confounds treatment. Controlling for X alone leaves residual bias.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nUnobserved confounding = 0: the CIA holds. Controlling for X eliminates all bias — the green and oracle bars match. This is the world where selection on observables works.\nUnobserved confounding = 1.5: now there’s a confounder you can’t see. Controlling for X helps (reduces bias vs naive) but doesn’t eliminate it. Only the oracle, who controls for both X and U, gets the right answer.\nUnobserved confounding = 3: controlling for X barely helps. The bias is large. No amount of regression, matching, or weighting on X can fix this — you need a different identification strategy.\nSet observed confounding = 0, unobserved = 2: all the confounding is unobserved. Naive and “control for X” give the same (biased) answer because X isn’t a confounder here.",
    "crumbs": [
      "Methods",
      "Selection on Observables"
    ]
  },
  {
    "objectID": "selection-observables.html#the-idea",
    "href": "selection-observables.html#the-idea",
    "title": "Selection on Observables",
    "section": "",
    "text": "You want the causal effect of a treatment, but people select into treatment based on their characteristics. Sicker patients seek medication, motivated students enroll in programs, richer firms adopt new technology.\nThe selection on observables strategy says: if you can observe everything that drives both treatment and outcome, you can condition on it and recover the causal effect. Once you hold those variables fixed, treatment is as good as random.\n\\[Y(0), Y(1) \\perp D \\mid X\\]\nThis is the conditional independence assumption (CIA), also called unconfoundedness or ignorability. It says: among people with the same \\(X\\), who gets treated is effectively random.\n\n\nIt’s the same logic, made precise. When you run \\(Y = \\alpha + \\tau D + \\beta X + \\varepsilon\\) and claim \\(\\hat{\\tau}\\) is causal, you’re implicitly assuming selection on observables — that \\(X\\) contains all the confounders. The difference is that causal inference makes this assumption explicit and offers multiple ways to implement it, each with different strengths:\n\n\n\nMethod\nHow it adjusts for X\n\n\n\n\nRegression adjustment\nModels the outcome as a function of X and D\n\n\nMatching\nPairs treated and control units with similar X\n\n\nIPW\nReweights units by their probability of treatment given X\n\n\nEntropy Balancing\nDirectly reweights controls to match treated group’s X distribution\n\n\nDoubly robust\nCombines regression and weighting — consistent if either model is correct\n\n\n\nAll of these rely on the same fundamental assumption. They differ in how they use X to make the comparison fair.\n\n\n\n\nConditional independence (CIA): all confounders are observed and included in X. If an unobserved variable affects both treatment and outcome, every method above is biased. This is untestable — you argue it based on institutional knowledge.\nOverlap (common support): for every value of X, there are both treated and untreated units — \\(0 &lt; P(D = 1 \\mid X) &lt; 1\\). If some covariate profiles always get treated, you can’t estimate the counterfactual for them.\nSUTVA: one unit’s treatment doesn’t affect another’s outcome.\n\n\n\n\nWhen there are unobserved confounders — variables that affect both treatment and outcome but aren’t in your data. No amount of regression, matching, or weighting can fix this.\nExamples:\n\nReturns to education: ability is unobserved. More able people get more education and earn more. Controlling for test scores helps but doesn’t fully capture ability. → You need IV.\nEffect of a new policy: states that adopt the policy may differ in unobservable ways (political will, citizen preferences). → You need DID or synthetic control.\nEffect of a drug: patients who take the drug may be sicker in ways the chart doesn’t capture. → You need an RCT.\n\nThe simulation below shows what happens when the CIA holds vs when it doesn’t.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_so\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate_so\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"obs_conf\", \"Observed confounding (X):\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      sliderInput(\"unobs_conf\", \"Unobserved confounding (U):\",\n                  min = 0, max = 3, value = 0, step = 0.25),\n\n      actionButton(\"go_so\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_so\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"so_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go_so\n    n   &lt;- input$n_so\n    ate &lt;- input$ate_so\n    gx  &lt;- input$obs_conf\n    gu  &lt;- input$unobs_conf\n\n    # Observed confounder\n    x &lt;- rnorm(n)\n\n    # Unobserved confounder\n    u &lt;- rnorm(n)\n\n    # Treatment depends on both\n    p &lt;- pnorm(gx * x + gu * u)\n    treat &lt;- rbinom(n, 1, p)\n\n    # Outcome depends on both\n    y &lt;- 1 + 2 * x + 1.5 * u + ate * treat + rnorm(n)\n\n    # Naive (no controls)\n    naive &lt;- coef(lm(y ~ treat))[2]\n\n    # Controlling for X only\n    ctrl_x &lt;- coef(lm(y ~ treat + x))[2]\n\n    # Oracle: controlling for X and U\n    oracle &lt;- coef(lm(y ~ treat + x + u))[2]\n\n    list(x = x, u = u, treat = treat, y = y,\n         naive = naive, ctrl_x = ctrl_x, oracle = oracle,\n         ate = ate, gx = gx, gu = gu)\n  })\n\n  output$so_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    estimates &lt;- c(d$naive, d$ctrl_x, d$oracle)\n    biases &lt;- estimates - d$ate\n    labels &lt;- c(\"Naive\\n(no controls)\", \"Control for X\\n(observed)\", \"Control for X + U\\n(oracle)\")\n    cols &lt;- c(\"#e74c3c\", ifelse(abs(biases[2]) &lt; 0.3, \"#27ae60\", \"#f39c12\"), \"#27ae60\")\n\n    bp &lt;- barplot(estimates, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.85,\n                  main = \"Estimated Treatment Effect by Method\",\n                  ylab = \"Estimate\", ylim = c(0, max(estimates) * 1.4))\n\n    # True ATE line\n    abline(h = d$ate, lty = 2, col = \"gray40\", lwd = 2)\n    text(0.2, d$ate + 0.15, paste0(\"True ATE = \", d$ate),\n         col = \"gray40\", cex = 0.85, adj = 0)\n\n    # Bias labels\n    text(bp, estimates + 0.15,\n         paste0(round(estimates, 2), \"\\n(bias: \", round(biases, 2), \")\"),\n         cex = 0.8)\n  })\n\n  output$results_so &lt;- renderUI({\n    d &lt;- dat()\n    bias_naive &lt;- d$naive - d$ate\n    bias_x &lt;- d$ctrl_x - d$ate\n    bias_oracle &lt;- d$oracle - d$ate\n    cia_holds &lt;- d$gu == 0\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; \", round(d$naive, 2),\n        \" &lt;span class='bad'&gt;(bias: \", round(bias_naive, 2), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Control X:&lt;/b&gt; \", round(d$ctrl_x, 2),\n        \" &lt;span class='\", ifelse(cia_holds, \"good\", \"bad\"), \"'&gt;(bias: \",\n        round(bias_x, 2), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Oracle (X+U):&lt;/b&gt; \", round(d$oracle, 2),\n        \" &lt;span class='good'&gt;(bias: \", round(bias_oracle, 2), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        if (cia_holds)\n          \"&lt;small&gt;CIA holds: controlling for X is enough.&lt;/small&gt;\"\n        else\n          \"&lt;small&gt;CIA violated: U confounds treatment. Controlling for X alone leaves residual bias.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nUnobserved confounding = 0: the CIA holds. Controlling for X eliminates all bias — the green and oracle bars match. This is the world where selection on observables works.\nUnobserved confounding = 1.5: now there’s a confounder you can’t see. Controlling for X helps (reduces bias vs naive) but doesn’t eliminate it. Only the oracle, who controls for both X and U, gets the right answer.\nUnobserved confounding = 3: controlling for X barely helps. The bias is large. No amount of regression, matching, or weighting on X can fix this — you need a different identification strategy.\nSet observed confounding = 0, unobserved = 2: all the confounding is unobserved. Naive and “control for X” give the same (biased) answer because X isn’t a confounder here.",
    "crumbs": [
      "Methods",
      "Selection on Observables"
    ]
  },
  {
    "objectID": "selection-observables.html#estimation-tools-not-just-for-selection-on-observables",
    "href": "selection-observables.html#estimation-tools-not-just-for-selection-on-observables",
    "title": "Selection on Observables",
    "section": "Estimation tools (not just for selection on observables)",
    "text": "Estimation tools (not just for selection on observables)\nOnce you have an identification strategy, you need a way to implement it. The tools below are often associated with selection on observables, but they’re general-purpose — they show up in other strategies too.\n\n\n\nTool\nUsed in selection on observables\nAlso used in\n\n\n\n\nRegression adjustment\nControl for X in a regression\nDID with covariates, RDD with covariates\n\n\nMatching\nPair treated/control on X\nDID matching estimators\n\n\nIPW\nReweight by propensity score\nIPW-DID (Abadie 2005, Callaway & Sant’Anna 2021)\n\n\nEntropy Balancing\nBalance covariates with weights\nWeighted DID\n\n\nDoubly robust\nCombine regression + weighting\nDR-DID (Sant’Anna & Zhao 2020)\n\n\n\nThe identification strategy tells you why your comparison is valid. The estimation tool tells you how to make the comparison. Don’t confuse the two — IPW is a tool, not a strategy.",
    "crumbs": [
      "Methods",
      "Selection on Observables"
    ]
  },
  {
    "objectID": "selection-observables.html#did-you-know",
    "href": "selection-observables.html#did-you-know",
    "title": "Selection on Observables",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe conditional independence assumption was formalized by Rosenbaum & Rubin (1983) in their foundational paper on propensity scores. They showed that conditioning on a scalar propensity score is sufficient — you don’t need to match on every covariate separately.\nThe term “selection on observables” is economics jargon. In statistics it’s called ignorability or no unmeasured confounding. In epidemiology it’s the exchangeability assumption. Same idea, different fields, different names.\nAltonji, Elder & Taber (2005) proposed a practical check: compare how much the estimate changes when you add observed controls. If adding strong predictors of the outcome barely moves the estimate, it’s less likely that unobservables would change it much either. Not a proof — but a useful heuristic.",
    "crumbs": [
      "Methods",
      "Selection on Observables"
    ]
  },
  {
    "objectID": "entropy-balancing.html",
    "href": "entropy-balancing.html",
    "title": "Entropy Balancing",
    "section": "",
    "text": "IPW relies on correctly specifying the propensity score model. If you get the model wrong, the weights are wrong, and the estimate is biased. Even if the model is right, extreme propensity scores create extreme weights and noisy estimates.",
    "crumbs": [
      "Estimation Tools",
      "Entropy Balancing"
    ]
  },
  {
    "objectID": "entropy-balancing.html#the-problem-with-ipw",
    "href": "entropy-balancing.html#the-problem-with-ipw",
    "title": "Entropy Balancing",
    "section": "",
    "text": "IPW relies on correctly specifying the propensity score model. If you get the model wrong, the weights are wrong, and the estimate is biased. Even if the model is right, extreme propensity scores create extreme weights and noisy estimates.",
    "crumbs": [
      "Estimation Tools",
      "Entropy Balancing"
    ]
  },
  {
    "objectID": "entropy-balancing.html#entropy-balancing-a-different-approach",
    "href": "entropy-balancing.html#entropy-balancing-a-different-approach",
    "title": "Entropy Balancing",
    "section": "Entropy balancing: a different approach",
    "text": "Entropy balancing: a different approach\nEntropy balancing (Hainmueller, 2012) skips the propensity score entirely. Instead, it directly finds weights for the control group that make the covariate distributions exactly match the treated group on specified moments (mean, variance, skewness).\nThe weights are chosen to be as close to uniform as possible (maximum entropy) subject to the balance constraints. This guarantees:\n\nExact balance on the moments you specify\nSmooth weights (no extreme values like IPW can produce)\n\n\nIPW vs Entropy Balancing\n\n\n\n\nIPW\nEntropy Balancing\n\n\n\n\nRequires a propensity score model\nYes\nNo\n\n\nBalance is…\nApproximate (check after)\nExact (by construction)\n\n\nExtreme weights?\nCan be severe\nControlled\n\n\nSensitive to misspecification?\nYes\nLess so\n\n\n\n\n\nAssumptions\n\nSelection on observables: conditional on the covariates you balance on, treatment is independent of potential outcomes. Same as IPW — if you’re missing a confounder, balancing the observed ones doesn’t help.\nOverlap: treated and control groups share common support in covariate space. You can’t reweight controls to look like treated units if no controls exist in that part of the distribution.\nCorrect moments: you need to balance the right moments. If the outcome depends on \\(X^2\\) but you only balance on \\(E[X]\\), you’ll miss the confounding.\n\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"confounding\", \"Confounding strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"balance_plot\", height = \"380px\")),\n        column(6, plotOutput(\"weight_plot\",  height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  # Simple entropy balancing: find weights for control group\n  # that match treated group mean of X\n  ebal &lt;- function(x_ctrl, target_mean, max_iter = 200, tol = 1e-6) {\n    n &lt;- length(x_ctrl)\n    lambda &lt;- 0\n    for (i in seq_len(max_iter)) {\n      w &lt;- exp(lambda * x_ctrl)\n      w &lt;- w / sum(w) * n\n      current &lt;- weighted.mean(x_ctrl, w)\n      grad &lt;- current - target_mean\n      if (abs(grad) &lt; tol) break\n      lambda &lt;- lambda - 0.5 * grad\n    }\n    w / sum(w) * n\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    ate  &lt;- input$ate\n    conf &lt;- input$confounding\n\n    x &lt;- rnorm(n)\n    p_true &lt;- pnorm(conf * x)\n    treat &lt;- rbinom(n, 1, p_true)\n\n    y &lt;- 1 + 2 * x + ate * treat + rnorm(n)\n\n    # Naive\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # IPW\n    ps &lt;- fitted(glm(treat ~ x, family = binomial))\n    w_ipw &lt;- ifelse(treat == 1, 1 / ps, 1 / (1 - ps))\n    ipw_est &lt;- weighted.mean(y[treat == 1], w_ipw[treat == 1]) -\n               weighted.mean(y[treat == 0], w_ipw[treat == 0])\n\n    # Entropy balancing (balance control to match treated mean of X)\n    x_ctrl &lt;- x[treat == 0]\n    x_treat_mean &lt;- mean(x[treat == 1])\n    w_eb &lt;- ebal(x_ctrl, x_treat_mean)\n\n    eb_est &lt;- mean(y[treat == 1]) - weighted.mean(y[treat == 0], w_eb)\n\n    # Balance diagnostics\n    ctrl_mean_raw &lt;- mean(x[treat == 0])\n    ctrl_mean_eb  &lt;- weighted.mean(x[treat == 0], w_eb)\n    treat_mean_x  &lt;- x_treat_mean\n\n    list(x = x, treat = treat, y = y,\n         w_ipw = w_ipw, w_eb = w_eb,\n         naive = naive, ipw_est = ipw_est, eb_est = eb_est,\n         ate = ate,\n         ctrl_mean_raw = ctrl_mean_raw,\n         ctrl_mean_eb = ctrl_mean_eb,\n         treat_mean_x = treat_mean_x)\n  })\n\n  output$balance_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 1, 3, 1))\n\n    means &lt;- c(d$treat_mean_x, d$ctrl_mean_raw, d$ctrl_mean_eb)\n    cols &lt;- c(\"#3498db\", \"#e74c3c\", \"#27ae60\")\n    labels &lt;- c(\"Treated\", \"Control\\n(unweighted)\", \"Control\\n(EB weighted)\")\n\n    bp &lt;- barplot(means, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.85,\n                  main = \"Mean of X: Balance Check\",\n                  ylab = \"\", ylim = range(means) * c(0.8, 1.3))\n    text(bp, means + 0.05, round(means, 3), cex = 0.9)\n  })\n\n  output$weight_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ctrl_idx &lt;- which(d$treat == 0)\n\n    w_ipw_ctrl &lt;- d$w_ipw[ctrl_idx]\n    w_eb_ctrl  &lt;- d$w_eb\n\n    xlim &lt;- c(0, max(c(w_ipw_ctrl, w_eb_ctrl)) * 1.1)\n\n    d_ipw &lt;- density(w_ipw_ctrl, from = 0)\n    d_eb  &lt;- density(w_eb_ctrl, from = 0)\n    ylim &lt;- c(0, max(d_ipw$y, d_eb$y) * 1.2)\n\n    plot(d_ipw, col = \"#e74c3c\", lwd = 2.5,\n         main = \"Weight Distributions (Control Units)\",\n         xlab = \"Weight\", ylab = \"Density\",\n         xlim = xlim, ylim = ylim)\n    lines(d_eb, col = \"#27ae60\", lwd = 2.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"IPW weights\", \"Entropy balancing weights\"),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = 2.5)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; &lt;span class='bad'&gt;\", round(d$naive, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(d$naive - d$ate, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;IPW:&lt;/b&gt; \", round(d$ipw_est, 3),\n        \" (bias: \", round(d$ipw_est - d$ate, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;Entropy Bal:&lt;/b&gt; &lt;span class='good'&gt;\", round(d$eb_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(d$eb_est - d$ate, 3), \")\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nConfounding = 1.5: look at the right plot. IPW weights have a long tail (some control units get huge weight). Entropy balancing weights are much smoother.\nConfounding = 3: IPW weights become extreme. EB stays stable.\nLeft plot: the green bar (EB-weighted control mean) exactly matches the blue bar (treated mean). That’s the guarantee — exact balance by construction.\nCompare the bias numbers in the sidebar: EB is typically closer to the true ATE, especially under strong confounding.",
    "crumbs": [
      "Estimation Tools",
      "Entropy Balancing"
    ]
  },
  {
    "objectID": "identification-estimation.html#the-math-where-bias-comes-from",
    "href": "identification-estimation.html#the-math-where-bias-comes-from",
    "title": "Identification vs Estimation",
    "section": "The math: where bias comes from",
    "text": "The math: where bias comes from\nWhen an identification assumption fails, it introduces a bias term that no estimator can remove. Here’s the decomposition for three methods.\n\nSelection on observables\nWe want the Average Treatment Effect on the Treated (ATT):\n\\[\\tau = E[Y(1) - Y(0) \\mid D = 1]\\]\nWe observe \\(E[Y \\mid D=1] = E[Y(1) \\mid D=1]\\) and \\(E[Y \\mid D=0] = E[Y(0) \\mid D=0]\\). The naive comparison is:\n\\[E[Y \\mid D=1] - E[Y \\mid D=0] = \\underbrace{E[Y(1) - Y(0) \\mid D=1]}_{\\text{ATT}} + \\underbrace{E[Y(0) \\mid D=1] - E[Y(0) \\mid D=0]}_{\\text{selection bias}}\\]\nThe second term is selection bias — the treated group would have had different outcomes even without treatment. The CIA says: conditional on \\(X\\), \\(E[Y(0) \\mid D=1, X] = E[Y(0) \\mid D=0, X]\\), so the selection bias is zero within each stratum of \\(X\\).\nIf the CIA fails — there’s an unobserved confounder \\(U\\) — then \\(E[Y(0) \\mid D=1, X] \\neq E[Y(0) \\mid D=0, X]\\) because \\(D\\) is still correlated with \\(Y(0)\\) through \\(U\\) even after conditioning on \\(X\\). The bias term is nonzero. Regression, IPW, matching — all give biased answers because the selection bias is baked into the estimand, not the estimator.\n\n\nDifference-in-differences\nThe DID estimand is:\n\\[\\hat{\\tau}_{DID} = \\big(E[Y_{1t}] - E[Y_{1,t-1}]\\big) - \\big(E[Y_{0t}] - E[Y_{0,t-1}]\\big)\\]\nwhere group 1 is treated, group 0 is control, \\(t\\) is post, \\(t-1\\) is pre. Substitute potential outcomes and add and subtract \\(E[Y_{1t}(0)]\\):\n\\[\\hat{\\tau}_{DID} = \\underbrace{E[Y_{1t}(1) - Y_{1t}(0)]}_{\\text{ATT}} + \\underbrace{\\big(E[Y_{1t}(0)] - E[Y_{1,t-1}(0)]\\big) - \\big(E[Y_{0t}(0)] - E[Y_{0,t-1}(0)]\\big)}_{\\text{differential trend bias}}\\]\nThe parallel trends assumption says the second term equals zero — the treated group’s untreated trajectory matches the control group’s trajectory. Then \\(\\hat{\\tau}_{DID} = \\text{ATT}\\).\nIf parallel trends fail — say the treated group was already trending upward faster — the differential trend term is positive. DID overestimates the effect. This bias doesn’t shrink with more data. It doesn’t go away if you switch from a 2×2 difference to TWFE or IPW-DID. It’s an identification failure, not an estimation failure.\n\n\nInstrumental variables\nWe have \\(Y = \\beta X + \\varepsilon\\) where \\(\\text{Cov}(X, \\varepsilon) \\neq 0\\) (endogeneity). The IV estimand is:\n\\[\\hat{\\beta}_{IV} = \\frac{\\text{Cov}(Z, Y)}{\\text{Cov}(Z, X)}\\]\nSubstitute \\(Y = \\beta X + \\varepsilon\\):\n\\[\\hat{\\beta}_{IV} = \\frac{\\text{Cov}(Z, \\beta X + \\varepsilon)}{\\text{Cov}(Z, X)} = \\beta + \\frac{\\text{Cov}(Z, \\varepsilon)}{\\text{Cov}(Z, X)}\\]\nThe exclusion restriction says \\(\\text{Cov}(Z, \\varepsilon) = 0\\) — the instrument is uncorrelated with the error. Then \\(\\hat{\\beta}_{IV} = \\beta\\).\nIf the exclusion restriction fails — \\(Z\\) directly affects \\(Y\\) through some channel other than \\(X\\) — then \\(\\text{Cov}(Z, \\varepsilon) \\neq 0\\) and the bias term \\(\\frac{\\text{Cov}(Z, \\varepsilon)}{\\text{Cov}(Z, X)}\\) is nonzero. No amount of data, no alternative estimator (LIML, GMM, jackknife) removes this. It’s baked in.\n\n\nThreats to identification\nEach method has specific threats — things that make the bias term nonzero:\n\n\n\n\n\n\n\n\n\nMethod\nIdentification assumption\nThreat (what breaks it)\nWhat the bias looks like\n\n\n\n\nSOO\nNo unobserved confounders\nOmitted variable that drives both \\(D\\) and \\(Y\\)\nSelection bias: treated group was different to begin with\n\n\nDID\nParallel trends\nTreated group was already on a different trajectory\nYou attribute the pre-existing trend to the treatment\n\n\nIV\nExclusion restriction\nInstrument affects \\(Y\\) through a channel other than \\(X\\)\nIV picks up the direct effect, not just the causal path\n\n\nRDD\nContinuity at cutoff\nUnits manipulate their score to sort across the cutoff; or another policy also kicks in at the same cutoff\nThe “jump” reflects sorting or a different treatment, not your treatment\n\n\nSynthetic control\nPre-treatment fit generalizes\nSpillovers from treated unit to donors; structural break changes the relationship\nCounterfactual is wrong, gap doesn’t reflect the treatment\n\n\n\nNotice: every threat is about the world, not about the math. You can’t test your way out of these — you argue them with institutional knowledge.\n\n\nThe pattern\nIn all three cases:\n\\[\\text{Estimate} = \\text{Causal effect} + \\text{Bias from violated assumption}\\]\nThe bias term is not a function of the estimator or sample size. It’s a function of how the world works. That’s why it’s an identification problem.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  }
]