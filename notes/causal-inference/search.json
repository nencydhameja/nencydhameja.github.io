[
  {
    "objectID": "entropy-balancing.html",
    "href": "entropy-balancing.html",
    "title": "Entropy Balancing",
    "section": "",
    "text": "IPW relies on correctly specifying the propensity score model. If you get the model wrong, the weights are wrong, and the estimate is biased. Even if the model is right, extreme propensity scores create extreme weights and noisy estimates.",
    "crumbs": [
      "Entropy Balancing"
    ]
  },
  {
    "objectID": "entropy-balancing.html#the-problem-with-ipw",
    "href": "entropy-balancing.html#the-problem-with-ipw",
    "title": "Entropy Balancing",
    "section": "",
    "text": "IPW relies on correctly specifying the propensity score model. If you get the model wrong, the weights are wrong, and the estimate is biased. Even if the model is right, extreme propensity scores create extreme weights and noisy estimates.",
    "crumbs": [
      "Entropy Balancing"
    ]
  },
  {
    "objectID": "entropy-balancing.html#entropy-balancing-a-different-approach",
    "href": "entropy-balancing.html#entropy-balancing-a-different-approach",
    "title": "Entropy Balancing",
    "section": "Entropy balancing: a different approach",
    "text": "Entropy balancing: a different approach\nEntropy balancing (Hainmueller, 2012) skips the propensity score entirely. Instead, it directly finds weights for the control group that make the covariate distributions exactly match the treated group on specified moments (mean, variance, skewness).\nThe weights are chosen to be as close to uniform as possible (maximum entropy) subject to the balance constraints. This guarantees:\n\nExact balance on the moments you specify\nSmooth weights (no extreme values like IPW can produce)\n\n\nIPW vs Entropy Balancing\n\n\n\n\nIPW\nEntropy Balancing\n\n\n\n\nRequires a propensity score model\nYes\nNo\n\n\nBalance is…\nApproximate (check after)\nExact (by construction)\n\n\nExtreme weights?\nCan be severe\nControlled\n\n\nSensitive to misspecification?\nYes\nLess so\n\n\n\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"confounding\", \"Confounding strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"balance_plot\", height = \"380px\")),\n        column(6, plotOutput(\"weight_plot\",  height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  # Simple entropy balancing: find weights for control group\n  # that match treated group mean of X\n  ebal &lt;- function(x_ctrl, target_mean, max_iter = 200, tol = 1e-6) {\n    n &lt;- length(x_ctrl)\n    lambda &lt;- 0\n    for (i in seq_len(max_iter)) {\n      w &lt;- exp(lambda * x_ctrl)\n      w &lt;- w / sum(w) * n\n      current &lt;- weighted.mean(x_ctrl, w)\n      grad &lt;- current - target_mean\n      if (abs(grad) &lt; tol) break\n      lambda &lt;- lambda - 0.5 * grad\n    }\n    w / sum(w) * n\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    ate  &lt;- input$ate\n    conf &lt;- input$confounding\n\n    x &lt;- rnorm(n)\n    p_true &lt;- pnorm(conf * x)\n    treat &lt;- rbinom(n, 1, p_true)\n\n    y &lt;- 1 + 2 * x + ate * treat + rnorm(n)\n\n    # Naive\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # IPW\n    ps &lt;- fitted(glm(treat ~ x, family = binomial))\n    w_ipw &lt;- ifelse(treat == 1, 1 / ps, 1 / (1 - ps))\n    ipw_est &lt;- weighted.mean(y[treat == 1], w_ipw[treat == 1]) -\n               weighted.mean(y[treat == 0], w_ipw[treat == 0])\n\n    # Entropy balancing (balance control to match treated mean of X)\n    x_ctrl &lt;- x[treat == 0]\n    x_treat_mean &lt;- mean(x[treat == 1])\n    w_eb &lt;- ebal(x_ctrl, x_treat_mean)\n\n    eb_est &lt;- mean(y[treat == 1]) - weighted.mean(y[treat == 0], w_eb)\n\n    # Balance diagnostics\n    ctrl_mean_raw &lt;- mean(x[treat == 0])\n    ctrl_mean_eb  &lt;- weighted.mean(x[treat == 0], w_eb)\n    treat_mean_x  &lt;- x_treat_mean\n\n    list(x = x, treat = treat, y = y,\n         w_ipw = w_ipw, w_eb = w_eb,\n         naive = naive, ipw_est = ipw_est, eb_est = eb_est,\n         ate = ate,\n         ctrl_mean_raw = ctrl_mean_raw,\n         ctrl_mean_eb = ctrl_mean_eb,\n         treat_mean_x = treat_mean_x)\n  })\n\n  output$balance_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 1, 3, 1))\n\n    means &lt;- c(d$treat_mean_x, d$ctrl_mean_raw, d$ctrl_mean_eb)\n    cols &lt;- c(\"#3498db\", \"#e74c3c\", \"#27ae60\")\n    labels &lt;- c(\"Treated\", \"Control\\n(unweighted)\", \"Control\\n(EB weighted)\")\n\n    bp &lt;- barplot(means, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.85,\n                  main = \"Mean of X: Balance Check\",\n                  ylab = \"\", ylim = range(means) * c(0.8, 1.3))\n    text(bp, means + 0.05, round(means, 3), cex = 0.9)\n  })\n\n  output$weight_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ctrl_idx &lt;- which(d$treat == 0)\n\n    w_ipw_ctrl &lt;- d$w_ipw[ctrl_idx]\n    w_eb_ctrl  &lt;- d$w_eb\n\n    xlim &lt;- c(0, max(c(w_ipw_ctrl, w_eb_ctrl)) * 1.1)\n\n    d_ipw &lt;- density(w_ipw_ctrl, from = 0)\n    d_eb  &lt;- density(w_eb_ctrl, from = 0)\n    ylim &lt;- c(0, max(d_ipw$y, d_eb$y) * 1.2)\n\n    plot(d_ipw, col = \"#e74c3c\", lwd = 2.5,\n         main = \"Weight Distributions (Control Units)\",\n         xlab = \"Weight\", ylab = \"Density\",\n         xlim = xlim, ylim = ylim)\n    lines(d_eb, col = \"#27ae60\", lwd = 2.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"IPW weights\", \"Entropy balancing weights\"),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = 2.5)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; &lt;span class='bad'&gt;\", round(d$naive, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(d$naive - d$ate, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;IPW:&lt;/b&gt; \", round(d$ipw_est, 3),\n        \" (bias: \", round(d$ipw_est - d$ate, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;Entropy Bal:&lt;/b&gt; &lt;span class='good'&gt;\", round(d$eb_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(d$eb_est - d$ate, 3), \")\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nConfounding = 1.5: look at the right plot. IPW weights have a long tail (some control units get huge weight). Entropy balancing weights are much smoother.\nConfounding = 3: IPW weights become extreme. EB stays stable.\nLeft plot: the green bar (EB-weighted control mean) exactly matches the blue bar (treated mean). That’s the guarantee — exact balance by construction.\nCompare the bias numbers in the sidebar: EB is typically closer to the true ATE, especially under strong confounding.",
    "crumbs": [
      "Entropy Balancing"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Causal Inference",
    "section": "",
    "text": "Causal inference methods — each topic pairs explanation with an interactive simulation you can run in the browser.\nBuilds on: Statistical Inference",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "Causal Inference",
    "section": "Topics",
    "text": "Topics\n\nPotential Outcomes & ATE — The framework behind all causal questions\nDifference-in-Differences — Parallel trends, treatment effects & event studies\nInverse Probability Weighting — Reweighting to balance treated and control\nEntropy Balancing — Exact moment balancing without a propensity score model",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "potential-outcomes.html",
    "href": "potential-outcomes.html",
    "title": "Potential Outcomes & ATE",
    "section": "",
    "text": "For every person, there are two potential outcomes:\n\n\\(Y_i(1)\\): what happens if they get the treatment\n\\(Y_i(0)\\): what happens if they don’t\n\nThe individual treatment effect is \\(\\tau_i = Y_i(1) - Y_i(0)\\). The problem? We only ever observe one of these. A person is either treated or not — never both. The unobserved outcome is the counterfactual.\nThe Average Treatment Effect (ATE) is:\n\\[\\text{ATE} = E[Y(1) - Y(0)]\\]\nSince we can’t observe both for anyone, we need assumptions (like random assignment) to estimate it.\n\n\nIf treatment is assigned randomly, the treated and control groups are comparable on average. The difference in group means is an unbiased estimator of the ATE. But if treatment isn’t random — if sicker people seek treatment — the naive comparison is biased. That’s selection bias.\nThe simulation below lets you see both potential outcomes (which you never get in real life), watch selection bias appear, and see how randomization fixes it.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Population size:\",\n                  min = 100, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = -2, max = 5, value = 2, step = 0.5),\n\n      selectInput(\"assign\", \"Treatment assignment:\",\n                  choices = c(\"Random (coin flip)\",\n                              \"Self-selection (high Y0 seek treatment)\",\n                              \"Self-selection (low Y0 seek treatment)\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"po_plot\", height = \"400px\")),\n        column(6, plotOutput(\"obs_plot\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    ate &lt;- input$ate\n\n    # Potential outcomes\n    y0 &lt;- rnorm(n, mean = 5, sd = 2)\n    y1 &lt;- y0 + ate + rnorm(n, sd = 0.5)\n\n    # Assignment\n    if (input$assign == \"Random (coin flip)\") {\n      treat &lt;- rbinom(n, 1, 0.5)\n    } else if (input$assign == \"Self-selection (high Y0 seek treatment)\") {\n      prob &lt;- pnorm(y0, mean = mean(y0), sd = sd(y0))\n      treat &lt;- rbinom(n, 1, prob)\n    } else {\n      prob &lt;- 1 - pnorm(y0, mean = mean(y0), sd = sd(y0))\n      treat &lt;- rbinom(n, 1, prob)\n    }\n\n    # Observed outcome\n    y_obs &lt;- ifelse(treat == 1, y1, y0)\n\n    # Estimates\n    naive &lt;- mean(y_obs[treat == 1]) - mean(y_obs[treat == 0])\n    true_ate &lt;- mean(y1 - y0)\n\n    list(y0 = y0, y1 = y1, treat = treat, y_obs = y_obs,\n         naive = naive, true_ate = true_ate, ate = ate)\n  })\n\n  output$po_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$y0, d$y1, pch = 16, cex = 0.6,\n         col = ifelse(d$treat == 1, \"#3498db80\", \"#e74c3c80\"),\n         xlab = \"Y(0) — outcome without treatment\",\n         ylab = \"Y(1) — outcome with treatment\",\n         main = \"Both Potential Outcomes (God's view)\")\n    abline(0, 1, lty = 2, col = \"gray40\", lwd = 1.5)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\", \"45° line (no effect)\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray40\"),\n           pch = c(16, 16, NA), lty = c(NA, NA, 2), lwd = c(NA, NA, 1.5))\n  })\n\n  output$obs_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    grp &lt;- factor(d$treat, labels = c(\"Control\", \"Treated\"))\n    boxplot(d$y_obs ~ grp,\n            col = c(\"#e74c3c40\", \"#3498db40\"),\n            border = c(\"#e74c3c\", \"#3498db\"),\n            main = \"What we actually observe\",\n            ylab = \"Observed Y\", xlab = \"\")\n\n    m0 &lt;- mean(d$y_obs[d$treat == 0])\n    m1 &lt;- mean(d$y_obs[d$treat == 1])\n    points(1:2, c(m0, m1), pch = 18, cex = 2.5, col = c(\"#e74c3c\", \"#3498db\"))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$naive - d$true_ate\n    biased &lt;- abs(bias) &gt; 0.3\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", round(d$true_ate, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive estimate:&lt;/b&gt; \", round(d$naive, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(biased, \"bad\", \"good\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        if (biased) \"&lt;br&gt;&lt;small&gt;Selection bias: treated & control groups aren't comparable.&lt;/small&gt;\"\n        else \"&lt;br&gt;&lt;small&gt;Random assignment makes groups comparable.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nStart with random assignment: the naive estimate is close to the true ATE.\nSwitch to self-selection (high Y₀ seek treatment): people who would have done well anyway are the ones getting treated. The naive estimate is too high — that’s positive selection bias.\nSwitch to self-selection (low Y₀ seek treatment): now the opposite. Sicker people seek treatment, making it look less effective than it is.\nThe left plot shows both potential outcomes — something you never see in real data. That’s the fundamental problem.",
    "crumbs": [
      "Potential Outcomes & ATE"
    ]
  },
  {
    "objectID": "potential-outcomes.html#the-fundamental-problem-of-causal-inference",
    "href": "potential-outcomes.html#the-fundamental-problem-of-causal-inference",
    "title": "Potential Outcomes & ATE",
    "section": "",
    "text": "For every person, there are two potential outcomes:\n\n\\(Y_i(1)\\): what happens if they get the treatment\n\\(Y_i(0)\\): what happens if they don’t\n\nThe individual treatment effect is \\(\\tau_i = Y_i(1) - Y_i(0)\\). The problem? We only ever observe one of these. A person is either treated or not — never both. The unobserved outcome is the counterfactual.\nThe Average Treatment Effect (ATE) is:\n\\[\\text{ATE} = E[Y(1) - Y(0)]\\]\nSince we can’t observe both for anyone, we need assumptions (like random assignment) to estimate it.\n\n\nIf treatment is assigned randomly, the treated and control groups are comparable on average. The difference in group means is an unbiased estimator of the ATE. But if treatment isn’t random — if sicker people seek treatment — the naive comparison is biased. That’s selection bias.\nThe simulation below lets you see both potential outcomes (which you never get in real life), watch selection bias appear, and see how randomization fixes it.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Population size:\",\n                  min = 100, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = -2, max = 5, value = 2, step = 0.5),\n\n      selectInput(\"assign\", \"Treatment assignment:\",\n                  choices = c(\"Random (coin flip)\",\n                              \"Self-selection (high Y0 seek treatment)\",\n                              \"Self-selection (low Y0 seek treatment)\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"po_plot\", height = \"400px\")),\n        column(6, plotOutput(\"obs_plot\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    ate &lt;- input$ate\n\n    # Potential outcomes\n    y0 &lt;- rnorm(n, mean = 5, sd = 2)\n    y1 &lt;- y0 + ate + rnorm(n, sd = 0.5)\n\n    # Assignment\n    if (input$assign == \"Random (coin flip)\") {\n      treat &lt;- rbinom(n, 1, 0.5)\n    } else if (input$assign == \"Self-selection (high Y0 seek treatment)\") {\n      prob &lt;- pnorm(y0, mean = mean(y0), sd = sd(y0))\n      treat &lt;- rbinom(n, 1, prob)\n    } else {\n      prob &lt;- 1 - pnorm(y0, mean = mean(y0), sd = sd(y0))\n      treat &lt;- rbinom(n, 1, prob)\n    }\n\n    # Observed outcome\n    y_obs &lt;- ifelse(treat == 1, y1, y0)\n\n    # Estimates\n    naive &lt;- mean(y_obs[treat == 1]) - mean(y_obs[treat == 0])\n    true_ate &lt;- mean(y1 - y0)\n\n    list(y0 = y0, y1 = y1, treat = treat, y_obs = y_obs,\n         naive = naive, true_ate = true_ate, ate = ate)\n  })\n\n  output$po_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$y0, d$y1, pch = 16, cex = 0.6,\n         col = ifelse(d$treat == 1, \"#3498db80\", \"#e74c3c80\"),\n         xlab = \"Y(0) — outcome without treatment\",\n         ylab = \"Y(1) — outcome with treatment\",\n         main = \"Both Potential Outcomes (God's view)\")\n    abline(0, 1, lty = 2, col = \"gray40\", lwd = 1.5)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\", \"45° line (no effect)\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray40\"),\n           pch = c(16, 16, NA), lty = c(NA, NA, 2), lwd = c(NA, NA, 1.5))\n  })\n\n  output$obs_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    grp &lt;- factor(d$treat, labels = c(\"Control\", \"Treated\"))\n    boxplot(d$y_obs ~ grp,\n            col = c(\"#e74c3c40\", \"#3498db40\"),\n            border = c(\"#e74c3c\", \"#3498db\"),\n            main = \"What we actually observe\",\n            ylab = \"Observed Y\", xlab = \"\")\n\n    m0 &lt;- mean(d$y_obs[d$treat == 0])\n    m1 &lt;- mean(d$y_obs[d$treat == 1])\n    points(1:2, c(m0, m1), pch = 18, cex = 2.5, col = c(\"#e74c3c\", \"#3498db\"))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$naive - d$true_ate\n    biased &lt;- abs(bias) &gt; 0.3\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", round(d$true_ate, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive estimate:&lt;/b&gt; \", round(d$naive, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(biased, \"bad\", \"good\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        if (biased) \"&lt;br&gt;&lt;small&gt;Selection bias: treated & control groups aren't comparable.&lt;/small&gt;\"\n        else \"&lt;br&gt;&lt;small&gt;Random assignment makes groups comparable.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nStart with random assignment: the naive estimate is close to the true ATE.\nSwitch to self-selection (high Y₀ seek treatment): people who would have done well anyway are the ones getting treated. The naive estimate is too high — that’s positive selection bias.\nSwitch to self-selection (low Y₀ seek treatment): now the opposite. Sicker people seek treatment, making it look less effective than it is.\nThe left plot shows both potential outcomes — something you never see in real data. That’s the fundamental problem.",
    "crumbs": [
      "Potential Outcomes & ATE"
    ]
  },
  {
    "objectID": "potential-outcomes.html#what-if-you-cant-randomize",
    "href": "potential-outcomes.html#what-if-you-cant-randomize",
    "title": "Potential Outcomes & ATE",
    "section": "What if you can’t randomize?",
    "text": "What if you can’t randomize?\nIn a true experiment (RCT):\n\nYou randomly assign people to treatment vs control\nBecause it’s random, the two groups are identical on average — so any difference in outcomes must be caused by the treatment\n\nBut most questions in economics, policy, and social science can’t be answered with an RCT. You can’t randomly assign poverty, or force some cities to build highways and others not to. So how do you estimate causal effects?\n\nNatural experiments\nA natural experiment is when something in the real world — a policy change, a rule, a geographic boundary, a disaster — creates treatment and control groups that are as-if randomly assigned. Nobody designed it as an experiment, but the logic is the same.\nExample: Say the government announced “all tracts in counties with food desert score &gt; X get a healthy food program.” Tracts at X+1 vs X−1 didn’t choose to be on different sides of that line — the cutoff did it for them. So comparing those tracts is like comparing treatment and control in an experiment.\n\nThe word “natural” = it happened in the real world, not in a lab.\nThe word “experiment” = it created as-if random variation in who got treated.\n\nThe whole point is to get around the selection bias problem the simulation above shows. If people (or firms, or cities) choose their treatment status, the naive comparison is biased. A natural experiment gives you variation that the units didn’t choose.\n\n\nThe rest of this course\nEvery method in this course is a strategy for exploiting natural experiments or otherwise correcting for selection bias:\n\n\n\nMethod\nThe idea\n\n\n\n\nDifference-in-Differences\nCompare changes over time between treated and control groups\n\n\nIPW\nReweight observations so treated and control look similar on observables\n\n\nEntropy Balancing\nDirectly balance covariates between groups without modeling the propensity score\n\n\n\nEach one makes a different assumption about why the comparison is valid. The art of causal inference is choosing the right method for your setting — and being honest about when the assumptions fail.",
    "crumbs": [
      "Potential Outcomes & ATE"
    ]
  },
  {
    "objectID": "did.html",
    "href": "did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "You have two groups: one gets treated at some point, the other never does. You observe both before and after treatment. The key assumption: absent treatment, both groups would have followed parallel trends.\n\\[\\hat{\\tau}_{DID} = (\\bar{Y}_{treat,post} - \\bar{Y}_{treat,pre}) - (\\bar{Y}_{ctrl,post} - \\bar{Y}_{ctrl,pre})\\]\nThe first difference removes time-invariant group characteristics. The second difference removes common time trends. What’s left is the treatment effect.\n\n\nWhen the parallel trends assumption is violated — if the treated group was already on a different trajectory before treatment. The simulation below lets you break this assumption and see the bias that results.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_units\", \"Units per group:\",\n                  min = 20, max = 200, value = 50, step = 10),\n\n      sliderInput(\"true_effect\", \"True treatment effect:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"trend_diff\", \"Differential pre-trend\\n(violation of parallel trends):\",\n                  min = -1, max = 1, value = 0, step = 0.1),\n\n      sliderInput(\"sigma\", \"Noise (SD):\",\n                  min = 0.5, max = 3, value = 1, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"did_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n_units\n    tau   &lt;- input$true_effect\n    delta &lt;- input$trend_diff\n    sigma &lt;- input$sigma\n\n    periods &lt;- -4:4\n    treat_time &lt;- 1  # treatment at t = 1\n\n    # Group means over time\n    ctrl_mean &lt;- 3 + 0.3 * periods\n    treat_mean &lt;- 5 + (0.3 + delta) * periods\n\n    # Add treatment effect post\n    treat_mean[periods &gt;= treat_time] &lt;- treat_mean[periods &gt;= treat_time] + tau\n\n    # Generate unit-level data\n    ctrl_data &lt;- sapply(ctrl_mean, function(m) m + rnorm(n, sd = sigma))\n    treat_data &lt;- sapply(treat_mean, function(m) m + rnorm(n, sd = sigma))\n\n    ctrl_means_obs &lt;- colMeans(ctrl_data)\n    treat_means_obs &lt;- colMeans(treat_data)\n\n    # DID estimate (using t=0 as pre, t=1 as post)\n    pre_idx  &lt;- which(periods == 0)\n    post_idx &lt;- which(periods == 1)\n\n    did_est &lt;- (treat_means_obs[post_idx] - treat_means_obs[pre_idx]) -\n               (ctrl_means_obs[post_idx] - ctrl_means_obs[pre_idx])\n\n    # Counterfactual for treated (parallel to control from t=0)\n    cf &lt;- treat_means_obs[pre_idx] + (ctrl_means_obs - ctrl_means_obs[pre_idx])\n\n    list(periods = periods, ctrl = ctrl_means_obs, treat = treat_means_obs,\n         cf = cf, did_est = did_est, tau = tau, delta = delta,\n         treat_time = treat_time)\n  })\n\n  output$did_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    ylim &lt;- range(c(d$ctrl, d$treat, d$cf)) + c(-0.5, 0.5)\n\n    plot(d$periods, d$treat, type = \"b\", pch = 19, lwd = 2.5, col = \"#3498db\",\n         xlab = \"Time period\", ylab = \"Mean outcome\",\n         main = \"Difference-in-Differences\",\n         ylim = ylim, xaxt = \"n\")\n    axis(1, at = d$periods)\n\n    lines(d$periods, d$ctrl, type = \"b\", pch = 19, lwd = 2.5, col = \"#e74c3c\")\n\n    # Counterfactual (dashed, post only)\n    post &lt;- d$periods &gt;= d$treat_time\n    lines(d$periods[post], d$cf[post], type = \"b\", pch = 1, lwd = 2, lty = 2,\n          col = \"#3498db80\")\n\n    # Treatment onset\n    abline(v = d$treat_time - 0.5, lty = 3, col = \"gray50\", lwd = 1.5)\n    text(d$treat_time - 0.5, ylim[2], \"Treatment\", pos = 4, cex = 0.85, col = \"gray40\")\n\n    # DID bracket\n    pre_idx  &lt;- which(d$periods == 0)\n    post_idx &lt;- which(d$periods == 1)\n    arrows(max(d$periods) - 0.3, d$cf[post_idx],\n           max(d$periods) - 0.3, d$treat[post_idx],\n           code = 3, lwd = 2, col = \"#27ae60\", length = 0.1)\n    text(max(d$periods) - 0.1, (d$cf[post_idx] + d$treat[post_idx]) / 2,\n         paste0(\"DID = \", round(d$did_est, 2)),\n         col = \"#27ae60\", cex = 0.9, adj = 0)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\", \"Counterfactual (parallel trends)\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"#3498db80\"),\n           pch = c(19, 19, 1), lty = c(1, 1, 2), lwd = c(2.5, 2.5, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$did_est - d$tau\n    biased &lt;- abs(d$delta) &gt; 0.05\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;DID estimate:&lt;/b&gt; \", round(d$did_est, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(biased, \"bad\", \"good\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        if (biased) \"&lt;br&gt;&lt;small&gt;Parallel trends violated &mdash; DID is biased.&lt;/small&gt;\"\n        else \"&lt;br&gt;&lt;small&gt;Parallel trends hold &mdash; DID is unbiased.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDifferential pre-trend = 0: parallel trends hold, DID nails the true effect.\nSlide the differential pre-trend to +0.5: the treated group was already rising faster. DID attributes some of that trend to the treatment — the estimate is biased upward.\nSet true effect = 0 with a differential trend: DID “finds” an effect that doesn’t exist. That’s how pre-trend violations create false positives.\nLook at the pre-treatment periods — if the lines aren’t parallel before treatment, you should worry.",
    "crumbs": [
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "did.html#the-idea",
    "href": "did.html#the-idea",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "You have two groups: one gets treated at some point, the other never does. You observe both before and after treatment. The key assumption: absent treatment, both groups would have followed parallel trends.\n\\[\\hat{\\tau}_{DID} = (\\bar{Y}_{treat,post} - \\bar{Y}_{treat,pre}) - (\\bar{Y}_{ctrl,post} - \\bar{Y}_{ctrl,pre})\\]\nThe first difference removes time-invariant group characteristics. The second difference removes common time trends. What’s left is the treatment effect.\n\n\nWhen the parallel trends assumption is violated — if the treated group was already on a different trajectory before treatment. The simulation below lets you break this assumption and see the bias that results.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_units\", \"Units per group:\",\n                  min = 20, max = 200, value = 50, step = 10),\n\n      sliderInput(\"true_effect\", \"True treatment effect:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"trend_diff\", \"Differential pre-trend\\n(violation of parallel trends):\",\n                  min = -1, max = 1, value = 0, step = 0.1),\n\n      sliderInput(\"sigma\", \"Noise (SD):\",\n                  min = 0.5, max = 3, value = 1, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"did_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n_units\n    tau   &lt;- input$true_effect\n    delta &lt;- input$trend_diff\n    sigma &lt;- input$sigma\n\n    periods &lt;- -4:4\n    treat_time &lt;- 1  # treatment at t = 1\n\n    # Group means over time\n    ctrl_mean &lt;- 3 + 0.3 * periods\n    treat_mean &lt;- 5 + (0.3 + delta) * periods\n\n    # Add treatment effect post\n    treat_mean[periods &gt;= treat_time] &lt;- treat_mean[periods &gt;= treat_time] + tau\n\n    # Generate unit-level data\n    ctrl_data &lt;- sapply(ctrl_mean, function(m) m + rnorm(n, sd = sigma))\n    treat_data &lt;- sapply(treat_mean, function(m) m + rnorm(n, sd = sigma))\n\n    ctrl_means_obs &lt;- colMeans(ctrl_data)\n    treat_means_obs &lt;- colMeans(treat_data)\n\n    # DID estimate (using t=0 as pre, t=1 as post)\n    pre_idx  &lt;- which(periods == 0)\n    post_idx &lt;- which(periods == 1)\n\n    did_est &lt;- (treat_means_obs[post_idx] - treat_means_obs[pre_idx]) -\n               (ctrl_means_obs[post_idx] - ctrl_means_obs[pre_idx])\n\n    # Counterfactual for treated (parallel to control from t=0)\n    cf &lt;- treat_means_obs[pre_idx] + (ctrl_means_obs - ctrl_means_obs[pre_idx])\n\n    list(periods = periods, ctrl = ctrl_means_obs, treat = treat_means_obs,\n         cf = cf, did_est = did_est, tau = tau, delta = delta,\n         treat_time = treat_time)\n  })\n\n  output$did_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    ylim &lt;- range(c(d$ctrl, d$treat, d$cf)) + c(-0.5, 0.5)\n\n    plot(d$periods, d$treat, type = \"b\", pch = 19, lwd = 2.5, col = \"#3498db\",\n         xlab = \"Time period\", ylab = \"Mean outcome\",\n         main = \"Difference-in-Differences\",\n         ylim = ylim, xaxt = \"n\")\n    axis(1, at = d$periods)\n\n    lines(d$periods, d$ctrl, type = \"b\", pch = 19, lwd = 2.5, col = \"#e74c3c\")\n\n    # Counterfactual (dashed, post only)\n    post &lt;- d$periods &gt;= d$treat_time\n    lines(d$periods[post], d$cf[post], type = \"b\", pch = 1, lwd = 2, lty = 2,\n          col = \"#3498db80\")\n\n    # Treatment onset\n    abline(v = d$treat_time - 0.5, lty = 3, col = \"gray50\", lwd = 1.5)\n    text(d$treat_time - 0.5, ylim[2], \"Treatment\", pos = 4, cex = 0.85, col = \"gray40\")\n\n    # DID bracket\n    pre_idx  &lt;- which(d$periods == 0)\n    post_idx &lt;- which(d$periods == 1)\n    arrows(max(d$periods) - 0.3, d$cf[post_idx],\n           max(d$periods) - 0.3, d$treat[post_idx],\n           code = 3, lwd = 2, col = \"#27ae60\", length = 0.1)\n    text(max(d$periods) - 0.1, (d$cf[post_idx] + d$treat[post_idx]) / 2,\n         paste0(\"DID = \", round(d$did_est, 2)),\n         col = \"#27ae60\", cex = 0.9, adj = 0)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\", \"Counterfactual (parallel trends)\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"#3498db80\"),\n           pch = c(19, 19, 1), lty = c(1, 1, 2), lwd = c(2.5, 2.5, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$did_est - d$tau\n    biased &lt;- abs(d$delta) &gt; 0.05\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;DID estimate:&lt;/b&gt; \", round(d$did_est, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(biased, \"bad\", \"good\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        if (biased) \"&lt;br&gt;&lt;small&gt;Parallel trends violated &mdash; DID is biased.&lt;/small&gt;\"\n        else \"&lt;br&gt;&lt;small&gt;Parallel trends hold &mdash; DID is unbiased.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDifferential pre-trend = 0: parallel trends hold, DID nails the true effect.\nSlide the differential pre-trend to +0.5: the treated group was already rising faster. DID attributes some of that trend to the treatment — the estimate is biased upward.\nSet true effect = 0 with a differential trend: DID “finds” an effect that doesn’t exist. That’s how pre-trend violations create false positives.\nLook at the pre-treatment periods — if the lines aren’t parallel before treatment, you should worry.",
    "crumbs": [
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "ipw.html",
    "href": "ipw.html",
    "title": "Inverse Probability Weighting",
    "section": "",
    "text": "In observational data, treatment isn’t random. People who get treated differ from those who don’t — they may be older, sicker, richer, etc. A naive comparison of outcomes is biased by these confounders.",
    "crumbs": [
      "IPW"
    ]
  },
  {
    "objectID": "ipw.html#the-problem",
    "href": "ipw.html#the-problem",
    "title": "Inverse Probability Weighting",
    "section": "",
    "text": "In observational data, treatment isn’t random. People who get treated differ from those who don’t — they may be older, sicker, richer, etc. A naive comparison of outcomes is biased by these confounders.",
    "crumbs": [
      "IPW"
    ]
  },
  {
    "objectID": "ipw.html#the-idea",
    "href": "ipw.html#the-idea",
    "title": "Inverse Probability Weighting",
    "section": "The idea",
    "text": "The idea\nInverse Probability Weighting (IPW) reweights observations so that the treated and control groups look alike on observed covariates. The steps:\n\nEstimate the propensity score \\(e(X) = P(\\text{treated} \\mid X)\\) — the probability of treatment given covariates.\nWeight each observation inversely by its probability of receiving the treatment it actually got:\n\nTreated units get weight \\(1 / e(X)\\)\nControl units get weight \\(1 / (1 - e(X))\\)\n\nCompute the weighted difference in means.\n\nIntuition: if a treated person had only a 20% chance of being treated (based on their X), they’re “surprising” — they represent 5 similar people who weren’t treated. So they get upweighted. This creates a pseudo-population where treatment is independent of X.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"confounding\", \"Confounding strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"balance_plot\", height = \"380px\")),\n        column(6, plotOutput(\"ps_plot\", height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    ate  &lt;- input$ate\n    conf &lt;- input$confounding\n\n    # Confounder\n    x &lt;- rnorm(n)\n\n    # Treatment depends on x (confounding)\n    p_true &lt;- pnorm(conf * x)\n    treat &lt;- rbinom(n, 1, p_true)\n\n    # Outcome depends on x and treatment\n    y &lt;- 1 + 2 * x + ate * treat + rnorm(n)\n\n    # Naive estimate\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # IPW estimate\n    ps &lt;- fitted(glm(treat ~ x, family = binomial))\n    w &lt;- ifelse(treat == 1, 1 / ps, 1 / (1 - ps))\n    ipw_est &lt;- weighted.mean(y[treat == 1], w[treat == 1]) -\n               weighted.mean(y[treat == 0], w[treat == 0])\n\n    list(x = x, treat = treat, y = y, ps = ps, w = w,\n         naive = naive, ipw_est = ipw_est, ate = ate)\n  })\n\n  output$balance_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Unweighted densities\n    x_t &lt;- d$x[d$treat == 1]\n    x_c &lt;- d$x[d$treat == 0]\n\n    rng &lt;- range(d$x)\n    dens_t &lt;- density(x_t, from = rng[1], to = rng[2])\n    dens_c &lt;- density(x_c, from = rng[1], to = rng[2])\n\n    ylim &lt;- c(0, max(dens_t$y, dens_c$y) * 1.2)\n\n    plot(dens_t, col = \"#3498db\", lwd = 2.5, main = \"Covariate Balance (X)\",\n         xlab = \"X (confounder)\", ylab = \"Density\", ylim = ylim)\n    lines(dens_c, col = \"#e74c3c\", lwd = 2.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\"),\n           col = c(\"#3498db\", \"#e74c3c\"), lwd = 2.5)\n  })\n\n  output$ps_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$ps, pch = 16, cex = 0.5,\n         col = ifelse(d$treat == 1, \"#3498db80\", \"#e74c3c80\"),\n         xlab = \"X (confounder)\", ylab = \"Propensity score e(X)\",\n         main = \"Propensity Score vs Confounder\")\n    abline(h = 0.5, lty = 2, col = \"gray50\")\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\"),\n           col = c(\"#3498db\", \"#e74c3c\"), pch = 16)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive estimate:&lt;/b&gt; &lt;span class='bad'&gt;\", round(d$naive, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;IPW estimate:&lt;/b&gt; &lt;span class='good'&gt;\", round(d$ipw_est, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Naive bias:&lt;/b&gt; \", round(d$naive - d$ate, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;IPW bias:&lt;/b&gt; \", round(d$ipw_est - d$ate, 3)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nConfounding = 0: treatment is random. Naive and IPW give the same answer.\nConfounding = 1.5: the covariate distributions for treated and control diverge (left plot). Naive is biased, IPW corrects it.\nConfounding = 3: extreme selection. The propensity scores are near 0 or 1 (right plot), meaning some units get huge weights. IPW becomes noisy — this is the extreme weights problem.",
    "crumbs": [
      "IPW"
    ]
  }
]