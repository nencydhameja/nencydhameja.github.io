[
  {
    "objectID": "iv.html",
    "href": "iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "You want the causal effect of \\(X\\) on \\(Y\\), but \\(X\\) is endogenous — correlated with the error term because of confounding, reverse causality, or measurement error. OLS is biased.\nThe fix: find a variable \\(Z\\) (the instrument) that:\n\nRelevance: \\(Z\\) is correlated with \\(X\\) — it actually moves \\(X\\)\nExclusion restriction: \\(Z\\) affects \\(Y\\) only through \\(X\\) — no back doors\n\n\\[Z \\to X \\to Y\\]\nIf both conditions hold, you can use \\(Z\\) to isolate the part of \\(X\\) that’s “as good as random” and estimate the causal effect.\n\n\nThe mechanics are simple:\nFirst stage: regress \\(X\\) on \\(Z\\) to get predicted values \\(\\hat{X}\\)\n\\[X = \\pi_0 + \\pi_1 Z + v\\]\nSecond stage: regress \\(Y\\) on \\(\\hat{X}\\) instead of \\(X\\)\n\\[Y = \\beta_0 + \\beta_1 \\hat{X} + \\varepsilon\\]\nWhy does this work? \\(\\hat{X}\\) contains only the variation in \\(X\\) that comes from \\(Z\\). Since \\(Z\\) is exogenous (by assumption), \\(\\hat{X}\\) is uncorrelated with the error term. The confounding is gone.\n\n\n\n\nRelevance: \\(Z\\) is correlated with \\(X\\) — the instrument actually moves the endogenous variable. Testable: check the first-stage F-statistic.\nExclusion restriction: \\(Z\\) affects \\(Y\\) only through \\(X\\) — no direct effect and no back-door paths. Not testable — you argue it.\nIndependence: \\(Z\\) is as good as randomly assigned — uncorrelated with the error term in the outcome equation\nMonotonicity (for LATE): the instrument moves everyone in the same direction — no “defiers” who do the opposite of what the instrument encourages\n\n\n\n\nReturns to education. You want to know if more schooling causes higher earnings. But ability confounds: smarter people get more education and earn more. OLS overstates the return.\nAngrist & Krueger (1991) used quarter of birth as an instrument. Because of compulsory schooling laws, people born in Q1 can drop out slightly earlier than Q4 births — so quarter of birth affects education (relevance) but presumably doesn’t affect earnings directly (exclusion).\n\n\n\n\nWeak instruments: if \\(Z\\) barely moves \\(X\\), the first stage is weak and the IV estimate becomes wildly noisy and biased. Rule of thumb: first-stage F-statistic &gt; 10.\nExclusion restriction violated: if \\(Z\\) affects \\(Y\\) through channels other than \\(X\\), the estimate is biased. This assumption is untestable — you argue it, you don’t prove it.\n\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 100, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"true_b\", \"True causal effect of X on Y:\",\n                  min = 0, max = 5, value = 2, step = 0.25),\n\n      sliderInput(\"confound\", \"Confounding strength:\",\n                  min = 0, max = 5, value = 3, step = 0.25),\n\n      sliderInput(\"inst_str\", \"Instrument strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.1),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"iv_plot\", height = \"470px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    b   &lt;- input$true_b\n    cf  &lt;- input$confound\n    pi1 &lt;- input$inst_str\n\n    # Confounder (unobserved ability)\n    u &lt;- rnorm(n)\n\n    # Instrument\n    z &lt;- rnorm(n)\n\n    # Endogenous regressor: driven by instrument + confounder\n    x &lt;- pi1 * z + cf * u + rnorm(n)\n\n    # Outcome: causal effect of x + confounder\n    y &lt;- b * x + cf * u + rnorm(n)\n\n    # OLS (biased)\n    ols &lt;- lm(y ~ x)\n\n    # 2SLS by hand\n    first &lt;- lm(x ~ z)\n    x_hat &lt;- fitted(first)\n    second &lt;- lm(y ~ x_hat)\n\n    # First-stage F\n    f_stat &lt;- summary(first)$fstatistic[1]\n\n    list(x = x, y = y, z = z, x_hat = x_hat,\n         b_ols = coef(ols)[2],\n         b_iv = coef(second)[2],\n         first_coef = coef(first)[2],\n         f_stat = f_stat,\n         true_b = b, confound = cf, inst_str = pi1)\n  })\n\n  output$iv_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mfrow = c(1, 2), mar = c(4.5, 4.5, 3, 1))\n\n    # Left: OLS scatter (X vs Y)\n    plot(d$x, d$y, pch = 16, cex = 0.4,\n         col = adjustcolor(\"#3498db\", 0.3),\n         xlab = \"X (endogenous)\", ylab = \"Y\",\n         main = \"OLS: Y on X\")\n    abline(lm(d$y ~ d$x), col = \"#e74c3c\", lwd = 3)\n    abline(a = 0, b = d$true_b, col = \"#27ae60\", lwd = 2, lty = 2)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(paste0(\"OLS = \", round(d$b_ols, 2)),\n                      paste0(\"True = \", d$true_b)),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = c(3, 2), lty = c(1, 2))\n\n    # Right: IV scatter (X-hat vs Y)\n    plot(d$x_hat, d$y, pch = 16, cex = 0.4,\n         col = adjustcolor(\"#9b59b6\", 0.3),\n         xlab = expression(hat(X) ~ \"(from first stage)\"), ylab = \"Y\",\n         main = expression(\"2SLS: Y on \" * hat(X)))\n    abline(lm(d$y ~ d$x_hat), col = \"#e74c3c\", lwd = 3)\n    abline(a = 0, b = d$true_b, col = \"#27ae60\", lwd = 2, lty = 2)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(paste0(\"IV = \", round(d$b_iv, 2)),\n                      paste0(\"True = \", d$true_b)),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = c(3, 2), lty = c(1, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    ols_bias &lt;- d$b_ols - d$true_b\n    iv_bias  &lt;- d$b_iv - d$true_b\n    weak &lt;- d$f_stat &lt; 10\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$true_b, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;OLS:&lt;/b&gt; \", round(d$b_ols, 3),\n        \" &nbsp; Bias: &lt;span class='bad'&gt;\", round(ols_bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;IV (2SLS):&lt;/b&gt; \", round(d$b_iv, 3),\n        \" &nbsp; Bias: &lt;span class='\", ifelse(abs(iv_bias) &lt; abs(ols_bias), \"good\", \"bad\"), \"'&gt;\",\n        round(iv_bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;First-stage F:&lt;/b&gt; \", round(d$f_stat, 1),\n        if (weak) \" &lt;span class='bad'&gt;&lt; 10 (weak!)&lt;/span&gt;\"\n        else \" &lt;span class='good'&gt;&ge; 10&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nConfounding = 3, instrument = 1.5: OLS is badly biased. IV recovers the true effect. This is the whole point.\nSet confounding = 0: OLS and IV agree — when there’s no endogeneity, you don’t need an instrument.\nSlide instrument strength toward 0: the first-stage F drops below 10. The IV estimate becomes erratic — sometimes worse than OLS. That’s the weak instrument problem.\nIncrease sample size with a weak instrument: it doesn’t help much. Weak instruments bias IV toward OLS, and more data doesn’t fix that.\nTrue effect = 0, confounding = 3: OLS “finds” a large effect. IV correctly shows ~0.",
    "crumbs": [
      "Methods",
      "Instrumental Variables"
    ]
  },
  {
    "objectID": "iv.html#the-idea",
    "href": "iv.html#the-idea",
    "title": "Instrumental Variables",
    "section": "",
    "text": "You want the causal effect of \\(X\\) on \\(Y\\), but \\(X\\) is endogenous — correlated with the error term because of confounding, reverse causality, or measurement error. OLS is biased.\nThe fix: find a variable \\(Z\\) (the instrument) that:\n\nRelevance: \\(Z\\) is correlated with \\(X\\) — it actually moves \\(X\\)\nExclusion restriction: \\(Z\\) affects \\(Y\\) only through \\(X\\) — no back doors\n\n\\[Z \\to X \\to Y\\]\nIf both conditions hold, you can use \\(Z\\) to isolate the part of \\(X\\) that’s “as good as random” and estimate the causal effect.\n\n\nThe mechanics are simple:\nFirst stage: regress \\(X\\) on \\(Z\\) to get predicted values \\(\\hat{X}\\)\n\\[X = \\pi_0 + \\pi_1 Z + v\\]\nSecond stage: regress \\(Y\\) on \\(\\hat{X}\\) instead of \\(X\\)\n\\[Y = \\beta_0 + \\beta_1 \\hat{X} + \\varepsilon\\]\nWhy does this work? \\(\\hat{X}\\) contains only the variation in \\(X\\) that comes from \\(Z\\). Since \\(Z\\) is exogenous (by assumption), \\(\\hat{X}\\) is uncorrelated with the error term. The confounding is gone.\n\n\n\n\nRelevance: \\(Z\\) is correlated with \\(X\\) — the instrument actually moves the endogenous variable. Testable: check the first-stage F-statistic.\nExclusion restriction: \\(Z\\) affects \\(Y\\) only through \\(X\\) — no direct effect and no back-door paths. Not testable — you argue it.\nIndependence: \\(Z\\) is as good as randomly assigned — uncorrelated with the error term in the outcome equation\nMonotonicity (for LATE): the instrument moves everyone in the same direction — no “defiers” who do the opposite of what the instrument encourages\n\n\n\n\nReturns to education. You want to know if more schooling causes higher earnings. But ability confounds: smarter people get more education and earn more. OLS overstates the return.\nAngrist & Krueger (1991) used quarter of birth as an instrument. Because of compulsory schooling laws, people born in Q1 can drop out slightly earlier than Q4 births — so quarter of birth affects education (relevance) but presumably doesn’t affect earnings directly (exclusion).\n\n\n\n\nWeak instruments: if \\(Z\\) barely moves \\(X\\), the first stage is weak and the IV estimate becomes wildly noisy and biased. Rule of thumb: first-stage F-statistic &gt; 10.\nExclusion restriction violated: if \\(Z\\) affects \\(Y\\) through channels other than \\(X\\), the estimate is biased. This assumption is untestable — you argue it, you don’t prove it.\n\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 100, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"true_b\", \"True causal effect of X on Y:\",\n                  min = 0, max = 5, value = 2, step = 0.25),\n\n      sliderInput(\"confound\", \"Confounding strength:\",\n                  min = 0, max = 5, value = 3, step = 0.25),\n\n      sliderInput(\"inst_str\", \"Instrument strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.1),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"iv_plot\", height = \"470px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    b   &lt;- input$true_b\n    cf  &lt;- input$confound\n    pi1 &lt;- input$inst_str\n\n    # Confounder (unobserved ability)\n    u &lt;- rnorm(n)\n\n    # Instrument\n    z &lt;- rnorm(n)\n\n    # Endogenous regressor: driven by instrument + confounder\n    x &lt;- pi1 * z + cf * u + rnorm(n)\n\n    # Outcome: causal effect of x + confounder\n    y &lt;- b * x + cf * u + rnorm(n)\n\n    # OLS (biased)\n    ols &lt;- lm(y ~ x)\n\n    # 2SLS by hand\n    first &lt;- lm(x ~ z)\n    x_hat &lt;- fitted(first)\n    second &lt;- lm(y ~ x_hat)\n\n    # First-stage F\n    f_stat &lt;- summary(first)$fstatistic[1]\n\n    list(x = x, y = y, z = z, x_hat = x_hat,\n         b_ols = coef(ols)[2],\n         b_iv = coef(second)[2],\n         first_coef = coef(first)[2],\n         f_stat = f_stat,\n         true_b = b, confound = cf, inst_str = pi1)\n  })\n\n  output$iv_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mfrow = c(1, 2), mar = c(4.5, 4.5, 3, 1))\n\n    # Left: OLS scatter (X vs Y)\n    plot(d$x, d$y, pch = 16, cex = 0.4,\n         col = adjustcolor(\"#3498db\", 0.3),\n         xlab = \"X (endogenous)\", ylab = \"Y\",\n         main = \"OLS: Y on X\")\n    abline(lm(d$y ~ d$x), col = \"#e74c3c\", lwd = 3)\n    abline(a = 0, b = d$true_b, col = \"#27ae60\", lwd = 2, lty = 2)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(paste0(\"OLS = \", round(d$b_ols, 2)),\n                      paste0(\"True = \", d$true_b)),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = c(3, 2), lty = c(1, 2))\n\n    # Right: IV scatter (X-hat vs Y)\n    plot(d$x_hat, d$y, pch = 16, cex = 0.4,\n         col = adjustcolor(\"#9b59b6\", 0.3),\n         xlab = expression(hat(X) ~ \"(from first stage)\"), ylab = \"Y\",\n         main = expression(\"2SLS: Y on \" * hat(X)))\n    abline(lm(d$y ~ d$x_hat), col = \"#e74c3c\", lwd = 3)\n    abline(a = 0, b = d$true_b, col = \"#27ae60\", lwd = 2, lty = 2)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(paste0(\"IV = \", round(d$b_iv, 2)),\n                      paste0(\"True = \", d$true_b)),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = c(3, 2), lty = c(1, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    ols_bias &lt;- d$b_ols - d$true_b\n    iv_bias  &lt;- d$b_iv - d$true_b\n    weak &lt;- d$f_stat &lt; 10\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$true_b, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;OLS:&lt;/b&gt; \", round(d$b_ols, 3),\n        \" &nbsp; Bias: &lt;span class='bad'&gt;\", round(ols_bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;IV (2SLS):&lt;/b&gt; \", round(d$b_iv, 3),\n        \" &nbsp; Bias: &lt;span class='\", ifelse(abs(iv_bias) &lt; abs(ols_bias), \"good\", \"bad\"), \"'&gt;\",\n        round(iv_bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;First-stage F:&lt;/b&gt; \", round(d$f_stat, 1),\n        if (weak) \" &lt;span class='bad'&gt;&lt; 10 (weak!)&lt;/span&gt;\"\n        else \" &lt;span class='good'&gt;&ge; 10&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nConfounding = 3, instrument = 1.5: OLS is badly biased. IV recovers the true effect. This is the whole point.\nSet confounding = 0: OLS and IV agree — when there’s no endogeneity, you don’t need an instrument.\nSlide instrument strength toward 0: the first-stage F drops below 10. The IV estimate becomes erratic — sometimes worse than OLS. That’s the weak instrument problem.\nIncrease sample size with a weak instrument: it doesn’t help much. Weak instruments bias IV toward OLS, and more data doesn’t fix that.\nTrue effect = 0, confounding = 3: OLS “finds” a large effect. IV correctly shows ~0.",
    "crumbs": [
      "Methods",
      "Instrumental Variables"
    ]
  },
  {
    "objectID": "iv.html#what-does-iv-actually-estimate",
    "href": "iv.html#what-does-iv-actually-estimate",
    "title": "Instrumental Variables",
    "section": "What does IV actually estimate?",
    "text": "What does IV actually estimate?\nA subtle point: IV doesn’t estimate the effect for everyone. It estimates the Local Average Treatment Effect (LATE) — the effect for compliers, people whose treatment status is actually changed by the instrument.\nIn the Angrist & Krueger example: IV estimates the return to education for people who would have dropped out if born in a different quarter. It says nothing about people who would have stayed in school regardless.\nThis means two different valid instruments can give you two different IV estimates — not because one is wrong, but because they’re identifying effects for different subpopulations.",
    "crumbs": [
      "Methods",
      "Instrumental Variables"
    ]
  },
  {
    "objectID": "iv.html#in-stata",
    "href": "iv.html#in-stata",
    "title": "Instrumental Variables",
    "section": "In Stata",
    "text": "In Stata\n* Two-stage least squares\nivregress 2sls outcome x1 x2 (treatment = instrument)\n\n* First-stage F statistic (check relevance)\nestat firststage\n\n* Overidentification test (with multiple instruments)\nivregress 2sls outcome x1 (treatment = inst1 inst2)\nestat overid\n\n* Manually run the two stages (to see what's happening)\nreg treatment instrument x1 x2          /* first stage */\npredict treatment_hat, xb\nreg outcome treatment_hat x1 x2         /* second stage */\nThe first-stage F should be well above 10 (Staiger & Stock rule of thumb). If it’s weak, the IV estimate is unreliable — possibly more biased than OLS.",
    "crumbs": [
      "Methods",
      "Instrumental Variables"
    ]
  },
  {
    "objectID": "iv.html#did-you-know",
    "href": "iv.html#did-you-know",
    "title": "Instrumental Variables",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe instrumental variables method dates back to Philip Wright (1928), who used it to estimate supply and demand curves for butter and flax seed. Some historians credit his son, Sewall Wright, with the actual derivation.\nThe “weak instruments” problem was formalized by Staiger & Stock (1997). They showed that when the first-stage F is below 10, IV can be more biased than OLS — the cure becomes worse than the disease.\nJoshua Angrist, one of the 2021 Nobel laureates, built much of his career on clever instruments: quarter of birth for schooling, draft lottery numbers for military service, religious composition for family size. The art is finding instruments that are both relevant and excludable.",
    "crumbs": [
      "Methods",
      "Instrumental Variables"
    ]
  },
  {
    "objectID": "panel-fe-re.html",
    "href": "panel-fe-re.html",
    "title": "Fixed vs Random Effects",
    "section": "",
    "text": "Most of the methods in this course use cross-sectional data — you see each unit once. But often you observe units repeatedly:\n\nThe same people surveyed each year (panel)\nStudents nested within schools (clusters)\nCensus tracts nested within counties (hierarchical)\n\nPanel/grouped data has a superpower: you can separate within-group variation (what changes over time for the same person) from between-group variation (how different people differ from each other). This distinction is the key to fixed vs random effects.\n\n\n\n\n\n\nExample: dollar stores and obesity. You have census tracts nested within counties, observed over multiple years. Some tracts get a new dollar store, others don’t. The question: does dollar store presence cause higher obesity? The problem: counties that attract dollar stores may already be different — poorer, more rural, less access to grocery stores. That’s a group-level confounder. Fixed effects can absorb it.",
    "crumbs": [
      "Panel Data",
      "Fixed vs Random Effects"
    ]
  },
  {
    "objectID": "panel-fe-re.html#panel-data-the-same-units-over-time-or-nested-units",
    "href": "panel-fe-re.html#panel-data-the-same-units-over-time-or-nested-units",
    "title": "Fixed vs Random Effects",
    "section": "",
    "text": "Most of the methods in this course use cross-sectional data — you see each unit once. But often you observe units repeatedly:\n\nThe same people surveyed each year (panel)\nStudents nested within schools (clusters)\nCensus tracts nested within counties (hierarchical)\n\nPanel/grouped data has a superpower: you can separate within-group variation (what changes over time for the same person) from between-group variation (how different people differ from each other). This distinction is the key to fixed vs random effects.\n\n\n\n\n\n\nExample: dollar stores and obesity. You have census tracts nested within counties, observed over multiple years. Some tracts get a new dollar store, others don’t. The question: does dollar store presence cause higher obesity? The problem: counties that attract dollar stores may already be different — poorer, more rural, less access to grocery stores. That’s a group-level confounder. Fixed effects can absorb it.",
    "crumbs": [
      "Panel Data",
      "Fixed vs Random Effects"
    ]
  },
  {
    "objectID": "panel-fe-re.html#fixed-effects-fe",
    "href": "panel-fe-re.html#fixed-effects-fe",
    "title": "Fixed vs Random Effects",
    "section": "Fixed effects (FE)",
    "text": "Fixed effects (FE)\nThe idea: include a separate intercept \\(\\alpha_j\\) for each group. This absorbs everything that is constant within the group — observed or unobserved.\n\\[Y_{it} = \\alpha_i + \\tau D_{it} + \\beta X_{it} + \\varepsilon_{it}\\]\nIn practice, you demean each variable by its group average:\n\\[\\tilde{Y}_{it} = Y_{it} - \\bar{Y}_i, \\quad \\tilde{D}_{it} = D_{it} - \\bar{D}_i, \\quad \\tilde{X}_{it} = X_{it} - \\bar{X}_i\\]\nThen regress \\(\\tilde{Y}\\) on \\(\\tilde{D}\\) and \\(\\tilde{X}\\). The demeaning eliminates \\(\\alpha_i\\) entirely — you only use within-group variation.\n\nWhat FE eliminates\nAny variable that is constant within a group gets differenced out:\n\nCounty-level: food culture, geography, baseline poverty level\nPerson-level (in a panel): ability, motivation, family background\nFirm-level: management quality, industry\n\nThis is the power of FE: you don’t need to measure or even name the confounders. If they’re constant within groups, they’re gone.\n\n\nWhat FE can’t do\n\nCan’t estimate effects of group-level variables. If you want to know the effect of being rural vs urban, FE absorbs that — it’s constant within county.\nCan’t handle time-varying unobservables. If something changes within the group and is correlated with treatment, FE doesn’t help.\nUses only within-group variation. If treatment barely varies within groups, FE has little to work with and estimates are imprecise.",
    "crumbs": [
      "Panel Data",
      "Fixed vs Random Effects"
    ]
  },
  {
    "objectID": "panel-fe-re.html#random-effects-re",
    "href": "panel-fe-re.html#random-effects-re",
    "title": "Fixed vs Random Effects",
    "section": "Random effects (RE)",
    "text": "Random effects (RE)\nThe idea: instead of treating each \\(\\alpha_j\\) as a fixed parameter, assume the group effects are random draws from a population:\n\\[\\alpha_j \\sim N(\\mu, \\tau^2)\\]\nThis is the same structure as the hierarchical model on the Bayesian course — RE is its frequentist counterpart. It produces partial pooling: small groups are shrunk toward the grand mean.\n\nThe critical assumption\nRE assumes group effects are uncorrelated with the covariates:\n\\[E[\\alpha_j \\mid X_{it}, D_{it}] = \\mu \\quad \\text{(no correlation)}\\]\nIf this holds, RE is more efficient than FE — it uses both within-group and between-group variation, so estimates are more precise.\nIf this fails — if groups with higher \\(\\alpha_j\\) systematically differ in their \\(X\\) or \\(D\\) values — RE is biased. This is the same selection bias problem from the potential outcomes page, but at the group level.",
    "crumbs": [
      "Panel Data",
      "Fixed vs Random Effects"
    ]
  },
  {
    "objectID": "panel-fe-re.html#the-tradeoff",
    "href": "panel-fe-re.html#the-tradeoff",
    "title": "Fixed vs Random Effects",
    "section": "The tradeoff",
    "text": "The tradeoff\n\n\n\n\n\n\n\n\n\nFixed effects\nRandom effects\n\n\n\n\nEliminates group-level confounders\nYes\nOnly if uncorrelated with X\n\n\nUses between-group variation\nNo (only within)\nYes (within + between)\n\n\nEfficiency\nLower (discards information)\nHigher (if assumption holds)\n\n\nCan estimate group-level predictors\nNo\nYes\n\n\nBias when \\(\\alpha_j\\) correlated with X\nNone\nBiased\n\n\n\nThe rule: if you’re after a causal effect and you’re worried about group-level confounders, use FE. If you believe the RE assumption (no correlation) or you need group-level predictors, use RE. When in doubt, FE is the safer choice.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_groups\", \"Number of groups:\",\n                  min = 10, max = 50, value = 20, step = 5),\n\n      sliderInput(\"n_per\", \"Observations per group:\",\n                  min = 5, max = 50, value = 10, step = 5),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"corr_alpha\", \"Correlation (\\u03B1 with D):\",\n                  min = 0, max = 3, value = 0, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"scatter_plot\", height = \"420px\")),\n        column(6, plotOutput(\"compare_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    J     &lt;- input$n_groups\n    n_per &lt;- input$n_per\n    ate   &lt;- input$ate\n    rho   &lt;- input$corr_alpha\n\n    # Group effects\n    alpha &lt;- rnorm(J, mean = 0, sd = 3)\n\n    # Expand to individual level\n    group &lt;- rep(1:J, each = n_per)\n    alpha_i &lt;- rep(alpha, each = n_per)\n    N &lt;- J * n_per\n\n    # Treatment: correlated with group effect (the confounding)\n    x &lt;- rnorm(N)\n    p_treat &lt;- pnorm(0.5 * x + rho * alpha_i / 3)\n    treat &lt;- rbinom(N, 1, p_treat)\n\n    # Outcome\n    y &lt;- alpha_i + ate * treat + 1.5 * x + rnorm(N)\n\n    # Naive (pooled OLS, no group effects)\n    naive &lt;- coef(lm(y ~ treat + x))[\"treat\"]\n\n    # Fixed effects (demean within group)\n    y_dm &lt;- y - ave(y, group)\n    d_dm &lt;- treat - ave(treat, group)\n    x_dm &lt;- x - ave(x, group)\n    fe_est &lt;- coef(lm(y_dm ~ d_dm + x_dm - 1))[\"d_dm\"]\n\n    # Random effects (partial pooling via weighted average)\n    # Simple RE: GLS with estimated variance components\n    # Use between and within estimators\n    y_bar_j &lt;- ave(y, group)\n    d_bar_j &lt;- ave(treat, group)\n    x_bar_j &lt;- ave(x, group)\n\n    # Within estimator = FE\n    # Between estimator\n    uj &lt;- tapply(y, group, mean)\n    dj &lt;- tapply(treat, group, mean)\n    xj &lt;- tapply(x, group, mean)\n    if (sd(dj) &gt; 0.01) {\n      be_est &lt;- coef(lm(uj ~ dj + xj))[\"dj\"]\n    } else {\n      be_est &lt;- naive\n    }\n\n    # RE is a weighted combo of within and between\n    sigma2_e &lt;- var(y_dm - fe_est * d_dm - coef(lm(y_dm ~ d_dm + x_dm - 1))[\"x_dm\"] * x_dm)\n    sigma2_a &lt;- max(var(uj - fe_est * dj) - sigma2_e / n_per, 0.01)\n    theta &lt;- 1 - sqrt(sigma2_e / (sigma2_e + n_per * sigma2_a))\n\n    y_re &lt;- y - theta * y_bar_j\n    d_re &lt;- treat - theta * d_bar_j\n    x_re &lt;- x - theta * x_bar_j\n    re_est &lt;- coef(lm(y_re ~ d_re + x_re))[\"d_re\"]\n\n    list(y = y, treat = treat, x = x, group = group, alpha_i = alpha_i,\n         naive = naive, fe_est = fe_est, re_est = re_est, be_est = be_est,\n         ate = ate, rho = rho, J = J, n_per = n_per,\n         alpha = alpha)\n  })\n\n  output$scatter_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Color by group (cycle through colors)\n    palette &lt;- rep(c(\"#3498db\", \"#e74c3c\", \"#27ae60\", \"#f39c12\",\n                     \"#9b59b6\", \"#1abc9c\", \"#e67e22\", \"#2c3e50\"),\n                   length.out = d$J)\n    cols &lt;- paste0(palette[d$group], \"50\")\n\n    plot(d$x, d$y, pch = 16, cex = 0.4, col = cols,\n         xlab = \"X (covariate)\", ylab = \"Y (outcome)\",\n         main = \"Data by Group (colors = groups)\")\n\n    # Show a few group means\n    show_groups &lt;- 1:min(5, d$J)\n    for (j in show_groups) {\n      idx &lt;- d$group == j\n      points(mean(d$x[idx]), mean(d$y[idx]),\n             pch = 17, cex = 1.5, col = palette[j])\n    }\n\n    if (d$rho &gt; 0) {\n      mtext(expression(alpha[j] * \" correlated with D — RE is biased\"),\n            side = 3, line = 0, cex = 0.8, col = \"#e74c3c\", font = 2)\n    } else {\n      mtext(expression(alpha[j] * \" independent of D — RE is valid\"),\n            side = 3, line = 0, cex = 0.8, col = \"#27ae60\", font = 2)\n    }\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 9, 3, 2))\n\n    ests &lt;- c(d$fe_est, d$re_est, d$naive)\n    labels &lt;- c(\"Fixed effects\", \"Random effects\", \"Pooled OLS\")\n    cols &lt;- c(\"#27ae60\", \"#3498db\", \"#e74c3c\")\n\n    xlim &lt;- range(c(ests, d$ate))\n    pad  &lt;- max(diff(xlim) * 0.4, 0.5)\n    xlim &lt;- xlim + c(-pad, pad)\n\n    plot(ests, 1:3, pch = 19, cex = 2, col = cols,\n         xlim = xlim, ylim = c(0.5, 3.5),\n         yaxt = \"n\", xlab = \"Estimated treatment effect\",\n         ylab = \"\", main = \"Estimator Comparison\")\n    axis(2, at = 1:3, labels = labels, las = 1, cex.axis = 0.9)\n\n    abline(v = d$ate, lty = 2, col = \"#2c3e50\", lwd = 2)\n    text(d$ate, 3.45, paste0(\"True ATE = \", d$ate),\n         cex = 0.85, font = 2, col = \"#2c3e50\")\n\n    segments(d$ate, 1:3, ests, 1:3, col = cols, lwd = 2, lty = 2)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    b_naive &lt;- d$naive - d$ate\n    b_fe    &lt;- d$fe_est - d$ate\n    b_re    &lt;- d$re_est - d$ate\n\n    fe_class &lt;- if (abs(b_fe) &lt; abs(b_naive) * 0.5) \"good\" else \"bad\"\n    re_class &lt;- if (abs(b_re) &lt; abs(b_naive) * 0.5) \"good\" else \"bad\"\n\n    hausman &lt;- abs(d$fe_est - d$re_est)\n    h_class &lt;- if (hausman &lt; 0.3) \"good\" else \"bad\"\n    h_label &lt;- if (hausman &lt; 0.3) \"FE \\u2248 RE (RE likely OK)\" else \"FE \\u2260 RE (use FE)\"\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Pooled OLS:&lt;/b&gt; &lt;span class='bad'&gt;\", round(d$naive, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_naive, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;Fixed effects:&lt;/b&gt; &lt;span class='\", fe_class, \"'&gt;\",\n        round(d$fe_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_fe, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;Random effects:&lt;/b&gt; &lt;span class='\", re_class, \"'&gt;\",\n        round(d$re_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_re, 3), \")&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;|FE \\u2212 RE|:&lt;/b&gt; &lt;span class='\", h_class, \"'&gt;\",\n        round(hausman, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;span class='\", h_class, \"'&gt;\", h_label, \"&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nCorrelation = 0: group effects are independent of treatment. All three estimators work, but RE and FE are close to the truth while pooled OLS may show some bias. The |FE − RE| gap is small — Hausman says RE is fine.\nCorrelation = 1.5: now groups with higher \\(\\alpha_j\\) are more likely to get treated. Pooled OLS is badly biased. FE still works (it eliminates \\(\\alpha_j\\)). RE is biased — it uses between-group variation that’s contaminated by the correlation. The |FE − RE| gap is large — Hausman says use FE.\nCorrelation = 3: extreme confounding. RE is nearly as biased as pooled OLS. FE remains unbiased. This is why FE is the default in applied economics.\nSmall groups (5 obs per group): FE is noisier because it uses less data. RE is more precise when valid — that’s the efficiency gain. But precision doesn’t help if the estimate is biased.",
    "crumbs": [
      "Panel Data",
      "Fixed vs Random Effects"
    ]
  },
  {
    "objectID": "panel-fe-re.html#the-hausman-test",
    "href": "panel-fe-re.html#the-hausman-test",
    "title": "Fixed vs Random Effects",
    "section": "The Hausman test",
    "text": "The Hausman test\nYou’ve seen it informally in the simulation: when |FE − RE| is large, RE is suspect. The Hausman test formalizes this.\n\nThe logic\nUnder the RE assumption (\\(\\alpha_j\\) uncorrelated with \\(X\\) and \\(D\\)):\n\nFE is consistent (unbiased) but inefficient (throws away between-group info)\nRE is consistent and efficient (uses all the data)\nSo FE and RE should give approximately the same answer\n\nIf the RE assumption fails:\n\nFE is still consistent (within-group variation is clean)\nRE is inconsistent (between-group variation is contaminated)\nFE and RE will diverge\n\nThe Hausman test exploits this:\n\\[H = (\\hat{\\tau}_{FE} - \\hat{\\tau}_{RE})' \\, [\\text{Var}(\\hat{\\tau}_{FE}) - \\text{Var}(\\hat{\\tau}_{RE})]^{-1} \\, (\\hat{\\tau}_{FE} - \\hat{\\tau}_{RE})\\]\nUnder \\(H_0\\) (RE is valid), \\(H \\sim \\chi^2_k\\) where \\(k\\) is the number of coefficients being compared.\n\nLarge \\(H\\) (small p-value): reject \\(H_0\\). FE and RE disagree → the RE assumption fails → use FE.\nSmall \\(H\\) (large p-value): fail to reject. FE and RE agree → RE assumption is plausible → RE is OK (and more efficient).\n\n\n\n\n\n\n\nExample: returns to education. You have panel data on workers observed over multiple years. You want the causal effect of an extra year of education on wages.\n\nRE assumes: workers who get more education don’t have systematically higher unobserved ability. Ability is random across education levels.\nFE says: doesn’t matter — I’ll control for each worker’s fixed ability by only using within-person wage changes when they get more education.\n\nYou run both. FE gives \\(\\hat{\\tau} = 0.06\\) (6% wage increase per year of education). RE gives \\(\\hat{\\tau} = 0.10\\) (10%). The gap is large.\nHausman test: \\(H = 15.3\\), \\(p &lt; 0.001\\). Reject \\(H_0\\). The RE estimate is inflated because high-ability workers get more education and earn more — classic selection bias at the group level. Use FE.\nThe FE estimate (6%) is the return to education holding ability constant. The RE estimate (10%) is contaminated by ability differences.\n\n\n\n\n\nHausman test in practice\n\n\n\n\n\n\n\n\nResult\nInterpretation\nAction\n\n\n\n\n\\(p &lt; 0.05\\)\nFE and RE significantly differ\nUse FE (RE assumption violated)\n\n\n\\(p &gt; 0.05\\)\nNo significant difference\nRE is OK — more efficient\n\n\n\nCaveats:\n\nThe Hausman test has low power with small samples — it may fail to reject even when RE is biased.\nA non-rejection doesn’t prove RE is valid. It means you can’t detect the violation with your data.\nIn practice, most applied economists default to FE for causal questions and use the Hausman test as a check, not as the primary decision tool.",
    "crumbs": [
      "Panel Data",
      "Fixed vs Random Effects"
    ]
  },
  {
    "objectID": "panel-fe-re.html#connection-to-other-methods",
    "href": "panel-fe-re.html#connection-to-other-methods",
    "title": "Fixed vs Random Effects",
    "section": "Connection to other methods",
    "text": "Connection to other methods\n\n\n\nMethod\nWhat it controls for\nAssumption\n\n\n\n\nFixed effects\nAll time-invariant group confounders\nNo time-varying confounders\n\n\nRandom effects\nGroup-level variation (partial pooling)\nGroup effects uncorrelated with X\n\n\nDID\nGroup FE + time FE\nParallel trends\n\n\nHierarchical models\nBayesian version of RE\nSame as RE, with priors\n\n\nClustered SEs\nNot a model — fixes standard errors\nDoesn’t change point estimates\n\n\n\nFE is an identification strategy. It tells you why your comparison is valid (within-group variation eliminates group-level confounders). RE is a modeling assumption that may or may not hold. When in doubt, FE is the conservative choice for causal inference.",
    "crumbs": [
      "Panel Data",
      "Fixed vs Random Effects"
    ]
  },
  {
    "objectID": "panel-fe-re.html#in-stata",
    "href": "panel-fe-re.html#in-stata",
    "title": "Fixed vs Random Effects",
    "section": "In Stata",
    "text": "In Stata\n* Declare panel structure\nxtset unit_id year\n\n* Fixed effects\nxtreg outcome treatment x1, fe cluster(unit_id)\n\n* Random effects\nxtreg outcome treatment x1, re\n\n* Hausman test (FE vs RE)\nquietly xtreg outcome treatment x1, fe\nestimates store fe_model\nquietly xtreg outcome treatment x1, re\nestimates store re_model\nhausman fe_model re_model\n\n* First-difference (alternative to FE)\nreg D.outcome D.treatment D.x1, cluster(unit_id)\nIf the Hausman test rejects, use FE — the RE assumption that group effects are uncorrelated with regressors doesn’t hold. For causal inference, FE is almost always the safer choice.",
    "crumbs": [
      "Panel Data",
      "Fixed vs Random Effects"
    ]
  },
  {
    "objectID": "panel-fe-re.html#did-you-know",
    "href": "panel-fe-re.html#did-you-know",
    "title": "Fixed vs Random Effects",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe Hausman test was introduced by Jerry Hausman (1978) in one of the most cited papers in econometrics. The general principle extends beyond FE/RE: any time you have a consistent-but-inefficient estimator and an efficient-but-possibly-inconsistent estimator, you can compare them. If they agree, the efficient one is probably fine.\nMundlak (1978) showed a clever alternative: add the group means \\(\\bar{X}_j\\) as regressors in the RE model. If the coefficient on \\(\\bar{X}_j\\) is significant, the RE assumption is violated (same information as the Hausman test, but easier to implement and interpret). This is called the correlated random effects approach.\nIn the machine learning world, random effects are closely related to mixed-effects models (e.g., lme4 in R). The terminology differs across fields: what economists call “fixed effects” (unit dummies), statisticians sometimes call “fixed effects” too, but the random effects tradition is much more developed in biostatistics and multilevel modeling (Raudenbush & Bryk, 2002).",
    "crumbs": [
      "Panel Data",
      "Fixed vs Random Effects"
    ]
  },
  {
    "objectID": "identification-estimation.html",
    "href": "identification-estimation.html",
    "title": "Identification vs Estimation",
    "section": "",
    "text": "Every causal inference project answers two fundamentally different questions:\n\nIdentification. Why is this comparison causal? What assumptions make the estimand equal to the causal parameter?\nEstimation. How do we compute the estimand from data? What statistical procedure do we use?\n\nThese are conceptually independent:\n\nYou can have correct identification with a poor estimator (unbiased but noisy).\nYou can have a highly efficient estimator with no identification (precise but biased toward the wrong parameter).\n\nIdentification comes first. If the assumptions fail, no estimator can recover the causal effect.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#two-separate-questions-in-every-causal-study",
    "href": "identification-estimation.html#two-separate-questions-in-every-causal-study",
    "title": "Identification vs Estimation",
    "section": "",
    "text": "Every causal inference project answers two fundamentally different questions:\n\nIdentification. Why is this comparison causal? What assumptions make the estimand equal to the causal parameter?\nEstimation. How do we compute the estimand from data? What statistical procedure do we use?\n\nThese are conceptually independent:\n\nYou can have correct identification with a poor estimator (unbiased but noisy).\nYou can have a highly efficient estimator with no identification (precise but biased toward the wrong parameter).\n\nIdentification comes first. If the assumptions fail, no estimator can recover the causal effect.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#identification-the-assumption",
    "href": "identification-estimation.html#identification-the-assumption",
    "title": "Identification vs Estimation",
    "section": "Identification: the assumption",
    "text": "Identification: the assumption\nIdentification is about the source of exogenous variation — why the variation in treatment you’re using is “as good as random” for estimating a causal effect.\n\n\n\n\n\n\n\n\nIdentification strategy\nThe assumption\nIn words\n\n\n\n\nSelection on observables\n\\(Y(0), Y(1) \\perp D \\mid X\\)\nConditional on X, treatment is as good as random\n\n\nParallel trends\n\\(E[Y(0)_t - Y(0)_{t-1} \\mid D=1] = E[Y(0)_t - Y(0)_{t-1} \\mid D=0]\\)\nAbsent treatment, both groups would have trended the same\n\n\nExclusion restriction\n\\(Z\\) affects \\(Y\\) only through \\(X\\)\nThe instrument has no direct effect on the outcome\n\n\nContinuity\n\\(E[Y(0) \\mid X=x]\\) is continuous at the cutoff\nNo other jump happens at the cutoff\n\n\n\nEach assumption is a claim about the world — not something you compute. You argue it using institutional knowledge, theory, and indirect evidence. Some are partially testable (you can check pre-trends for DID, run a McCrary test for RDD), but none can be fully proven from data.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#estimation-the-computation",
    "href": "identification-estimation.html#estimation-the-computation",
    "title": "Identification vs Estimation",
    "section": "Estimation: the computation",
    "text": "Estimation: the computation\nEstimation is about how you turn data into a number, given that you believe your identification assumption holds.\n\n\n\n\n\n\n\nEstimator\nWhat it does\n\n\n\n\nOLS regression\nFits a linear model, uses coefficients\n\n\nMatching\nPairs treated/control units with similar covariates\n\n\nIPW\nReweights observations by inverse propensity scores\n\n\nEntropy balancing\nFinds weights that exactly balance covariate moments\n\n\nDoubly robust\nCombines regression and weighting\n\n\n2SLS\nTwo-stage regression using predicted values from the first stage\n\n\nLocal polynomial\nFits flexible curves on each side of a cutoff\n\n\nSynthetic control weights\nConstrained optimization to match pre-treatment trends\n\n\nTWFE\nTwo-way fixed effects regression\n\n\n\nThese are tools — they can often be combined with different identification strategies. IPW can implement selection on observables or be used in a DID design (IPW-DID). Regression can adjust for covariates in an RDD or in a cross-sectional study. The estimator doesn’t determine identification; the assumption does.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#research-designs-bundle-both",
    "href": "identification-estimation.html#research-designs-bundle-both",
    "title": "Identification vs Estimation",
    "section": "Research designs bundle both",
    "text": "Research designs bundle both\nWhat we usually call “methods” in applied work — DID, IV, RDD — are really research designs that bundle an identification strategy with a default estimator:\n\n\n\n\n\n\n\n\nResearch design\nIdentification\nCommon estimators\n\n\n\n\nSOO study\nConditional independence\nRegression, matching, IPW, EB, doubly robust\n\n\nDID\nParallel trends\n2×2 difference, TWFE, IPW-DID (Abadie 2005), DR-DID (Sant’Anna & Zhao 2020)\n\n\nIV\nExclusion restriction + relevance\n2SLS, LIML, GMM\n\n\nRDD\nContinuity at cutoff\nLocal polynomial, local randomization\n\n\nSynthetic control\nPre-treatment fit → valid counterfactual\nConstrained weight optimization, augmented SCM\n\n\n\nThis is why the same estimation tool shows up in multiple designs. IPW appears in the SOO column and the DID column — because it’s a tool, not a strategy.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#the-math-where-bias-comes-from",
    "href": "identification-estimation.html#the-math-where-bias-comes-from",
    "title": "Identification vs Estimation",
    "section": "The math: where bias comes from",
    "text": "The math: where bias comes from\nWhen an identification assumption fails, it introduces a bias term that no estimator can remove. Here’s the decomposition for three methods.\n\nSelection on observables\nWe want the Average Treatment Effect on the Treated (ATT):\n\\[\\tau = E[Y(1) - Y(0) \\mid D = 1]\\]\nWe observe \\(E[Y \\mid D=1] = E[Y(1) \\mid D=1]\\) and \\(E[Y \\mid D=0] = E[Y(0) \\mid D=0]\\). The naive comparison is:\n\\[E[Y \\mid D=1] - E[Y \\mid D=0] = \\underbrace{E[Y(1) - Y(0) \\mid D=1]}_{\\text{ATT}} + \\underbrace{E[Y(0) \\mid D=1] - E[Y(0) \\mid D=0]}_{\\text{selection bias}}\\]\nThe second term is selection bias — the treated group would have had different outcomes even without treatment. The CIA says: conditional on \\(X\\), \\(E[Y(0) \\mid D=1, X] = E[Y(0) \\mid D=0, X]\\), so the selection bias is zero within each stratum of \\(X\\).\nIf the CIA fails — there’s an unobserved confounder \\(U\\) — then \\(E[Y(0) \\mid D=1, X] \\neq E[Y(0) \\mid D=0, X]\\) because \\(D\\) is still correlated with \\(Y(0)\\) through \\(U\\) even after conditioning on \\(X\\). The bias term is nonzero. Regression, IPW, matching — all give biased answers because the selection bias is baked into the estimand, not the estimator.\n\n\nDifference-in-differences\nThe DID estimand is:\n\\[\\hat{\\tau}_{DID} = \\big(E[Y_{1t}] - E[Y_{1,t-1}]\\big) - \\big(E[Y_{0t}] - E[Y_{0,t-1}]\\big)\\]\nwhere group 1 is treated, group 0 is control, \\(t\\) is post, \\(t-1\\) is pre. Substitute potential outcomes and add and subtract \\(E[Y_{1t}(0)]\\):\n\\[\\hat{\\tau}_{DID} = \\underbrace{E[Y_{1t}(1) - Y_{1t}(0)]}_{\\text{ATT}} + \\underbrace{\\big(E[Y_{1t}(0)] - E[Y_{1,t-1}(0)]\\big) - \\big(E[Y_{0t}(0)] - E[Y_{0,t-1}(0)]\\big)}_{\\text{differential trend bias}}\\]\nThe parallel trends assumption says the second term equals zero — the treated group’s untreated trajectory matches the control group’s trajectory. Then \\(\\hat{\\tau}_{DID} = \\text{ATT}\\).\nIf parallel trends fail — say the treated group was already trending upward faster — the differential trend term is positive. DID overestimates the effect. This bias doesn’t shrink with more data. It doesn’t go away if you switch from a 2×2 difference to TWFE or IPW-DID. It’s an identification failure, not an estimation failure.\n\n\nInstrumental variables\nWe have \\(Y = \\beta X + \\varepsilon\\) where \\(\\text{Cov}(X, \\varepsilon) \\neq 0\\) (endogeneity). The IV estimand is:\n\\[\\hat{\\beta}_{IV} = \\frac{\\text{Cov}(Z, Y)}{\\text{Cov}(Z, X)}\\]\nSubstitute \\(Y = \\beta X + \\varepsilon\\):\n\\[\\hat{\\beta}_{IV} = \\frac{\\text{Cov}(Z, \\beta X + \\varepsilon)}{\\text{Cov}(Z, X)} = \\beta + \\frac{\\text{Cov}(Z, \\varepsilon)}{\\text{Cov}(Z, X)}\\]\nThe exclusion restriction says \\(\\text{Cov}(Z, \\varepsilon) = 0\\) — the instrument is uncorrelated with the error. Then \\(\\hat{\\beta}_{IV} = \\beta\\).\nIf the exclusion restriction fails — \\(Z\\) directly affects \\(Y\\) through some channel other than \\(X\\) — then \\(\\text{Cov}(Z, \\varepsilon) \\neq 0\\) and the bias term \\(\\frac{\\text{Cov}(Z, \\varepsilon)}{\\text{Cov}(Z, X)}\\) is nonzero. No amount of data, no alternative estimator (LIML, GMM, jackknife) removes this. It’s baked in.\n\n\nThreats to identification\nEach method has specific threats — things that make the bias term nonzero:\n\n\n\n\n\n\n\n\n\nMethod\nIdentification assumption\nThreat (what breaks it)\nWhat the bias looks like\n\n\n\n\nSOO\nNo unobserved confounders\nOmitted variable that drives both \\(D\\) and \\(Y\\)\nSelection bias: treated group was different to begin with\n\n\nDID\nParallel trends\nTreated group was already on a different trajectory\nYou attribute the pre-existing trend to the treatment\n\n\nIV\nExclusion restriction\nInstrument affects \\(Y\\) through a channel other than \\(X\\)\nIV picks up the direct effect, not just the causal path\n\n\nRDD\nContinuity at cutoff\nUnits manipulate their score to sort across the cutoff; or another policy also kicks in at the same cutoff\nThe “jump” reflects sorting or a different treatment, not your treatment\n\n\nSynthetic control\nPre-treatment fit generalizes\nSpillovers from treated unit to donors; structural break changes the relationship\nCounterfactual is wrong, gap doesn’t reflect the treatment\n\n\n\nNotice: every threat is about the world, not about the math. You can’t test your way out of these — you argue them with institutional knowledge.\n\n\nThe pattern\nIn all three cases:\n\\[\\text{Estimate} = \\text{Causal effect} + \\text{Identification bias} + \\text{Estimation bias}\\]\nIdentification bias comes from violated assumptions — it’s a function of how the world works. Estimation bias comes from the estimator — it’s a function of how you computed the number. Identification bias dominates and can’t be fixed. Estimation bias is usually smaller and can be fixed by choosing a better estimator. That’s why identification comes first.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#types-of-bias",
    "href": "identification-estimation.html#types-of-bias",
    "title": "Identification vs Estimation",
    "section": "Types of bias",
    "text": "Types of bias\n\nIdentification biases\nThese come from the world, not the estimator. More data doesn’t help. A fancier estimator doesn’t help. You need a different identification strategy or better data.\nOmitted variable bias (OVB). The most common. An unobserved variable \\(U\\) affects both treatment and outcome. For the simple regression \\(Y = \\beta X + \\gamma U + \\varepsilon\\) where you omit \\(U\\):\n\\[\\text{OVB} = \\gamma \\cdot \\frac{\\text{Cov}(X, U)}{\\text{Var}(X)}\\]\nThe bias is the effect of \\(U\\) on \\(Y\\) (\\(\\gamma\\)) times how much \\(U\\) correlates with \\(X\\). If \\(U\\) drives people toward treatment and improves outcomes, both terms are positive and you overestimate the effect.\nSelection bias. The treated group would have had different outcomes even without treatment: \\(E[Y(0) \\mid D=1] \\neq E[Y(0) \\mid D=0]\\). This is OVB rephrased in potential outcomes language — the “omitted variable” is whatever drives people to select into treatment.\nSimultaneity bias. \\(X\\) causes \\(Y\\) but \\(Y\\) also causes \\(X\\). Regressing \\(Y\\) on \\(X\\) picks up both directions. Common in macro (do interest rates affect GDP, or does GDP affect interest rates?) and in supply/demand estimation.\nCollider bias (sample selection bias). You condition on a variable caused by both treatment and outcome — this opens a fake path between them. The correlation-causation page covers this in detail.\nDifferential trends bias. In DID: the treated group was already on a different trajectory before treatment. The estimate captures the pre-existing divergence, not the treatment effect.\n\n\nEstimation biases\nThese come from the estimator, not the world. They can be reduced or eliminated by choosing a better estimator, using more data, or fixing the specification.\nFunctional form misspecification. You fit a linear model but the truth is nonlinear. In RDD: a straight line through curved data creates a fake “jump” at the cutoff. Fix: use local polynomials, check robustness to specification.\nFinite sample / weak instrument bias. With weak instruments (\\(F &lt; 10\\)), 2SLS is biased toward OLS in finite samples — even if the exclusion restriction holds. This shrinks with stronger instruments or alternative estimators (LIML, Anderson-Rubin).\nNegative weighting (TWFE with staggered treatment). Goodman-Bacon (2021) and de Chaisemartin & d’Haultfoeuille (2020) showed that two-way fixed effects can produce negative weights on some treatment effects when treatment is staggered across time — giving biased estimates even when parallel trends holds. Fix: use Callaway & Sant’Anna, Sun & Abraham, or other heterogeneity-robust estimators.\nExtreme weights. In IPW: when propensity scores are near 0 or 1, some observations get enormous weights, making the estimate noisy and potentially biased in finite samples. Fix: trim extreme scores, use entropy balancing, or use doubly robust estimators.\nAttenuation bias (measurement error). When \\(X\\) is measured with noise, OLS is biased toward zero. The noisier the measurement relative to the true signal, the more the estimate shrinks. Can be fixed with better measurement or IV. See the measurement error page for the signal-to-noise ratio formula.\n\n\nSummary\n\n\n\n\n\n\n\n\n\nBias\nType\nGoes away with more data?\nFix\n\n\n\n\nOmitted variable / confounding\nIdentification\nNo\nBetter controls, different strategy (IV, DID, RDD)\n\n\nSelection bias\nIdentification\nNo\nRandomization, or argue CIA\n\n\nSimultaneity\nIdentification\nNo\nIV, timing restrictions\n\n\nCollider / sample selection\nIdentification\nNo\nDon’t condition on colliders\n\n\nDifferential trends\nIdentification\nNo\nDifferent comparison group, different strategy\n\n\nFunctional form\nEstimation\nPartially\nFlexible specifications, local methods\n\n\nWeak instruments\nEstimation\nPartially\nStronger instruments, LIML\n\n\nTWFE negative weighting\nEstimation\nNo\nHeterogeneity-robust DID estimators\n\n\nExtreme IPW weights\nEstimation\nYes (slowly)\nTrimming, EB, doubly robust\n\n\nAttenuation (measurement error)\nBoth\nNo\nBetter data, IV",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#simulation-identification-matters-estimation-is-secondary",
    "href": "identification-estimation.html#simulation-identification-matters-estimation-is-secondary",
    "title": "Identification vs Estimation",
    "section": "Simulation: identification matters, estimation is secondary",
    "text": "Simulation: identification matters, estimation is secondary\nSame data, same identification assumption, three different estimators. When the assumption holds, they all work. When it doesn’t, they all fail.\n#| standalone: true\n#| viewerHeight: 580\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 13px; line-height: 1.8;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_ie\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate_ie\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"obs_ie\", \"Observed confounding (X):\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      sliderInput(\"unobs_ie\", \"Unobserved confounding (U):\",\n                  min = 0, max = 3, value = 0, step = 0.25),\n\n      actionButton(\"go_ie\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_ie\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"ie_plot\", height = \"420px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go_ie\n    n   &lt;- input$n_ie\n    ate &lt;- input$ate_ie\n    gx  &lt;- input$obs_ie\n    gu  &lt;- input$unobs_ie\n\n    x &lt;- rnorm(n)\n    u &lt;- rnorm(n)\n\n    p &lt;- pnorm(gx * x + gu * u)\n    treat &lt;- rbinom(n, 1, p)\n\n    y &lt;- 1 + 2 * x + 1.5 * u + ate * treat + rnorm(n)\n\n    # Estimator 1: OLS regression controlling for X\n    est_reg &lt;- coef(lm(y ~ treat + x))[2]\n\n    # Estimator 2: IPW\n    ps &lt;- fitted(glm(treat ~ x, family = binomial))\n    ps &lt;- pmin(pmax(ps, 0.01), 0.99)\n    w &lt;- ifelse(treat == 1, 1 / ps, 1 / (1 - ps))\n    est_ipw &lt;- weighted.mean(y[treat == 1], w[treat == 1]) -\n               weighted.mean(y[treat == 0], w[treat == 0])\n\n    # Estimator 3: Matching (simple: nearest neighbor on X)\n    matched_y &lt;- numeric(sum(treat == 1))\n    x_t &lt;- x[treat == 1]\n    y_t &lt;- y[treat == 1]\n    x_c &lt;- x[treat == 0]\n    y_c &lt;- y[treat == 0]\n    for (i in seq_along(x_t)) {\n      nearest &lt;- which.min(abs(x_c - x_t[i]))\n      matched_y[i] &lt;- y_c[nearest]\n    }\n    est_match &lt;- mean(y_t) - mean(matched_y)\n\n    list(est_reg = est_reg, est_ipw = est_ipw, est_match = est_match,\n         ate = ate, gu = gu)\n  })\n\n  output$ie_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(5, 4.5, 3, 1))\n\n    estimates &lt;- c(d$est_reg, d$est_ipw, d$est_match)\n    biases &lt;- estimates - d$ate\n    labels &lt;- c(\"OLS\\nregression\", \"IPW\", \"Nearest-neighbor\\nmatching\")\n\n    cia_holds &lt;- d$gu == 0\n    cols &lt;- ifelse(abs(biases) &lt; 0.5, \"#27ae60\", \"#e74c3c\")\n\n    bp &lt;- barplot(estimates, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.85,\n                  main = ifelse(cia_holds,\n                    \"CIA holds: all estimators work\",\n                    \"CIA violated: all estimators fail\"),\n                  ylab = \"Estimate\",\n                  ylim = c(0, max(estimates, d$ate) * 1.5))\n\n    abline(h = d$ate, lty = 2, col = \"gray40\", lwd = 2)\n    text(0.2, d$ate + 0.15, paste0(\"True ATE = \", d$ate),\n         col = \"gray40\", cex = 0.85, adj = 0)\n\n    text(bp, estimates + 0.2,\n         paste0(round(estimates, 2)),\n         cex = 0.9, font = 2)\n  })\n\n  output$results_ie &lt;- renderUI({\n    d &lt;- dat()\n    cia_holds &lt;- d$gu == 0\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Regression:&lt;/b&gt; \", round(d$est_reg, 2),\n        \" (bias: \", round(d$est_reg - d$ate, 2), \")&lt;br&gt;\",\n        \"&lt;b&gt;IPW:&lt;/b&gt; \", round(d$est_ipw, 2),\n        \" (bias: \", round(d$est_ipw - d$ate, 2), \")&lt;br&gt;\",\n        \"&lt;b&gt;Matching:&lt;/b&gt; \", round(d$est_match, 2),\n        \" (bias: \", round(d$est_match - d$ate, 2), \")&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        if (cia_holds)\n          \"&lt;span class='good'&gt;CIA holds.&lt;/span&gt; All three estimators give similar, roughly unbiased answers. The choice of estimator is secondary.\"\n        else\n          \"&lt;span class='bad'&gt;CIA violated.&lt;/span&gt; All three estimators are biased. Switching estimators doesn't help — you need a different identification strategy.\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nUnobserved confounding = 0: the CIA holds. All three estimators — regression, IPW, matching — give roughly the same answer, close to the true ATE. The choice between them is about efficiency, not bias.\nUnobserved confounding = 2: the CIA is violated. All three estimators are biased in the same direction. Switching from regression to IPW to matching doesn’t help — the problem is identification, not estimation.\nIncrease sample size with unobserved confounding: all three get more precise but stay biased. More data doesn’t fix a broken assumption.\n\nThe lesson: spend your energy on identification, not on the fanciest estimator.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#in-stata-identification-estimation-cheat-sheet",
    "href": "identification-estimation.html#in-stata-identification-estimation-cheat-sheet",
    "title": "Identification vs Estimation",
    "section": "In Stata: identification → estimation cheat sheet",
    "text": "In Stata: identification → estimation cheat sheet\n\n\n\n\n\n\n\nIdentification strategy\nStata command\n\n\n\n\nRandom assignment\nreg outcome treatment\n\n\nSelection on observables\nteffects ra (outcome x1 x2) (treatment)\n\n\nInverse probability weighting\nteffects ipw (outcome) (treatment x1 x2)\n\n\nMatching\nteffects nnmatch (outcome x1 x2) (treatment)\n\n\nDoubly robust\nteffects aipw (outcome x1 x2) (treatment x1 x2)\n\n\nDifference-in-differences\nreg outcome treated##post, cluster(group)\n\n\nInstrumental variables\nivregress 2sls outcome (treatment = instrument)\n\n\nRegression discontinuity\nrdrobust outcome running_var, c(0)\n\n\nFixed effects\nxtreg outcome treatment x1, fe cluster(id)\n\n\n\nThe right column is the easy part. The hard part is arguing that the left column holds.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#did-you-know",
    "href": "identification-estimation.html#did-you-know",
    "title": "Identification vs Estimation",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe distinction between identification and estimation was articulated clearly by Charles Manski in his 1995 book Identification Problems in the Social Sciences. He argued that most debates in empirical work are really about identification, not estimation.\nAngrist & Pischke (Mostly Harmless Econometrics, 2009) organized their entire textbook around identification strategies — regression, IV, DID, RDD — rather than estimators. This framing reshaped how a generation of economists thinks about empirical work.\nA common mistake in applied papers: spending pages discussing the estimator (clustered SEs, bootstrap, semiparametric methods) while spending one paragraph on identification. The estimator is the easy part. The hard part is arguing that your comparison is causal.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "doubly-robust.html",
    "href": "doubly-robust.html",
    "title": "Doubly Robust Estimation",
    "section": "",
    "text": "Every estimation tool under selection on observables relies on getting a model right:\n\nRegression adjustment needs the outcome model \\(E[Y \\mid X, D]\\) to be correct.\nIPW needs the propensity score model \\(P(D = 1 \\mid X)\\) to be correct.\n\nWhat if you’re not sure which one you got right? Doubly robust (DR) estimation combines both models and gives you a consistent estimate if either one is correctly specified.\n\n\n\n\n\n\nExample: vaccine effectiveness. You want to know if a vaccine reduces hospitalization. You model who gets vaccinated (propensity score: older people and healthcare workers vaccinate more) and you model hospitalization risk (outcome model: depends on age, comorbidities, obesity). DR combines both. If your outcome model misses a nonlinear age effect but your propensity score is right — you’re fine. If your propensity score ignores rural vs urban but your outcome model captures it — also fine. You only fail if both models are wrong simultaneously.\n\n\n\n\n\nThe most common DR estimator is Augmented Inverse Probability Weighting (AIPW):\n\\[\\hat{\\tau}_{DR} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\hat{\\mu}_1(X_i) - \\hat{\\mu}_0(X_i) + \\frac{D_i (Y_i - \\hat{\\mu}_1(X_i))}{\\hat{e}(X_i)} - \\frac{(1 - D_i)(Y_i - \\hat{\\mu}_0(X_i))}{1 - \\hat{e}(X_i)} \\right]\\]\nwhere \\(\\hat{\\mu}_d(X)\\) is the estimated outcome under treatment \\(d\\), and \\(\\hat{e}(X)\\) is the estimated propensity score.\nIntuition: start with the regression prediction (\\(\\hat{\\mu}_1 - \\hat{\\mu}_0\\)), then correct it using the IPW-weighted residuals. If the outcome model is perfect, the residuals are zero and the correction vanishes. If the outcome model is wrong but the propensity score is right, the correction fixes the bias.\n\n\n\n\n\n\nOutcome model\nPropensity score\nDR consistent?\n\n\n\n\nCorrect\nCorrect\nYes\n\n\nCorrect\nWrong\nYes\n\n\nWrong\nCorrect\nYes\n\n\nWrong\nWrong\nNo\n\n\n\nThree out of four — you get two shots at a consistent estimate. This is the key advantage over methods that rely on a single model.\n\n\n\nTo see DR in action, we need a setting where models can be wrong:\n\nTrue outcome: \\(Y = 1 + 2X^2 + \\tau D + \\varepsilon\\) (quadratic in \\(X\\))\nTrue treatment: \\(P(D=1 \\mid X) = \\Phi(1.5 \\cdot X)\\) (probit)\n“Correct” outcome model: includes \\(X^2\\)\n“Wrong” outcome model: linear only (\\(X\\), no \\(X^2\\))\n“Correct” PS model: logit on \\(X\\) (close enough to probit)\n“Wrong” PS model: constant 0.5 (ignores \\(X\\) entirely)\n\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 500, max = 3000, value = 1000, step = 250),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      selectInput(\"scenario\", \"Scenario:\",\n                  choices = c(\n                    \"Both correct\"         = \"both\",\n                    \"Only outcome correct\"  = \"outcome_only\",\n                    \"Only PS correct\"       = \"ps_only\",\n                    \"Neither correct\"       = \"neither\"\n                  )),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"fit_plot\", height = \"430px\")),\n        column(6, plotOutput(\"compare_plot\", height = \"430px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    ate &lt;- input$ate\n    sc  &lt;- input$scenario\n\n    # Data generating process\n    x &lt;- rnorm(n)\n\n    # True PS: probit\n    p_true &lt;- pnorm(1.5 * x)\n    treat &lt;- rbinom(n, 1, p_true)\n\n    # True outcome: quadratic\n    y &lt;- 1 + 2 * x^2 + ate * treat + rnorm(n)\n\n    # --- Outcome models ---\n    outcome_correct &lt;- sc %in% c(\"both\", \"outcome_only\")\n    if (outcome_correct) {\n      x2 &lt;- x^2\n      fit1 &lt;- lm(y ~ treat + x + x2)\n      mu1 &lt;- predict(fit1, newdata = data.frame(treat = 1, x = x, x2 = x^2))\n      mu0 &lt;- predict(fit1, newdata = data.frame(treat = 0, x = x, x2 = x^2))\n    } else {\n      fit1 &lt;- lm(y ~ treat + x)\n      mu1 &lt;- predict(fit1, newdata = data.frame(treat = 1, x = x))\n      mu0 &lt;- predict(fit1, newdata = data.frame(treat = 0, x = x))\n    }\n\n    # Regression adjustment estimate\n    reg_est &lt;- mean(mu1 - mu0)\n\n    # --- PS models ---\n    ps_correct &lt;- sc %in% c(\"both\", \"ps_only\")\n    if (ps_correct) {\n      ps &lt;- fitted(glm(treat ~ x, family = binomial))\n    } else {\n      ps &lt;- rep(0.5, n)\n    }\n\n    # IPW estimate\n    w &lt;- ifelse(treat == 1, 1 / ps, 1 / (1 - ps))\n    ipw_est &lt;- weighted.mean(y[treat == 1], w[treat == 1]) -\n               weighted.mean(y[treat == 0], w[treat == 0])\n\n    # Doubly robust (AIPW)\n    dr_est &lt;- mean(mu1 - mu0 +\n      treat * (y - mu1) / ps -\n      (1 - treat) * (y - mu0) / (1 - ps))\n\n    # Naive\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # For plotting: outcome model fit curves\n    x_seq &lt;- seq(min(x), max(x), length.out = 200)\n    if (outcome_correct) {\n      pred_t &lt;- predict(fit1, newdata = data.frame(treat = 1, x = x_seq, x2 = x_seq^2))\n      pred_c &lt;- predict(fit1, newdata = data.frame(treat = 0, x = x_seq, x2 = x_seq^2))\n    } else {\n      pred_t &lt;- predict(fit1, newdata = data.frame(treat = 1, x = x_seq))\n      pred_c &lt;- predict(fit1, newdata = data.frame(treat = 0, x = x_seq))\n    }\n    true_t &lt;- 1 + 2 * x_seq^2 + ate\n    true_c &lt;- 1 + 2 * x_seq^2\n\n    list(x = x, y = y, treat = treat,\n         x_seq = x_seq, pred_t = pred_t, pred_c = pred_c,\n         true_t = true_t, true_c = true_c,\n         naive = naive, reg_est = reg_est, ipw_est = ipw_est, dr_est = dr_est,\n         ate = ate, sc = sc,\n         outcome_correct = outcome_correct, ps_correct = ps_correct)\n  })\n\n  output$fit_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$y, pch = 16, cex = 0.3,\n         col = ifelse(d$treat == 1, \"#3498db30\", \"#e74c3c30\"),\n         xlab = \"X (confounder)\", ylab = \"Y (outcome)\",\n         main = \"Outcome Model Fit vs Truth\")\n\n    # Fitted curves (solid)\n    lines(d$x_seq, d$pred_t, col = \"#3498db\", lwd = 2.5)\n    lines(d$x_seq, d$pred_c, col = \"#e74c3c\", lwd = 2.5)\n\n    # True curves (dashed)\n    lines(d$x_seq, d$true_t, col = \"#3498db\", lwd = 1.5, lty = 3)\n    lines(d$x_seq, d$true_c, col = \"#e74c3c\", lwd = 1.5, lty = 3)\n\n    outcome_label &lt;- if (d$outcome_correct) \"Correct (quadratic)\" else \"Wrong (linear)\"\n    ps_label &lt;- if (d$ps_correct) \"Correct (logit)\" else \"Wrong (constant)\"\n\n    legend(\"topleft\", bty = \"n\", cex = 0.75,\n           legend = c(\"Treated data\", \"Control data\",\n                      \"Model fit (solid)\", \"True E[Y|X,D] (dotted)\",\n                      paste0(\"Outcome: \", outcome_label),\n                      paste0(\"PS: \", ps_label)),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray40\", \"gray40\", NA, NA),\n           pch = c(16, 16, NA, NA, NA, NA),\n           lwd = c(NA, NA, 2.5, 1.5, NA, NA),\n           lty = c(NA, NA, 1, 3, NA, NA))\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 9, 3, 2))\n\n    ests &lt;- c(d$dr_est, d$ipw_est, d$reg_est, d$naive)\n    labels &lt;- c(\"Doubly Robust\", \"IPW\", \"Regression adj.\", \"Naive\")\n    cols &lt;- c(\"#9b59b6\", \"#27ae60\", \"#3498db\", \"#e74c3c\")\n\n    xlim &lt;- range(c(ests, d$ate))\n    pad  &lt;- max(diff(xlim) * 0.4, 0.5)\n    xlim &lt;- xlim + c(-pad, pad)\n\n    plot(ests, 1:4, pch = 19, cex = 2, col = cols,\n         xlim = xlim, ylim = c(0.5, 4.5),\n         yaxt = \"n\", xlab = \"Estimated treatment effect\",\n         ylab = \"\", main = \"Estimator Comparison\")\n    axis(2, at = 1:4, labels = labels, las = 1, cex.axis = 0.9)\n\n    abline(v = d$ate, lty = 2, col = \"#2c3e50\", lwd = 2)\n    text(d$ate, 4.45, paste0(\"True ATE = \", d$ate),\n         cex = 0.85, font = 2, col = \"#2c3e50\")\n\n    segments(d$ate, 1:4, ests, 1:4, col = cols, lwd = 2, lty = 2)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    b_naive &lt;- d$naive - d$ate\n    b_reg   &lt;- d$reg_est - d$ate\n    b_ipw   &lt;- d$ipw_est - d$ate\n    b_dr    &lt;- d$dr_est - d$ate\n\n    scenario_label &lt;- switch(d$sc,\n      both         = \"Both models correct\",\n      outcome_only = \"Only outcome model correct\",\n      ps_only      = \"Only propensity score correct\",\n      neither      = \"Neither model correct\")\n\n    dr_ok &lt;- abs(b_dr) &lt; abs(b_naive) * 0.5\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Scenario:&lt;/b&gt; \", scenario_label, \"&lt;br&gt;\",\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; &lt;span class='bad'&gt;\", round(d$naive, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_naive, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;Reg. adj:&lt;/b&gt; \", round(d$reg_est, 3),\n        \" (bias: \", round(b_reg, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;IPW:&lt;/b&gt; \", round(d$ipw_est, 3),\n        \" (bias: \", round(b_ipw, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;DR:&lt;/b&gt; &lt;span class='\", ifelse(dr_ok, \"good\", \"bad\"), \"'&gt;\",\n        round(d$dr_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_dr, 3), \")\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nWalk through all four scenarios:\n\nBoth correct: all three adjusted estimators (regression, IPW, DR) are close to the truth. The left plot shows the fitted curves matching the true curves. DR works.\nOnly outcome correct: the PS is constant (ignores X), so IPW is just the naive difference — biased. But regression gets the outcome right, and DR inherits that. DR works.\nOnly PS correct: the outcome model fits a line when the truth is quadratic — you can see the misfit on the left plot. Regression is biased. But the PS is right, so the IPW correction rescues DR. DR works.\nNeither correct: both models are wrong. The left plot shows a bad fit, and the PS can’t correct it. DR fails — garbage in, garbage out.\n\nKey insight: DR doesn’t require both models to be right. It requires at least one to be right. That’s the “double robustness” — two chances instead of one.",
    "crumbs": [
      "Estimation Tools",
      "Doubly Robust"
    ]
  },
  {
    "objectID": "doubly-robust.html#the-idea",
    "href": "doubly-robust.html#the-idea",
    "title": "Doubly Robust Estimation",
    "section": "",
    "text": "Every estimation tool under selection on observables relies on getting a model right:\n\nRegression adjustment needs the outcome model \\(E[Y \\mid X, D]\\) to be correct.\nIPW needs the propensity score model \\(P(D = 1 \\mid X)\\) to be correct.\n\nWhat if you’re not sure which one you got right? Doubly robust (DR) estimation combines both models and gives you a consistent estimate if either one is correctly specified.\n\n\n\n\n\n\nExample: vaccine effectiveness. You want to know if a vaccine reduces hospitalization. You model who gets vaccinated (propensity score: older people and healthcare workers vaccinate more) and you model hospitalization risk (outcome model: depends on age, comorbidities, obesity). DR combines both. If your outcome model misses a nonlinear age effect but your propensity score is right — you’re fine. If your propensity score ignores rural vs urban but your outcome model captures it — also fine. You only fail if both models are wrong simultaneously.\n\n\n\n\n\nThe most common DR estimator is Augmented Inverse Probability Weighting (AIPW):\n\\[\\hat{\\tau}_{DR} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\hat{\\mu}_1(X_i) - \\hat{\\mu}_0(X_i) + \\frac{D_i (Y_i - \\hat{\\mu}_1(X_i))}{\\hat{e}(X_i)} - \\frac{(1 - D_i)(Y_i - \\hat{\\mu}_0(X_i))}{1 - \\hat{e}(X_i)} \\right]\\]\nwhere \\(\\hat{\\mu}_d(X)\\) is the estimated outcome under treatment \\(d\\), and \\(\\hat{e}(X)\\) is the estimated propensity score.\nIntuition: start with the regression prediction (\\(\\hat{\\mu}_1 - \\hat{\\mu}_0\\)), then correct it using the IPW-weighted residuals. If the outcome model is perfect, the residuals are zero and the correction vanishes. If the outcome model is wrong but the propensity score is right, the correction fixes the bias.\n\n\n\n\n\n\nOutcome model\nPropensity score\nDR consistent?\n\n\n\n\nCorrect\nCorrect\nYes\n\n\nCorrect\nWrong\nYes\n\n\nWrong\nCorrect\nYes\n\n\nWrong\nWrong\nNo\n\n\n\nThree out of four — you get two shots at a consistent estimate. This is the key advantage over methods that rely on a single model.\n\n\n\nTo see DR in action, we need a setting where models can be wrong:\n\nTrue outcome: \\(Y = 1 + 2X^2 + \\tau D + \\varepsilon\\) (quadratic in \\(X\\))\nTrue treatment: \\(P(D=1 \\mid X) = \\Phi(1.5 \\cdot X)\\) (probit)\n“Correct” outcome model: includes \\(X^2\\)\n“Wrong” outcome model: linear only (\\(X\\), no \\(X^2\\))\n“Correct” PS model: logit on \\(X\\) (close enough to probit)\n“Wrong” PS model: constant 0.5 (ignores \\(X\\) entirely)\n\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 500, max = 3000, value = 1000, step = 250),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      selectInput(\"scenario\", \"Scenario:\",\n                  choices = c(\n                    \"Both correct\"         = \"both\",\n                    \"Only outcome correct\"  = \"outcome_only\",\n                    \"Only PS correct\"       = \"ps_only\",\n                    \"Neither correct\"       = \"neither\"\n                  )),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"fit_plot\", height = \"430px\")),\n        column(6, plotOutput(\"compare_plot\", height = \"430px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    ate &lt;- input$ate\n    sc  &lt;- input$scenario\n\n    # Data generating process\n    x &lt;- rnorm(n)\n\n    # True PS: probit\n    p_true &lt;- pnorm(1.5 * x)\n    treat &lt;- rbinom(n, 1, p_true)\n\n    # True outcome: quadratic\n    y &lt;- 1 + 2 * x^2 + ate * treat + rnorm(n)\n\n    # --- Outcome models ---\n    outcome_correct &lt;- sc %in% c(\"both\", \"outcome_only\")\n    if (outcome_correct) {\n      x2 &lt;- x^2\n      fit1 &lt;- lm(y ~ treat + x + x2)\n      mu1 &lt;- predict(fit1, newdata = data.frame(treat = 1, x = x, x2 = x^2))\n      mu0 &lt;- predict(fit1, newdata = data.frame(treat = 0, x = x, x2 = x^2))\n    } else {\n      fit1 &lt;- lm(y ~ treat + x)\n      mu1 &lt;- predict(fit1, newdata = data.frame(treat = 1, x = x))\n      mu0 &lt;- predict(fit1, newdata = data.frame(treat = 0, x = x))\n    }\n\n    # Regression adjustment estimate\n    reg_est &lt;- mean(mu1 - mu0)\n\n    # --- PS models ---\n    ps_correct &lt;- sc %in% c(\"both\", \"ps_only\")\n    if (ps_correct) {\n      ps &lt;- fitted(glm(treat ~ x, family = binomial))\n    } else {\n      ps &lt;- rep(0.5, n)\n    }\n\n    # IPW estimate\n    w &lt;- ifelse(treat == 1, 1 / ps, 1 / (1 - ps))\n    ipw_est &lt;- weighted.mean(y[treat == 1], w[treat == 1]) -\n               weighted.mean(y[treat == 0], w[treat == 0])\n\n    # Doubly robust (AIPW)\n    dr_est &lt;- mean(mu1 - mu0 +\n      treat * (y - mu1) / ps -\n      (1 - treat) * (y - mu0) / (1 - ps))\n\n    # Naive\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # For plotting: outcome model fit curves\n    x_seq &lt;- seq(min(x), max(x), length.out = 200)\n    if (outcome_correct) {\n      pred_t &lt;- predict(fit1, newdata = data.frame(treat = 1, x = x_seq, x2 = x_seq^2))\n      pred_c &lt;- predict(fit1, newdata = data.frame(treat = 0, x = x_seq, x2 = x_seq^2))\n    } else {\n      pred_t &lt;- predict(fit1, newdata = data.frame(treat = 1, x = x_seq))\n      pred_c &lt;- predict(fit1, newdata = data.frame(treat = 0, x = x_seq))\n    }\n    true_t &lt;- 1 + 2 * x_seq^2 + ate\n    true_c &lt;- 1 + 2 * x_seq^2\n\n    list(x = x, y = y, treat = treat,\n         x_seq = x_seq, pred_t = pred_t, pred_c = pred_c,\n         true_t = true_t, true_c = true_c,\n         naive = naive, reg_est = reg_est, ipw_est = ipw_est, dr_est = dr_est,\n         ate = ate, sc = sc,\n         outcome_correct = outcome_correct, ps_correct = ps_correct)\n  })\n\n  output$fit_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$y, pch = 16, cex = 0.3,\n         col = ifelse(d$treat == 1, \"#3498db30\", \"#e74c3c30\"),\n         xlab = \"X (confounder)\", ylab = \"Y (outcome)\",\n         main = \"Outcome Model Fit vs Truth\")\n\n    # Fitted curves (solid)\n    lines(d$x_seq, d$pred_t, col = \"#3498db\", lwd = 2.5)\n    lines(d$x_seq, d$pred_c, col = \"#e74c3c\", lwd = 2.5)\n\n    # True curves (dashed)\n    lines(d$x_seq, d$true_t, col = \"#3498db\", lwd = 1.5, lty = 3)\n    lines(d$x_seq, d$true_c, col = \"#e74c3c\", lwd = 1.5, lty = 3)\n\n    outcome_label &lt;- if (d$outcome_correct) \"Correct (quadratic)\" else \"Wrong (linear)\"\n    ps_label &lt;- if (d$ps_correct) \"Correct (logit)\" else \"Wrong (constant)\"\n\n    legend(\"topleft\", bty = \"n\", cex = 0.75,\n           legend = c(\"Treated data\", \"Control data\",\n                      \"Model fit (solid)\", \"True E[Y|X,D] (dotted)\",\n                      paste0(\"Outcome: \", outcome_label),\n                      paste0(\"PS: \", ps_label)),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray40\", \"gray40\", NA, NA),\n           pch = c(16, 16, NA, NA, NA, NA),\n           lwd = c(NA, NA, 2.5, 1.5, NA, NA),\n           lty = c(NA, NA, 1, 3, NA, NA))\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 9, 3, 2))\n\n    ests &lt;- c(d$dr_est, d$ipw_est, d$reg_est, d$naive)\n    labels &lt;- c(\"Doubly Robust\", \"IPW\", \"Regression adj.\", \"Naive\")\n    cols &lt;- c(\"#9b59b6\", \"#27ae60\", \"#3498db\", \"#e74c3c\")\n\n    xlim &lt;- range(c(ests, d$ate))\n    pad  &lt;- max(diff(xlim) * 0.4, 0.5)\n    xlim &lt;- xlim + c(-pad, pad)\n\n    plot(ests, 1:4, pch = 19, cex = 2, col = cols,\n         xlim = xlim, ylim = c(0.5, 4.5),\n         yaxt = \"n\", xlab = \"Estimated treatment effect\",\n         ylab = \"\", main = \"Estimator Comparison\")\n    axis(2, at = 1:4, labels = labels, las = 1, cex.axis = 0.9)\n\n    abline(v = d$ate, lty = 2, col = \"#2c3e50\", lwd = 2)\n    text(d$ate, 4.45, paste0(\"True ATE = \", d$ate),\n         cex = 0.85, font = 2, col = \"#2c3e50\")\n\n    segments(d$ate, 1:4, ests, 1:4, col = cols, lwd = 2, lty = 2)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    b_naive &lt;- d$naive - d$ate\n    b_reg   &lt;- d$reg_est - d$ate\n    b_ipw   &lt;- d$ipw_est - d$ate\n    b_dr    &lt;- d$dr_est - d$ate\n\n    scenario_label &lt;- switch(d$sc,\n      both         = \"Both models correct\",\n      outcome_only = \"Only outcome model correct\",\n      ps_only      = \"Only propensity score correct\",\n      neither      = \"Neither model correct\")\n\n    dr_ok &lt;- abs(b_dr) &lt; abs(b_naive) * 0.5\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Scenario:&lt;/b&gt; \", scenario_label, \"&lt;br&gt;\",\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; &lt;span class='bad'&gt;\", round(d$naive, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_naive, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;Reg. adj:&lt;/b&gt; \", round(d$reg_est, 3),\n        \" (bias: \", round(b_reg, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;IPW:&lt;/b&gt; \", round(d$ipw_est, 3),\n        \" (bias: \", round(b_ipw, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;DR:&lt;/b&gt; &lt;span class='\", ifelse(dr_ok, \"good\", \"bad\"), \"'&gt;\",\n        round(d$dr_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_dr, 3), \")\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nWalk through all four scenarios:\n\nBoth correct: all three adjusted estimators (regression, IPW, DR) are close to the truth. The left plot shows the fitted curves matching the true curves. DR works.\nOnly outcome correct: the PS is constant (ignores X), so IPW is just the naive difference — biased. But regression gets the outcome right, and DR inherits that. DR works.\nOnly PS correct: the outcome model fits a line when the truth is quadratic — you can see the misfit on the left plot. Regression is biased. But the PS is right, so the IPW correction rescues DR. DR works.\nNeither correct: both models are wrong. The left plot shows a bad fit, and the PS can’t correct it. DR fails — garbage in, garbage out.\n\nKey insight: DR doesn’t require both models to be right. It requires at least one to be right. That’s the “double robustness” — two chances instead of one.",
    "crumbs": [
      "Estimation Tools",
      "Doubly Robust"
    ]
  },
  {
    "objectID": "doubly-robust.html#why-not-always-use-dr",
    "href": "doubly-robust.html#why-not-always-use-dr",
    "title": "Doubly Robust Estimation",
    "section": "Why not always use DR?",
    "text": "Why not always use DR?\nIf DR is better than regression alone and IPW alone, why not always use it?\n\nYou still need at least one model right. DR isn’t magic — it fails when both models are misspecified. In the “neither correct” scenario above, DR is just as biased as the others.\nVariance. DR can have higher variance than a correctly specified single model. The extra robustness comes at the cost of efficiency. In the “both correct” scenario, regression alone is often more precise.\nComplexity. Two models to specify, diagnose, and justify instead of one.\n\nIn practice, DR is most valuable when you’re uncertain about your models — which is most of the time.",
    "crumbs": [
      "Estimation Tools",
      "Doubly Robust"
    ]
  },
  {
    "objectID": "doubly-robust.html#in-stata",
    "href": "doubly-robust.html#in-stata",
    "title": "Doubly Robust Estimation",
    "section": "In Stata",
    "text": "In Stata\n* Augmented IPW (doubly robust)\nteffects aipw (outcome x1 x2) (treatment x1 x2)\n\n* Check overlap\nteffects overlap\n\n* Doubly robust DID (Sant'Anna & Zhao 2020)\n* ssc install drdid\ndrdid outcome x1 x2, ivar(id) time(year) treatment(treated)\nteffects aipw specifies two models in one command: the outcome model (first parentheses) and the treatment model (second parentheses). You get double robustness — consistent if either model is correct.",
    "crumbs": [
      "Estimation Tools",
      "Doubly Robust"
    ]
  },
  {
    "objectID": "doubly-robust.html#did-you-know",
    "href": "doubly-robust.html#did-you-know",
    "title": "Doubly Robust Estimation",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe doubly robust property was established by Scharfstein, Rotnitzky & Robins (1999) and Bang & Robins (2005). The key insight is that AIPW belongs to a class of semiparametrically efficient estimators — it achieves the lowest possible variance among regular estimators when both models are correct, and remains consistent when either is correct.\nDoubly robust DID (DR-DID) extends the idea to difference-in-differences. Sant’Anna & Zhao (2020) showed how to combine outcome regression and propensity score weighting in the DID setting, getting double robustness for treatment effect estimation under parallel trends. This is now standard in the staggered DID literature.\nThe DR estimator is connected to targeted learning (van der Laan & Rose,\n\nand debiased machine learning (Chernozhukov et al., 2018). These frameworks use machine learning for the nuisance models (\\(\\hat{\\mu}\\) and \\(\\hat{e}\\)) while maintaining valid inference for the causal parameter — DR structure is what makes this possible.",
    "crumbs": [
      "Estimation Tools",
      "Doubly Robust"
    ]
  },
  {
    "objectID": "ipw.html",
    "href": "ipw.html",
    "title": "Inverse Probability Weighting",
    "section": "",
    "text": "In observational data, treatment isn’t random. People who get treated differ from those who don’t — they may be older, sicker, richer, etc. A naive comparison of outcomes is biased by these confounders.",
    "crumbs": [
      "Estimation Tools",
      "IPW"
    ]
  },
  {
    "objectID": "ipw.html#the-problem",
    "href": "ipw.html#the-problem",
    "title": "Inverse Probability Weighting",
    "section": "",
    "text": "In observational data, treatment isn’t random. People who get treated differ from those who don’t — they may be older, sicker, richer, etc. A naive comparison of outcomes is biased by these confounders.",
    "crumbs": [
      "Estimation Tools",
      "IPW"
    ]
  },
  {
    "objectID": "ipw.html#the-idea",
    "href": "ipw.html#the-idea",
    "title": "Inverse Probability Weighting",
    "section": "The idea",
    "text": "The idea\nInverse Probability Weighting (IPW) reweights observations so that the treated and control groups look alike on observed covariates. The steps:\n\nEstimate the propensity score \\(e(X) = P(\\text{treated} \\mid X)\\) — the probability of treatment given covariates.\nWeight each observation inversely by its probability of receiving the treatment it actually got:\n\nTreated units get weight \\(1 / e(X)\\)\nControl units get weight \\(1 / (1 - e(X))\\)\n\nCompute the weighted difference in means.\n\nIntuition: if a treated person had only a 20% chance of being treated (based on their X), they’re “surprising” — they represent 5 similar people who weren’t treated. So they get upweighted. This creates a pseudo-population where treatment is independent of X.\n\nAssumptions\n\nUnconfoundedness (selection on observables): conditional on observed covariates \\(X\\), treatment is independent of potential outcomes — \\(Y(0), Y(1) \\perp D \\mid X\\). All confounders are observed and included.\nOverlap (positivity): every unit has a non-zero probability of being in either group — \\(0 &lt; e(X) &lt; 1\\) for all \\(X\\). No one is deterministically treated or untreated.\nCorrect propensity score model: the functional form of \\(e(X)\\) is correctly specified. If you use a logit and the true relationship is nonlinear, the weights are wrong.\n\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"confounding\", \"Confounding strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"balance_plot\", height = \"380px\")),\n        column(6, plotOutput(\"ps_plot\", height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    ate  &lt;- input$ate\n    conf &lt;- input$confounding\n\n    # Confounder\n    x &lt;- rnorm(n)\n\n    # Treatment depends on x (confounding)\n    p_true &lt;- pnorm(conf * x)\n    treat &lt;- rbinom(n, 1, p_true)\n\n    # Outcome depends on x and treatment\n    y &lt;- 1 + 2 * x + ate * treat + rnorm(n)\n\n    # Naive estimate\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # IPW estimate\n    ps &lt;- fitted(glm(treat ~ x, family = binomial))\n    w &lt;- ifelse(treat == 1, 1 / ps, 1 / (1 - ps))\n    ipw_est &lt;- weighted.mean(y[treat == 1], w[treat == 1]) -\n               weighted.mean(y[treat == 0], w[treat == 0])\n\n    list(x = x, treat = treat, y = y, ps = ps, w = w,\n         naive = naive, ipw_est = ipw_est, ate = ate)\n  })\n\n  output$balance_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Unweighted densities\n    x_t &lt;- d$x[d$treat == 1]\n    x_c &lt;- d$x[d$treat == 0]\n\n    rng &lt;- range(d$x)\n    dens_t &lt;- density(x_t, from = rng[1], to = rng[2])\n    dens_c &lt;- density(x_c, from = rng[1], to = rng[2])\n\n    ylim &lt;- c(0, max(dens_t$y, dens_c$y) * 1.2)\n\n    plot(dens_t, col = \"#3498db\", lwd = 2.5, main = \"Covariate Balance (X)\",\n         xlab = \"X (confounder)\", ylab = \"Density\", ylim = ylim)\n    lines(dens_c, col = \"#e74c3c\", lwd = 2.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\"),\n           col = c(\"#3498db\", \"#e74c3c\"), lwd = 2.5)\n  })\n\n  output$ps_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$ps, pch = 16, cex = 0.5,\n         col = ifelse(d$treat == 1, \"#3498db80\", \"#e74c3c80\"),\n         xlab = \"X (confounder)\", ylab = \"Propensity score e(X)\",\n         main = \"Propensity Score vs Confounder\")\n    abline(h = 0.5, lty = 2, col = \"gray50\")\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\"),\n           col = c(\"#3498db\", \"#e74c3c\"), pch = 16)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive estimate:&lt;/b&gt; &lt;span class='bad'&gt;\", round(d$naive, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;IPW estimate:&lt;/b&gt; &lt;span class='good'&gt;\", round(d$ipw_est, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Naive bias:&lt;/b&gt; \", round(d$naive - d$ate, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;IPW bias:&lt;/b&gt; \", round(d$ipw_est - d$ate, 3)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nConfounding = 0: treatment is random. Naive and IPW give the same answer.\nConfounding = 1.5: the covariate distributions for treated and control diverge (left plot). Naive is biased, IPW corrects it.\nConfounding = 3: extreme selection. The propensity scores are near 0 or 1 (right plot), meaning some units get huge weights. IPW becomes noisy — this is the extreme weights problem.",
    "crumbs": [
      "Estimation Tools",
      "IPW"
    ]
  },
  {
    "objectID": "ipw.html#in-stata",
    "href": "ipw.html#in-stata",
    "title": "Inverse Probability Weighting",
    "section": "In Stata",
    "text": "In Stata\n* IPW with Stata's teffects\nteffects ipw (outcome) (treatment x1 x2)\n\n* Check propensity score overlap\nteffects overlap\n\n* Manual approach: estimate propensity score, then weight\nlogit treatment x1 x2\npredict pscore, pr\ngen ipw = treatment/pscore + (1-treatment)/(1-pscore)\nreg outcome treatment [pw=ipw]\nteffects ipw handles everything — propensity score estimation, weighting, and correct standard errors. The manual approach is useful for understanding what’s happening under the hood.",
    "crumbs": [
      "Estimation Tools",
      "IPW"
    ]
  },
  {
    "objectID": "did.html",
    "href": "did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "You have two groups: one gets treated at some point, the other never does. You observe both before and after treatment. The key assumption: absent treatment, both groups would have followed parallel trends.\n\\[\\hat{\\tau}_{DID} = (\\bar{Y}_{treat,post} - \\bar{Y}_{treat,pre}) - (\\bar{Y}_{ctrl,post} - \\bar{Y}_{ctrl,pre})\\]\nThe first difference removes time-invariant group characteristics. The second difference removes common time trends. What’s left is the treatment effect.\n\n\n\nParallel trends: absent treatment, the treated and control groups would have followed the same trajectory over time — the key assumption\nNo anticipation: treated units don’t change behavior before the treatment date\nSUTVA: treatment of one group doesn’t spill over to the control group\nStable composition: the groups don’t change membership over time (no differential attrition)\n\n\n\n\nWhen the parallel trends assumption is violated — if the treated group was already on a different trajectory before treatment. The simulation below lets you break this assumption and see the bias that results.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_units\", \"Units per group:\",\n                  min = 20, max = 200, value = 50, step = 10),\n\n      sliderInput(\"true_effect\", \"True treatment effect:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"trend_diff\", \"Differential pre-trend\\n(violation of parallel trends):\",\n                  min = -1, max = 1, value = 0, step = 0.1),\n\n      sliderInput(\"sigma\", \"Noise (SD):\",\n                  min = 0.5, max = 3, value = 1, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"did_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n_units\n    tau   &lt;- input$true_effect\n    delta &lt;- input$trend_diff\n    sigma &lt;- input$sigma\n\n    periods &lt;- -4:4\n    treat_time &lt;- 1  # treatment at t = 1\n\n    # Group means over time\n    ctrl_mean &lt;- 3 + 0.3 * periods\n    treat_mean &lt;- 5 + (0.3 + delta) * periods\n\n    # Add treatment effect post\n    treat_mean[periods &gt;= treat_time] &lt;- treat_mean[periods &gt;= treat_time] + tau\n\n    # Generate unit-level data\n    ctrl_data &lt;- sapply(ctrl_mean, function(m) m + rnorm(n, sd = sigma))\n    treat_data &lt;- sapply(treat_mean, function(m) m + rnorm(n, sd = sigma))\n\n    ctrl_means_obs &lt;- colMeans(ctrl_data)\n    treat_means_obs &lt;- colMeans(treat_data)\n\n    # DID estimate (using t=0 as pre, t=1 as post)\n    pre_idx  &lt;- which(periods == 0)\n    post_idx &lt;- which(periods == 1)\n\n    did_est &lt;- (treat_means_obs[post_idx] - treat_means_obs[pre_idx]) -\n               (ctrl_means_obs[post_idx] - ctrl_means_obs[pre_idx])\n\n    # Counterfactual for treated (parallel to control from t=0)\n    cf &lt;- treat_means_obs[pre_idx] + (ctrl_means_obs - ctrl_means_obs[pre_idx])\n\n    list(periods = periods, ctrl = ctrl_means_obs, treat = treat_means_obs,\n         cf = cf, did_est = did_est, tau = tau, delta = delta,\n         treat_time = treat_time)\n  })\n\n  output$did_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    ylim &lt;- range(c(d$ctrl, d$treat, d$cf)) + c(-0.5, 0.5)\n\n    plot(d$periods, d$treat, type = \"b\", pch = 19, lwd = 2.5, col = \"#3498db\",\n         xlab = \"Time period\", ylab = \"Mean outcome\",\n         main = \"Difference-in-Differences\",\n         ylim = ylim, xaxt = \"n\")\n    axis(1, at = d$periods)\n\n    lines(d$periods, d$ctrl, type = \"b\", pch = 19, lwd = 2.5, col = \"#e74c3c\")\n\n    # Counterfactual (dashed, post only)\n    post &lt;- d$periods &gt;= d$treat_time\n    lines(d$periods[post], d$cf[post], type = \"b\", pch = 1, lwd = 2, lty = 2,\n          col = \"#3498db80\")\n\n    # Treatment onset\n    abline(v = d$treat_time - 0.5, lty = 3, col = \"gray50\", lwd = 1.5)\n    text(d$treat_time - 0.5, ylim[2], \"Treatment\", pos = 4, cex = 0.85, col = \"gray40\")\n\n    # DID bracket\n    pre_idx  &lt;- which(d$periods == 0)\n    post_idx &lt;- which(d$periods == 1)\n    arrows(max(d$periods) - 0.3, d$cf[post_idx],\n           max(d$periods) - 0.3, d$treat[post_idx],\n           code = 3, lwd = 2, col = \"#27ae60\", length = 0.1)\n    text(max(d$periods) - 0.1, (d$cf[post_idx] + d$treat[post_idx]) / 2,\n         paste0(\"DID = \", round(d$did_est, 2)),\n         col = \"#27ae60\", cex = 0.9, adj = 0)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\", \"Counterfactual (parallel trends)\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"#3498db80\"),\n           pch = c(19, 19, 1), lty = c(1, 1, 2), lwd = c(2.5, 2.5, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$did_est - d$tau\n    biased &lt;- abs(d$delta) &gt; 0.05\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;DID estimate:&lt;/b&gt; \", round(d$did_est, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(biased, \"bad\", \"good\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        if (biased) \"&lt;br&gt;&lt;small&gt;Parallel trends violated &mdash; DID is biased.&lt;/small&gt;\"\n        else \"&lt;br&gt;&lt;small&gt;Parallel trends hold &mdash; DID is unbiased.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDifferential pre-trend = 0: parallel trends hold, DID nails the true effect.\nSlide the differential pre-trend to +0.5: the treated group was already rising faster. DID attributes some of that trend to the treatment — the estimate is biased upward.\nSet true effect = 0 with a differential trend: DID “finds” an effect that doesn’t exist. That’s how pre-trend violations create false positives.\nLook at the pre-treatment periods — if the lines aren’t parallel before treatment, you should worry.",
    "crumbs": [
      "Methods",
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "did.html#the-idea",
    "href": "did.html#the-idea",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "You have two groups: one gets treated at some point, the other never does. You observe both before and after treatment. The key assumption: absent treatment, both groups would have followed parallel trends.\n\\[\\hat{\\tau}_{DID} = (\\bar{Y}_{treat,post} - \\bar{Y}_{treat,pre}) - (\\bar{Y}_{ctrl,post} - \\bar{Y}_{ctrl,pre})\\]\nThe first difference removes time-invariant group characteristics. The second difference removes common time trends. What’s left is the treatment effect.\n\n\n\nParallel trends: absent treatment, the treated and control groups would have followed the same trajectory over time — the key assumption\nNo anticipation: treated units don’t change behavior before the treatment date\nSUTVA: treatment of one group doesn’t spill over to the control group\nStable composition: the groups don’t change membership over time (no differential attrition)\n\n\n\n\nWhen the parallel trends assumption is violated — if the treated group was already on a different trajectory before treatment. The simulation below lets you break this assumption and see the bias that results.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_units\", \"Units per group:\",\n                  min = 20, max = 200, value = 50, step = 10),\n\n      sliderInput(\"true_effect\", \"True treatment effect:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"trend_diff\", \"Differential pre-trend\\n(violation of parallel trends):\",\n                  min = -1, max = 1, value = 0, step = 0.1),\n\n      sliderInput(\"sigma\", \"Noise (SD):\",\n                  min = 0.5, max = 3, value = 1, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"did_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n_units\n    tau   &lt;- input$true_effect\n    delta &lt;- input$trend_diff\n    sigma &lt;- input$sigma\n\n    periods &lt;- -4:4\n    treat_time &lt;- 1  # treatment at t = 1\n\n    # Group means over time\n    ctrl_mean &lt;- 3 + 0.3 * periods\n    treat_mean &lt;- 5 + (0.3 + delta) * periods\n\n    # Add treatment effect post\n    treat_mean[periods &gt;= treat_time] &lt;- treat_mean[periods &gt;= treat_time] + tau\n\n    # Generate unit-level data\n    ctrl_data &lt;- sapply(ctrl_mean, function(m) m + rnorm(n, sd = sigma))\n    treat_data &lt;- sapply(treat_mean, function(m) m + rnorm(n, sd = sigma))\n\n    ctrl_means_obs &lt;- colMeans(ctrl_data)\n    treat_means_obs &lt;- colMeans(treat_data)\n\n    # DID estimate (using t=0 as pre, t=1 as post)\n    pre_idx  &lt;- which(periods == 0)\n    post_idx &lt;- which(periods == 1)\n\n    did_est &lt;- (treat_means_obs[post_idx] - treat_means_obs[pre_idx]) -\n               (ctrl_means_obs[post_idx] - ctrl_means_obs[pre_idx])\n\n    # Counterfactual for treated (parallel to control from t=0)\n    cf &lt;- treat_means_obs[pre_idx] + (ctrl_means_obs - ctrl_means_obs[pre_idx])\n\n    list(periods = periods, ctrl = ctrl_means_obs, treat = treat_means_obs,\n         cf = cf, did_est = did_est, tau = tau, delta = delta,\n         treat_time = treat_time)\n  })\n\n  output$did_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    ylim &lt;- range(c(d$ctrl, d$treat, d$cf)) + c(-0.5, 0.5)\n\n    plot(d$periods, d$treat, type = \"b\", pch = 19, lwd = 2.5, col = \"#3498db\",\n         xlab = \"Time period\", ylab = \"Mean outcome\",\n         main = \"Difference-in-Differences\",\n         ylim = ylim, xaxt = \"n\")\n    axis(1, at = d$periods)\n\n    lines(d$periods, d$ctrl, type = \"b\", pch = 19, lwd = 2.5, col = \"#e74c3c\")\n\n    # Counterfactual (dashed, post only)\n    post &lt;- d$periods &gt;= d$treat_time\n    lines(d$periods[post], d$cf[post], type = \"b\", pch = 1, lwd = 2, lty = 2,\n          col = \"#3498db80\")\n\n    # Treatment onset\n    abline(v = d$treat_time - 0.5, lty = 3, col = \"gray50\", lwd = 1.5)\n    text(d$treat_time - 0.5, ylim[2], \"Treatment\", pos = 4, cex = 0.85, col = \"gray40\")\n\n    # DID bracket\n    pre_idx  &lt;- which(d$periods == 0)\n    post_idx &lt;- which(d$periods == 1)\n    arrows(max(d$periods) - 0.3, d$cf[post_idx],\n           max(d$periods) - 0.3, d$treat[post_idx],\n           code = 3, lwd = 2, col = \"#27ae60\", length = 0.1)\n    text(max(d$periods) - 0.1, (d$cf[post_idx] + d$treat[post_idx]) / 2,\n         paste0(\"DID = \", round(d$did_est, 2)),\n         col = \"#27ae60\", cex = 0.9, adj = 0)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\", \"Counterfactual (parallel trends)\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"#3498db80\"),\n           pch = c(19, 19, 1), lty = c(1, 1, 2), lwd = c(2.5, 2.5, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$did_est - d$tau\n    biased &lt;- abs(d$delta) &gt; 0.05\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;DID estimate:&lt;/b&gt; \", round(d$did_est, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(biased, \"bad\", \"good\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        if (biased) \"&lt;br&gt;&lt;small&gt;Parallel trends violated &mdash; DID is biased.&lt;/small&gt;\"\n        else \"&lt;br&gt;&lt;small&gt;Parallel trends hold &mdash; DID is unbiased.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDifferential pre-trend = 0: parallel trends hold, DID nails the true effect.\nSlide the differential pre-trend to +0.5: the treated group was already rising faster. DID attributes some of that trend to the treatment — the estimate is biased upward.\nSet true effect = 0 with a differential trend: DID “finds” an effect that doesn’t exist. That’s how pre-trend violations create false positives.\nLook at the pre-treatment periods — if the lines aren’t parallel before treatment, you should worry.",
    "crumbs": [
      "Methods",
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "did.html#in-stata",
    "href": "did.html#in-stata",
    "title": "Difference-in-Differences",
    "section": "In Stata",
    "text": "In Stata\n* Classic 2x2 DID with interaction\nreg outcome treated##post\n\n* With controls\nreg outcome treated##post x1 x2, cluster(state)\n\n* Event study (test parallel trends visually)\nreg outcome i.treated#i.year i.year i.treated, cluster(state)\n\n* Modern staggered DID (Callaway & Sant'Anna 2021)\n* ssc install csdid\ncsdid outcome x1 x2, ivar(id) time(year) gvar(first_treated)\nThe coefficient on treated#post (or 1.treated#1.post) is the DID estimate. Clustering standard errors at the group level (e.g., state) is standard practice.",
    "crumbs": [
      "Methods",
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "regression-adjustment.html",
    "href": "regression-adjustment.html",
    "title": "Regression Adjustment",
    "section": "",
    "text": "The simplest way to estimate a causal effect from observational data: run a regression of the outcome on the treatment indicator and the confounders.\n\\[Y_i = \\alpha + \\tau D_i + \\beta X_i + \\varepsilon_i\\]\nThe coefficient \\(\\hat{\\tau}\\) is your treatment effect estimate, “adjusted” for \\(X\\). OLS compares treated and control units within levels of X — holding the confounder constant to isolate the effect of treatment.\nThe gap between the two regression lines (treated vs control) is \\(\\hat{\\tau}\\).\n\n\n\nSelection on observables (CIA): all confounders are included in \\(X\\). If an unobserved variable drives both treatment and outcome, the estimate is biased — same as every other tool under SOO.\nCorrect functional form: the relationship between \\(X\\) and \\(Y\\) is correctly specified. If you fit a linear model but the truth is quadratic, the adjustment is wrong and \\(\\hat{\\tau}\\) is biased. This is the key vulnerability.\nOverlap: treated and control groups share common support in \\(X\\).\n\n\n\n\nRegression adjustment models the outcome — it writes down a specific equation for how \\(Y\\) relates to \\(X\\) and \\(D\\). That equation is the functional form:\n\nLinear: \\(Y = \\alpha + \\tau D + \\beta X\\) — assumes \\(Y\\) changes at a constant rate with \\(X\\). A straight line.\nQuadratic: \\(Y = \\alpha + \\tau D + \\beta_1 X + \\beta_2 X^2\\) — allows curvature.\nLog: \\(Y = \\alpha + \\tau D + \\beta \\log(X)\\) — assumes diminishing returns.\n\nWhen you run lm(y ~ treat + x), you’ve chosen the linear form. If the truth is quadratic but you fit a line, the line can’t capture the curvature — and that misfit contaminates your estimate of \\(\\hat{\\tau}\\). The model is wrong, so the estimate is biased, even when all confounders are observed. This is an estimation bias, not an identification failure.\nCompare this to IPW, which models the treatment assignment instead — it writes down an equation for \\(P(D = 1 \\mid X)\\), not for \\(Y\\). IPW never touches the outcome. It just reweights the raw \\(Y\\) values using the propensity score. So if the outcome-\\(X\\) relationship is nonlinear, IPW doesn’t care — as long as the propensity score model is right.\n\n\n\n\nWhat it models\nWhat can go wrong\n\n\n\n\nRegression adjustment\n\\(E[Y \\mid X, D]\\) (the outcome)\nWrong functional form for \\(Y\\)\n\n\nIPW\n\\(P(D=1 \\mid X)\\) (the treatment)\nWrong propensity score model\n\n\nDoubly robust\nBoth\nFails only if both are wrong\n\n\n\nEach method bets on getting one model right. Doubly robust estimation hedges by combining both — it only needs one to be correct.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"confounding\", \"Confounding strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      selectInput(\"shape\", \"True outcome model:\",\n                  choices = c(\"Linear (Y ~ X)\"      = \"linear\",\n                              \"Quadratic (Y ~ X\\u00b2)\" = \"quadratic\")),\n\n      actionButton(\"go\", \"New draw\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"scatter_plot\", height = \"400px\")),\n        column(6, plotOutput(\"compare_plot\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    ate  &lt;- input$ate\n    conf &lt;- input$confounding\n    is_linear &lt;- input$shape == \"linear\"\n\n    # Confounder\n    x &lt;- rnorm(n)\n\n    # Treatment depends on x\n    p_true &lt;- pnorm(conf * x)\n    treat  &lt;- rbinom(n, 1, p_true)\n\n    # Outcome depends on x (linearly or quadratically) + treatment\n    if (is_linear) {\n      y &lt;- 1 + 2 * x + ate * treat + rnorm(n)\n    } else {\n      y &lt;- 1 + 2 * x^2 + ate * treat + rnorm(n)\n    }\n\n    # Naive (no controls)\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # Regression adjustment (always linear: Y ~ D + X)\n    fit &lt;- lm(y ~ treat + x)\n    reg_est &lt;- coef(fit)[\"treat\"]\n\n    # IPW for comparison\n    ps &lt;- fitted(glm(treat ~ x, family = binomial))\n    w &lt;- ifelse(treat == 1, 1 / ps, 1 / (1 - ps))\n    ipw_est &lt;- weighted.mean(y[treat == 1], w[treat == 1]) -\n               weighted.mean(y[treat == 0], w[treat == 0])\n\n    list(x = x, treat = treat, y = y, fit = fit,\n         naive = naive, reg_est = reg_est, ipw_est = ipw_est,\n         ate = ate, is_linear = is_linear)\n  })\n\n  output$scatter_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$y, pch = 16, cex = 0.5,\n         col = ifelse(d$treat == 1, \"#3498db50\", \"#e74c3c50\"),\n         xlab = \"X (confounder)\", ylab = \"Y (outcome)\",\n         main = \"Data + Regression Fit\")\n\n    # Fitted regression lines (always linear)\n    x_seq &lt;- seq(min(d$x), max(d$x), length.out = 200)\n    pred_t &lt;- predict(d$fit,\n      newdata = data.frame(treat = 1, x = x_seq))\n    pred_c &lt;- predict(d$fit,\n      newdata = data.frame(treat = 0, x = x_seq))\n    lines(x_seq, pred_t, col = \"#3498db\", lwd = 2.5)\n    lines(x_seq, pred_c, col = \"#e74c3c\", lwd = 2.5)\n\n    # True conditional means (dashed)\n    if (d$is_linear) {\n      true_t &lt;- 1 + 2 * x_seq + d$ate\n      true_c &lt;- 1 + 2 * x_seq\n    } else {\n      true_t &lt;- 1 + 2 * x_seq^2 + d$ate\n      true_c &lt;- 1 + 2 * x_seq^2\n    }\n    lines(x_seq, true_t, col = \"#3498db\", lwd = 1.5, lty = 3)\n    lines(x_seq, true_c, col = \"#e74c3c\", lwd = 1.5, lty = 3)\n\n    # Gap arrow at x = 0\n    mid_t &lt;- predict(d$fit,\n      newdata = data.frame(treat = 1, x = 0))\n    mid_c &lt;- predict(d$fit,\n      newdata = data.frame(treat = 0, x = 0))\n    arrows(0, mid_c, 0, mid_t, code = 3,\n           col = \"#2c3e50\", lwd = 2, length = 0.08)\n    text(0.3, (mid_t + mid_c) / 2,\n         bquote(hat(tau) == .(round(coef(d$fit)[\"treat\"], 2))),\n         cex = 0.95, font = 2)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.8,\n           legend = c(\"Treated\", \"Control\",\n                      \"Regression fit\", \"True E[Y|X,D]\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray40\", \"gray40\"),\n           pch = c(16, 16, NA, NA),\n           lwd = c(NA, NA, 2.5, 1.5),\n           lty = c(NA, NA, 1, 3))\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 9, 3, 2))\n\n    ests &lt;- c(d$ipw_est, d$reg_est, d$naive)\n    labels &lt;- c(\"IPW\", \"Regression adj.\", \"Naive\")\n    cols &lt;- c(\"#27ae60\", \"#3498db\", \"#e74c3c\")\n\n    xlim &lt;- range(c(ests, d$ate))\n    pad  &lt;- max(diff(xlim) * 0.4, 0.5)\n    xlim &lt;- xlim + c(-pad, pad)\n\n    plot(ests, 1:3, pch = 19, cex = 2, col = cols,\n         xlim = xlim, ylim = c(0.5, 3.5),\n         yaxt = \"n\", xlab = \"Estimated treatment effect\",\n         ylab = \"\", main = \"Estimator Comparison\")\n    axis(2, at = 1:3, labels = labels,\n         las = 1, cex.axis = 0.9)\n\n    abline(v = d$ate, lty = 2, col = \"#2c3e50\", lwd = 2)\n    text(d$ate, 3.45,\n         paste0(\"True ATE = \", d$ate),\n         cex = 0.85, font = 2, col = \"#2c3e50\")\n\n    # Lines from true to estimate\n    segments(d$ate, 1:3, ests, 1:3,\n             col = cols, lwd = 2, lty = 2)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    b_naive &lt;- d$naive - d$ate\n    b_reg   &lt;- d$reg_est - d$ate\n    b_ipw   &lt;- d$ipw_est - d$ate\n    reg_ok  &lt;- abs(b_reg) &lt; abs(b_naive) * 0.5\n    ipw_ok  &lt;- abs(b_ipw) &lt; abs(b_naive) * 0.5\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; &lt;span class='bad'&gt;\",\n        round(d$naive, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_naive, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;Reg. adj:&lt;/b&gt; &lt;span class='\",\n        ifelse(reg_ok, \"good\", \"bad\"), \"'&gt;\",\n        round(d$reg_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_reg, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;IPW:&lt;/b&gt; &lt;span class='\",\n        ifelse(ipw_ok, \"good\", \"bad\"), \"'&gt;\",\n        round(d$ipw_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_ipw, 3), \")\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nLinear outcome, confounding = 1.5: regression adjustment works well. The fitted lines (solid) match the true curves (dotted). The estimate is close to the true ATE.\nSwitch to Quadratic: now the true outcome is \\(Y = 1 + 2X^2 + \\tau D\\), but regression still fits a line. The solid lines miss the curvature (compare to the dotted true curves). The regression estimate is biased — it’s a functional form problem, not a confounding problem.\nQuadratic outcome, compare to IPW: IPW doesn’t model the outcome, so it still works. The green dot (IPW) is close to the true ATE; the blue dot (regression) is not.\nConfounding = 0: treatment is random. All three estimators agree, and functional form doesn’t matter (no confounding to adjust for).\n\n\n\n\nRegression adjustment is the simplest and most common tool. It works great when the outcome model is correctly specified. But it’s fragile to functional form misspecification — fitting a line when the truth is curved introduces bias even when all confounders are observed.\nThis is why alternatives like IPW and entropy balancing exist: they avoid modeling the outcome entirely. And doubly robust methods hedge both bets.",
    "crumbs": [
      "Estimation Tools",
      "Regression Adjustment"
    ]
  },
  {
    "objectID": "regression-adjustment.html#the-idea",
    "href": "regression-adjustment.html#the-idea",
    "title": "Regression Adjustment",
    "section": "",
    "text": "The simplest way to estimate a causal effect from observational data: run a regression of the outcome on the treatment indicator and the confounders.\n\\[Y_i = \\alpha + \\tau D_i + \\beta X_i + \\varepsilon_i\\]\nThe coefficient \\(\\hat{\\tau}\\) is your treatment effect estimate, “adjusted” for \\(X\\). OLS compares treated and control units within levels of X — holding the confounder constant to isolate the effect of treatment.\nThe gap between the two regression lines (treated vs control) is \\(\\hat{\\tau}\\).\n\n\n\nSelection on observables (CIA): all confounders are included in \\(X\\). If an unobserved variable drives both treatment and outcome, the estimate is biased — same as every other tool under SOO.\nCorrect functional form: the relationship between \\(X\\) and \\(Y\\) is correctly specified. If you fit a linear model but the truth is quadratic, the adjustment is wrong and \\(\\hat{\\tau}\\) is biased. This is the key vulnerability.\nOverlap: treated and control groups share common support in \\(X\\).\n\n\n\n\nRegression adjustment models the outcome — it writes down a specific equation for how \\(Y\\) relates to \\(X\\) and \\(D\\). That equation is the functional form:\n\nLinear: \\(Y = \\alpha + \\tau D + \\beta X\\) — assumes \\(Y\\) changes at a constant rate with \\(X\\). A straight line.\nQuadratic: \\(Y = \\alpha + \\tau D + \\beta_1 X + \\beta_2 X^2\\) — allows curvature.\nLog: \\(Y = \\alpha + \\tau D + \\beta \\log(X)\\) — assumes diminishing returns.\n\nWhen you run lm(y ~ treat + x), you’ve chosen the linear form. If the truth is quadratic but you fit a line, the line can’t capture the curvature — and that misfit contaminates your estimate of \\(\\hat{\\tau}\\). The model is wrong, so the estimate is biased, even when all confounders are observed. This is an estimation bias, not an identification failure.\nCompare this to IPW, which models the treatment assignment instead — it writes down an equation for \\(P(D = 1 \\mid X)\\), not for \\(Y\\). IPW never touches the outcome. It just reweights the raw \\(Y\\) values using the propensity score. So if the outcome-\\(X\\) relationship is nonlinear, IPW doesn’t care — as long as the propensity score model is right.\n\n\n\n\nWhat it models\nWhat can go wrong\n\n\n\n\nRegression adjustment\n\\(E[Y \\mid X, D]\\) (the outcome)\nWrong functional form for \\(Y\\)\n\n\nIPW\n\\(P(D=1 \\mid X)\\) (the treatment)\nWrong propensity score model\n\n\nDoubly robust\nBoth\nFails only if both are wrong\n\n\n\nEach method bets on getting one model right. Doubly robust estimation hedges by combining both — it only needs one to be correct.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"confounding\", \"Confounding strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      selectInput(\"shape\", \"True outcome model:\",\n                  choices = c(\"Linear (Y ~ X)\"      = \"linear\",\n                              \"Quadratic (Y ~ X\\u00b2)\" = \"quadratic\")),\n\n      actionButton(\"go\", \"New draw\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"scatter_plot\", height = \"400px\")),\n        column(6, plotOutput(\"compare_plot\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    ate  &lt;- input$ate\n    conf &lt;- input$confounding\n    is_linear &lt;- input$shape == \"linear\"\n\n    # Confounder\n    x &lt;- rnorm(n)\n\n    # Treatment depends on x\n    p_true &lt;- pnorm(conf * x)\n    treat  &lt;- rbinom(n, 1, p_true)\n\n    # Outcome depends on x (linearly or quadratically) + treatment\n    if (is_linear) {\n      y &lt;- 1 + 2 * x + ate * treat + rnorm(n)\n    } else {\n      y &lt;- 1 + 2 * x^2 + ate * treat + rnorm(n)\n    }\n\n    # Naive (no controls)\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # Regression adjustment (always linear: Y ~ D + X)\n    fit &lt;- lm(y ~ treat + x)\n    reg_est &lt;- coef(fit)[\"treat\"]\n\n    # IPW for comparison\n    ps &lt;- fitted(glm(treat ~ x, family = binomial))\n    w &lt;- ifelse(treat == 1, 1 / ps, 1 / (1 - ps))\n    ipw_est &lt;- weighted.mean(y[treat == 1], w[treat == 1]) -\n               weighted.mean(y[treat == 0], w[treat == 0])\n\n    list(x = x, treat = treat, y = y, fit = fit,\n         naive = naive, reg_est = reg_est, ipw_est = ipw_est,\n         ate = ate, is_linear = is_linear)\n  })\n\n  output$scatter_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$y, pch = 16, cex = 0.5,\n         col = ifelse(d$treat == 1, \"#3498db50\", \"#e74c3c50\"),\n         xlab = \"X (confounder)\", ylab = \"Y (outcome)\",\n         main = \"Data + Regression Fit\")\n\n    # Fitted regression lines (always linear)\n    x_seq &lt;- seq(min(d$x), max(d$x), length.out = 200)\n    pred_t &lt;- predict(d$fit,\n      newdata = data.frame(treat = 1, x = x_seq))\n    pred_c &lt;- predict(d$fit,\n      newdata = data.frame(treat = 0, x = x_seq))\n    lines(x_seq, pred_t, col = \"#3498db\", lwd = 2.5)\n    lines(x_seq, pred_c, col = \"#e74c3c\", lwd = 2.5)\n\n    # True conditional means (dashed)\n    if (d$is_linear) {\n      true_t &lt;- 1 + 2 * x_seq + d$ate\n      true_c &lt;- 1 + 2 * x_seq\n    } else {\n      true_t &lt;- 1 + 2 * x_seq^2 + d$ate\n      true_c &lt;- 1 + 2 * x_seq^2\n    }\n    lines(x_seq, true_t, col = \"#3498db\", lwd = 1.5, lty = 3)\n    lines(x_seq, true_c, col = \"#e74c3c\", lwd = 1.5, lty = 3)\n\n    # Gap arrow at x = 0\n    mid_t &lt;- predict(d$fit,\n      newdata = data.frame(treat = 1, x = 0))\n    mid_c &lt;- predict(d$fit,\n      newdata = data.frame(treat = 0, x = 0))\n    arrows(0, mid_c, 0, mid_t, code = 3,\n           col = \"#2c3e50\", lwd = 2, length = 0.08)\n    text(0.3, (mid_t + mid_c) / 2,\n         bquote(hat(tau) == .(round(coef(d$fit)[\"treat\"], 2))),\n         cex = 0.95, font = 2)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.8,\n           legend = c(\"Treated\", \"Control\",\n                      \"Regression fit\", \"True E[Y|X,D]\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray40\", \"gray40\"),\n           pch = c(16, 16, NA, NA),\n           lwd = c(NA, NA, 2.5, 1.5),\n           lty = c(NA, NA, 1, 3))\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 9, 3, 2))\n\n    ests &lt;- c(d$ipw_est, d$reg_est, d$naive)\n    labels &lt;- c(\"IPW\", \"Regression adj.\", \"Naive\")\n    cols &lt;- c(\"#27ae60\", \"#3498db\", \"#e74c3c\")\n\n    xlim &lt;- range(c(ests, d$ate))\n    pad  &lt;- max(diff(xlim) * 0.4, 0.5)\n    xlim &lt;- xlim + c(-pad, pad)\n\n    plot(ests, 1:3, pch = 19, cex = 2, col = cols,\n         xlim = xlim, ylim = c(0.5, 3.5),\n         yaxt = \"n\", xlab = \"Estimated treatment effect\",\n         ylab = \"\", main = \"Estimator Comparison\")\n    axis(2, at = 1:3, labels = labels,\n         las = 1, cex.axis = 0.9)\n\n    abline(v = d$ate, lty = 2, col = \"#2c3e50\", lwd = 2)\n    text(d$ate, 3.45,\n         paste0(\"True ATE = \", d$ate),\n         cex = 0.85, font = 2, col = \"#2c3e50\")\n\n    # Lines from true to estimate\n    segments(d$ate, 1:3, ests, 1:3,\n             col = cols, lwd = 2, lty = 2)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    b_naive &lt;- d$naive - d$ate\n    b_reg   &lt;- d$reg_est - d$ate\n    b_ipw   &lt;- d$ipw_est - d$ate\n    reg_ok  &lt;- abs(b_reg) &lt; abs(b_naive) * 0.5\n    ipw_ok  &lt;- abs(b_ipw) &lt; abs(b_naive) * 0.5\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; &lt;span class='bad'&gt;\",\n        round(d$naive, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_naive, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;Reg. adj:&lt;/b&gt; &lt;span class='\",\n        ifelse(reg_ok, \"good\", \"bad\"), \"'&gt;\",\n        round(d$reg_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_reg, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;IPW:&lt;/b&gt; &lt;span class='\",\n        ifelse(ipw_ok, \"good\", \"bad\"), \"'&gt;\",\n        round(d$ipw_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_ipw, 3), \")\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nLinear outcome, confounding = 1.5: regression adjustment works well. The fitted lines (solid) match the true curves (dotted). The estimate is close to the true ATE.\nSwitch to Quadratic: now the true outcome is \\(Y = 1 + 2X^2 + \\tau D\\), but regression still fits a line. The solid lines miss the curvature (compare to the dotted true curves). The regression estimate is biased — it’s a functional form problem, not a confounding problem.\nQuadratic outcome, compare to IPW: IPW doesn’t model the outcome, so it still works. The green dot (IPW) is close to the true ATE; the blue dot (regression) is not.\nConfounding = 0: treatment is random. All three estimators agree, and functional form doesn’t matter (no confounding to adjust for).\n\n\n\n\nRegression adjustment is the simplest and most common tool. It works great when the outcome model is correctly specified. But it’s fragile to functional form misspecification — fitting a line when the truth is curved introduces bias even when all confounders are observed.\nThis is why alternatives like IPW and entropy balancing exist: they avoid modeling the outcome entirely. And doubly robust methods hedge both bets.",
    "crumbs": [
      "Estimation Tools",
      "Regression Adjustment"
    ]
  },
  {
    "objectID": "regression-adjustment.html#in-stata",
    "href": "regression-adjustment.html#in-stata",
    "title": "Regression Adjustment",
    "section": "In Stata",
    "text": "In Stata\n* Basic regression adjustment\nreg outcome treatment x1 x2\n\n* Stata's teffects version (potential outcomes framework)\nteffects ra (outcome x1 x2) (treatment)\n\n* Lin (2013) fix: interact treatment with demeaned covariates\n* (robust to functional form misspecification)\nforeach var of varlist x1 x2 {\n    sum `var', meanonly\n    gen `var'_dm = `var' - r(mean)\n}\nreg outcome treatment c.treatment#c.(x1_dm x2_dm) x1 x2\nPlain reg outcome treatment x1 x2 and teffects ra give the same point estimate under linearity. The Lin (2013) interaction approach is safer when you’re unsure about functional form.",
    "crumbs": [
      "Estimation Tools",
      "Regression Adjustment"
    ]
  },
  {
    "objectID": "regression-adjustment.html#did-you-know",
    "href": "regression-adjustment.html#did-you-know",
    "title": "Regression Adjustment",
    "section": "Did you know?",
    "text": "Did you know?\n\nRegression adjustment is so natural that most applied researchers use it without thinking of it as a “method.” But Freedman (2008) showed that even in randomized experiments, regression adjustment can be biased in finite samples if the model is wrong — the linear adjustment can introduce bias that wasn’t there. Lin (2013) showed how to fix this: interact the treatment indicator with all covariates, and the resulting estimator is asymptotically unbiased regardless of the true functional form.\nThe vulnerability of regression to functional form is a key motivation for semiparametric and nonparametric methods. Instead of assuming \\(E[Y \\mid X] = \\alpha + \\beta X\\) (which might be wrong), methods like kernel regression or series estimation let the data determine the shape.",
    "crumbs": [
      "Estimation Tools",
      "Regression Adjustment"
    ]
  },
  {
    "objectID": "synth.html",
    "href": "synth.html",
    "title": "Synthetic Control",
    "section": "",
    "text": "You have one treated unit — a state that passed a law, a country hit by a crisis, a company that changed policy. You want to know what would have happened without the treatment. But there’s no single control unit that’s a good comparison.\nThe synthetic control method builds a weighted combination of untreated units that matches the treated unit’s pre-treatment trajectory. That weighted combination — the “synthetic” version — serves as the counterfactual.\n\\[\\hat{Y}_{1t}^{N} = \\sum_{j=2}^{J+1} w_j \\, Y_{jt}\\]\nwhere \\(w_j \\geq 0\\) and \\(\\sum w_j = 1\\). The weights are chosen so that the synthetic unit tracks the treated unit closely before treatment. After treatment, the gap between the treated unit and its synthetic version is the estimated effect.\n\n\nAbadie, Diamond & Hainmueller (2010): California passed Proposition 99 in 1988, a major tobacco control program. No single state is a good comparison — some are too urban, some too rural, some already had anti-smoking laws. The synthetic California is a weighted mix of states (Utah, Nevada, Colorado, Connecticut, Montana…) that together match California’s pre-1988 smoking trend almost exactly. After 1988, actual California diverges sharply below its synthetic version — that gap is the treatment effect.\n\n\n\n\nNo interference / SUTVA: the treatment of the treated unit doesn’t affect the donor units (no spillovers)\nConvex hull: the treated unit’s pre-treatment outcomes can be expressed as a weighted average of the donors — the treated unit isn’t an outlier that no combination of donors can match\nNo anticipation: the treated unit doesn’t change behavior before the treatment date\nCommon factors: treated and donor units are driven by the same underlying factors, just with different loadings — the weights that work pre-treatment continue to work post-treatment\n\n\n\n\n\nBad pre-treatment fit: if the synthetic unit can’t track the treated unit before treatment, you can’t trust the post-treatment gap. There’s no magic — if no combination of donors resembles the treated unit, the method doesn’t work.\nSpillovers: if the treatment affects the donor units too (e.g., smokers move from California to Nevada), the synthetic control is contaminated.\nToo few donors: with very few comparison units, the weights are forced and the match may be poor.\n\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n    .wt-table { font-size: 12px; margin-top: 6px; }\n    .wt-table td { padding: 1px 6px; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"tau_sc\", \"True treatment effect:\",\n                  min = -5, max = 5, value = -3, step = 0.5),\n\n      sliderInput(\"n_donors\", \"Number of donor units:\",\n                  min = 3, max = 15, value = 8, step = 1),\n\n      sliderInput(\"sigma_sc\", \"Noise (SD):\",\n                  min = 0.1, max = 2, value = 0.5, step = 0.1),\n\n      sliderInput(\"treat_time\", \"Treatment period:\",\n                  min = 8, max = 18, value = 12, step = 1),\n\n      actionButton(\"go_sc\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_sc\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"synth_plot\", height = \"500px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go_sc\n    tau   &lt;- input$tau_sc\n    J     &lt;- input$n_donors\n    sigma &lt;- input$sigma_sc\n    T0    &lt;- input$treat_time\n    TT    &lt;- 25\n\n    # Generate donor unit trajectories\n    # Each donor has its own intercept and slope\n    set.seed(NULL)\n    intercepts &lt;- rnorm(J, mean = 10, sd = 2)\n    slopes     &lt;- rnorm(J, mean = 0.3, sd = 0.15)\n    donors &lt;- matrix(NA, nrow = TT, ncol = J)\n    for (j in 1:J) {\n      donors[, j] &lt;- intercepts[j] + slopes[j] * (1:TT) + rnorm(TT, sd = sigma)\n    }\n\n    # True weights (sparse: pick 3-4 donors that matter)\n    n_active &lt;- min(4, J)\n    active &lt;- sample(1:J, n_active)\n    true_w &lt;- rep(0, J)\n    raw &lt;- runif(n_active, 0.1, 1)\n    true_w[active] &lt;- raw / sum(raw)\n\n    # Treated unit = weighted combo of donors + treatment effect after T0\n    treated &lt;- donors %*% true_w + rnorm(TT, sd = sigma * 0.5)\n    treated[(T0 + 1):TT] &lt;- treated[(T0 + 1):TT] + tau\n\n    # Estimate synthetic control weights (OLS on pre-period, constrained to sum to 1)\n    # Simple approach: non-negative least squares via iterative projection\n    pre &lt;- 1:T0\n    Y1_pre &lt;- treated[pre]\n    Y0_pre &lt;- donors[pre, ]\n\n    # Use a simple regression + normalize approach\n    # Unconstrained OLS, then clip negatives and renormalize\n    if (J &lt;= T0) {\n      fit &lt;- lm(Y1_pre ~ Y0_pre - 1)\n      w_hat &lt;- coef(fit)\n    } else {\n      # More donors than periods: use ridge-like approach\n      lambda &lt;- 0.01\n      w_hat &lt;- solve(t(Y0_pre) %*% Y0_pre + lambda * diag(J),\n                     t(Y0_pre) %*% Y1_pre)\n    }\n    w_hat[w_hat &lt; 0] &lt;- 0\n    if (sum(w_hat) &gt; 0) w_hat &lt;- w_hat / sum(w_hat) else w_hat &lt;- rep(1/J, J)\n\n    # Synthetic control trajectory\n    synth &lt;- donors %*% w_hat\n\n    # Estimated effect (post-treatment gap)\n    post &lt;- (T0 + 1):TT\n    gaps &lt;- treated[post] - synth[post]\n    avg_effect &lt;- mean(gaps)\n\n    # Pre-treatment fit (RMSPE)\n    pre_rmspe &lt;- sqrt(mean((treated[pre] - synth[pre])^2))\n\n    list(time = 1:TT, treated = as.numeric(treated),\n         synth = as.numeric(synth), donors = donors,\n         w_hat = w_hat, T0 = T0, tau = tau,\n         avg_effect = avg_effect, pre_rmspe = pre_rmspe, J = J)\n  })\n\n  output$synth_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ylim &lt;- range(c(d$treated, d$synth, d$donors)) + c(-1, 1)\n\n    # Donor units (gray background)\n    plot(d$time, d$donors[, 1], type = \"n\",\n         xlab = \"Time\", ylab = \"Outcome\",\n         main = \"Synthetic Control Method\",\n         ylim = ylim)\n\n    for (j in 1:d$J) {\n      lines(d$time, d$donors[, j], col = adjustcolor(\"gray70\", 0.4), lwd = 0.8)\n    }\n\n    # Synthetic control\n    lines(d$time, d$synth, col = \"#e74c3c\", lwd = 3, lty = 2)\n\n    # Treated unit\n    lines(d$time, d$treated, col = \"#3498db\", lwd = 3)\n\n    # Treatment line\n    abline(v = d$T0 + 0.5, lty = 3, col = \"gray40\", lwd = 1.5)\n    text(d$T0 + 0.5, ylim[2], \"Treatment\", pos = 4, cex = 0.85, col = \"gray40\")\n\n    # Gap shading in post period\n    post &lt;- (d$T0 + 1):length(d$time)\n    polygon(c(d$time[post], rev(d$time[post])),\n            c(d$treated[post], rev(d$synth[post])),\n            col = adjustcolor(\"#27ae60\", 0.15), border = NA)\n\n    # Gap label\n    mid_post &lt;- d$time[round(median(post))]\n    mid_gap &lt;- (d$treated[round(median(post))] + d$synth[round(median(post))]) / 2\n    text(mid_post, mid_gap,\n         paste0(\"Avg gap = \", round(d$avg_effect, 2)),\n         col = \"#27ae60\", font = 2, cex = 0.9)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated unit\", \"Synthetic control\", \"Donor units\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray70\"),\n           lwd = c(3, 3, 1), lty = c(1, 2, 1))\n  })\n\n  output$results_sc &lt;- renderUI({\n    d &lt;- dat()\n\n    # Top weights\n    ord &lt;- order(d$w_hat, decreasing = TRUE)\n    top &lt;- ord[d$w_hat[ord] &gt; 0.01]\n    wt_rows &lt;- paste0(\n      sapply(top, function(j) {\n        paste0(\"&lt;tr&gt;&lt;td&gt;Donor \", j, \"&lt;/td&gt;&lt;td&gt;&lt;b&gt;\",\n               round(d$w_hat[j] * 100), \"%&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;\")\n      }),\n      collapse = \"\"\n    )\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;Avg post-treatment gap:&lt;/b&gt; \",\n        \"&lt;span class='\", ifelse(abs(d$avg_effect - d$tau) &lt; 1, \"good\", \"bad\"), \"'&gt;\",\n        round(d$avg_effect, 2), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Pre-treatment RMSPE:&lt;/b&gt; \", round(d$pre_rmspe, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Weights:&lt;/b&gt;\",\n        \"&lt;table class='wt-table'&gt;\", wt_rows, \"&lt;/table&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDefault settings (effect = -3): the synthetic control (red dashed) tracks the treated unit closely before treatment, then diverges. The green-shaded gap is the estimated effect.\nSet true effect = 0: the two lines should stay close after treatment too. If you see a big gap, it’s noise — this is why pre-treatment fit matters.\nReduce donors to 3: fewer building blocks means a worse pre-treatment fit. The estimate gets noisier.\nIncrease donors to 15: more building blocks, better fit. But watch the weights — most donors get zero weight. The method is naturally sparse.\nMove treatment period later (18): short post-period, harder to judge whether the gap is real or just noise.\nCrank up noise: the pre-treatment fit deteriorates and the post-treatment gap becomes unreliable.",
    "crumbs": [
      "Methods",
      "Synthetic Control"
    ]
  },
  {
    "objectID": "synth.html#the-idea",
    "href": "synth.html#the-idea",
    "title": "Synthetic Control",
    "section": "",
    "text": "You have one treated unit — a state that passed a law, a country hit by a crisis, a company that changed policy. You want to know what would have happened without the treatment. But there’s no single control unit that’s a good comparison.\nThe synthetic control method builds a weighted combination of untreated units that matches the treated unit’s pre-treatment trajectory. That weighted combination — the “synthetic” version — serves as the counterfactual.\n\\[\\hat{Y}_{1t}^{N} = \\sum_{j=2}^{J+1} w_j \\, Y_{jt}\\]\nwhere \\(w_j \\geq 0\\) and \\(\\sum w_j = 1\\). The weights are chosen so that the synthetic unit tracks the treated unit closely before treatment. After treatment, the gap between the treated unit and its synthetic version is the estimated effect.\n\n\nAbadie, Diamond & Hainmueller (2010): California passed Proposition 99 in 1988, a major tobacco control program. No single state is a good comparison — some are too urban, some too rural, some already had anti-smoking laws. The synthetic California is a weighted mix of states (Utah, Nevada, Colorado, Connecticut, Montana…) that together match California’s pre-1988 smoking trend almost exactly. After 1988, actual California diverges sharply below its synthetic version — that gap is the treatment effect.\n\n\n\n\nNo interference / SUTVA: the treatment of the treated unit doesn’t affect the donor units (no spillovers)\nConvex hull: the treated unit’s pre-treatment outcomes can be expressed as a weighted average of the donors — the treated unit isn’t an outlier that no combination of donors can match\nNo anticipation: the treated unit doesn’t change behavior before the treatment date\nCommon factors: treated and donor units are driven by the same underlying factors, just with different loadings — the weights that work pre-treatment continue to work post-treatment\n\n\n\n\n\nBad pre-treatment fit: if the synthetic unit can’t track the treated unit before treatment, you can’t trust the post-treatment gap. There’s no magic — if no combination of donors resembles the treated unit, the method doesn’t work.\nSpillovers: if the treatment affects the donor units too (e.g., smokers move from California to Nevada), the synthetic control is contaminated.\nToo few donors: with very few comparison units, the weights are forced and the match may be poor.\n\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n    .wt-table { font-size: 12px; margin-top: 6px; }\n    .wt-table td { padding: 1px 6px; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"tau_sc\", \"True treatment effect:\",\n                  min = -5, max = 5, value = -3, step = 0.5),\n\n      sliderInput(\"n_donors\", \"Number of donor units:\",\n                  min = 3, max = 15, value = 8, step = 1),\n\n      sliderInput(\"sigma_sc\", \"Noise (SD):\",\n                  min = 0.1, max = 2, value = 0.5, step = 0.1),\n\n      sliderInput(\"treat_time\", \"Treatment period:\",\n                  min = 8, max = 18, value = 12, step = 1),\n\n      actionButton(\"go_sc\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_sc\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"synth_plot\", height = \"500px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go_sc\n    tau   &lt;- input$tau_sc\n    J     &lt;- input$n_donors\n    sigma &lt;- input$sigma_sc\n    T0    &lt;- input$treat_time\n    TT    &lt;- 25\n\n    # Generate donor unit trajectories\n    # Each donor has its own intercept and slope\n    set.seed(NULL)\n    intercepts &lt;- rnorm(J, mean = 10, sd = 2)\n    slopes     &lt;- rnorm(J, mean = 0.3, sd = 0.15)\n    donors &lt;- matrix(NA, nrow = TT, ncol = J)\n    for (j in 1:J) {\n      donors[, j] &lt;- intercepts[j] + slopes[j] * (1:TT) + rnorm(TT, sd = sigma)\n    }\n\n    # True weights (sparse: pick 3-4 donors that matter)\n    n_active &lt;- min(4, J)\n    active &lt;- sample(1:J, n_active)\n    true_w &lt;- rep(0, J)\n    raw &lt;- runif(n_active, 0.1, 1)\n    true_w[active] &lt;- raw / sum(raw)\n\n    # Treated unit = weighted combo of donors + treatment effect after T0\n    treated &lt;- donors %*% true_w + rnorm(TT, sd = sigma * 0.5)\n    treated[(T0 + 1):TT] &lt;- treated[(T0 + 1):TT] + tau\n\n    # Estimate synthetic control weights (OLS on pre-period, constrained to sum to 1)\n    # Simple approach: non-negative least squares via iterative projection\n    pre &lt;- 1:T0\n    Y1_pre &lt;- treated[pre]\n    Y0_pre &lt;- donors[pre, ]\n\n    # Use a simple regression + normalize approach\n    # Unconstrained OLS, then clip negatives and renormalize\n    if (J &lt;= T0) {\n      fit &lt;- lm(Y1_pre ~ Y0_pre - 1)\n      w_hat &lt;- coef(fit)\n    } else {\n      # More donors than periods: use ridge-like approach\n      lambda &lt;- 0.01\n      w_hat &lt;- solve(t(Y0_pre) %*% Y0_pre + lambda * diag(J),\n                     t(Y0_pre) %*% Y1_pre)\n    }\n    w_hat[w_hat &lt; 0] &lt;- 0\n    if (sum(w_hat) &gt; 0) w_hat &lt;- w_hat / sum(w_hat) else w_hat &lt;- rep(1/J, J)\n\n    # Synthetic control trajectory\n    synth &lt;- donors %*% w_hat\n\n    # Estimated effect (post-treatment gap)\n    post &lt;- (T0 + 1):TT\n    gaps &lt;- treated[post] - synth[post]\n    avg_effect &lt;- mean(gaps)\n\n    # Pre-treatment fit (RMSPE)\n    pre_rmspe &lt;- sqrt(mean((treated[pre] - synth[pre])^2))\n\n    list(time = 1:TT, treated = as.numeric(treated),\n         synth = as.numeric(synth), donors = donors,\n         w_hat = w_hat, T0 = T0, tau = tau,\n         avg_effect = avg_effect, pre_rmspe = pre_rmspe, J = J)\n  })\n\n  output$synth_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ylim &lt;- range(c(d$treated, d$synth, d$donors)) + c(-1, 1)\n\n    # Donor units (gray background)\n    plot(d$time, d$donors[, 1], type = \"n\",\n         xlab = \"Time\", ylab = \"Outcome\",\n         main = \"Synthetic Control Method\",\n         ylim = ylim)\n\n    for (j in 1:d$J) {\n      lines(d$time, d$donors[, j], col = adjustcolor(\"gray70\", 0.4), lwd = 0.8)\n    }\n\n    # Synthetic control\n    lines(d$time, d$synth, col = \"#e74c3c\", lwd = 3, lty = 2)\n\n    # Treated unit\n    lines(d$time, d$treated, col = \"#3498db\", lwd = 3)\n\n    # Treatment line\n    abline(v = d$T0 + 0.5, lty = 3, col = \"gray40\", lwd = 1.5)\n    text(d$T0 + 0.5, ylim[2], \"Treatment\", pos = 4, cex = 0.85, col = \"gray40\")\n\n    # Gap shading in post period\n    post &lt;- (d$T0 + 1):length(d$time)\n    polygon(c(d$time[post], rev(d$time[post])),\n            c(d$treated[post], rev(d$synth[post])),\n            col = adjustcolor(\"#27ae60\", 0.15), border = NA)\n\n    # Gap label\n    mid_post &lt;- d$time[round(median(post))]\n    mid_gap &lt;- (d$treated[round(median(post))] + d$synth[round(median(post))]) / 2\n    text(mid_post, mid_gap,\n         paste0(\"Avg gap = \", round(d$avg_effect, 2)),\n         col = \"#27ae60\", font = 2, cex = 0.9)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated unit\", \"Synthetic control\", \"Donor units\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray70\"),\n           lwd = c(3, 3, 1), lty = c(1, 2, 1))\n  })\n\n  output$results_sc &lt;- renderUI({\n    d &lt;- dat()\n\n    # Top weights\n    ord &lt;- order(d$w_hat, decreasing = TRUE)\n    top &lt;- ord[d$w_hat[ord] &gt; 0.01]\n    wt_rows &lt;- paste0(\n      sapply(top, function(j) {\n        paste0(\"&lt;tr&gt;&lt;td&gt;Donor \", j, \"&lt;/td&gt;&lt;td&gt;&lt;b&gt;\",\n               round(d$w_hat[j] * 100), \"%&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;\")\n      }),\n      collapse = \"\"\n    )\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;Avg post-treatment gap:&lt;/b&gt; \",\n        \"&lt;span class='\", ifelse(abs(d$avg_effect - d$tau) &lt; 1, \"good\", \"bad\"), \"'&gt;\",\n        round(d$avg_effect, 2), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Pre-treatment RMSPE:&lt;/b&gt; \", round(d$pre_rmspe, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Weights:&lt;/b&gt;\",\n        \"&lt;table class='wt-table'&gt;\", wt_rows, \"&lt;/table&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDefault settings (effect = -3): the synthetic control (red dashed) tracks the treated unit closely before treatment, then diverges. The green-shaded gap is the estimated effect.\nSet true effect = 0: the two lines should stay close after treatment too. If you see a big gap, it’s noise — this is why pre-treatment fit matters.\nReduce donors to 3: fewer building blocks means a worse pre-treatment fit. The estimate gets noisier.\nIncrease donors to 15: more building blocks, better fit. But watch the weights — most donors get zero weight. The method is naturally sparse.\nMove treatment period later (18): short post-period, harder to judge whether the gap is real or just noise.\nCrank up noise: the pre-treatment fit deteriorates and the post-treatment gap becomes unreliable.",
    "crumbs": [
      "Methods",
      "Synthetic Control"
    ]
  },
  {
    "objectID": "synth.html#inference-placebo-tests",
    "href": "synth.html#inference-placebo-tests",
    "title": "Synthetic Control",
    "section": "Inference: placebo tests",
    "text": "Inference: placebo tests\nWith one treated unit, you can’t do standard inference. Instead, you run placebo tests:\nIn-space placebos. Apply the synthetic control method to each donor unit — pretend it was treated and build a synthetic version from the remaining donors. If the treated unit’s gap is much larger than the placebo gaps, the effect is likely real.\nIn-time placebos. Move the treatment date earlier (to a period when no treatment occurred). If you find a gap in the placebo period, your method is picking up something other than the treatment.\nThese aren’t formal p-values, but they give you a sense of whether the effect is distinguishable from noise.",
    "crumbs": [
      "Methods",
      "Synthetic Control"
    ]
  },
  {
    "objectID": "synth.html#in-stata",
    "href": "synth.html#in-stata",
    "title": "Synthetic Control",
    "section": "In Stata",
    "text": "In Stata\n* Install synthetic control\n* ssc install synth\n\n* Set up panel data\ntsset unit_id year\n\n* Synthetic control\nsynth outcome x1 x2 outcome(1985) outcome(1988) outcome(1990), ///\n      trunit(3) trperiod(1994) fig\n\n* Placebo tests (permute treatment across donors)\n* ssc install synth_runner\nsynth_runner outcome x1 x2, trunit(3) trperiod(1994) gen_vars\nsynth finds weights on donor units that match the treated unit’s pre-treatment trajectory. Always run placebo tests — if the treated unit’s gap isn’t larger than the placebos, the effect isn’t credible.",
    "crumbs": [
      "Methods",
      "Synthetic Control"
    ]
  },
  {
    "objectID": "synth.html#did-you-know",
    "href": "synth.html#did-you-know",
    "title": "Synthetic Control",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe synthetic control method was developed by Abadie & Gardeazabal (2003) to study the economic impact of terrorism in the Basque Country, and formalized by Abadie, Diamond & Hainmueller (2010) in the California tobacco study. It has since become one of the most widely used methods in policy evaluation.\nAthey & Imbens (2017) called synthetic control “arguably the most important innovation in the policy evaluation literature in the last 15 years.”\nThe method works best when you have long pre-treatment panels (many time periods before treatment) and a moderate number of donor units. It struggles with short panels or when no combination of donors can approximate the treated unit.",
    "crumbs": [
      "Methods",
      "Synthetic Control"
    ]
  },
  {
    "objectID": "potential-outcomes.html",
    "href": "potential-outcomes.html",
    "title": "Potential Outcomes & ATE",
    "section": "",
    "text": "For every person, there are two potential outcomes:\n\n\\(Y_i(1)\\): what happens if they get the treatment\n\\(Y_i(0)\\): what happens if they don’t\n\nThe individual treatment effect is \\(\\tau_i = Y_i(1) - Y_i(0)\\). The problem? We only ever observe one of these. A person is either treated or not — never both. The unobserved outcome is the counterfactual.\nThe Average Treatment Effect (ATE) is:\n\\[\\text{ATE} = E[Y(1) - Y(0)]\\]\nSince we can’t observe both for anyone, we need assumptions (like random assignment) to estimate it.\n\n\n\nSUTVA (Stable Unit Treatment Value Assumption): one person’s treatment doesn’t affect another person’s outcome — no interference between units\nConsistency: the observed outcome for a treated person equals their potential outcome under treatment, \\(Y_i = Y_i(1)\\) if treated\nRandom assignment (for unbiased estimation): treatment is independent of potential outcomes — \\(Y(0), Y(1) \\perp D\\)\n\n\n\n\nStart with the only thing we can compute from data — the difference in observed group means:\n\\[E[Y \\mid D=1] - E[Y \\mid D=0]\\]\nBy consistency (\\(Y_i = D_i Y_i(1) + (1 - D_i) Y_i(0)\\)), this equals:\n\\[E[Y(1) \\mid D=1] - E[Y(0) \\mid D=0]\\]\nNow add and subtract \\(E[Y(0) \\mid D=1]\\):\n\\[\\underbrace{E[Y(1) \\mid D=1] - E[Y(0) \\mid D=1]}_{\\text{ATT}} + \\underbrace{E[Y(0) \\mid D=1] - E[Y(0) \\mid D=0]}_{\\text{Selection bias}}\\]\nThis is the fundamental decomposition:\n\\[\\text{Difference in means} = \\text{ATT} + \\text{Selection bias}\\]\nThe selection bias term asks: would the treated group have had different outcomes even without treatment? If sicker people seek treatment, then \\(E[Y(0) \\mid D=1] &lt; E[Y(0) \\mid D=0]\\) — the treated group would have done worse anyway — and the naive comparison underestimates the effect.\n\n\n\n\n\n\nExample: the college earnings premium. People who go to college earn more than those who don’t. But is that because college causes higher earnings, or because the same people who go to college — smart, motivated, from wealthier families — would have earned more anyway? That’s selection bias: \\(E[Y(0) \\mid D=1] &gt; E[Y(0) \\mid D=0]\\). College-goers would have out-earned non-college-goers even without college. The naive earnings gap overestimates the causal effect of college because it bundles the true effect with the selection bias.\n\n\n\n\n\n\nIf \\(D\\) is independent of potential outcomes — \\(Y(0), Y(1) \\perp D\\) — then conditioning on \\(D\\) doesn’t matter:\n\\[E[Y(0) \\mid D=1] = E[Y(0) \\mid D=0] = E[Y(0)]\\]\nThe selection bias term is exactly zero. And the ATT simplifies:\n\\[\\text{ATT} = E[Y(1) \\mid D=1] - E[Y(0) \\mid D=1] = E[Y(1)] - E[Y(0)] = \\text{ATE}\\]\nSo under random assignment:\n\\[\\boxed{\\text{Difference in means} = \\text{ATT} = \\text{ATE}}\\]\nThe naive estimator — just comparing group averages — gives you the causal effect. No modeling, no assumptions about functional form. That’s why randomization is the gold standard.\nWithout randomization, the selection bias term is generally nonzero, and the difference in means ≠ ATE. Every method in this course is a strategy for eliminating or working around that selection bias.\nThe simulation below lets you see both potential outcomes (which you never get in real life), watch selection bias appear, and see how randomization kills it.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Population size:\",\n                  min = 100, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = -2, max = 5, value = 2, step = 0.5),\n\n      selectInput(\"assign\", \"Treatment assignment:\",\n                  choices = c(\"Random (coin flip)\",\n                              \"Self-selection (high Y0 seek treatment)\",\n                              \"Self-selection (low Y0 seek treatment)\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"po_plot\", height = \"400px\")),\n        column(6, plotOutput(\"obs_plot\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    ate &lt;- input$ate\n\n    # Potential outcomes\n    y0 &lt;- rnorm(n, mean = 5, sd = 2)\n    y1 &lt;- y0 + ate + rnorm(n, sd = 0.5)\n\n    # Assignment\n    if (input$assign == \"Random (coin flip)\") {\n      treat &lt;- rbinom(n, 1, 0.5)\n    } else if (input$assign == \"Self-selection (high Y0 seek treatment)\") {\n      prob &lt;- pnorm(y0, mean = mean(y0), sd = sd(y0))\n      treat &lt;- rbinom(n, 1, prob)\n    } else {\n      prob &lt;- 1 - pnorm(y0, mean = mean(y0), sd = sd(y0))\n      treat &lt;- rbinom(n, 1, prob)\n    }\n\n    # Observed outcome\n    y_obs &lt;- ifelse(treat == 1, y1, y0)\n\n    # Estimates\n    naive &lt;- mean(y_obs[treat == 1]) - mean(y_obs[treat == 0])\n    true_ate &lt;- mean(y1 - y0)\n\n    list(y0 = y0, y1 = y1, treat = treat, y_obs = y_obs,\n         naive = naive, true_ate = true_ate, ate = ate)\n  })\n\n  output$po_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$y0, d$y1, pch = 16, cex = 0.6,\n         col = ifelse(d$treat == 1, \"#3498db80\", \"#e74c3c80\"),\n         xlab = \"Y(0) — outcome without treatment\",\n         ylab = \"Y(1) — outcome with treatment\",\n         main = \"Both Potential Outcomes (God's view)\")\n    abline(0, 1, lty = 2, col = \"gray40\", lwd = 1.5)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\", \"45° line (no effect)\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray40\"),\n           pch = c(16, 16, NA), lty = c(NA, NA, 2), lwd = c(NA, NA, 1.5))\n  })\n\n  output$obs_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    grp &lt;- factor(d$treat, labels = c(\"Control\", \"Treated\"))\n    boxplot(d$y_obs ~ grp,\n            col = c(\"#e74c3c40\", \"#3498db40\"),\n            border = c(\"#e74c3c\", \"#3498db\"),\n            main = \"What we actually observe\",\n            ylab = \"Observed Y\", xlab = \"\")\n\n    m0 &lt;- mean(d$y_obs[d$treat == 0])\n    m1 &lt;- mean(d$y_obs[d$treat == 1])\n    points(1:2, c(m0, m1), pch = 18, cex = 2.5, col = c(\"#e74c3c\", \"#3498db\"))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$naive - d$true_ate\n    biased &lt;- abs(bias) &gt; 0.3\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", round(d$true_ate, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive estimate:&lt;/b&gt; \", round(d$naive, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(biased, \"bad\", \"good\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        if (biased) \"&lt;br&gt;&lt;small&gt;Selection bias: treated & control groups aren't comparable.&lt;/small&gt;\"\n        else \"&lt;br&gt;&lt;small&gt;Random assignment makes groups comparable.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nStart with random assignment: the naive estimate is close to the true ATE.\nSwitch to self-selection (high Y₀ seek treatment): people who would have done well anyway are the ones getting treated. The naive estimate is too high — that’s positive selection bias.\nSwitch to self-selection (low Y₀ seek treatment): now the opposite. Sicker people seek treatment, making it look less effective than it is.\nThe left plot shows both potential outcomes — something you never see in real data. That’s the fundamental problem.",
    "crumbs": [
      "Framework",
      "Potential Outcomes & ATE"
    ]
  },
  {
    "objectID": "potential-outcomes.html#the-fundamental-problem-of-causal-inference",
    "href": "potential-outcomes.html#the-fundamental-problem-of-causal-inference",
    "title": "Potential Outcomes & ATE",
    "section": "",
    "text": "For every person, there are two potential outcomes:\n\n\\(Y_i(1)\\): what happens if they get the treatment\n\\(Y_i(0)\\): what happens if they don’t\n\nThe individual treatment effect is \\(\\tau_i = Y_i(1) - Y_i(0)\\). The problem? We only ever observe one of these. A person is either treated or not — never both. The unobserved outcome is the counterfactual.\nThe Average Treatment Effect (ATE) is:\n\\[\\text{ATE} = E[Y(1) - Y(0)]\\]\nSince we can’t observe both for anyone, we need assumptions (like random assignment) to estimate it.\n\n\n\nSUTVA (Stable Unit Treatment Value Assumption): one person’s treatment doesn’t affect another person’s outcome — no interference between units\nConsistency: the observed outcome for a treated person equals their potential outcome under treatment, \\(Y_i = Y_i(1)\\) if treated\nRandom assignment (for unbiased estimation): treatment is independent of potential outcomes — \\(Y(0), Y(1) \\perp D\\)\n\n\n\n\nStart with the only thing we can compute from data — the difference in observed group means:\n\\[E[Y \\mid D=1] - E[Y \\mid D=0]\\]\nBy consistency (\\(Y_i = D_i Y_i(1) + (1 - D_i) Y_i(0)\\)), this equals:\n\\[E[Y(1) \\mid D=1] - E[Y(0) \\mid D=0]\\]\nNow add and subtract \\(E[Y(0) \\mid D=1]\\):\n\\[\\underbrace{E[Y(1) \\mid D=1] - E[Y(0) \\mid D=1]}_{\\text{ATT}} + \\underbrace{E[Y(0) \\mid D=1] - E[Y(0) \\mid D=0]}_{\\text{Selection bias}}\\]\nThis is the fundamental decomposition:\n\\[\\text{Difference in means} = \\text{ATT} + \\text{Selection bias}\\]\nThe selection bias term asks: would the treated group have had different outcomes even without treatment? If sicker people seek treatment, then \\(E[Y(0) \\mid D=1] &lt; E[Y(0) \\mid D=0]\\) — the treated group would have done worse anyway — and the naive comparison underestimates the effect.\n\n\n\n\n\n\nExample: the college earnings premium. People who go to college earn more than those who don’t. But is that because college causes higher earnings, or because the same people who go to college — smart, motivated, from wealthier families — would have earned more anyway? That’s selection bias: \\(E[Y(0) \\mid D=1] &gt; E[Y(0) \\mid D=0]\\). College-goers would have out-earned non-college-goers even without college. The naive earnings gap overestimates the causal effect of college because it bundles the true effect with the selection bias.\n\n\n\n\n\n\nIf \\(D\\) is independent of potential outcomes — \\(Y(0), Y(1) \\perp D\\) — then conditioning on \\(D\\) doesn’t matter:\n\\[E[Y(0) \\mid D=1] = E[Y(0) \\mid D=0] = E[Y(0)]\\]\nThe selection bias term is exactly zero. And the ATT simplifies:\n\\[\\text{ATT} = E[Y(1) \\mid D=1] - E[Y(0) \\mid D=1] = E[Y(1)] - E[Y(0)] = \\text{ATE}\\]\nSo under random assignment:\n\\[\\boxed{\\text{Difference in means} = \\text{ATT} = \\text{ATE}}\\]\nThe naive estimator — just comparing group averages — gives you the causal effect. No modeling, no assumptions about functional form. That’s why randomization is the gold standard.\nWithout randomization, the selection bias term is generally nonzero, and the difference in means ≠ ATE. Every method in this course is a strategy for eliminating or working around that selection bias.\nThe simulation below lets you see both potential outcomes (which you never get in real life), watch selection bias appear, and see how randomization kills it.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Population size:\",\n                  min = 100, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = -2, max = 5, value = 2, step = 0.5),\n\n      selectInput(\"assign\", \"Treatment assignment:\",\n                  choices = c(\"Random (coin flip)\",\n                              \"Self-selection (high Y0 seek treatment)\",\n                              \"Self-selection (low Y0 seek treatment)\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"po_plot\", height = \"400px\")),\n        column(6, plotOutput(\"obs_plot\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    ate &lt;- input$ate\n\n    # Potential outcomes\n    y0 &lt;- rnorm(n, mean = 5, sd = 2)\n    y1 &lt;- y0 + ate + rnorm(n, sd = 0.5)\n\n    # Assignment\n    if (input$assign == \"Random (coin flip)\") {\n      treat &lt;- rbinom(n, 1, 0.5)\n    } else if (input$assign == \"Self-selection (high Y0 seek treatment)\") {\n      prob &lt;- pnorm(y0, mean = mean(y0), sd = sd(y0))\n      treat &lt;- rbinom(n, 1, prob)\n    } else {\n      prob &lt;- 1 - pnorm(y0, mean = mean(y0), sd = sd(y0))\n      treat &lt;- rbinom(n, 1, prob)\n    }\n\n    # Observed outcome\n    y_obs &lt;- ifelse(treat == 1, y1, y0)\n\n    # Estimates\n    naive &lt;- mean(y_obs[treat == 1]) - mean(y_obs[treat == 0])\n    true_ate &lt;- mean(y1 - y0)\n\n    list(y0 = y0, y1 = y1, treat = treat, y_obs = y_obs,\n         naive = naive, true_ate = true_ate, ate = ate)\n  })\n\n  output$po_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$y0, d$y1, pch = 16, cex = 0.6,\n         col = ifelse(d$treat == 1, \"#3498db80\", \"#e74c3c80\"),\n         xlab = \"Y(0) — outcome without treatment\",\n         ylab = \"Y(1) — outcome with treatment\",\n         main = \"Both Potential Outcomes (God's view)\")\n    abline(0, 1, lty = 2, col = \"gray40\", lwd = 1.5)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\", \"45° line (no effect)\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray40\"),\n           pch = c(16, 16, NA), lty = c(NA, NA, 2), lwd = c(NA, NA, 1.5))\n  })\n\n  output$obs_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    grp &lt;- factor(d$treat, labels = c(\"Control\", \"Treated\"))\n    boxplot(d$y_obs ~ grp,\n            col = c(\"#e74c3c40\", \"#3498db40\"),\n            border = c(\"#e74c3c\", \"#3498db\"),\n            main = \"What we actually observe\",\n            ylab = \"Observed Y\", xlab = \"\")\n\n    m0 &lt;- mean(d$y_obs[d$treat == 0])\n    m1 &lt;- mean(d$y_obs[d$treat == 1])\n    points(1:2, c(m0, m1), pch = 18, cex = 2.5, col = c(\"#e74c3c\", \"#3498db\"))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$naive - d$true_ate\n    biased &lt;- abs(bias) &gt; 0.3\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", round(d$true_ate, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive estimate:&lt;/b&gt; \", round(d$naive, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(biased, \"bad\", \"good\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        if (biased) \"&lt;br&gt;&lt;small&gt;Selection bias: treated & control groups aren't comparable.&lt;/small&gt;\"\n        else \"&lt;br&gt;&lt;small&gt;Random assignment makes groups comparable.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nStart with random assignment: the naive estimate is close to the true ATE.\nSwitch to self-selection (high Y₀ seek treatment): people who would have done well anyway are the ones getting treated. The naive estimate is too high — that’s positive selection bias.\nSwitch to self-selection (low Y₀ seek treatment): now the opposite. Sicker people seek treatment, making it look less effective than it is.\nThe left plot shows both potential outcomes — something you never see in real data. That’s the fundamental problem.",
    "crumbs": [
      "Framework",
      "Potential Outcomes & ATE"
    ]
  },
  {
    "objectID": "potential-outcomes.html#what-if-you-cant-randomize",
    "href": "potential-outcomes.html#what-if-you-cant-randomize",
    "title": "Potential Outcomes & ATE",
    "section": "What if you can’t randomize?",
    "text": "What if you can’t randomize?\nIn a true experiment (RCT):\n\nYou randomly assign people to treatment vs control\nBecause it’s random, the two groups are identical on average — so any difference in outcomes must be caused by the treatment\n\nBut most questions in economics, policy, and social science can’t be answered with an RCT. You can’t randomly assign poverty, or force some cities to build highways and others not to. So how do you estimate causal effects?\n\nNatural experiments\nA natural experiment is when something in the real world — a policy change, a rule, a geographic boundary, a disaster — creates treatment and control groups that are as-if randomly assigned. Nobody designed it as an experiment, but the logic is the same.\nExample: Say the government announced “all tracts in counties with food desert score &gt; X get a healthy food program.” Tracts at X+1 vs X−1 didn’t choose to be on different sides of that line — the cutoff did it for them. So comparing those tracts is like comparing treatment and control in an experiment.\n\nThe word “natural” = it happened in the real world, not in a lab.\nThe word “experiment” = it created as-if random variation in who got treated.\n\nThe whole point is to get around the selection bias problem the simulation above shows. If people (or firms, or cities) choose their treatment status, the naive comparison is biased. A natural experiment gives you variation that the units didn’t choose.\n\n\nThe rest of this course\nEvery method in this course is a strategy for exploiting natural experiments or otherwise correcting for selection bias:\n\n\n\nMethod\nThe idea\n\n\n\n\nSelection on Observables\nCondition on everything that drives both treatment and outcome\n\n\nIPW\nReweight observations so treated and control look similar on observables\n\n\nEntropy Balancing\nDirectly balance covariates between groups without modeling the propensity score\n\n\nDifference-in-Differences\nCompare changes over time between treated and control groups\n\n\nInstrumental Variables\nUse exogenous variation to isolate the causal effect\n\n\nRegression Discontinuity\nExploit a cutoff rule as a natural experiment\n\n\nSynthetic Control\nBuild a weighted counterfactual from donor units\n\n\n\nEach one makes a different assumption about why the comparison is valid. The art of causal inference is choosing the right method for your setting — and being honest about when the assumptions fail.",
    "crumbs": [
      "Framework",
      "Potential Outcomes & ATE"
    ]
  },
  {
    "objectID": "potential-outcomes.html#in-stata",
    "href": "potential-outcomes.html#in-stata",
    "title": "Potential Outcomes & ATE",
    "section": "In Stata",
    "text": "In Stata\nThe simplest causal estimate — a difference in means under random assignment:\n* Difference in means (t-test)\nttest outcome, by(treatment)\n\n* Same thing as a regression (identical point estimate)\nreg outcome treatment\n\n* With controls (gains precision in an RCT, required under SOO)\nreg outcome treatment x1 x2 x3\nWith random assignment, reg outcome treatment gives you an unbiased estimate of the ATE. The coefficient on treatment is the causal effect. Adding controls doesn’t change the estimate (in expectation) but can shrink the standard errors.",
    "crumbs": [
      "Framework",
      "Potential Outcomes & ATE"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Causal Inference",
    "section": "",
    "text": "Causal inference methods — each topic pairs explanation with an interactive simulation you can run in the browser.\nBuilds on: Statistical Inference",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#framework",
    "href": "index.html#framework",
    "title": "Applied Causal Inference",
    "section": "Framework",
    "text": "Framework\n\nPotential Outcomes & ATE — The framework behind all causal questions\nIdentification vs Estimation — The distinction that organizes everything",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#methods",
    "href": "index.html#methods",
    "title": "Applied Causal Inference",
    "section": "Methods",
    "text": "Methods\n\nSelection on Observables — When conditioning on X is enough\nDifference-in-Differences — Parallel trends & treatment effects\nInstrumental Variables — Exogenous variation to isolate causal effects\nRegression Discontinuity — Cutoff rules as natural experiments\nSynthetic Control — Building a counterfactual from weighted donors",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#panel-data",
    "href": "index.html#panel-data",
    "title": "Applied Causal Inference",
    "section": "Panel Data",
    "text": "Panel Data\n\nFixed vs Random Effects — Within vs between variation, the Hausman test, and when each estimator works",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#estimation-tools",
    "href": "index.html#estimation-tools",
    "title": "Applied Causal Inference",
    "section": "Estimation Tools",
    "text": "Estimation Tools\nTools that can be paired with different research designs.\n\nRegression Adjustment — Model the outcome, adjust for confounders\nMatching — Pair treated and control units on covariates\nInverse Probability Weighting — Reweighting to balance treated and control\nEntropy Balancing — Exact moment balancing without a propensity score model\nDoubly Robust — Combine outcome model + propensity score for double protection",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "rdd.html",
    "href": "rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "Some treatments are assigned by a cutoff rule: you get a scholarship if your test score is above 80, you qualify for a program if your income is below $30k, you win an election if your vote share exceeds 50%.\nRight around the cutoff, people just above and just below are nearly identical — they differ by a fraction of a point. The cutoff creates near-random assignment in a narrow window. That’s the identifying variation.\n\\[\\tau_{RDD} = \\lim_{x \\downarrow c} E[Y \\mid X = x] - \\lim_{x \\uparrow c} E[Y \\mid X = x]\\]\nThe treatment effect is the jump in the outcome at the cutoff. If the outcome is smooth everywhere except at the cutoff, that discontinuity is the causal effect.\n\n\n\nSharp RDD: treatment is a deterministic function of the running variable. Score \\(\\geq\\) 80 → treated, period. Everyone complies.\nFuzzy RDD: the cutoff changes the probability of treatment, but not perfectly. Some people above the cutoff don’t take treatment, some below do. This is essentially an IV problem — the cutoff is the instrument.\n\n\n\n\n\n\n\nWhy fuzzy RDD is IV. Map it directly: the instrument \\(Z\\) is “above the cutoff” (yes/no), the endogenous variable \\(D\\) is actually receiving treatment, and \\(Y\\) is your outcome. The cutoff satisfies the IV assumptions — it’s relevant (shifts treatment probability), excludable (scoring 80 vs 79 doesn’t directly change outcomes), and monotone (crossing the cutoff doesn’t make anyone less likely to be treated). The fuzzy RDD estimate is the Wald estimator: \\(\\tau = \\frac{\\text{jump in outcome at cutoff}}{\\text{jump in treatment probability at cutoff}}\\) — reduced form divided by first stage. Like IV, this estimates a LATE for compliers (people whose treatment status actually changes at the cutoff).\n\n\n\n\n\n\n\nContinuity: the potential outcomes \\(E[Y(0) \\mid X = x]\\) and \\(E[Y(1) \\mid X = x]\\) are continuous at the cutoff — no other jump happens at exactly that point\nNo manipulation: units cannot precisely control their running variable to sort across the cutoff. If they can, the “as good as random” logic breaks.\nLocal randomization: units just above and just below the cutoff are comparable on all observed and unobserved characteristics\nCorrect functional form: the relationship between the running variable and outcome is correctly modeled within the bandwidth window\n\n\n\n\n\nManipulation: if people can precisely control their score to land just above or below the cutoff, the “as good as random” logic breaks. Check for bunching at the cutoff (McCrary test).\nWrong functional form: if you fit a straight line but the true relationship is curved, you might mistake curvature for a jump. Use local polynomials and check sensitivity to bandwidth.\n\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"tau\", \"True treatment effect:\",\n                  min = 0, max = 5, value = 2, step = 0.25),\n\n      sliderInput(\"sigma\", \"Noise (SD):\",\n                  min = 0.5, max = 4, value = 1.5, step = 0.25),\n\n      sliderInput(\"bw\", \"Bandwidth around cutoff:\",\n                  min = 0.05, max = 0.5, value = 0.2, step = 0.05),\n\n      selectInput(\"curve\", \"True relationship:\",\n                  choices = c(\"Linear\" = \"linear\",\n                              \"Quadratic\" = \"quad\",\n                              \"Flat\" = \"flat\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"rdd_plot\", height = \"480px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n\n    tau   &lt;- input$tau\n    sigma &lt;- input$sigma\n    bw    &lt;- input$bw\n    curve &lt;- input$curve\n\n    # Running variable: uniform on [0, 1], cutoff at 0.5\n    x &lt;- runif(n)\n    cutoff &lt;- 0.5\n    treat &lt;- as.numeric(x &gt;= cutoff)\n\n    # Potential outcome (smooth function of x)\n    if (curve == \"linear\") {\n      mu &lt;- 2 + 1.5 * x\n    } else if (curve == \"quad\") {\n      mu &lt;- 2 + 3 * (x - 0.5)^2\n    } else {\n      mu &lt;- rep(3, n)\n    }\n\n    y &lt;- mu + tau * treat + rnorm(n, sd = sigma)\n\n    # Local linear regression within bandwidth\n    in_bw &lt;- abs(x - cutoff) &lt;= bw\n    x_bw &lt;- x[in_bw]\n    y_bw &lt;- y[in_bw]\n    t_bw &lt;- treat[in_bw]\n\n    # Separate regressions left and right\n    left  &lt;- x_bw &lt; cutoff\n    right &lt;- x_bw &gt;= cutoff\n\n    if (sum(left) &gt; 2 && sum(right) &gt; 2) {\n      fit_l &lt;- lm(y_bw[left] ~ x_bw[left])\n      fit_r &lt;- lm(y_bw[right] ~ x_bw[right])\n\n      # Predicted values at cutoff\n      pred_l &lt;- coef(fit_l)[1] + coef(fit_l)[2] * cutoff\n      pred_r &lt;- coef(fit_r)[1] + coef(fit_r)[2] * cutoff\n\n      rdd_est &lt;- pred_r - pred_l\n\n      # Fitted lines for plotting\n      xseq_l &lt;- seq(cutoff - bw, cutoff, length.out = 100)\n      xseq_r &lt;- seq(cutoff, cutoff + bw, length.out = 100)\n      yhat_l &lt;- coef(fit_l)[1] + coef(fit_l)[2] * xseq_l\n      yhat_r &lt;- coef(fit_r)[1] + coef(fit_r)[2] * xseq_r\n    } else {\n      rdd_est &lt;- NA\n      xseq_l &lt;- xseq_r &lt;- yhat_l &lt;- yhat_r &lt;- NULL\n      pred_l &lt;- pred_r &lt;- NA\n    }\n\n    list(x = x, y = y, treat = treat, cutoff = cutoff,\n         bw = bw, in_bw = in_bw, rdd_est = rdd_est,\n         tau = tau, sigma = sigma,\n         xseq_l = xseq_l, xseq_r = xseq_r,\n         yhat_l = yhat_l, yhat_r = yhat_r,\n         pred_l = pred_l, pred_r = pred_r)\n  })\n\n  output$rdd_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Color by treatment\n    cols &lt;- ifelse(d$treat == 1, adjustcolor(\"#3498db\", 0.25),\n                   adjustcolor(\"#e74c3c\", 0.25))\n\n    # Dim points outside bandwidth\n    cols[!d$in_bw] &lt;- adjustcolor(\"gray70\", 0.15)\n\n    plot(d$x, d$y, pch = 16, cex = 0.5, col = cols,\n         xlab = \"Running variable (X)\", ylab = \"Outcome (Y)\",\n         main = \"Regression Discontinuity Design\")\n\n    # Cutoff line\n    abline(v = d$cutoff, lty = 2, col = \"gray40\", lwd = 1.5)\n\n    # Bandwidth shading\n    rect(d$cutoff - d$bw, par(\"usr\")[3],\n         d$cutoff + d$bw, par(\"usr\")[4],\n         col = adjustcolor(\"#f39c12\", 0.08), border = NA)\n\n    # Local linear fits\n    if (!is.null(d$xseq_l)) {\n      lines(d$xseq_l, d$yhat_l, col = \"#e74c3c\", lwd = 3)\n      lines(d$xseq_r, d$yhat_r, col = \"#3498db\", lwd = 3)\n\n      # Jump arrow\n      arrows(d$cutoff + 0.01, d$pred_l, d$cutoff + 0.01, d$pred_r,\n             code = 3, lwd = 2.5, col = \"#27ae60\", length = 0.1)\n\n      text(d$cutoff + 0.03,\n           (d$pred_l + d$pred_r) / 2,\n           paste0(\"Jump = \", round(d$rdd_est, 2)),\n           col = \"#27ae60\", cex = 0.95, adj = 0, font = 2)\n    }\n\n    text(d$cutoff, par(\"usr\")[4] * 0.98, \"Cutoff\",\n         col = \"gray40\", cex = 0.8, pos = 4)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Control (below cutoff)\", \"Treated (above cutoff)\",\n                      \"Estimation window\"),\n           pch = c(16, 16, 15),\n           col = c(\"#e74c3c\", \"#3498db\", adjustcolor(\"#f39c12\", 0.3)))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    if (is.na(d$rdd_est)) {\n      return(tags$div(class = \"stats-box\",\n        HTML(\"&lt;b&gt;Not enough observations in bandwidth.&lt;/b&gt; Widen it.\")))\n    }\n\n    bias &lt;- d$rdd_est - d$tau\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;RDD estimate:&lt;/b&gt; \", round(d$rdd_est, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(abs(bias) &lt; 0.3, \"good\", \"bad\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;small&gt;Bandwidth: &plusmn;\", d$bw, \" around cutoff&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDefault settings: the jump at the cutoff is clear. The RDD estimate is close to the true effect.\nNarrow the bandwidth (0.05): fewer observations, noisier estimate — but less bias from misspecification. Widen it (0.5): more data, more precise, but you’re using observations far from the cutoff.\nSwitch to quadratic: with a narrow bandwidth, the local linear fit still works fine. But widen the bandwidth with a quadratic DGP and the estimate gets biased — the line can’t capture the curve.\nSet true effect = 0: there should be no visible jump. If you see one, it’s noise (or a bad bandwidth choice).\nCrank up noise: the jump gets harder to see, and you need more data or a wider bandwidth to detect it. This is the bias-variance tradeoff of bandwidth selection.",
    "crumbs": [
      "Methods",
      "Regression Discontinuity"
    ]
  },
  {
    "objectID": "rdd.html#the-idea",
    "href": "rdd.html#the-idea",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "Some treatments are assigned by a cutoff rule: you get a scholarship if your test score is above 80, you qualify for a program if your income is below $30k, you win an election if your vote share exceeds 50%.\nRight around the cutoff, people just above and just below are nearly identical — they differ by a fraction of a point. The cutoff creates near-random assignment in a narrow window. That’s the identifying variation.\n\\[\\tau_{RDD} = \\lim_{x \\downarrow c} E[Y \\mid X = x] - \\lim_{x \\uparrow c} E[Y \\mid X = x]\\]\nThe treatment effect is the jump in the outcome at the cutoff. If the outcome is smooth everywhere except at the cutoff, that discontinuity is the causal effect.\n\n\n\nSharp RDD: treatment is a deterministic function of the running variable. Score \\(\\geq\\) 80 → treated, period. Everyone complies.\nFuzzy RDD: the cutoff changes the probability of treatment, but not perfectly. Some people above the cutoff don’t take treatment, some below do. This is essentially an IV problem — the cutoff is the instrument.\n\n\n\n\n\n\n\nWhy fuzzy RDD is IV. Map it directly: the instrument \\(Z\\) is “above the cutoff” (yes/no), the endogenous variable \\(D\\) is actually receiving treatment, and \\(Y\\) is your outcome. The cutoff satisfies the IV assumptions — it’s relevant (shifts treatment probability), excludable (scoring 80 vs 79 doesn’t directly change outcomes), and monotone (crossing the cutoff doesn’t make anyone less likely to be treated). The fuzzy RDD estimate is the Wald estimator: \\(\\tau = \\frac{\\text{jump in outcome at cutoff}}{\\text{jump in treatment probability at cutoff}}\\) — reduced form divided by first stage. Like IV, this estimates a LATE for compliers (people whose treatment status actually changes at the cutoff).\n\n\n\n\n\n\n\nContinuity: the potential outcomes \\(E[Y(0) \\mid X = x]\\) and \\(E[Y(1) \\mid X = x]\\) are continuous at the cutoff — no other jump happens at exactly that point\nNo manipulation: units cannot precisely control their running variable to sort across the cutoff. If they can, the “as good as random” logic breaks.\nLocal randomization: units just above and just below the cutoff are comparable on all observed and unobserved characteristics\nCorrect functional form: the relationship between the running variable and outcome is correctly modeled within the bandwidth window\n\n\n\n\n\nManipulation: if people can precisely control their score to land just above or below the cutoff, the “as good as random” logic breaks. Check for bunching at the cutoff (McCrary test).\nWrong functional form: if you fit a straight line but the true relationship is curved, you might mistake curvature for a jump. Use local polynomials and check sensitivity to bandwidth.\n\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"tau\", \"True treatment effect:\",\n                  min = 0, max = 5, value = 2, step = 0.25),\n\n      sliderInput(\"sigma\", \"Noise (SD):\",\n                  min = 0.5, max = 4, value = 1.5, step = 0.25),\n\n      sliderInput(\"bw\", \"Bandwidth around cutoff:\",\n                  min = 0.05, max = 0.5, value = 0.2, step = 0.05),\n\n      selectInput(\"curve\", \"True relationship:\",\n                  choices = c(\"Linear\" = \"linear\",\n                              \"Quadratic\" = \"quad\",\n                              \"Flat\" = \"flat\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"rdd_plot\", height = \"480px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n\n    tau   &lt;- input$tau\n    sigma &lt;- input$sigma\n    bw    &lt;- input$bw\n    curve &lt;- input$curve\n\n    # Running variable: uniform on [0, 1], cutoff at 0.5\n    x &lt;- runif(n)\n    cutoff &lt;- 0.5\n    treat &lt;- as.numeric(x &gt;= cutoff)\n\n    # Potential outcome (smooth function of x)\n    if (curve == \"linear\") {\n      mu &lt;- 2 + 1.5 * x\n    } else if (curve == \"quad\") {\n      mu &lt;- 2 + 3 * (x - 0.5)^2\n    } else {\n      mu &lt;- rep(3, n)\n    }\n\n    y &lt;- mu + tau * treat + rnorm(n, sd = sigma)\n\n    # Local linear regression within bandwidth\n    in_bw &lt;- abs(x - cutoff) &lt;= bw\n    x_bw &lt;- x[in_bw]\n    y_bw &lt;- y[in_bw]\n    t_bw &lt;- treat[in_bw]\n\n    # Separate regressions left and right\n    left  &lt;- x_bw &lt; cutoff\n    right &lt;- x_bw &gt;= cutoff\n\n    if (sum(left) &gt; 2 && sum(right) &gt; 2) {\n      fit_l &lt;- lm(y_bw[left] ~ x_bw[left])\n      fit_r &lt;- lm(y_bw[right] ~ x_bw[right])\n\n      # Predicted values at cutoff\n      pred_l &lt;- coef(fit_l)[1] + coef(fit_l)[2] * cutoff\n      pred_r &lt;- coef(fit_r)[1] + coef(fit_r)[2] * cutoff\n\n      rdd_est &lt;- pred_r - pred_l\n\n      # Fitted lines for plotting\n      xseq_l &lt;- seq(cutoff - bw, cutoff, length.out = 100)\n      xseq_r &lt;- seq(cutoff, cutoff + bw, length.out = 100)\n      yhat_l &lt;- coef(fit_l)[1] + coef(fit_l)[2] * xseq_l\n      yhat_r &lt;- coef(fit_r)[1] + coef(fit_r)[2] * xseq_r\n    } else {\n      rdd_est &lt;- NA\n      xseq_l &lt;- xseq_r &lt;- yhat_l &lt;- yhat_r &lt;- NULL\n      pred_l &lt;- pred_r &lt;- NA\n    }\n\n    list(x = x, y = y, treat = treat, cutoff = cutoff,\n         bw = bw, in_bw = in_bw, rdd_est = rdd_est,\n         tau = tau, sigma = sigma,\n         xseq_l = xseq_l, xseq_r = xseq_r,\n         yhat_l = yhat_l, yhat_r = yhat_r,\n         pred_l = pred_l, pred_r = pred_r)\n  })\n\n  output$rdd_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Color by treatment\n    cols &lt;- ifelse(d$treat == 1, adjustcolor(\"#3498db\", 0.25),\n                   adjustcolor(\"#e74c3c\", 0.25))\n\n    # Dim points outside bandwidth\n    cols[!d$in_bw] &lt;- adjustcolor(\"gray70\", 0.15)\n\n    plot(d$x, d$y, pch = 16, cex = 0.5, col = cols,\n         xlab = \"Running variable (X)\", ylab = \"Outcome (Y)\",\n         main = \"Regression Discontinuity Design\")\n\n    # Cutoff line\n    abline(v = d$cutoff, lty = 2, col = \"gray40\", lwd = 1.5)\n\n    # Bandwidth shading\n    rect(d$cutoff - d$bw, par(\"usr\")[3],\n         d$cutoff + d$bw, par(\"usr\")[4],\n         col = adjustcolor(\"#f39c12\", 0.08), border = NA)\n\n    # Local linear fits\n    if (!is.null(d$xseq_l)) {\n      lines(d$xseq_l, d$yhat_l, col = \"#e74c3c\", lwd = 3)\n      lines(d$xseq_r, d$yhat_r, col = \"#3498db\", lwd = 3)\n\n      # Jump arrow\n      arrows(d$cutoff + 0.01, d$pred_l, d$cutoff + 0.01, d$pred_r,\n             code = 3, lwd = 2.5, col = \"#27ae60\", length = 0.1)\n\n      text(d$cutoff + 0.03,\n           (d$pred_l + d$pred_r) / 2,\n           paste0(\"Jump = \", round(d$rdd_est, 2)),\n           col = \"#27ae60\", cex = 0.95, adj = 0, font = 2)\n    }\n\n    text(d$cutoff, par(\"usr\")[4] * 0.98, \"Cutoff\",\n         col = \"gray40\", cex = 0.8, pos = 4)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Control (below cutoff)\", \"Treated (above cutoff)\",\n                      \"Estimation window\"),\n           pch = c(16, 16, 15),\n           col = c(\"#e74c3c\", \"#3498db\", adjustcolor(\"#f39c12\", 0.3)))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    if (is.na(d$rdd_est)) {\n      return(tags$div(class = \"stats-box\",\n        HTML(\"&lt;b&gt;Not enough observations in bandwidth.&lt;/b&gt; Widen it.\")))\n    }\n\n    bias &lt;- d$rdd_est - d$tau\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;RDD estimate:&lt;/b&gt; \", round(d$rdd_est, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(abs(bias) &lt; 0.3, \"good\", \"bad\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;small&gt;Bandwidth: &plusmn;\", d$bw, \" around cutoff&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDefault settings: the jump at the cutoff is clear. The RDD estimate is close to the true effect.\nNarrow the bandwidth (0.05): fewer observations, noisier estimate — but less bias from misspecification. Widen it (0.5): more data, more precise, but you’re using observations far from the cutoff.\nSwitch to quadratic: with a narrow bandwidth, the local linear fit still works fine. But widen the bandwidth with a quadratic DGP and the estimate gets biased — the line can’t capture the curve.\nSet true effect = 0: there should be no visible jump. If you see one, it’s noise (or a bad bandwidth choice).\nCrank up noise: the jump gets harder to see, and you need more data or a wider bandwidth to detect it. This is the bias-variance tradeoff of bandwidth selection.",
    "crumbs": [
      "Methods",
      "Regression Discontinuity"
    ]
  },
  {
    "objectID": "rdd.html#the-bandwidth-tradeoff",
    "href": "rdd.html#the-bandwidth-tradeoff",
    "title": "Regression Discontinuity",
    "section": "The bandwidth tradeoff",
    "text": "The bandwidth tradeoff\nBandwidth is the central tuning parameter in RDD:\n\n\n\n\n\n\n\n\n\nNarrow bandwidth\nWide bandwidth\n\n\n\n\nBias\nLow — only using near-identical units\nHigh — far-away units may differ systematically\n\n\nVariance\nHigh — few observations\nLow — more data\n\n\nRisk\nNoisy, imprecise estimate\nPrecise but potentially wrong\n\n\n\nIn practice, researchers use data-driven bandwidth selectors (Imbens & Kalyanaraman 2012, Calonico, Cattaneo & Titiunik 2014) that optimize this tradeoff. You should always check that your results are robust to different bandwidth choices.",
    "crumbs": [
      "Methods",
      "Regression Discontinuity"
    ]
  },
  {
    "objectID": "rdd.html#in-stata",
    "href": "rdd.html#in-stata",
    "title": "Regression Discontinuity",
    "section": "In Stata",
    "text": "In Stata\n* Install rdrobust (Cattaneo, Idrobo & Titiunik)\n* ssc install rdrobust\n\n* Sharp RDD with optimal bandwidth\nrdrobust outcome running_var, c(0)\n\n* RDD plot\nrdplot outcome running_var, c(0)\n\n* Check for manipulation of the running variable\nrddensity running_var, c(0)\n\n* Fuzzy RDD (treatment is probabilistic at cutoff)\nrdrobust outcome running_var, c(0) fuzzy(treatment)\nrdrobust handles bandwidth selection, bias correction, and robust standard errors automatically. Always show the rdplot — if you can’t see the jump visually, be skeptical of the estimate.",
    "crumbs": [
      "Methods",
      "Regression Discontinuity"
    ]
  },
  {
    "objectID": "rdd.html#did-you-know",
    "href": "rdd.html#did-you-know",
    "title": "Regression Discontinuity",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe RDD idea goes back to Thistlethwaite & Campbell (1960), who studied the effect of merit scholarships on career outcomes using the score cutoff. It was largely ignored for decades until economists rediscovered it in the late 1990s.\nLee (2008) used RDD to study the incumbency advantage in US elections — candidates who barely win vs barely lose. This paper helped establish RDD as a workhorse method in political economy.\nCattaneo, Idrobo & Titiunik wrote an excellent practical guide: A Practical Introduction to Regression Discontinuity Designs. It’s freely available and covers everything from basic plots to formal inference.",
    "crumbs": [
      "Methods",
      "Regression Discontinuity"
    ]
  },
  {
    "objectID": "selection-observables.html",
    "href": "selection-observables.html",
    "title": "Selection on Observables",
    "section": "",
    "text": "You want the causal effect of a treatment, but people select into treatment based on their characteristics. Sicker patients seek medication, motivated students enroll in programs, richer firms adopt new technology.\nThe selection on observables strategy says: if you can observe everything that drives both treatment and outcome, you can condition on it and recover the causal effect. Once you hold those variables fixed, treatment is as good as random.\n\\[Y(0), Y(1) \\perp D \\mid X\\]\nThis is the conditional independence assumption (CIA), also called unconfoundedness or ignorability. It says: among people with the same \\(X\\), who gets treated is effectively random.\n\n\nIt’s the same logic, made precise. When you run \\(Y = \\alpha + \\tau D + \\beta X + \\varepsilon\\) and claim \\(\\hat{\\tau}\\) is causal, you’re implicitly assuming selection on observables — that \\(X\\) contains all the confounders. The difference is that causal inference makes this assumption explicit and offers multiple ways to implement it, each with different strengths:\n\n\n\nMethod\nHow it adjusts for X\n\n\n\n\nRegression Adjustment\nModels the outcome as a function of X and D\n\n\nMatching\nPairs treated and control units with similar X\n\n\nIPW\nReweights units by their probability of treatment given X\n\n\nEntropy Balancing\nDirectly reweights controls to match treated group’s X distribution\n\n\nDoubly robust\nCombines regression and weighting — consistent if either model is correct\n\n\n\nAll of these rely on the same fundamental assumption. They differ in how they use X to make the comparison fair.\n\n\n\n\nConditional independence (CIA): all confounders are observed and included in X. If an unobserved variable affects both treatment and outcome, every method above is biased. This is untestable — you argue it based on institutional knowledge.\nOverlap (common support): for every value of X, there are both treated and untreated units — \\(0 &lt; P(D = 1 \\mid X) &lt; 1\\). If some covariate profiles always get treated, you can’t estimate the counterfactual for them.\nSUTVA: one unit’s treatment doesn’t affect another’s outcome.\n\n\n\n\nWhen there are unobserved confounders — variables that affect both treatment and outcome but aren’t in your data. No amount of regression, matching, or weighting can fix this.\nExamples:\n\nReturns to education: ability is unobserved. More able people get more education and earn more. Controlling for test scores helps but doesn’t fully capture ability. → You need IV.\nEffect of a new policy: states that adopt the policy may differ in unobservable ways (political will, citizen preferences). → You need DID or synthetic control.\nEffect of a drug: patients who take the drug may be sicker in ways the chart doesn’t capture. → You need an RCT.\n\nThe simulation below shows what happens when the CIA holds vs when it doesn’t.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_so\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate_so\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"obs_conf\", \"Observed confounding (X):\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      sliderInput(\"unobs_conf\", \"Unobserved confounding (U):\",\n                  min = 0, max = 3, value = 0, step = 0.25),\n\n      actionButton(\"go_so\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_so\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"so_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go_so\n    n   &lt;- input$n_so\n    ate &lt;- input$ate_so\n    gx  &lt;- input$obs_conf\n    gu  &lt;- input$unobs_conf\n\n    # Observed confounder\n    x &lt;- rnorm(n)\n\n    # Unobserved confounder\n    u &lt;- rnorm(n)\n\n    # Treatment depends on both\n    p &lt;- pnorm(gx * x + gu * u)\n    treat &lt;- rbinom(n, 1, p)\n\n    # Outcome depends on both\n    y &lt;- 1 + 2 * x + 1.5 * u + ate * treat + rnorm(n)\n\n    # Naive (no controls)\n    naive &lt;- coef(lm(y ~ treat))[2]\n\n    # Controlling for X only\n    ctrl_x &lt;- coef(lm(y ~ treat + x))[2]\n\n    # Oracle: controlling for X and U\n    oracle &lt;- coef(lm(y ~ treat + x + u))[2]\n\n    list(x = x, u = u, treat = treat, y = y,\n         naive = naive, ctrl_x = ctrl_x, oracle = oracle,\n         ate = ate, gx = gx, gu = gu)\n  })\n\n  output$so_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    estimates &lt;- c(d$naive, d$ctrl_x, d$oracle)\n    biases &lt;- estimates - d$ate\n    labels &lt;- c(\"Naive\\n(no controls)\", \"Control for X\\n(observed)\", \"Control for X + U\\n(oracle)\")\n    cols &lt;- c(\"#e74c3c\", ifelse(abs(biases[2]) &lt; 0.3, \"#27ae60\", \"#f39c12\"), \"#27ae60\")\n\n    bp &lt;- barplot(estimates, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.85,\n                  main = \"Estimated Treatment Effect by Method\",\n                  ylab = \"Estimate\", ylim = c(0, max(estimates) * 1.4))\n\n    # True ATE line\n    abline(h = d$ate, lty = 2, col = \"gray40\", lwd = 2)\n    text(0.2, d$ate + 0.15, paste0(\"True ATE = \", d$ate),\n         col = \"gray40\", cex = 0.85, adj = 0)\n\n    # Bias labels\n    text(bp, estimates + 0.15,\n         paste0(round(estimates, 2), \"\\n(bias: \", round(biases, 2), \")\"),\n         cex = 0.8)\n  })\n\n  output$results_so &lt;- renderUI({\n    d &lt;- dat()\n    bias_naive &lt;- d$naive - d$ate\n    bias_x &lt;- d$ctrl_x - d$ate\n    bias_oracle &lt;- d$oracle - d$ate\n    cia_holds &lt;- d$gu == 0\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; \", round(d$naive, 2),\n        \" &lt;span class='bad'&gt;(bias: \", round(bias_naive, 2), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Control X:&lt;/b&gt; \", round(d$ctrl_x, 2),\n        \" &lt;span class='\", ifelse(cia_holds, \"good\", \"bad\"), \"'&gt;(bias: \",\n        round(bias_x, 2), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Oracle (X+U):&lt;/b&gt; \", round(d$oracle, 2),\n        \" &lt;span class='good'&gt;(bias: \", round(bias_oracle, 2), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        if (cia_holds)\n          \"&lt;small&gt;CIA holds: controlling for X is enough.&lt;/small&gt;\"\n        else\n          \"&lt;small&gt;CIA violated: U confounds treatment. Controlling for X alone leaves residual bias.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nUnobserved confounding = 0: the CIA holds. Controlling for X eliminates all bias — the green and oracle bars match. This is the world where selection on observables works.\nUnobserved confounding = 1.5: now there’s a confounder you can’t see. Controlling for X helps (reduces bias vs naive) but doesn’t eliminate it. Only the oracle, who controls for both X and U, gets the right answer.\nUnobserved confounding = 3: controlling for X barely helps. The bias is large. No amount of regression, matching, or weighting on X can fix this — you need a different identification strategy.\nSet observed confounding = 0, unobserved = 2: all the confounding is unobserved. Naive and “control for X” give the same (biased) answer because X isn’t a confounder here.",
    "crumbs": [
      "Methods",
      "Selection on Observables"
    ]
  },
  {
    "objectID": "selection-observables.html#the-idea",
    "href": "selection-observables.html#the-idea",
    "title": "Selection on Observables",
    "section": "",
    "text": "You want the causal effect of a treatment, but people select into treatment based on their characteristics. Sicker patients seek medication, motivated students enroll in programs, richer firms adopt new technology.\nThe selection on observables strategy says: if you can observe everything that drives both treatment and outcome, you can condition on it and recover the causal effect. Once you hold those variables fixed, treatment is as good as random.\n\\[Y(0), Y(1) \\perp D \\mid X\\]\nThis is the conditional independence assumption (CIA), also called unconfoundedness or ignorability. It says: among people with the same \\(X\\), who gets treated is effectively random.\n\n\nIt’s the same logic, made precise. When you run \\(Y = \\alpha + \\tau D + \\beta X + \\varepsilon\\) and claim \\(\\hat{\\tau}\\) is causal, you’re implicitly assuming selection on observables — that \\(X\\) contains all the confounders. The difference is that causal inference makes this assumption explicit and offers multiple ways to implement it, each with different strengths:\n\n\n\nMethod\nHow it adjusts for X\n\n\n\n\nRegression Adjustment\nModels the outcome as a function of X and D\n\n\nMatching\nPairs treated and control units with similar X\n\n\nIPW\nReweights units by their probability of treatment given X\n\n\nEntropy Balancing\nDirectly reweights controls to match treated group’s X distribution\n\n\nDoubly robust\nCombines regression and weighting — consistent if either model is correct\n\n\n\nAll of these rely on the same fundamental assumption. They differ in how they use X to make the comparison fair.\n\n\n\n\nConditional independence (CIA): all confounders are observed and included in X. If an unobserved variable affects both treatment and outcome, every method above is biased. This is untestable — you argue it based on institutional knowledge.\nOverlap (common support): for every value of X, there are both treated and untreated units — \\(0 &lt; P(D = 1 \\mid X) &lt; 1\\). If some covariate profiles always get treated, you can’t estimate the counterfactual for them.\nSUTVA: one unit’s treatment doesn’t affect another’s outcome.\n\n\n\n\nWhen there are unobserved confounders — variables that affect both treatment and outcome but aren’t in your data. No amount of regression, matching, or weighting can fix this.\nExamples:\n\nReturns to education: ability is unobserved. More able people get more education and earn more. Controlling for test scores helps but doesn’t fully capture ability. → You need IV.\nEffect of a new policy: states that adopt the policy may differ in unobservable ways (political will, citizen preferences). → You need DID or synthetic control.\nEffect of a drug: patients who take the drug may be sicker in ways the chart doesn’t capture. → You need an RCT.\n\nThe simulation below shows what happens when the CIA holds vs when it doesn’t.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_so\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate_so\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"obs_conf\", \"Observed confounding (X):\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      sliderInput(\"unobs_conf\", \"Unobserved confounding (U):\",\n                  min = 0, max = 3, value = 0, step = 0.25),\n\n      actionButton(\"go_so\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_so\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"so_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go_so\n    n   &lt;- input$n_so\n    ate &lt;- input$ate_so\n    gx  &lt;- input$obs_conf\n    gu  &lt;- input$unobs_conf\n\n    # Observed confounder\n    x &lt;- rnorm(n)\n\n    # Unobserved confounder\n    u &lt;- rnorm(n)\n\n    # Treatment depends on both\n    p &lt;- pnorm(gx * x + gu * u)\n    treat &lt;- rbinom(n, 1, p)\n\n    # Outcome depends on both\n    y &lt;- 1 + 2 * x + 1.5 * u + ate * treat + rnorm(n)\n\n    # Naive (no controls)\n    naive &lt;- coef(lm(y ~ treat))[2]\n\n    # Controlling for X only\n    ctrl_x &lt;- coef(lm(y ~ treat + x))[2]\n\n    # Oracle: controlling for X and U\n    oracle &lt;- coef(lm(y ~ treat + x + u))[2]\n\n    list(x = x, u = u, treat = treat, y = y,\n         naive = naive, ctrl_x = ctrl_x, oracle = oracle,\n         ate = ate, gx = gx, gu = gu)\n  })\n\n  output$so_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    estimates &lt;- c(d$naive, d$ctrl_x, d$oracle)\n    biases &lt;- estimates - d$ate\n    labels &lt;- c(\"Naive\\n(no controls)\", \"Control for X\\n(observed)\", \"Control for X + U\\n(oracle)\")\n    cols &lt;- c(\"#e74c3c\", ifelse(abs(biases[2]) &lt; 0.3, \"#27ae60\", \"#f39c12\"), \"#27ae60\")\n\n    bp &lt;- barplot(estimates, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.85,\n                  main = \"Estimated Treatment Effect by Method\",\n                  ylab = \"Estimate\", ylim = c(0, max(estimates) * 1.4))\n\n    # True ATE line\n    abline(h = d$ate, lty = 2, col = \"gray40\", lwd = 2)\n    text(0.2, d$ate + 0.15, paste0(\"True ATE = \", d$ate),\n         col = \"gray40\", cex = 0.85, adj = 0)\n\n    # Bias labels\n    text(bp, estimates + 0.15,\n         paste0(round(estimates, 2), \"\\n(bias: \", round(biases, 2), \")\"),\n         cex = 0.8)\n  })\n\n  output$results_so &lt;- renderUI({\n    d &lt;- dat()\n    bias_naive &lt;- d$naive - d$ate\n    bias_x &lt;- d$ctrl_x - d$ate\n    bias_oracle &lt;- d$oracle - d$ate\n    cia_holds &lt;- d$gu == 0\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; \", round(d$naive, 2),\n        \" &lt;span class='bad'&gt;(bias: \", round(bias_naive, 2), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Control X:&lt;/b&gt; \", round(d$ctrl_x, 2),\n        \" &lt;span class='\", ifelse(cia_holds, \"good\", \"bad\"), \"'&gt;(bias: \",\n        round(bias_x, 2), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Oracle (X+U):&lt;/b&gt; \", round(d$oracle, 2),\n        \" &lt;span class='good'&gt;(bias: \", round(bias_oracle, 2), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        if (cia_holds)\n          \"&lt;small&gt;CIA holds: controlling for X is enough.&lt;/small&gt;\"\n        else\n          \"&lt;small&gt;CIA violated: U confounds treatment. Controlling for X alone leaves residual bias.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nUnobserved confounding = 0: the CIA holds. Controlling for X eliminates all bias — the green and oracle bars match. This is the world where selection on observables works.\nUnobserved confounding = 1.5: now there’s a confounder you can’t see. Controlling for X helps (reduces bias vs naive) but doesn’t eliminate it. Only the oracle, who controls for both X and U, gets the right answer.\nUnobserved confounding = 3: controlling for X barely helps. The bias is large. No amount of regression, matching, or weighting on X can fix this — you need a different identification strategy.\nSet observed confounding = 0, unobserved = 2: all the confounding is unobserved. Naive and “control for X” give the same (biased) answer because X isn’t a confounder here.",
    "crumbs": [
      "Methods",
      "Selection on Observables"
    ]
  },
  {
    "objectID": "selection-observables.html#estimation-tools-not-just-for-selection-on-observables",
    "href": "selection-observables.html#estimation-tools-not-just-for-selection-on-observables",
    "title": "Selection on Observables",
    "section": "Estimation tools (not just for selection on observables)",
    "text": "Estimation tools (not just for selection on observables)\nOnce you have an identification strategy, you need a way to implement it. The tools below are often associated with selection on observables, but they’re general-purpose — they show up in other strategies too.\n\n\n\nTool\nUsed in selection on observables\nAlso used in\n\n\n\n\nRegression adjustment\nControl for X in a regression\nDID with covariates, RDD with covariates\n\n\nMatching\nPair treated/control on X\nDID matching estimators\n\n\nIPW\nReweight by propensity score\nIPW-DID (Abadie 2005, Callaway & Sant’Anna 2021)\n\n\nEntropy Balancing\nBalance covariates with weights\nWeighted DID\n\n\nDoubly robust\nCombine regression + weighting\nDR-DID (Sant’Anna & Zhao 2020)\n\n\n\nThe identification strategy tells you why your comparison is valid. The estimation tool tells you how to make the comparison. Don’t confuse the two — IPW is a tool, not a strategy.",
    "crumbs": [
      "Methods",
      "Selection on Observables"
    ]
  },
  {
    "objectID": "selection-observables.html#in-stata",
    "href": "selection-observables.html#in-stata",
    "title": "Selection on Observables",
    "section": "In Stata",
    "text": "In Stata\nAll SOO estimators live under teffects:\n* Regression adjustment\nteffects ra (outcome x1 x2) (treatment)\n\n* Inverse probability weighting\nteffects ipw (outcome) (treatment x1 x2)\n\n* Nearest-neighbor matching\nteffects nnmatch (outcome x1 x2) (treatment), nneighbor(1)\n\n* Doubly robust (AIPW)\nteffects aipw (outcome x1 x2) (treatment x1 x2)\n\n* Check overlap after any teffects command\nteffects overlap\nSame identification assumption (CIA) behind all of them — different ways to use \\(X\\) to make the comparison fair. See each page for details.",
    "crumbs": [
      "Methods",
      "Selection on Observables"
    ]
  },
  {
    "objectID": "selection-observables.html#did-you-know",
    "href": "selection-observables.html#did-you-know",
    "title": "Selection on Observables",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe conditional independence assumption was formalized by Rosenbaum & Rubin (1983) in their foundational paper on propensity scores. They showed that conditioning on a scalar propensity score is sufficient — you don’t need to match on every covariate separately.\nThe term “selection on observables” is economics jargon. In statistics it’s called ignorability or no unmeasured confounding. In epidemiology it’s the exchangeability assumption. Same idea, different fields, different names.\nAltonji, Elder & Taber (2005) proposed a practical check: compare how much the estimate changes when you add observed controls. If adding strong predictors of the outcome barely moves the estimate, it’s less likely that unobservables would change it much either. Not a proof — but a useful heuristic.",
    "crumbs": [
      "Methods",
      "Selection on Observables"
    ]
  },
  {
    "objectID": "entropy-balancing.html",
    "href": "entropy-balancing.html",
    "title": "Entropy Balancing",
    "section": "",
    "text": "IPW relies on correctly specifying the propensity score model. If you get the model wrong, the weights are wrong, and the estimate is biased. Even if the model is right, extreme propensity scores create extreme weights and noisy estimates.",
    "crumbs": [
      "Estimation Tools",
      "Entropy Balancing"
    ]
  },
  {
    "objectID": "entropy-balancing.html#the-problem-with-ipw",
    "href": "entropy-balancing.html#the-problem-with-ipw",
    "title": "Entropy Balancing",
    "section": "",
    "text": "IPW relies on correctly specifying the propensity score model. If you get the model wrong, the weights are wrong, and the estimate is biased. Even if the model is right, extreme propensity scores create extreme weights and noisy estimates.",
    "crumbs": [
      "Estimation Tools",
      "Entropy Balancing"
    ]
  },
  {
    "objectID": "entropy-balancing.html#entropy-balancing-a-different-approach",
    "href": "entropy-balancing.html#entropy-balancing-a-different-approach",
    "title": "Entropy Balancing",
    "section": "Entropy balancing: a different approach",
    "text": "Entropy balancing: a different approach\nEntropy balancing (Hainmueller, 2012) skips the propensity score entirely. Instead, it directly finds weights for the control group that make the covariate distributions exactly match the treated group on specified moments (mean, variance, skewness).\nThe weights are chosen to be as close to uniform as possible (maximum entropy) subject to the balance constraints. This guarantees:\n\nExact balance on the moments you specify\nSmooth weights (no extreme values like IPW can produce)\n\n\nIPW vs Entropy Balancing\n\n\n\n\nIPW\nEntropy Balancing\n\n\n\n\nRequires a propensity score model\nYes\nNo\n\n\nBalance is…\nApproximate (check after)\nExact (by construction)\n\n\nExtreme weights?\nCan be severe\nControlled\n\n\nSensitive to misspecification?\nYes\nLess so\n\n\n\n\n\nAssumptions\n\nSelection on observables: conditional on the covariates you balance on, treatment is independent of potential outcomes. Same as IPW — if you’re missing a confounder, balancing the observed ones doesn’t help.\nOverlap: treated and control groups share common support in covariate space. You can’t reweight controls to look like treated units if no controls exist in that part of the distribution.\nCorrect moments: you need to balance the right moments. If the outcome depends on \\(X^2\\) but you only balance on \\(E[X]\\), you’ll miss the confounding.\n\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"confounding\", \"Confounding strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"balance_plot\", height = \"380px\")),\n        column(6, plotOutput(\"weight_plot\",  height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  # Simple entropy balancing: find weights for control group\n  # that match treated group mean of X\n  ebal &lt;- function(x_ctrl, target_mean, max_iter = 200, tol = 1e-6) {\n    n &lt;- length(x_ctrl)\n    lambda &lt;- 0\n    for (i in seq_len(max_iter)) {\n      w &lt;- exp(lambda * x_ctrl)\n      w &lt;- w / sum(w) * n\n      current &lt;- weighted.mean(x_ctrl, w)\n      grad &lt;- current - target_mean\n      if (abs(grad) &lt; tol) break\n      lambda &lt;- lambda - 0.5 * grad\n    }\n    w / sum(w) * n\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    ate  &lt;- input$ate\n    conf &lt;- input$confounding\n\n    x &lt;- rnorm(n)\n    p_true &lt;- pnorm(conf * x)\n    treat &lt;- rbinom(n, 1, p_true)\n\n    y &lt;- 1 + 2 * x + ate * treat + rnorm(n)\n\n    # Naive\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # IPW\n    ps &lt;- fitted(glm(treat ~ x, family = binomial))\n    w_ipw &lt;- ifelse(treat == 1, 1 / ps, 1 / (1 - ps))\n    ipw_est &lt;- weighted.mean(y[treat == 1], w_ipw[treat == 1]) -\n               weighted.mean(y[treat == 0], w_ipw[treat == 0])\n\n    # Entropy balancing (balance control to match treated mean of X)\n    x_ctrl &lt;- x[treat == 0]\n    x_treat_mean &lt;- mean(x[treat == 1])\n    w_eb &lt;- ebal(x_ctrl, x_treat_mean)\n\n    eb_est &lt;- mean(y[treat == 1]) - weighted.mean(y[treat == 0], w_eb)\n\n    # Balance diagnostics\n    ctrl_mean_raw &lt;- mean(x[treat == 0])\n    ctrl_mean_eb  &lt;- weighted.mean(x[treat == 0], w_eb)\n    treat_mean_x  &lt;- x_treat_mean\n\n    list(x = x, treat = treat, y = y,\n         w_ipw = w_ipw, w_eb = w_eb,\n         naive = naive, ipw_est = ipw_est, eb_est = eb_est,\n         ate = ate,\n         ctrl_mean_raw = ctrl_mean_raw,\n         ctrl_mean_eb = ctrl_mean_eb,\n         treat_mean_x = treat_mean_x)\n  })\n\n  output$balance_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 1, 3, 1))\n\n    means &lt;- c(d$treat_mean_x, d$ctrl_mean_raw, d$ctrl_mean_eb)\n    cols &lt;- c(\"#3498db\", \"#e74c3c\", \"#27ae60\")\n    labels &lt;- c(\"Treated\", \"Control\\n(unweighted)\", \"Control\\n(EB weighted)\")\n\n    bp &lt;- barplot(means, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.85,\n                  main = \"Mean of X: Balance Check\",\n                  ylab = \"\", ylim = range(means) * c(0.8, 1.3))\n    text(bp, means + 0.05, round(means, 3), cex = 0.9)\n  })\n\n  output$weight_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ctrl_idx &lt;- which(d$treat == 0)\n\n    w_ipw_ctrl &lt;- d$w_ipw[ctrl_idx]\n    w_eb_ctrl  &lt;- d$w_eb\n\n    xlim &lt;- c(0, max(c(w_ipw_ctrl, w_eb_ctrl)) * 1.1)\n\n    d_ipw &lt;- density(w_ipw_ctrl, from = 0)\n    d_eb  &lt;- density(w_eb_ctrl, from = 0)\n    ylim &lt;- c(0, max(d_ipw$y, d_eb$y) * 1.2)\n\n    plot(d_ipw, col = \"#e74c3c\", lwd = 2.5,\n         main = \"Weight Distributions (Control Units)\",\n         xlab = \"Weight\", ylab = \"Density\",\n         xlim = xlim, ylim = ylim)\n    lines(d_eb, col = \"#27ae60\", lwd = 2.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"IPW weights\", \"Entropy balancing weights\"),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = 2.5)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; &lt;span class='bad'&gt;\", round(d$naive, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(d$naive - d$ate, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;IPW:&lt;/b&gt; \", round(d$ipw_est, 3),\n        \" (bias: \", round(d$ipw_est - d$ate, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;Entropy Bal:&lt;/b&gt; &lt;span class='good'&gt;\", round(d$eb_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(d$eb_est - d$ate, 3), \")\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nConfounding = 1.5: look at the right plot. IPW weights have a long tail (some control units get huge weight). Entropy balancing weights are much smoother.\nConfounding = 3: IPW weights become extreme. EB stays stable.\nLeft plot: the green bar (EB-weighted control mean) exactly matches the blue bar (treated mean). That’s the guarantee — exact balance by construction.\nCompare the bias numbers in the sidebar: EB is typically closer to the true ATE, especially under strong confounding.",
    "crumbs": [
      "Estimation Tools",
      "Entropy Balancing"
    ]
  },
  {
    "objectID": "entropy-balancing.html#in-stata",
    "href": "entropy-balancing.html#in-stata",
    "title": "Entropy Balancing",
    "section": "In Stata",
    "text": "In Stata\n* Install entropy balancing\n* ssc install ebalance\n\n* Generate weights that balance x1, x2, x3 between treated and control\nebalance treatment x1 x2 x3, gen(eb_weight)\n\n* Use the weights in your outcome regression\nreg outcome treatment x1 x2 x3 [pw=eb_weight]\nebalance finds the smoothest set of control-group weights that exactly matches the treated group on the moments you specify (means by default; add targets(2) to also match variances). No propensity score model needed.",
    "crumbs": [
      "Estimation Tools",
      "Entropy Balancing"
    ]
  },
  {
    "objectID": "matching.html",
    "href": "matching.html",
    "title": "Matching",
    "section": "",
    "text": "You want the causal effect of a treatment, and you believe selection on observables holds — all confounders are observed. Matching implements this by finding, for each treated unit, a control unit that looks similar on covariates \\(X\\).\nThe logic is simple: if two people have the same \\(X\\), and one got treated while the other didn’t, the difference in their outcomes estimates the treatment effect for that value of \\(X\\). Average across all matched pairs and you get the ATT.\n\n\n\n\n\n\nExample: job training programs. The government runs a job training program. People who enroll earn more afterward — but is that the program, or did motivated people self-select? Matching says: for each enrollee, find a non-enrollee with the same age, education, prior income, and industry. Compare their earnings. If the enrollee still earns more after matching on everything observable, that’s the program’s effect. This is exactly what LaLonde (1986) studied — and found that naive comparisons badly overestimated the program’s impact.\n\n\n\n\\[\\hat{\\tau}_{match} = \\frac{1}{N_1} \\sum_{i: D_i=1} \\Big[ Y_i - Y_{j(i)} \\Big]\\]\nwhere \\(j(i)\\) is the control unit matched to treated unit \\(i\\).\n\n\n\n\n\n\n\n\n\n\n\nMethod\nHow it matches\nPros\nCons\n\n\n\n\nExact\nIdentical X values\nNo approximation error\nInfeasible with continuous or many covariates\n\n\nNearest neighbor (on X)\nClosest \\(\\|X_i - X_j\\|\\)\nSimple, intuitive\nCurse of dimensionality with many covariates\n\n\nNearest neighbor (on PS)\nClosest \\(\\|e(X_i) - e(X_j)\\|\\)\nReduces to 1D; Rosenbaum & Rubin (1983)\nInherits PS model risk\n\n\nCaliper\nNN, but reject if distance \\(&gt; c\\)\nDrops bad matches\nLoses observations\n\n\nMahalanobis\nDistance accounts for correlations\nBetter than Euclidean for correlated X\nStill suffers in high dimensions\n\n\n\n\n\n\nSame as all selection on observables methods:\n\nConditional independence (CIA): \\(Y(0), Y(1) \\perp D \\mid X\\). All confounders are observed.\nOverlap (common support): \\(0 &lt; P(D = 1 \\mid X) &lt; 1\\). For every treated unit, a comparable control exists.\nSUTVA: no interference between units.\n\n\n\n\nMatching finds comparisons rather than modeling outcomes. But the quality of those comparisons depends on:\n\nOverlap: if treated and control units live in different parts of the covariate space, the “nearest” neighbor may be far away. The match is bad and the estimate is biased.\nDimensionality: with many covariates, even the nearest neighbor can be quite distant (“curse of dimensionality”). This is why propensity score matching collapses \\(X\\) to a single dimension.\n\nCompare this to regression adjustment, which extrapolates using a model, and IPW, which reweights instead of pairing. Each has different failure modes.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"confounding\", \"Confounding strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      selectInput(\"match_type\", \"Matching method:\",\n                  choices = c(\"NN on X\" = \"nn_x\",\n                              \"NN on propensity score\" = \"nn_ps\",\n                              \"Caliper (on X)\" = \"caliper\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"match_plot\", height = \"420px\")),\n        column(6, plotOutput(\"compare_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    ate  &lt;- input$ate\n    conf &lt;- input$confounding\n    mtype &lt;- input$match_type\n\n    # Confounder\n    x &lt;- rnorm(n)\n\n    # Treatment depends on x\n    p_true &lt;- pnorm(conf * x)\n    treat &lt;- rbinom(n, 1, p_true)\n\n    # Outcome\n    y &lt;- 1 + 2 * x + ate * treat + rnorm(n)\n\n    idx_t &lt;- which(treat == 1)\n    idx_c &lt;- which(treat == 0)\n\n    # Propensity score (for PS matching)\n    ps &lt;- fitted(glm(treat ~ x, family = binomial))\n\n    # Matching\n    matched_j &lt;- integer(length(idx_t))\n    match_dist &lt;- numeric(length(idx_t))\n\n    if (mtype == \"nn_x\") {\n      for (k in seq_along(idx_t)) {\n        dists &lt;- abs(x[idx_t[k]] - x[idx_c])\n        best &lt;- which.min(dists)\n        matched_j[k] &lt;- idx_c[best]\n        match_dist[k] &lt;- dists[best]\n      }\n    } else if (mtype == \"nn_ps\") {\n      for (k in seq_along(idx_t)) {\n        dists &lt;- abs(ps[idx_t[k]] - ps[idx_c])\n        best &lt;- which.min(dists)\n        matched_j[k] &lt;- idx_c[best]\n        match_dist[k] &lt;- abs(x[idx_t[k]] - x[idx_c[best]])\n      }\n    } else {\n      # Caliper on X (caliper = 0.25 * sd(x))\n      cal &lt;- 0.25 * sd(x)\n      for (k in seq_along(idx_t)) {\n        dists &lt;- abs(x[idx_t[k]] - x[idx_c])\n        best &lt;- which.min(dists)\n        if (dists[best] &lt;= cal) {\n          matched_j[k] &lt;- idx_c[best]\n          match_dist[k] &lt;- dists[best]\n        } else {\n          matched_j[k] &lt;- NA\n          match_dist[k] &lt;- NA\n        }\n      }\n    }\n\n    # Matching estimate\n    valid &lt;- !is.na(matched_j)\n    n_matched &lt;- sum(valid)\n    if (n_matched &gt; 0) {\n      match_est &lt;- mean(y[idx_t[valid]] - y[matched_j[valid]])\n    } else {\n      match_est &lt;- NA\n    }\n    avg_dist &lt;- mean(match_dist[valid])\n\n    # Naive\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # Regression adjustment\n    reg_est &lt;- coef(lm(y ~ treat + x))[\"treat\"]\n\n    list(x = x, y = y, treat = treat, ps = ps,\n         idx_t = idx_t, idx_c = idx_c,\n         matched_j = matched_j, valid = valid,\n         match_est = match_est, naive = naive, reg_est = reg_est,\n         ate = ate, n_matched = n_matched, avg_dist = avg_dist,\n         mtype = mtype)\n  })\n\n  output$match_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$y, pch = 16, cex = 0.5,\n         col = ifelse(d$treat == 1, \"#3498db40\", \"#e74c3c40\"),\n         xlab = \"X (confounder)\", ylab = \"Y (outcome)\",\n         main = \"Matched Pairs\")\n\n    # Draw match lines for a subset (first 30 valid matches)\n    valid_idx &lt;- which(d$valid)\n    show &lt;- valid_idx[seq_len(min(30, length(valid_idx)))]\n\n    for (k in show) {\n      i &lt;- d$idx_t[k]\n      j &lt;- d$matched_j[k]\n      segments(d$x[i], d$y[i], d$x[j], d$y[j],\n               col = \"#9b59b680\", lwd = 1)\n    }\n\n    # Re-draw matched points on top\n    for (k in show) {\n      i &lt;- d$idx_t[k]\n      j &lt;- d$matched_j[k]\n      points(d$x[i], d$y[i], pch = 16, cex = 0.7, col = \"#3498db\")\n      points(d$x[j], d$y[j], pch = 16, cex = 0.7, col = \"#e74c3c\")\n    }\n\n    n_show &lt;- length(show)\n    legend(\"topleft\", bty = \"n\", cex = 0.8,\n           legend = c(\"Treated\", \"Control\",\n                      paste0(\"Match link (\", n_show, \" shown)\")),\n           col = c(\"#3498db\", \"#e74c3c\", \"#9b59b6\"),\n           pch = c(16, 16, NA), lwd = c(NA, NA, 1.5))\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 9, 3, 2))\n\n    ests &lt;- c(d$reg_est, d$match_est, d$naive)\n    labels &lt;- c(\"Regression adj.\", \"Matching\", \"Naive\")\n    cols &lt;- c(\"#3498db\", \"#9b59b6\", \"#e74c3c\")\n\n    valid_ests &lt;- !is.na(ests)\n    xlim &lt;- range(c(ests[valid_ests], d$ate))\n    pad  &lt;- max(diff(xlim) * 0.4, 0.5)\n    xlim &lt;- xlim + c(-pad, pad)\n\n    plot(ests, 1:3, pch = 19, cex = 2, col = cols,\n         xlim = xlim, ylim = c(0.5, 3.5),\n         yaxt = \"n\", xlab = \"Estimated treatment effect\",\n         ylab = \"\", main = \"Estimator Comparison\")\n    axis(2, at = 1:3, labels = labels, las = 1, cex.axis = 0.9)\n\n    abline(v = d$ate, lty = 2, col = \"#2c3e50\", lwd = 2)\n    text(d$ate, 3.45, paste0(\"True ATE = \", d$ate),\n         cex = 0.85, font = 2, col = \"#2c3e50\")\n\n    segments(d$ate, 1:3, ests, 1:3, col = cols, lwd = 2, lty = 2)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    b_naive &lt;- d$naive - d$ate\n    b_match &lt;- if (!is.na(d$match_est)) d$match_est - d$ate else NA\n    b_reg   &lt;- d$reg_est - d$ate\n\n    match_class &lt;- if (!is.na(b_match) && abs(b_match) &lt; abs(b_naive) * 0.5) \"good\" else \"bad\"\n    reg_class   &lt;- if (abs(b_reg) &lt; abs(b_naive) * 0.5) \"good\" else \"bad\"\n\n    method_label &lt;- switch(d$mtype,\n      nn_x    = \"NN on X\",\n      nn_ps   = \"NN on propensity score\",\n      caliper = \"Caliper on X\")\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Method:&lt;/b&gt; \", method_label, \"&lt;br&gt;\",\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; &lt;span class='bad'&gt;\", round(d$naive, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_naive, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;Matching:&lt;/b&gt; &lt;span class='\", match_class, \"'&gt;\",\n        if (!is.na(d$match_est)) round(d$match_est, 3) else \"N/A\",\n        \"&lt;/span&gt;\",\n        if (!is.na(b_match)) paste0(\" (bias: \", round(b_match, 3), \")\") else \"\", \"&lt;br&gt;\",\n        \"&lt;b&gt;Reg. adj:&lt;/b&gt; &lt;span class='\", reg_class, \"'&gt;\",\n        round(d$reg_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_reg, 3), \")&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Pairs matched:&lt;/b&gt; \", d$n_matched, \" / \", length(d$idx_t), \"&lt;br&gt;\",\n        \"&lt;b&gt;Avg match distance:&lt;/b&gt; \",\n        if (!is.na(d$avg_dist)) round(d$avg_dist, 3) else \"N/A\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nNN on X, confounding = 1.5: matching works well. The purple lines connecting matched pairs are short — each treated unit finds a similar control. The matching estimate is close to the true ATE.\nSwitch to caliper: some treated units in the tails can’t find a close match and get dropped. “Pairs matched” drops below the total. The remaining matches are better quality (shorter distances).\nConfounding = 3: treated and control units are far apart in X space. Match distances increase, match quality degrades, and the estimate gets noisier. This is the overlap problem — matching can’t fix it.\nSmall sample (n = 200) with high confounding: matching struggles because there aren’t enough control units in the right part of the covariate space. Regression adjustment extrapolates, which helps here but can hurt elsewhere (see the regression adjustment page).\nNN on propensity score: matches on the estimated \\(e(X)\\) instead of \\(X\\) directly. With one confounder, results are similar. The advantage appears with multiple covariates (not shown here).",
    "crumbs": [
      "Estimation Tools",
      "Matching"
    ]
  },
  {
    "objectID": "matching.html#the-idea",
    "href": "matching.html#the-idea",
    "title": "Matching",
    "section": "",
    "text": "You want the causal effect of a treatment, and you believe selection on observables holds — all confounders are observed. Matching implements this by finding, for each treated unit, a control unit that looks similar on covariates \\(X\\).\nThe logic is simple: if two people have the same \\(X\\), and one got treated while the other didn’t, the difference in their outcomes estimates the treatment effect for that value of \\(X\\). Average across all matched pairs and you get the ATT.\n\n\n\n\n\n\nExample: job training programs. The government runs a job training program. People who enroll earn more afterward — but is that the program, or did motivated people self-select? Matching says: for each enrollee, find a non-enrollee with the same age, education, prior income, and industry. Compare their earnings. If the enrollee still earns more after matching on everything observable, that’s the program’s effect. This is exactly what LaLonde (1986) studied — and found that naive comparisons badly overestimated the program’s impact.\n\n\n\n\\[\\hat{\\tau}_{match} = \\frac{1}{N_1} \\sum_{i: D_i=1} \\Big[ Y_i - Y_{j(i)} \\Big]\\]\nwhere \\(j(i)\\) is the control unit matched to treated unit \\(i\\).\n\n\n\n\n\n\n\n\n\n\n\nMethod\nHow it matches\nPros\nCons\n\n\n\n\nExact\nIdentical X values\nNo approximation error\nInfeasible with continuous or many covariates\n\n\nNearest neighbor (on X)\nClosest \\(\\|X_i - X_j\\|\\)\nSimple, intuitive\nCurse of dimensionality with many covariates\n\n\nNearest neighbor (on PS)\nClosest \\(\\|e(X_i) - e(X_j)\\|\\)\nReduces to 1D; Rosenbaum & Rubin (1983)\nInherits PS model risk\n\n\nCaliper\nNN, but reject if distance \\(&gt; c\\)\nDrops bad matches\nLoses observations\n\n\nMahalanobis\nDistance accounts for correlations\nBetter than Euclidean for correlated X\nStill suffers in high dimensions\n\n\n\n\n\n\nSame as all selection on observables methods:\n\nConditional independence (CIA): \\(Y(0), Y(1) \\perp D \\mid X\\). All confounders are observed.\nOverlap (common support): \\(0 &lt; P(D = 1 \\mid X) &lt; 1\\). For every treated unit, a comparable control exists.\nSUTVA: no interference between units.\n\n\n\n\nMatching finds comparisons rather than modeling outcomes. But the quality of those comparisons depends on:\n\nOverlap: if treated and control units live in different parts of the covariate space, the “nearest” neighbor may be far away. The match is bad and the estimate is biased.\nDimensionality: with many covariates, even the nearest neighbor can be quite distant (“curse of dimensionality”). This is why propensity score matching collapses \\(X\\) to a single dimension.\n\nCompare this to regression adjustment, which extrapolates using a model, and IPW, which reweights instead of pairing. Each has different failure modes.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"confounding\", \"Confounding strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      selectInput(\"match_type\", \"Matching method:\",\n                  choices = c(\"NN on X\" = \"nn_x\",\n                              \"NN on propensity score\" = \"nn_ps\",\n                              \"Caliper (on X)\" = \"caliper\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"match_plot\", height = \"420px\")),\n        column(6, plotOutput(\"compare_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    ate  &lt;- input$ate\n    conf &lt;- input$confounding\n    mtype &lt;- input$match_type\n\n    # Confounder\n    x &lt;- rnorm(n)\n\n    # Treatment depends on x\n    p_true &lt;- pnorm(conf * x)\n    treat &lt;- rbinom(n, 1, p_true)\n\n    # Outcome\n    y &lt;- 1 + 2 * x + ate * treat + rnorm(n)\n\n    idx_t &lt;- which(treat == 1)\n    idx_c &lt;- which(treat == 0)\n\n    # Propensity score (for PS matching)\n    ps &lt;- fitted(glm(treat ~ x, family = binomial))\n\n    # Matching\n    matched_j &lt;- integer(length(idx_t))\n    match_dist &lt;- numeric(length(idx_t))\n\n    if (mtype == \"nn_x\") {\n      for (k in seq_along(idx_t)) {\n        dists &lt;- abs(x[idx_t[k]] - x[idx_c])\n        best &lt;- which.min(dists)\n        matched_j[k] &lt;- idx_c[best]\n        match_dist[k] &lt;- dists[best]\n      }\n    } else if (mtype == \"nn_ps\") {\n      for (k in seq_along(idx_t)) {\n        dists &lt;- abs(ps[idx_t[k]] - ps[idx_c])\n        best &lt;- which.min(dists)\n        matched_j[k] &lt;- idx_c[best]\n        match_dist[k] &lt;- abs(x[idx_t[k]] - x[idx_c[best]])\n      }\n    } else {\n      # Caliper on X (caliper = 0.25 * sd(x))\n      cal &lt;- 0.25 * sd(x)\n      for (k in seq_along(idx_t)) {\n        dists &lt;- abs(x[idx_t[k]] - x[idx_c])\n        best &lt;- which.min(dists)\n        if (dists[best] &lt;= cal) {\n          matched_j[k] &lt;- idx_c[best]\n          match_dist[k] &lt;- dists[best]\n        } else {\n          matched_j[k] &lt;- NA\n          match_dist[k] &lt;- NA\n        }\n      }\n    }\n\n    # Matching estimate\n    valid &lt;- !is.na(matched_j)\n    n_matched &lt;- sum(valid)\n    if (n_matched &gt; 0) {\n      match_est &lt;- mean(y[idx_t[valid]] - y[matched_j[valid]])\n    } else {\n      match_est &lt;- NA\n    }\n    avg_dist &lt;- mean(match_dist[valid])\n\n    # Naive\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # Regression adjustment\n    reg_est &lt;- coef(lm(y ~ treat + x))[\"treat\"]\n\n    list(x = x, y = y, treat = treat, ps = ps,\n         idx_t = idx_t, idx_c = idx_c,\n         matched_j = matched_j, valid = valid,\n         match_est = match_est, naive = naive, reg_est = reg_est,\n         ate = ate, n_matched = n_matched, avg_dist = avg_dist,\n         mtype = mtype)\n  })\n\n  output$match_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$y, pch = 16, cex = 0.5,\n         col = ifelse(d$treat == 1, \"#3498db40\", \"#e74c3c40\"),\n         xlab = \"X (confounder)\", ylab = \"Y (outcome)\",\n         main = \"Matched Pairs\")\n\n    # Draw match lines for a subset (first 30 valid matches)\n    valid_idx &lt;- which(d$valid)\n    show &lt;- valid_idx[seq_len(min(30, length(valid_idx)))]\n\n    for (k in show) {\n      i &lt;- d$idx_t[k]\n      j &lt;- d$matched_j[k]\n      segments(d$x[i], d$y[i], d$x[j], d$y[j],\n               col = \"#9b59b680\", lwd = 1)\n    }\n\n    # Re-draw matched points on top\n    for (k in show) {\n      i &lt;- d$idx_t[k]\n      j &lt;- d$matched_j[k]\n      points(d$x[i], d$y[i], pch = 16, cex = 0.7, col = \"#3498db\")\n      points(d$x[j], d$y[j], pch = 16, cex = 0.7, col = \"#e74c3c\")\n    }\n\n    n_show &lt;- length(show)\n    legend(\"topleft\", bty = \"n\", cex = 0.8,\n           legend = c(\"Treated\", \"Control\",\n                      paste0(\"Match link (\", n_show, \" shown)\")),\n           col = c(\"#3498db\", \"#e74c3c\", \"#9b59b6\"),\n           pch = c(16, 16, NA), lwd = c(NA, NA, 1.5))\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 9, 3, 2))\n\n    ests &lt;- c(d$reg_est, d$match_est, d$naive)\n    labels &lt;- c(\"Regression adj.\", \"Matching\", \"Naive\")\n    cols &lt;- c(\"#3498db\", \"#9b59b6\", \"#e74c3c\")\n\n    valid_ests &lt;- !is.na(ests)\n    xlim &lt;- range(c(ests[valid_ests], d$ate))\n    pad  &lt;- max(diff(xlim) * 0.4, 0.5)\n    xlim &lt;- xlim + c(-pad, pad)\n\n    plot(ests, 1:3, pch = 19, cex = 2, col = cols,\n         xlim = xlim, ylim = c(0.5, 3.5),\n         yaxt = \"n\", xlab = \"Estimated treatment effect\",\n         ylab = \"\", main = \"Estimator Comparison\")\n    axis(2, at = 1:3, labels = labels, las = 1, cex.axis = 0.9)\n\n    abline(v = d$ate, lty = 2, col = \"#2c3e50\", lwd = 2)\n    text(d$ate, 3.45, paste0(\"True ATE = \", d$ate),\n         cex = 0.85, font = 2, col = \"#2c3e50\")\n\n    segments(d$ate, 1:3, ests, 1:3, col = cols, lwd = 2, lty = 2)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    b_naive &lt;- d$naive - d$ate\n    b_match &lt;- if (!is.na(d$match_est)) d$match_est - d$ate else NA\n    b_reg   &lt;- d$reg_est - d$ate\n\n    match_class &lt;- if (!is.na(b_match) && abs(b_match) &lt; abs(b_naive) * 0.5) \"good\" else \"bad\"\n    reg_class   &lt;- if (abs(b_reg) &lt; abs(b_naive) * 0.5) \"good\" else \"bad\"\n\n    method_label &lt;- switch(d$mtype,\n      nn_x    = \"NN on X\",\n      nn_ps   = \"NN on propensity score\",\n      caliper = \"Caliper on X\")\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Method:&lt;/b&gt; \", method_label, \"&lt;br&gt;\",\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; &lt;span class='bad'&gt;\", round(d$naive, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_naive, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;Matching:&lt;/b&gt; &lt;span class='\", match_class, \"'&gt;\",\n        if (!is.na(d$match_est)) round(d$match_est, 3) else \"N/A\",\n        \"&lt;/span&gt;\",\n        if (!is.na(b_match)) paste0(\" (bias: \", round(b_match, 3), \")\") else \"\", \"&lt;br&gt;\",\n        \"&lt;b&gt;Reg. adj:&lt;/b&gt; &lt;span class='\", reg_class, \"'&gt;\",\n        round(d$reg_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_reg, 3), \")&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Pairs matched:&lt;/b&gt; \", d$n_matched, \" / \", length(d$idx_t), \"&lt;br&gt;\",\n        \"&lt;b&gt;Avg match distance:&lt;/b&gt; \",\n        if (!is.na(d$avg_dist)) round(d$avg_dist, 3) else \"N/A\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nNN on X, confounding = 1.5: matching works well. The purple lines connecting matched pairs are short — each treated unit finds a similar control. The matching estimate is close to the true ATE.\nSwitch to caliper: some treated units in the tails can’t find a close match and get dropped. “Pairs matched” drops below the total. The remaining matches are better quality (shorter distances).\nConfounding = 3: treated and control units are far apart in X space. Match distances increase, match quality degrades, and the estimate gets noisier. This is the overlap problem — matching can’t fix it.\nSmall sample (n = 200) with high confounding: matching struggles because there aren’t enough control units in the right part of the covariate space. Regression adjustment extrapolates, which helps here but can hurt elsewhere (see the regression adjustment page).\nNN on propensity score: matches on the estimated \\(e(X)\\) instead of \\(X\\) directly. With one confounder, results are similar. The advantage appears with multiple covariates (not shown here).",
    "crumbs": [
      "Estimation Tools",
      "Matching"
    ]
  },
  {
    "objectID": "matching.html#how-does-matching-compare-to-other-tools",
    "href": "matching.html#how-does-matching-compare-to-other-tools",
    "title": "Matching",
    "section": "How does matching compare to other tools?",
    "text": "How does matching compare to other tools?\n\n\n\nTool\nStrategy\nVulnerability\n\n\n\n\nMatching\nFind similar units\nBad matches when overlap is poor\n\n\nRegression adj.\nModel the outcome\nWrong functional form\n\n\nIPW\nReweight by propensity score\nExtreme weights\n\n\nEntropy Balancing\nBalance moments exactly\nMissing higher-order moments\n\n\nDoubly Robust\nCombine outcome model + PS\nFails only if both models wrong\n\n\n\nAll rely on the same identification assumption (CIA). They differ in how they use \\(X\\) to make the comparison fair.",
    "crumbs": [
      "Estimation Tools",
      "Matching"
    ]
  },
  {
    "objectID": "matching.html#in-stata",
    "href": "matching.html#in-stata",
    "title": "Matching",
    "section": "In Stata",
    "text": "In Stata\n* Nearest-neighbor matching (on covariates)\nteffects nnmatch (outcome x1 x2) (treatment), nneighbor(1)\n\n* Bias-corrected matching (Abadie & Imbens)\nteffects nnmatch (outcome x1 x2) (treatment), nneighbor(1) biasadj(x1 x2)\n\n* Propensity score matching\nteffects psmatch (outcome) (treatment x1 x2)\n\n* Coarsened exact matching\n* ssc install cem\ncem x1 (#5) x2 (#3), treatment(treatment)\nreg outcome treatment x1 x2 [iw=cem_weights]\nteffects nnmatch with biasadj() corrects for the remaining covariate imbalance within matched pairs — important when matching on continuous variables.",
    "crumbs": [
      "Estimation Tools",
      "Matching"
    ]
  },
  {
    "objectID": "matching.html#did-you-know",
    "href": "matching.html#did-you-know",
    "title": "Matching",
    "section": "Did you know?",
    "text": "Did you know?\n\nAbadie & Imbens (2006) showed that nearest-neighbor matching is not \\(\\sqrt{n}\\)-consistent — its convergence rate is slower than parametric estimators when matching on more than one continuous covariate. The bias from imperfect matches doesn’t vanish fast enough. Their bias-corrected matching estimator fixes this by running a regression within each matched pair to adjust for the remaining covariate difference.\nPropensity score matching was proposed by Rosenbaum & Rubin (1983), who proved that matching on the scalar \\(e(X) = P(D=1 \\mid X)\\) is sufficient for removing confounding — you don’t need to match on every covariate separately. This was a breakthrough because it collapsed the high-dimensional matching problem to one dimension.\nKing & Nielsen (2019) argued that propensity score matching can paradoxically increase imbalance and model dependence. Their critique centers on the fact that exact PS matches don’t guarantee covariate balance — two units with the same propensity score can have very different covariate values.",
    "crumbs": [
      "Estimation Tools",
      "Matching"
    ]
  }
]