[
  {
    "objectID": "iv.html",
    "href": "iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "You want the causal effect of \\(X\\) on \\(Y\\), but \\(X\\) is endogenous — correlated with the error term because of confounding, reverse causality, or measurement error. OLS is biased.\nThe fix: find a variable \\(Z\\) (the instrument) that:\n\nRelevance: \\(Z\\) is correlated with \\(X\\) — it actually moves \\(X\\)\nExclusion restriction: \\(Z\\) affects \\(Y\\) only through \\(X\\) — no back doors\n\n\\[Z \\to X \\to Y\\]\nIf both conditions hold, you can use \\(Z\\) to isolate the part of \\(X\\) that’s “as good as random” and estimate the causal effect.\n\n\nThe mechanics are simple:\nFirst stage: regress \\(X\\) on \\(Z\\) to get predicted values \\(\\hat{X}\\)\n\\[X = \\pi_0 + \\pi_1 Z + v\\]\nSecond stage: regress \\(Y\\) on \\(\\hat{X}\\) instead of \\(X\\)\n\\[Y = \\beta_0 + \\beta_1 \\hat{X} + \\varepsilon\\]\nWhy does this work? \\(\\hat{X}\\) contains only the variation in \\(X\\) that comes from \\(Z\\). Since \\(Z\\) is exogenous (by assumption), \\(\\hat{X}\\) is uncorrelated with the error term. The confounding is gone.\n\n\n\n\nRelevance: \\(Z\\) is correlated with \\(X\\) — the instrument actually moves the endogenous variable. Testable: check the first-stage F-statistic.\nExclusion restriction: \\(Z\\) affects \\(Y\\) only through \\(X\\) — no direct effect and no back-door paths. Not testable — you argue it.\nIndependence: \\(Z\\) is as good as randomly assigned — uncorrelated with the error term in the outcome equation\nMonotonicity (for LATE): the instrument moves everyone in the same direction — no “defiers” who do the opposite of what the instrument encourages\n\n\n\n\nReturns to education. You want to know if more schooling causes higher earnings. But ability confounds: smarter people get more education and earn more. OLS overstates the return.\nAngrist & Krueger (1991) used quarter of birth as an instrument. Because of compulsory schooling laws, people born in Q1 can drop out slightly earlier than Q4 births — so quarter of birth affects education (relevance) but presumably doesn’t affect earnings directly (exclusion).\n\n\n\n\nWeak instruments: if \\(Z\\) barely moves \\(X\\), the first stage is weak and the IV estimate becomes wildly noisy and biased. Rule of thumb: first-stage F-statistic &gt; 10.\nExclusion restriction violated: if \\(Z\\) affects \\(Y\\) through channels other than \\(X\\), the estimate is biased. This assumption is untestable — you argue it, you don’t prove it.\n\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 100, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"true_b\", \"True causal effect of X on Y:\",\n                  min = 0, max = 5, value = 2, step = 0.25),\n\n      sliderInput(\"confound\", \"Confounding strength:\",\n                  min = 0, max = 5, value = 3, step = 0.25),\n\n      sliderInput(\"inst_str\", \"Instrument strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.1),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"iv_plot\", height = \"470px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    b   &lt;- input$true_b\n    cf  &lt;- input$confound\n    pi1 &lt;- input$inst_str\n\n    # Confounder (unobserved ability)\n    u &lt;- rnorm(n)\n\n    # Instrument\n    z &lt;- rnorm(n)\n\n    # Endogenous regressor: driven by instrument + confounder\n    x &lt;- pi1 * z + cf * u + rnorm(n)\n\n    # Outcome: causal effect of x + confounder\n    y &lt;- b * x + cf * u + rnorm(n)\n\n    # OLS (biased)\n    ols &lt;- lm(y ~ x)\n\n    # 2SLS by hand\n    first &lt;- lm(x ~ z)\n    x_hat &lt;- fitted(first)\n    second &lt;- lm(y ~ x_hat)\n\n    # First-stage F\n    f_stat &lt;- summary(first)$fstatistic[1]\n\n    list(x = x, y = y, z = z, x_hat = x_hat,\n         b_ols = coef(ols)[2],\n         b_iv = coef(second)[2],\n         first_coef = coef(first)[2],\n         f_stat = f_stat,\n         true_b = b, confound = cf, inst_str = pi1)\n  })\n\n  output$iv_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mfrow = c(1, 2), mar = c(4.5, 4.5, 3, 1))\n\n    # Left: OLS scatter (X vs Y)\n    plot(d$x, d$y, pch = 16, cex = 0.4,\n         col = adjustcolor(\"#3498db\", 0.3),\n         xlab = \"X (endogenous)\", ylab = \"Y\",\n         main = \"OLS: Y on X\")\n    abline(lm(d$y ~ d$x), col = \"#e74c3c\", lwd = 3)\n    abline(a = 0, b = d$true_b, col = \"#27ae60\", lwd = 2, lty = 2)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(paste0(\"OLS = \", round(d$b_ols, 2)),\n                      paste0(\"True = \", d$true_b)),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = c(3, 2), lty = c(1, 2))\n\n    # Right: IV scatter (X-hat vs Y)\n    plot(d$x_hat, d$y, pch = 16, cex = 0.4,\n         col = adjustcolor(\"#9b59b6\", 0.3),\n         xlab = expression(hat(X) ~ \"(from first stage)\"), ylab = \"Y\",\n         main = expression(\"2SLS: Y on \" * hat(X)))\n    abline(lm(d$y ~ d$x_hat), col = \"#e74c3c\", lwd = 3)\n    abline(a = 0, b = d$true_b, col = \"#27ae60\", lwd = 2, lty = 2)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(paste0(\"IV = \", round(d$b_iv, 2)),\n                      paste0(\"True = \", d$true_b)),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = c(3, 2), lty = c(1, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    ols_bias &lt;- d$b_ols - d$true_b\n    iv_bias  &lt;- d$b_iv - d$true_b\n    weak &lt;- d$f_stat &lt; 10\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$true_b, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;OLS:&lt;/b&gt; \", round(d$b_ols, 3),\n        \" &nbsp; Bias: &lt;span class='bad'&gt;\", round(ols_bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;IV (2SLS):&lt;/b&gt; \", round(d$b_iv, 3),\n        \" &nbsp; Bias: &lt;span class='\", ifelse(abs(iv_bias) &lt; abs(ols_bias), \"good\", \"bad\"), \"'&gt;\",\n        round(iv_bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;First-stage F:&lt;/b&gt; \", round(d$f_stat, 1),\n        if (weak) \" &lt;span class='bad'&gt;&lt; 10 (weak!)&lt;/span&gt;\"\n        else \" &lt;span class='good'&gt;&ge; 10&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nConfounding = 3, instrument = 1.5: OLS is badly biased. IV recovers the true effect. This is the whole point.\nSet confounding = 0: OLS and IV agree — when there’s no endogeneity, you don’t need an instrument.\nSlide instrument strength toward 0: the first-stage F drops below 10. The IV estimate becomes erratic — sometimes worse than OLS. That’s the weak instrument problem.\nIncrease sample size with a weak instrument: it doesn’t help much. Weak instruments bias IV toward OLS, and more data doesn’t fix that.\nTrue effect = 0, confounding = 3: OLS “finds” a large effect. IV correctly shows ~0.",
    "crumbs": [
      "Methods",
      "Instrumental Variables"
    ]
  },
  {
    "objectID": "iv.html#the-idea",
    "href": "iv.html#the-idea",
    "title": "Instrumental Variables",
    "section": "",
    "text": "You want the causal effect of \\(X\\) on \\(Y\\), but \\(X\\) is endogenous — correlated with the error term because of confounding, reverse causality, or measurement error. OLS is biased.\nThe fix: find a variable \\(Z\\) (the instrument) that:\n\nRelevance: \\(Z\\) is correlated with \\(X\\) — it actually moves \\(X\\)\nExclusion restriction: \\(Z\\) affects \\(Y\\) only through \\(X\\) — no back doors\n\n\\[Z \\to X \\to Y\\]\nIf both conditions hold, you can use \\(Z\\) to isolate the part of \\(X\\) that’s “as good as random” and estimate the causal effect.\n\n\nThe mechanics are simple:\nFirst stage: regress \\(X\\) on \\(Z\\) to get predicted values \\(\\hat{X}\\)\n\\[X = \\pi_0 + \\pi_1 Z + v\\]\nSecond stage: regress \\(Y\\) on \\(\\hat{X}\\) instead of \\(X\\)\n\\[Y = \\beta_0 + \\beta_1 \\hat{X} + \\varepsilon\\]\nWhy does this work? \\(\\hat{X}\\) contains only the variation in \\(X\\) that comes from \\(Z\\). Since \\(Z\\) is exogenous (by assumption), \\(\\hat{X}\\) is uncorrelated with the error term. The confounding is gone.\n\n\n\n\nRelevance: \\(Z\\) is correlated with \\(X\\) — the instrument actually moves the endogenous variable. Testable: check the first-stage F-statistic.\nExclusion restriction: \\(Z\\) affects \\(Y\\) only through \\(X\\) — no direct effect and no back-door paths. Not testable — you argue it.\nIndependence: \\(Z\\) is as good as randomly assigned — uncorrelated with the error term in the outcome equation\nMonotonicity (for LATE): the instrument moves everyone in the same direction — no “defiers” who do the opposite of what the instrument encourages\n\n\n\n\nReturns to education. You want to know if more schooling causes higher earnings. But ability confounds: smarter people get more education and earn more. OLS overstates the return.\nAngrist & Krueger (1991) used quarter of birth as an instrument. Because of compulsory schooling laws, people born in Q1 can drop out slightly earlier than Q4 births — so quarter of birth affects education (relevance) but presumably doesn’t affect earnings directly (exclusion).\n\n\n\n\nWeak instruments: if \\(Z\\) barely moves \\(X\\), the first stage is weak and the IV estimate becomes wildly noisy and biased. Rule of thumb: first-stage F-statistic &gt; 10.\nExclusion restriction violated: if \\(Z\\) affects \\(Y\\) through channels other than \\(X\\), the estimate is biased. This assumption is untestable — you argue it, you don’t prove it.\n\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 100, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"true_b\", \"True causal effect of X on Y:\",\n                  min = 0, max = 5, value = 2, step = 0.25),\n\n      sliderInput(\"confound\", \"Confounding strength:\",\n                  min = 0, max = 5, value = 3, step = 0.25),\n\n      sliderInput(\"inst_str\", \"Instrument strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.1),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"iv_plot\", height = \"470px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    b   &lt;- input$true_b\n    cf  &lt;- input$confound\n    pi1 &lt;- input$inst_str\n\n    # Confounder (unobserved ability)\n    u &lt;- rnorm(n)\n\n    # Instrument\n    z &lt;- rnorm(n)\n\n    # Endogenous regressor: driven by instrument + confounder\n    x &lt;- pi1 * z + cf * u + rnorm(n)\n\n    # Outcome: causal effect of x + confounder\n    y &lt;- b * x + cf * u + rnorm(n)\n\n    # OLS (biased)\n    ols &lt;- lm(y ~ x)\n\n    # 2SLS by hand\n    first &lt;- lm(x ~ z)\n    x_hat &lt;- fitted(first)\n    second &lt;- lm(y ~ x_hat)\n\n    # First-stage F\n    f_stat &lt;- summary(first)$fstatistic[1]\n\n    list(x = x, y = y, z = z, x_hat = x_hat,\n         b_ols = coef(ols)[2],\n         b_iv = coef(second)[2],\n         first_coef = coef(first)[2],\n         f_stat = f_stat,\n         true_b = b, confound = cf, inst_str = pi1)\n  })\n\n  output$iv_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mfrow = c(1, 2), mar = c(4.5, 4.5, 3, 1))\n\n    # Left: OLS scatter (X vs Y)\n    plot(d$x, d$y, pch = 16, cex = 0.4,\n         col = adjustcolor(\"#3498db\", 0.3),\n         xlab = \"X (endogenous)\", ylab = \"Y\",\n         main = \"OLS: Y on X\")\n    abline(lm(d$y ~ d$x), col = \"#e74c3c\", lwd = 3)\n    abline(a = 0, b = d$true_b, col = \"#27ae60\", lwd = 2, lty = 2)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(paste0(\"OLS = \", round(d$b_ols, 2)),\n                      paste0(\"True = \", d$true_b)),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = c(3, 2), lty = c(1, 2))\n\n    # Right: IV scatter (X-hat vs Y)\n    plot(d$x_hat, d$y, pch = 16, cex = 0.4,\n         col = adjustcolor(\"#9b59b6\", 0.3),\n         xlab = expression(hat(X) ~ \"(from first stage)\"), ylab = \"Y\",\n         main = expression(\"2SLS: Y on \" * hat(X)))\n    abline(lm(d$y ~ d$x_hat), col = \"#e74c3c\", lwd = 3)\n    abline(a = 0, b = d$true_b, col = \"#27ae60\", lwd = 2, lty = 2)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(paste0(\"IV = \", round(d$b_iv, 2)),\n                      paste0(\"True = \", d$true_b)),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = c(3, 2), lty = c(1, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    ols_bias &lt;- d$b_ols - d$true_b\n    iv_bias  &lt;- d$b_iv - d$true_b\n    weak &lt;- d$f_stat &lt; 10\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$true_b, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;OLS:&lt;/b&gt; \", round(d$b_ols, 3),\n        \" &nbsp; Bias: &lt;span class='bad'&gt;\", round(ols_bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;IV (2SLS):&lt;/b&gt; \", round(d$b_iv, 3),\n        \" &nbsp; Bias: &lt;span class='\", ifelse(abs(iv_bias) &lt; abs(ols_bias), \"good\", \"bad\"), \"'&gt;\",\n        round(iv_bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;First-stage F:&lt;/b&gt; \", round(d$f_stat, 1),\n        if (weak) \" &lt;span class='bad'&gt;&lt; 10 (weak!)&lt;/span&gt;\"\n        else \" &lt;span class='good'&gt;&ge; 10&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nConfounding = 3, instrument = 1.5: OLS is badly biased. IV recovers the true effect. This is the whole point.\nSet confounding = 0: OLS and IV agree — when there’s no endogeneity, you don’t need an instrument.\nSlide instrument strength toward 0: the first-stage F drops below 10. The IV estimate becomes erratic — sometimes worse than OLS. That’s the weak instrument problem.\nIncrease sample size with a weak instrument: it doesn’t help much. Weak instruments bias IV toward OLS, and more data doesn’t fix that.\nTrue effect = 0, confounding = 3: OLS “finds” a large effect. IV correctly shows ~0.",
    "crumbs": [
      "Methods",
      "Instrumental Variables"
    ]
  },
  {
    "objectID": "iv.html#what-does-iv-actually-estimate",
    "href": "iv.html#what-does-iv-actually-estimate",
    "title": "Instrumental Variables",
    "section": "What does IV actually estimate?",
    "text": "What does IV actually estimate?\nA subtle point: IV doesn’t estimate the effect for everyone. It estimates the Local Average Treatment Effect (LATE) — the effect for compliers, people whose treatment status is actually changed by the instrument.\nIn the Angrist & Krueger example: IV estimates the return to education for people who would have dropped out if born in a different quarter. It says nothing about people who would have stayed in school regardless.\nThis means two different valid instruments can give you two different IV estimates — not because one is wrong, but because they’re identifying effects for different subpopulations.",
    "crumbs": [
      "Methods",
      "Instrumental Variables"
    ]
  },
  {
    "objectID": "iv.html#in-stata",
    "href": "iv.html#in-stata",
    "title": "Instrumental Variables",
    "section": "In Stata",
    "text": "In Stata\n* Two-stage least squares\nivregress 2sls outcome x1 x2 (treatment = instrument)\n\n* First-stage F statistic (check relevance)\nestat firststage\n\n* Overidentification test (with multiple instruments)\nivregress 2sls outcome x1 (treatment = inst1 inst2)\nestat overid\n\n* Manually run the two stages (to see what's happening)\nreg treatment instrument x1 x2          /* first stage */\npredict treatment_hat, xb\nreg outcome treatment_hat x1 x2         /* second stage */\nThe first-stage F should be well above 10 (Staiger & Stock rule of thumb). If it’s weak, the IV estimate is unreliable — possibly more biased than OLS.",
    "crumbs": [
      "Methods",
      "Instrumental Variables"
    ]
  },
  {
    "objectID": "iv.html#did-you-know",
    "href": "iv.html#did-you-know",
    "title": "Instrumental Variables",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe instrumental variables method dates back to Philip Wright (1928), who used it to estimate supply and demand curves for butter and flax seed. Some historians credit his son, Sewall Wright, with the actual derivation.\nThe “weak instruments” problem was formalized by Staiger & Stock (1997). They showed that when the first-stage F is below 10, IV can be more biased than OLS — the cure becomes worse than the disease.\nJoshua Angrist, one of the 2021 Nobel laureates, built much of his career on clever instruments: quarter of birth for schooling, draft lottery numbers for military service, religious composition for family size. The art is finding instruments that are both relevant and excludable.",
    "crumbs": [
      "Methods",
      "Instrumental Variables"
    ]
  },
  {
    "objectID": "panel-fe-re.html",
    "href": "panel-fe-re.html",
    "title": "Fixed vs Random Effects",
    "section": "",
    "text": "Most of the methods in this course use cross-sectional data — you see each unit once. But often you observe units repeatedly:\n\nThe same people surveyed each year (panel)\nStudents nested within schools (clusters)\nCensus tracts nested within counties (hierarchical)\n\nPanel/grouped data has a superpower: you can separate within-group variation (what changes over time for the same person) from between-group variation (how different people differ from each other). This distinction is the key to fixed vs random effects.\n\n\n\n\n\n\nExample: dollar stores and obesity. You have census tracts nested within counties, observed over multiple years. Some tracts get a new dollar store, others don’t. The question: does dollar store presence cause higher obesity? The problem: counties that attract dollar stores may already be different — poorer, more rural, less access to grocery stores. That’s a group-level confounder. Fixed effects can absorb it.",
    "crumbs": [
      "Panel Data",
      "Fixed vs Random Effects"
    ]
  },
  {
    "objectID": "panel-fe-re.html#panel-data-the-same-units-over-time-or-nested-units",
    "href": "panel-fe-re.html#panel-data-the-same-units-over-time-or-nested-units",
    "title": "Fixed vs Random Effects",
    "section": "",
    "text": "Most of the methods in this course use cross-sectional data — you see each unit once. But often you observe units repeatedly:\n\nThe same people surveyed each year (panel)\nStudents nested within schools (clusters)\nCensus tracts nested within counties (hierarchical)\n\nPanel/grouped data has a superpower: you can separate within-group variation (what changes over time for the same person) from between-group variation (how different people differ from each other). This distinction is the key to fixed vs random effects.\n\n\n\n\n\n\nExample: dollar stores and obesity. You have census tracts nested within counties, observed over multiple years. Some tracts get a new dollar store, others don’t. The question: does dollar store presence cause higher obesity? The problem: counties that attract dollar stores may already be different — poorer, more rural, less access to grocery stores. That’s a group-level confounder. Fixed effects can absorb it.",
    "crumbs": [
      "Panel Data",
      "Fixed vs Random Effects"
    ]
  },
  {
    "objectID": "panel-fe-re.html#fixed-effects-fe",
    "href": "panel-fe-re.html#fixed-effects-fe",
    "title": "Fixed vs Random Effects",
    "section": "Fixed effects (FE)",
    "text": "Fixed effects (FE)\nThe idea: include a separate intercept \\(\\alpha_j\\) for each group. This absorbs everything that is constant within the group — observed or unobserved.\n\\[Y_{it} = \\alpha_i + \\tau D_{it} + \\beta X_{it} + \\varepsilon_{it}\\]\nIn practice, you demean each variable by its group average:\n\\[\\tilde{Y}_{it} = Y_{it} - \\bar{Y}_i, \\quad \\tilde{D}_{it} = D_{it} - \\bar{D}_i, \\quad \\tilde{X}_{it} = X_{it} - \\bar{X}_i\\]\nThen regress \\(\\tilde{Y}\\) on \\(\\tilde{D}\\) and \\(\\tilde{X}\\). The demeaning eliminates \\(\\alpha_i\\) entirely — you only use within-group variation.\n\nWhat FE eliminates\nAny variable that is constant within a group gets differenced out:\n\nCounty-level: food culture, geography, baseline poverty level\nPerson-level (in a panel): ability, motivation, family background\nFirm-level: management quality, industry\n\nThis is the power of FE: you don’t need to measure or even name the confounders. If they’re constant within groups, they’re gone.\n\n\nWhat FE can’t do\n\nCan’t estimate effects of group-level variables. If you want to know the effect of being rural vs urban, FE absorbs that — it’s constant within county.\nCan’t handle time-varying unobservables. If something changes within the group and is correlated with treatment, FE doesn’t help.\nUses only within-group variation. If treatment barely varies within groups, FE has little to work with and estimates are imprecise.",
    "crumbs": [
      "Panel Data",
      "Fixed vs Random Effects"
    ]
  },
  {
    "objectID": "panel-fe-re.html#random-effects-re",
    "href": "panel-fe-re.html#random-effects-re",
    "title": "Fixed vs Random Effects",
    "section": "Random effects (RE)",
    "text": "Random effects (RE)\nThe idea: instead of treating each \\(\\alpha_j\\) as a fixed parameter, assume the group effects are random draws from a population:\n\\[\\alpha_j \\sim N(\\mu, \\tau^2)\\]\nThis is the same structure as the hierarchical model on the Bayesian course — RE is its frequentist counterpart. It produces partial pooling: small groups are shrunk toward the grand mean.\n\nThe critical assumption\nRE assumes group effects are uncorrelated with the covariates:\n\\[E[\\alpha_j \\mid X_{it}, D_{it}] = \\mu \\quad \\text{(no correlation)}\\]\nIf this holds, RE is more efficient than FE — it uses both within-group and between-group variation, so estimates are more precise.\nIf this fails — if groups with higher \\(\\alpha_j\\) systematically differ in their \\(X\\) or \\(D\\) values — RE is biased. This is the same selection bias problem from the potential outcomes page, but at the group level.",
    "crumbs": [
      "Panel Data",
      "Fixed vs Random Effects"
    ]
  },
  {
    "objectID": "panel-fe-re.html#the-tradeoff",
    "href": "panel-fe-re.html#the-tradeoff",
    "title": "Fixed vs Random Effects",
    "section": "The tradeoff",
    "text": "The tradeoff\n\n\n\n\n\n\n\n\n\nFixed effects\nRandom effects\n\n\n\n\nEliminates group-level confounders\nYes\nOnly if uncorrelated with X\n\n\nUses between-group variation\nNo (only within)\nYes (within + between)\n\n\nEfficiency\nLower (discards information)\nHigher (if assumption holds)\n\n\nCan estimate group-level predictors\nNo\nYes\n\n\nBias when \\(\\alpha_j\\) correlated with X\nNone\nBiased\n\n\n\nThe rule: if you’re after a causal effect and you’re worried about group-level confounders, use FE. If you believe the RE assumption (no correlation) or you need group-level predictors, use RE. When in doubt, FE is the safer choice.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_groups\", \"Number of groups:\",\n                  min = 10, max = 50, value = 20, step = 5),\n\n      sliderInput(\"n_per\", \"Observations per group:\",\n                  min = 5, max = 50, value = 10, step = 5),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"corr_alpha\", \"Correlation (\\u03B1 with D):\",\n                  min = 0, max = 3, value = 0, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"scatter_plot\", height = \"420px\")),\n        column(6, plotOutput(\"compare_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    J     &lt;- input$n_groups\n    n_per &lt;- input$n_per\n    ate   &lt;- input$ate\n    rho   &lt;- input$corr_alpha\n\n    # Group effects\n    alpha &lt;- rnorm(J, mean = 0, sd = 3)\n\n    # Expand to individual level\n    group &lt;- rep(1:J, each = n_per)\n    alpha_i &lt;- rep(alpha, each = n_per)\n    N &lt;- J * n_per\n\n    # Treatment: correlated with group effect (the confounding)\n    x &lt;- rnorm(N)\n    p_treat &lt;- pnorm(0.5 * x + rho * alpha_i / 3)\n    treat &lt;- rbinom(N, 1, p_treat)\n\n    # Outcome\n    y &lt;- alpha_i + ate * treat + 1.5 * x + rnorm(N)\n\n    # Naive (pooled OLS, no group effects)\n    naive &lt;- coef(lm(y ~ treat + x))[\"treat\"]\n\n    # Fixed effects (demean within group)\n    y_dm &lt;- y - ave(y, group)\n    d_dm &lt;- treat - ave(treat, group)\n    x_dm &lt;- x - ave(x, group)\n    fe_est &lt;- coef(lm(y_dm ~ d_dm + x_dm - 1))[\"d_dm\"]\n\n    # Random effects (partial pooling via weighted average)\n    # Simple RE: GLS with estimated variance components\n    # Use between and within estimators\n    y_bar_j &lt;- ave(y, group)\n    d_bar_j &lt;- ave(treat, group)\n    x_bar_j &lt;- ave(x, group)\n\n    # Within estimator = FE\n    # Between estimator\n    uj &lt;- tapply(y, group, mean)\n    dj &lt;- tapply(treat, group, mean)\n    xj &lt;- tapply(x, group, mean)\n    if (sd(dj) &gt; 0.01) {\n      be_est &lt;- coef(lm(uj ~ dj + xj))[\"dj\"]\n    } else {\n      be_est &lt;- naive\n    }\n\n    # RE is a weighted combo of within and between\n    sigma2_e &lt;- var(y_dm - fe_est * d_dm - coef(lm(y_dm ~ d_dm + x_dm - 1))[\"x_dm\"] * x_dm)\n    sigma2_a &lt;- max(var(uj - fe_est * dj) - sigma2_e / n_per, 0.01)\n    theta &lt;- 1 - sqrt(sigma2_e / (sigma2_e + n_per * sigma2_a))\n\n    y_re &lt;- y - theta * y_bar_j\n    d_re &lt;- treat - theta * d_bar_j\n    x_re &lt;- x - theta * x_bar_j\n    re_est &lt;- coef(lm(y_re ~ d_re + x_re))[\"d_re\"]\n\n    list(y = y, treat = treat, x = x, group = group, alpha_i = alpha_i,\n         naive = naive, fe_est = fe_est, re_est = re_est, be_est = be_est,\n         ate = ate, rho = rho, J = J, n_per = n_per,\n         alpha = alpha)\n  })\n\n  output$scatter_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Color by group (cycle through colors)\n    palette &lt;- rep(c(\"#3498db\", \"#e74c3c\", \"#27ae60\", \"#f39c12\",\n                     \"#9b59b6\", \"#1abc9c\", \"#e67e22\", \"#2c3e50\"),\n                   length.out = d$J)\n    cols &lt;- paste0(palette[d$group], \"50\")\n\n    plot(d$x, d$y, pch = 16, cex = 0.4, col = cols,\n         xlab = \"X (covariate)\", ylab = \"Y (outcome)\",\n         main = \"Data by Group (colors = groups)\")\n\n    # Show a few group means\n    show_groups &lt;- 1:min(5, d$J)\n    for (j in show_groups) {\n      idx &lt;- d$group == j\n      points(mean(d$x[idx]), mean(d$y[idx]),\n             pch = 17, cex = 1.5, col = palette[j])\n    }\n\n    if (d$rho &gt; 0) {\n      mtext(expression(alpha[j] * \" correlated with D — RE is biased\"),\n            side = 3, line = 0, cex = 0.8, col = \"#e74c3c\", font = 2)\n    } else {\n      mtext(expression(alpha[j] * \" independent of D — RE is valid\"),\n            side = 3, line = 0, cex = 0.8, col = \"#27ae60\", font = 2)\n    }\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 9, 3, 2))\n\n    ests &lt;- c(d$fe_est, d$re_est, d$naive)\n    labels &lt;- c(\"Fixed effects\", \"Random effects\", \"Pooled OLS\")\n    cols &lt;- c(\"#27ae60\", \"#3498db\", \"#e74c3c\")\n\n    xlim &lt;- range(c(ests, d$ate))\n    pad  &lt;- max(diff(xlim) * 0.4, 0.5)\n    xlim &lt;- xlim + c(-pad, pad)\n\n    plot(ests, 1:3, pch = 19, cex = 2, col = cols,\n         xlim = xlim, ylim = c(0.5, 3.5),\n         yaxt = \"n\", xlab = \"Estimated treatment effect\",\n         ylab = \"\", main = \"Estimator Comparison\")\n    axis(2, at = 1:3, labels = labels, las = 1, cex.axis = 0.9)\n\n    abline(v = d$ate, lty = 2, col = \"#2c3e50\", lwd = 2)\n    text(d$ate, 3.45, paste0(\"True ATE = \", d$ate),\n         cex = 0.85, font = 2, col = \"#2c3e50\")\n\n    segments(d$ate, 1:3, ests, 1:3, col = cols, lwd = 2, lty = 2)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    b_naive &lt;- d$naive - d$ate\n    b_fe    &lt;- d$fe_est - d$ate\n    b_re    &lt;- d$re_est - d$ate\n\n    fe_class &lt;- if (abs(b_fe) &lt; abs(b_naive) * 0.5) \"good\" else \"bad\"\n    re_class &lt;- if (abs(b_re) &lt; abs(b_naive) * 0.5) \"good\" else \"bad\"\n\n    hausman &lt;- abs(d$fe_est - d$re_est)\n    h_class &lt;- if (hausman &lt; 0.3) \"good\" else \"bad\"\n    h_label &lt;- if (hausman &lt; 0.3) \"FE \\u2248 RE (RE likely OK)\" else \"FE \\u2260 RE (use FE)\"\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Pooled OLS:&lt;/b&gt; &lt;span class='bad'&gt;\", round(d$naive, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_naive, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;Fixed effects:&lt;/b&gt; &lt;span class='\", fe_class, \"'&gt;\",\n        round(d$fe_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_fe, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;Random effects:&lt;/b&gt; &lt;span class='\", re_class, \"'&gt;\",\n        round(d$re_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_re, 3), \")&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;|FE \\u2212 RE|:&lt;/b&gt; &lt;span class='\", h_class, \"'&gt;\",\n        round(hausman, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;span class='\", h_class, \"'&gt;\", h_label, \"&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nCorrelation = 0: group effects are independent of treatment. All three estimators work, but RE and FE are close to the truth while pooled OLS may show some bias. The |FE − RE| gap is small — Hausman says RE is fine.\nCorrelation = 1.5: now groups with higher \\(\\alpha_j\\) are more likely to get treated. Pooled OLS is badly biased. FE still works (it eliminates \\(\\alpha_j\\)). RE is biased — it uses between-group variation that’s contaminated by the correlation. The |FE − RE| gap is large — Hausman says use FE.\nCorrelation = 3: extreme confounding. RE is nearly as biased as pooled OLS. FE remains unbiased. This is why FE is the default in applied economics.\nSmall groups (5 obs per group): FE is noisier because it uses less data. RE is more precise when valid — that’s the efficiency gain. But precision doesn’t help if the estimate is biased.",
    "crumbs": [
      "Panel Data",
      "Fixed vs Random Effects"
    ]
  },
  {
    "objectID": "panel-fe-re.html#the-hausman-test",
    "href": "panel-fe-re.html#the-hausman-test",
    "title": "Fixed vs Random Effects",
    "section": "The Hausman test",
    "text": "The Hausman test\nYou’ve seen it informally in the simulation: when |FE − RE| is large, RE is suspect. The Hausman test formalizes this.\n\nThe logic\nUnder the RE assumption (\\(\\alpha_j\\) uncorrelated with \\(X\\) and \\(D\\)):\n\nFE is consistent (unbiased) but inefficient (throws away between-group info)\nRE is consistent and efficient (uses all the data)\nSo FE and RE should give approximately the same answer\n\nIf the RE assumption fails:\n\nFE is still consistent (within-group variation is clean)\nRE is inconsistent (between-group variation is contaminated)\nFE and RE will diverge\n\nThe Hausman test exploits this:\n\\[H = (\\hat{\\tau}_{FE} - \\hat{\\tau}_{RE})' \\, [\\text{Var}(\\hat{\\tau}_{FE}) - \\text{Var}(\\hat{\\tau}_{RE})]^{-1} \\, (\\hat{\\tau}_{FE} - \\hat{\\tau}_{RE})\\]\nUnder \\(H_0\\) (RE is valid), \\(H \\sim \\chi^2_k\\) where \\(k\\) is the number of coefficients being compared.\n\nLarge \\(H\\) (small p-value): reject \\(H_0\\). FE and RE disagree → the RE assumption fails → use FE.\nSmall \\(H\\) (large p-value): fail to reject. FE and RE agree → RE assumption is plausible → RE is OK (and more efficient).\n\n\n\n\n\n\n\nExample: returns to education. You have panel data on workers observed over multiple years. You want the causal effect of an extra year of education on wages.\n\nRE assumes: workers who get more education don’t have systematically higher unobserved ability. Ability is random across education levels.\nFE says: doesn’t matter — I’ll control for each worker’s fixed ability by only using within-person wage changes when they get more education.\n\nYou run both. FE gives \\(\\hat{\\tau} = 0.06\\) (6% wage increase per year of education). RE gives \\(\\hat{\\tau} = 0.10\\) (10%). The gap is large.\nHausman test: \\(H = 15.3\\), \\(p &lt; 0.001\\). Reject \\(H_0\\). The RE estimate is inflated because high-ability workers get more education and earn more — classic selection bias at the group level. Use FE.\nThe FE estimate (6%) is the return to education holding ability constant. The RE estimate (10%) is contaminated by ability differences.\n\n\n\n\n\nHausman test in practice\n\n\n\n\n\n\n\n\nResult\nInterpretation\nAction\n\n\n\n\n\\(p &lt; 0.05\\)\nFE and RE significantly differ\nUse FE (RE assumption violated)\n\n\n\\(p &gt; 0.05\\)\nNo significant difference\nRE is OK — more efficient\n\n\n\nCaveats:\n\nThe Hausman test has low power with small samples — it may fail to reject even when RE is biased.\nA non-rejection doesn’t prove RE is valid. It means you can’t detect the violation with your data.\nIn practice, most applied economists default to FE for causal questions and use the Hausman test as a check, not as the primary decision tool.",
    "crumbs": [
      "Panel Data",
      "Fixed vs Random Effects"
    ]
  },
  {
    "objectID": "panel-fe-re.html#connection-to-other-methods",
    "href": "panel-fe-re.html#connection-to-other-methods",
    "title": "Fixed vs Random Effects",
    "section": "Connection to other methods",
    "text": "Connection to other methods\n\n\n\nMethod\nWhat it controls for\nAssumption\n\n\n\n\nFixed effects\nAll time-invariant group confounders\nNo time-varying confounders\n\n\nRandom effects\nGroup-level variation (partial pooling)\nGroup effects uncorrelated with X\n\n\nDID\nGroup FE + time FE\nParallel trends\n\n\nHierarchical models\nBayesian version of RE\nSame as RE, with priors\n\n\nClustered SEs\nNot a model — fixes standard errors\nDoesn’t change point estimates\n\n\n\nFE is an identification strategy. It tells you why your comparison is valid (within-group variation eliminates group-level confounders). RE is a modeling assumption that may or may not hold. When in doubt, FE is the conservative choice for causal inference.",
    "crumbs": [
      "Panel Data",
      "Fixed vs Random Effects"
    ]
  },
  {
    "objectID": "panel-fe-re.html#in-stata",
    "href": "panel-fe-re.html#in-stata",
    "title": "Fixed vs Random Effects",
    "section": "In Stata",
    "text": "In Stata\n* Declare panel structure\nxtset unit_id year\n\n* Fixed effects\nxtreg outcome treatment x1, fe cluster(unit_id)\n\n* Random effects\nxtreg outcome treatment x1, re\n\n* Hausman test (FE vs RE)\nquietly xtreg outcome treatment x1, fe\nestimates store fe_model\nquietly xtreg outcome treatment x1, re\nestimates store re_model\nhausman fe_model re_model\n\n* First-difference (alternative to FE)\nreg D.outcome D.treatment D.x1, cluster(unit_id)\nIf the Hausman test rejects, use FE — the RE assumption that group effects are uncorrelated with regressors doesn’t hold. For causal inference, FE is almost always the safer choice.",
    "crumbs": [
      "Panel Data",
      "Fixed vs Random Effects"
    ]
  },
  {
    "objectID": "panel-fe-re.html#did-you-know",
    "href": "panel-fe-re.html#did-you-know",
    "title": "Fixed vs Random Effects",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe Hausman test was introduced by Jerry Hausman (1978) in one of the most cited papers in econometrics. The general principle extends beyond FE/RE: any time you have a consistent-but-inefficient estimator and an efficient-but-possibly-inconsistent estimator, you can compare them. If they agree, the efficient one is probably fine.\nMundlak (1978) showed a clever alternative: add the group means \\(\\bar{X}_j\\) as regressors in the RE model. If the coefficient on \\(\\bar{X}_j\\) is significant, the RE assumption is violated (same information as the Hausman test, but easier to implement and interpret). This is called the correlated random effects approach.\nIn the machine learning world, random effects are closely related to mixed-effects models (e.g., lme4 in R). The terminology differs across fields: what economists call “fixed effects” (unit dummies), statisticians sometimes call “fixed effects” too, but the random effects tradition is much more developed in biostatistics and multilevel modeling (Raudenbush & Bryk, 2002).",
    "crumbs": [
      "Panel Data",
      "Fixed vs Random Effects"
    ]
  },
  {
    "objectID": "identification-estimation.html",
    "href": "identification-estimation.html",
    "title": "Identification vs Estimation",
    "section": "",
    "text": "Every causal inference project answers two fundamentally different questions:\n\nIdentification. Why is this comparison causal? What assumptions make the estimand equal to the causal parameter?\nEstimation. How do we compute the estimand from data? What statistical procedure do we use?\n\nThese are conceptually independent:\n\nYou can have correct identification with a poor estimator (unbiased but noisy).\nYou can have a highly efficient estimator with no identification (precise but biased toward the wrong parameter).\n\nIdentification comes first. If the assumptions fail, no estimator can recover the causal effect.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#two-separate-questions-in-every-causal-study",
    "href": "identification-estimation.html#two-separate-questions-in-every-causal-study",
    "title": "Identification vs Estimation",
    "section": "",
    "text": "Every causal inference project answers two fundamentally different questions:\n\nIdentification. Why is this comparison causal? What assumptions make the estimand equal to the causal parameter?\nEstimation. How do we compute the estimand from data? What statistical procedure do we use?\n\nThese are conceptually independent:\n\nYou can have correct identification with a poor estimator (unbiased but noisy).\nYou can have a highly efficient estimator with no identification (precise but biased toward the wrong parameter).\n\nIdentification comes first. If the assumptions fail, no estimator can recover the causal effect.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#identification-the-assumption",
    "href": "identification-estimation.html#identification-the-assumption",
    "title": "Identification vs Estimation",
    "section": "Identification: the assumption",
    "text": "Identification: the assumption\nIdentification is about the source of exogenous variation — why the variation in treatment you’re using is “as good as random” for estimating a causal effect.\n\n\n\n\n\n\n\n\nIdentification strategy\nThe assumption\nIn words\n\n\n\n\nSelection on observables\n\\(Y(0), Y(1) \\perp D \\mid X\\)\nConditional on X, treatment is as good as random\n\n\nParallel trends\n\\(E[Y(0)_t - Y(0)_{t-1} \\mid D=1] = E[Y(0)_t - Y(0)_{t-1} \\mid D=0]\\)\nAbsent treatment, both groups would have trended the same\n\n\nExclusion restriction\n\\(Z\\) affects \\(Y\\) only through \\(X\\)\nThe instrument has no direct effect on the outcome\n\n\nContinuity\n\\(E[Y(0) \\mid X=x]\\) is continuous at the cutoff\nNo other jump happens at the cutoff\n\n\n\nEach assumption is a claim about the world — not something you compute. You argue it using institutional knowledge, theory, and indirect evidence. Some are partially testable (you can check pre-trends for DID, run a McCrary test for RDD), but none can be fully proven from data.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#estimation-the-computation",
    "href": "identification-estimation.html#estimation-the-computation",
    "title": "Identification vs Estimation",
    "section": "Estimation: the computation",
    "text": "Estimation: the computation\nEstimation is about how you turn data into a number, given that you believe your identification assumption holds.\n\n\n\n\n\n\n\nEstimator\nWhat it does\n\n\n\n\nOLS regression\nFits a linear model, uses coefficients\n\n\nMatching\nPairs treated/control units with similar covariates\n\n\nIPW\nReweights observations by inverse propensity scores\n\n\nEntropy balancing\nFinds weights that exactly balance covariate moments\n\n\nDoubly robust\nCombines regression and weighting\n\n\n2SLS\nTwo-stage regression using predicted values from the first stage\n\n\nLocal polynomial\nFits flexible curves on each side of a cutoff\n\n\nSynthetic control weights\nConstrained optimization to match pre-treatment trends\n\n\nTWFE\nTwo-way fixed effects regression\n\n\n\nThese are tools — they can often be combined with different identification strategies. IPW can implement selection on observables or be used in a DID design (IPW-DID). Regression can adjust for covariates in an RDD or in a cross-sectional study. The estimator doesn’t determine identification; the assumption does.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#research-designs-bundle-both",
    "href": "identification-estimation.html#research-designs-bundle-both",
    "title": "Identification vs Estimation",
    "section": "Research designs bundle both",
    "text": "Research designs bundle both\nWhat we usually call “methods” in applied work — DID, IV, RDD — are really research designs that bundle an identification strategy with a default estimator:\n\n\n\n\n\n\n\n\nResearch design\nIdentification\nCommon estimators\n\n\n\n\nSOO study\nConditional independence\nRegression, matching, IPW, EB, doubly robust\n\n\nDID\nParallel trends\n2×2 difference, TWFE, IPW-DID (Abadie 2005), DR-DID (Sant’Anna & Zhao 2020)\n\n\nIV\nExclusion restriction + relevance\n2SLS, LIML, GMM\n\n\nRDD\nContinuity at cutoff\nLocal polynomial, local randomization\n\n\nSynthetic control\nPre-treatment fit → valid counterfactual\nConstrained weight optimization, augmented SCM\n\n\n\nThis is why the same estimation tool shows up in multiple designs. IPW appears in the SOO column and the DID column — because it’s a tool, not a strategy.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#the-math-where-bias-comes-from",
    "href": "identification-estimation.html#the-math-where-bias-comes-from",
    "title": "Identification vs Estimation",
    "section": "The math: where bias comes from",
    "text": "The math: where bias comes from\nWhen an identification assumption fails, it introduces a bias term that no estimator can remove. Here’s the decomposition for three methods.\n\nSelection on observables\nWe want the Average Treatment Effect on the Treated (ATT):\n\\[\\tau = E[Y(1) - Y(0) \\mid D = 1]\\]\nWe observe \\(E[Y \\mid D=1] = E[Y(1) \\mid D=1]\\) and \\(E[Y \\mid D=0] = E[Y(0) \\mid D=0]\\). The naive comparison is:\n\\[E[Y \\mid D=1] - E[Y \\mid D=0] = \\underbrace{E[Y(1) - Y(0) \\mid D=1]}_{\\text{ATT}} + \\underbrace{E[Y(0) \\mid D=1] - E[Y(0) \\mid D=0]}_{\\text{selection bias}}\\]\nThe second term is selection bias — the treated group would have had different outcomes even without treatment. The CIA says: conditional on \\(X\\), \\(E[Y(0) \\mid D=1, X] = E[Y(0) \\mid D=0, X]\\), so the selection bias is zero within each stratum of \\(X\\).\nIf the CIA fails — there’s an unobserved confounder \\(U\\) — then \\(E[Y(0) \\mid D=1, X] \\neq E[Y(0) \\mid D=0, X]\\) because \\(D\\) is still correlated with \\(Y(0)\\) through \\(U\\) even after conditioning on \\(X\\). The bias term is nonzero. Regression, IPW, matching — all give biased answers because the selection bias is baked into the estimand, not the estimator.\n\n\nDifference-in-differences\nThe DID estimand is:\n\\[\\hat{\\tau}_{DID} = \\big(E[Y_{1t}] - E[Y_{1,t-1}]\\big) - \\big(E[Y_{0t}] - E[Y_{0,t-1}]\\big)\\]\nwhere group 1 is treated, group 0 is control, \\(t\\) is post, \\(t-1\\) is pre. Substitute potential outcomes and add and subtract \\(E[Y_{1t}(0)]\\):\n\\[\\hat{\\tau}_{DID} = \\underbrace{E[Y_{1t}(1) - Y_{1t}(0)]}_{\\text{ATT}} + \\underbrace{\\big(E[Y_{1t}(0)] - E[Y_{1,t-1}(0)]\\big) - \\big(E[Y_{0t}(0)] - E[Y_{0,t-1}(0)]\\big)}_{\\text{differential trend bias}}\\]\nThe parallel trends assumption says the second term equals zero — the treated group’s untreated trajectory matches the control group’s trajectory. Then \\(\\hat{\\tau}_{DID} = \\text{ATT}\\).\nIf parallel trends fail — say the treated group was already trending upward faster — the differential trend term is positive. DID overestimates the effect. This bias doesn’t shrink with more data. It doesn’t go away if you switch from a 2×2 difference to TWFE or IPW-DID. It’s an identification failure, not an estimation failure.\n\n\nInstrumental variables\nWe have \\(Y = \\beta X + \\varepsilon\\) where \\(\\text{Cov}(X, \\varepsilon) \\neq 0\\) (endogeneity). The IV estimand is:\n\\[\\hat{\\beta}_{IV} = \\frac{\\text{Cov}(Z, Y)}{\\text{Cov}(Z, X)}\\]\nSubstitute \\(Y = \\beta X + \\varepsilon\\):\n\\[\\hat{\\beta}_{IV} = \\frac{\\text{Cov}(Z, \\beta X + \\varepsilon)}{\\text{Cov}(Z, X)} = \\beta + \\frac{\\text{Cov}(Z, \\varepsilon)}{\\text{Cov}(Z, X)}\\]\nThe exclusion restriction says \\(\\text{Cov}(Z, \\varepsilon) = 0\\) — the instrument is uncorrelated with the error. Then \\(\\hat{\\beta}_{IV} = \\beta\\).\nIf the exclusion restriction fails — \\(Z\\) directly affects \\(Y\\) through some channel other than \\(X\\) — then \\(\\text{Cov}(Z, \\varepsilon) \\neq 0\\) and the bias term \\(\\frac{\\text{Cov}(Z, \\varepsilon)}{\\text{Cov}(Z, X)}\\) is nonzero. No amount of data, no alternative estimator (LIML, GMM, jackknife) removes this. It’s baked in.\n\n\nThreats to identification\nEach method has specific threats — things that make the bias term nonzero:\n\n\n\n\n\n\n\n\n\nMethod\nIdentification assumption\nThreat (what breaks it)\nWhat the bias looks like\n\n\n\n\nSOO\nNo unobserved confounders\nOmitted variable that drives both \\(D\\) and \\(Y\\)\nSelection bias: treated group was different to begin with\n\n\nDID\nParallel trends\nTreated group was already on a different trajectory\nYou attribute the pre-existing trend to the treatment\n\n\nIV\nExclusion restriction\nInstrument affects \\(Y\\) through a channel other than \\(X\\)\nIV picks up the direct effect, not just the causal path\n\n\nRDD\nContinuity at cutoff\nUnits manipulate their score to sort across the cutoff; or another policy also kicks in at the same cutoff\nThe “jump” reflects sorting or a different treatment, not your treatment\n\n\nSynthetic control\nPre-treatment fit generalizes\nSpillovers from treated unit to donors; structural break changes the relationship\nCounterfactual is wrong, gap doesn’t reflect the treatment\n\n\n\nNotice: every threat is about the world, not about the math. You can’t test your way out of these — you argue them with institutional knowledge.\n\n\nThe pattern\nIn all three cases:\n\\[\\text{Estimate} = \\text{Causal effect} + \\text{Identification bias} + \\text{Estimation bias}\\]\nIdentification bias comes from violated assumptions — it’s a function of how the world works. Estimation bias comes from the estimator — it’s a function of how you computed the number. Identification bias dominates and can’t be fixed. Estimation bias is usually smaller and can be fixed by choosing a better estimator. That’s why identification comes first.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#types-of-bias",
    "href": "identification-estimation.html#types-of-bias",
    "title": "Identification vs Estimation",
    "section": "Types of bias",
    "text": "Types of bias\n\nIdentification biases\nThese come from the world, not the estimator. More data doesn’t help. A fancier estimator doesn’t help. You need a different identification strategy or better data.\nOmitted variable bias (OVB). The most common. An unobserved variable \\(U\\) affects both treatment and outcome. For the simple regression \\(Y = \\beta X + \\gamma U + \\varepsilon\\) where you omit \\(U\\):\n\\[\\text{OVB} = \\gamma \\cdot \\frac{\\text{Cov}(X, U)}{\\text{Var}(X)}\\]\nThe bias is the effect of \\(U\\) on \\(Y\\) (\\(\\gamma\\)) times how much \\(U\\) correlates with \\(X\\). If \\(U\\) drives people toward treatment and improves outcomes, both terms are positive and you overestimate the effect.\nSelection bias. The treated group would have had different outcomes even without treatment: \\(E[Y(0) \\mid D=1] \\neq E[Y(0) \\mid D=0]\\). This is OVB rephrased in potential outcomes language — the “omitted variable” is whatever drives people to select into treatment.\nSimultaneity bias. \\(X\\) causes \\(Y\\) but \\(Y\\) also causes \\(X\\). Regressing \\(Y\\) on \\(X\\) picks up both directions. Common in macro (do interest rates affect GDP, or does GDP affect interest rates?) and in supply/demand estimation.\nCollider bias (sample selection bias). You condition on a variable caused by both treatment and outcome — this opens a fake path between them. The correlation-causation page covers this in detail.\nDifferential trends bias. In DID: the treated group was already on a different trajectory before treatment. The estimate captures the pre-existing divergence, not the treatment effect.\n\n\nEstimation biases\nThese come from the estimator, not the world. They can be reduced or eliminated by choosing a better estimator, using more data, or fixing the specification.\nFunctional form misspecification. You fit a linear model but the truth is nonlinear. In RDD: a straight line through curved data creates a fake “jump” at the cutoff. Fix: use local polynomials, check robustness to specification.\nFinite sample / weak instrument bias. With weak instruments (\\(F &lt; 10\\)), 2SLS is biased toward OLS in finite samples — even if the exclusion restriction holds. This shrinks with stronger instruments or alternative estimators (LIML, Anderson-Rubin).\nNegative weighting (TWFE with staggered treatment). Goodman-Bacon (2021) and de Chaisemartin & d’Haultfoeuille (2020) showed that two-way fixed effects can produce negative weights on some treatment effects when treatment is staggered across time — giving biased estimates even when parallel trends holds. Fix: use Callaway & Sant’Anna, Sun & Abraham, or other heterogeneity-robust estimators.\nExtreme weights. In IPW: when propensity scores are near 0 or 1, some observations get enormous weights, making the estimate noisy and potentially biased in finite samples. Fix: trim extreme scores, use entropy balancing, or use doubly robust estimators.\nAttenuation bias (measurement error). When \\(X\\) is measured with noise, OLS is biased toward zero. The noisier the measurement relative to the true signal, the more the estimate shrinks. Can be fixed with better measurement or IV. See the measurement error page for the signal-to-noise ratio formula.\n\n\nSummary\n\n\n\n\n\n\n\n\n\nBias\nType\nGoes away with more data?\nFix\n\n\n\n\nOmitted variable / confounding\nIdentification\nNo\nBetter controls, different strategy (IV, DID, RDD)\n\n\nSelection bias\nIdentification\nNo\nRandomization, or argue CIA\n\n\nSimultaneity\nIdentification\nNo\nIV, timing restrictions\n\n\nCollider / sample selection\nIdentification\nNo\nDon’t condition on colliders\n\n\nDifferential trends\nIdentification\nNo\nDifferent comparison group, different strategy\n\n\nFunctional form\nEstimation\nPartially\nFlexible specifications, local methods\n\n\nWeak instruments\nEstimation\nPartially\nStronger instruments, LIML\n\n\nTWFE negative weighting\nEstimation\nNo\nHeterogeneity-robust DID estimators\n\n\nExtreme IPW weights\nEstimation\nYes (slowly)\nTrimming, EB, doubly robust\n\n\nAttenuation (measurement error)\nBoth\nNo\nBetter data, IV",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#simulation-identification-matters-estimation-is-secondary",
    "href": "identification-estimation.html#simulation-identification-matters-estimation-is-secondary",
    "title": "Identification vs Estimation",
    "section": "Simulation: identification matters, estimation is secondary",
    "text": "Simulation: identification matters, estimation is secondary\nSame data, same identification assumption, three different estimators. When the assumption holds, they all work. When it doesn’t, they all fail.\n#| standalone: true\n#| viewerHeight: 580\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 13px; line-height: 1.8;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_ie\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate_ie\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"obs_ie\", \"Observed confounding (X):\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      sliderInput(\"unobs_ie\", \"Unobserved confounding (U):\",\n                  min = 0, max = 3, value = 0, step = 0.25),\n\n      actionButton(\"go_ie\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_ie\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"ie_plot\", height = \"420px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go_ie\n    n   &lt;- input$n_ie\n    ate &lt;- input$ate_ie\n    gx  &lt;- input$obs_ie\n    gu  &lt;- input$unobs_ie\n\n    x &lt;- rnorm(n)\n    u &lt;- rnorm(n)\n\n    p &lt;- pnorm(gx * x + gu * u)\n    treat &lt;- rbinom(n, 1, p)\n\n    y &lt;- 1 + 2 * x + 1.5 * u + ate * treat + rnorm(n)\n\n    # Estimator 1: OLS regression controlling for X\n    est_reg &lt;- coef(lm(y ~ treat + x))[2]\n\n    # Estimator 2: IPW\n    ps &lt;- fitted(glm(treat ~ x, family = binomial))\n    ps &lt;- pmin(pmax(ps, 0.01), 0.99)\n    w &lt;- ifelse(treat == 1, 1 / ps, 1 / (1 - ps))\n    est_ipw &lt;- weighted.mean(y[treat == 1], w[treat == 1]) -\n               weighted.mean(y[treat == 0], w[treat == 0])\n\n    # Estimator 3: Matching (simple: nearest neighbor on X)\n    matched_y &lt;- numeric(sum(treat == 1))\n    x_t &lt;- x[treat == 1]\n    y_t &lt;- y[treat == 1]\n    x_c &lt;- x[treat == 0]\n    y_c &lt;- y[treat == 0]\n    for (i in seq_along(x_t)) {\n      nearest &lt;- which.min(abs(x_c - x_t[i]))\n      matched_y[i] &lt;- y_c[nearest]\n    }\n    est_match &lt;- mean(y_t) - mean(matched_y)\n\n    list(est_reg = est_reg, est_ipw = est_ipw, est_match = est_match,\n         ate = ate, gu = gu)\n  })\n\n  output$ie_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(5, 4.5, 3, 1))\n\n    estimates &lt;- c(d$est_reg, d$est_ipw, d$est_match)\n    biases &lt;- estimates - d$ate\n    labels &lt;- c(\"OLS\\nregression\", \"IPW\", \"Nearest-neighbor\\nmatching\")\n\n    cia_holds &lt;- d$gu == 0\n    cols &lt;- ifelse(abs(biases) &lt; 0.5, \"#27ae60\", \"#e74c3c\")\n\n    bp &lt;- barplot(estimates, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.85,\n                  main = ifelse(cia_holds,\n                    \"CIA holds: all estimators work\",\n                    \"CIA violated: all estimators fail\"),\n                  ylab = \"Estimate\",\n                  ylim = c(0, max(estimates, d$ate) * 1.5))\n\n    abline(h = d$ate, lty = 2, col = \"gray40\", lwd = 2)\n    text(0.2, d$ate + 0.15, paste0(\"True ATE = \", d$ate),\n         col = \"gray40\", cex = 0.85, adj = 0)\n\n    text(bp, estimates + 0.2,\n         paste0(round(estimates, 2)),\n         cex = 0.9, font = 2)\n  })\n\n  output$results_ie &lt;- renderUI({\n    d &lt;- dat()\n    cia_holds &lt;- d$gu == 0\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Regression:&lt;/b&gt; \", round(d$est_reg, 2),\n        \" (bias: \", round(d$est_reg - d$ate, 2), \")&lt;br&gt;\",\n        \"&lt;b&gt;IPW:&lt;/b&gt; \", round(d$est_ipw, 2),\n        \" (bias: \", round(d$est_ipw - d$ate, 2), \")&lt;br&gt;\",\n        \"&lt;b&gt;Matching:&lt;/b&gt; \", round(d$est_match, 2),\n        \" (bias: \", round(d$est_match - d$ate, 2), \")&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        if (cia_holds)\n          \"&lt;span class='good'&gt;CIA holds.&lt;/span&gt; All three estimators give similar, roughly unbiased answers. The choice of estimator is secondary.\"\n        else\n          \"&lt;span class='bad'&gt;CIA violated.&lt;/span&gt; All three estimators are biased. Switching estimators doesn't help — you need a different identification strategy.\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nUnobserved confounding = 0: the CIA holds. All three estimators — regression, IPW, matching — give roughly the same answer, close to the true ATE. The choice between them is about efficiency, not bias.\nUnobserved confounding = 2: the CIA is violated. All three estimators are biased in the same direction. Switching from regression to IPW to matching doesn’t help — the problem is identification, not estimation.\nIncrease sample size with unobserved confounding: all three get more precise but stay biased. More data doesn’t fix a broken assumption.\n\nThe lesson: spend your energy on identification, not on the fanciest estimator.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#in-stata-identification-estimation-cheat-sheet",
    "href": "identification-estimation.html#in-stata-identification-estimation-cheat-sheet",
    "title": "Identification vs Estimation",
    "section": "In Stata: identification → estimation cheat sheet",
    "text": "In Stata: identification → estimation cheat sheet\n\n\n\n\n\n\n\nIdentification strategy\nStata command\n\n\n\n\nRandom assignment\nreg outcome treatment\n\n\nSelection on observables\nteffects ra (outcome x1 x2) (treatment)\n\n\nInverse probability weighting\nteffects ipw (outcome) (treatment x1 x2)\n\n\nMatching\nteffects nnmatch (outcome x1 x2) (treatment)\n\n\nDoubly robust\nteffects aipw (outcome x1 x2) (treatment x1 x2)\n\n\nDifference-in-differences\nreg outcome treated##post, cluster(group)\n\n\nInstrumental variables\nivregress 2sls outcome (treatment = instrument)\n\n\nRegression discontinuity\nrdrobust outcome running_var, c(0)\n\n\nFixed effects\nxtreg outcome treatment x1, fe cluster(id)\n\n\n\nThe right column is the easy part. The hard part is arguing that the left column holds.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "identification-estimation.html#did-you-know",
    "href": "identification-estimation.html#did-you-know",
    "title": "Identification vs Estimation",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe distinction between identification and estimation was articulated clearly by Charles Manski in his 1995 book Identification Problems in the Social Sciences. He argued that most debates in empirical work are really about identification, not estimation.\nAngrist & Pischke (Mostly Harmless Econometrics, 2009) organized their entire textbook around identification strategies — regression, IV, DID, RDD — rather than estimators. This framing reshaped how a generation of economists thinks about empirical work.\nA common mistake in applied papers: spending pages discussing the estimator (clustered SEs, bootstrap, semiparametric methods) while spending one paragraph on identification. The estimator is the easy part. The hard part is arguing that your comparison is causal.",
    "crumbs": [
      "Framework",
      "Identification vs Estimation"
    ]
  },
  {
    "objectID": "selection-observables.html",
    "href": "selection-observables.html",
    "title": "Selection on Observables",
    "section": "",
    "text": "You want the causal effect of a treatment, but people select into treatment based on their characteristics. Sicker patients seek medication, motivated students enroll in programs, richer firms adopt new technology.\nThe selection on observables strategy says: if you can observe everything that drives both treatment and outcome, you can condition on it and recover the causal effect. Once you hold those variables fixed, treatment is as good as random.\n\\[Y(0), Y(1) \\perp D \\mid X\\]\nThis is the conditional independence assumption (CIA), also called unconfoundedness or ignorability. It says: among people with the same \\(X\\), who gets treated is effectively random.\n\n\nIt’s the same logic, made precise. When you run \\(Y = \\alpha + \\tau D + \\beta X + \\varepsilon\\) and claim \\(\\hat{\\tau}\\) is causal, you’re implicitly assuming selection on observables — that \\(X\\) contains all the confounders. The difference is that causal inference makes this assumption explicit and offers multiple ways to implement it, each with different strengths:\n\n\n\nMethod\nHow it adjusts for X\n\n\n\n\nRegression Adjustment\nModels the outcome as a function of X and D\n\n\nMatching\nPairs treated and control units with similar X\n\n\nIPW\nReweights units by their probability of treatment given X\n\n\nEntropy Balancing\nDirectly reweights controls to match treated group’s X distribution\n\n\nDoubly robust\nCombines regression and weighting — consistent if either model is correct\n\n\n\nAll of these rely on the same fundamental assumption. They differ in how they use X to make the comparison fair.\n\n\n\n\nConditional independence (CIA): all confounders are observed and included in X. If an unobserved variable affects both treatment and outcome, every method above is biased. This is untestable — you argue it based on institutional knowledge.\nOverlap (common support): for every value of X, there are both treated and untreated units — \\(0 &lt; P(D = 1 \\mid X) &lt; 1\\). If some covariate profiles always get treated, you can’t estimate the counterfactual for them.\nSUTVA: one unit’s treatment doesn’t affect another’s outcome.\n\n\n\n\nWhen there are unobserved confounders — variables that affect both treatment and outcome but aren’t in your data. No amount of regression, matching, or weighting can fix this.\nExamples:\n\nReturns to education: ability is unobserved. More able people get more education and earn more. Controlling for test scores helps but doesn’t fully capture ability. → You need IV.\nEffect of a new policy: states that adopt the policy may differ in unobservable ways (political will, citizen preferences). → You need DID or synthetic control.\nEffect of a drug: patients who take the drug may be sicker in ways the chart doesn’t capture. → You need an RCT.\n\nThe simulation below shows what happens when the CIA holds vs when it doesn’t.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_so\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate_so\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"obs_conf\", \"Observed confounding (X):\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      sliderInput(\"unobs_conf\", \"Unobserved confounding (U):\",\n                  min = 0, max = 3, value = 0, step = 0.25),\n\n      actionButton(\"go_so\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_so\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"so_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go_so\n    n   &lt;- input$n_so\n    ate &lt;- input$ate_so\n    gx  &lt;- input$obs_conf\n    gu  &lt;- input$unobs_conf\n\n    # Observed confounder\n    x &lt;- rnorm(n)\n\n    # Unobserved confounder\n    u &lt;- rnorm(n)\n\n    # Treatment depends on both\n    p &lt;- pnorm(gx * x + gu * u)\n    treat &lt;- rbinom(n, 1, p)\n\n    # Outcome depends on both\n    y &lt;- 1 + 2 * x + 1.5 * u + ate * treat + rnorm(n)\n\n    # Naive (no controls)\n    naive &lt;- coef(lm(y ~ treat))[2]\n\n    # Controlling for X only\n    ctrl_x &lt;- coef(lm(y ~ treat + x))[2]\n\n    # Oracle: controlling for X and U\n    oracle &lt;- coef(lm(y ~ treat + x + u))[2]\n\n    list(x = x, u = u, treat = treat, y = y,\n         naive = naive, ctrl_x = ctrl_x, oracle = oracle,\n         ate = ate, gx = gx, gu = gu)\n  })\n\n  output$so_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    estimates &lt;- c(d$naive, d$ctrl_x, d$oracle)\n    biases &lt;- estimates - d$ate\n    labels &lt;- c(\"Naive\\n(no controls)\", \"Control for X\\n(observed)\", \"Control for X + U\\n(oracle)\")\n    cols &lt;- c(\"#e74c3c\", ifelse(abs(biases[2]) &lt; 0.3, \"#27ae60\", \"#f39c12\"), \"#27ae60\")\n\n    bp &lt;- barplot(estimates, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.85,\n                  main = \"Estimated Treatment Effect by Method\",\n                  ylab = \"Estimate\", ylim = c(0, max(estimates) * 1.4))\n\n    # True ATE line\n    abline(h = d$ate, lty = 2, col = \"gray40\", lwd = 2)\n    text(0.2, d$ate + 0.15, paste0(\"True ATE = \", d$ate),\n         col = \"gray40\", cex = 0.85, adj = 0)\n\n    # Bias labels\n    text(bp, estimates + 0.15,\n         paste0(round(estimates, 2), \"\\n(bias: \", round(biases, 2), \")\"),\n         cex = 0.8)\n  })\n\n  output$results_so &lt;- renderUI({\n    d &lt;- dat()\n    bias_naive &lt;- d$naive - d$ate\n    bias_x &lt;- d$ctrl_x - d$ate\n    bias_oracle &lt;- d$oracle - d$ate\n    cia_holds &lt;- d$gu == 0\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; \", round(d$naive, 2),\n        \" &lt;span class='bad'&gt;(bias: \", round(bias_naive, 2), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Control X:&lt;/b&gt; \", round(d$ctrl_x, 2),\n        \" &lt;span class='\", ifelse(cia_holds, \"good\", \"bad\"), \"'&gt;(bias: \",\n        round(bias_x, 2), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Oracle (X+U):&lt;/b&gt; \", round(d$oracle, 2),\n        \" &lt;span class='good'&gt;(bias: \", round(bias_oracle, 2), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        if (cia_holds)\n          \"&lt;small&gt;CIA holds: controlling for X is enough.&lt;/small&gt;\"\n        else\n          \"&lt;small&gt;CIA violated: U confounds treatment. Controlling for X alone leaves residual bias.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nUnobserved confounding = 0: the CIA holds. Controlling for X eliminates all bias — the green and oracle bars match. This is the world where selection on observables works.\nUnobserved confounding = 1.5: now there’s a confounder you can’t see. Controlling for X helps (reduces bias vs naive) but doesn’t eliminate it. Only the oracle, who controls for both X and U, gets the right answer.\nUnobserved confounding = 3: controlling for X barely helps. The bias is large. No amount of regression, matching, or weighting on X can fix this — you need a different identification strategy.\nSet observed confounding = 0, unobserved = 2: all the confounding is unobserved. Naive and “control for X” give the same (biased) answer because X isn’t a confounder here.",
    "crumbs": [
      "Methods",
      "Selection on Observables"
    ]
  },
  {
    "objectID": "selection-observables.html#the-idea",
    "href": "selection-observables.html#the-idea",
    "title": "Selection on Observables",
    "section": "",
    "text": "You want the causal effect of a treatment, but people select into treatment based on their characteristics. Sicker patients seek medication, motivated students enroll in programs, richer firms adopt new technology.\nThe selection on observables strategy says: if you can observe everything that drives both treatment and outcome, you can condition on it and recover the causal effect. Once you hold those variables fixed, treatment is as good as random.\n\\[Y(0), Y(1) \\perp D \\mid X\\]\nThis is the conditional independence assumption (CIA), also called unconfoundedness or ignorability. It says: among people with the same \\(X\\), who gets treated is effectively random.\n\n\nIt’s the same logic, made precise. When you run \\(Y = \\alpha + \\tau D + \\beta X + \\varepsilon\\) and claim \\(\\hat{\\tau}\\) is causal, you’re implicitly assuming selection on observables — that \\(X\\) contains all the confounders. The difference is that causal inference makes this assumption explicit and offers multiple ways to implement it, each with different strengths:\n\n\n\nMethod\nHow it adjusts for X\n\n\n\n\nRegression Adjustment\nModels the outcome as a function of X and D\n\n\nMatching\nPairs treated and control units with similar X\n\n\nIPW\nReweights units by their probability of treatment given X\n\n\nEntropy Balancing\nDirectly reweights controls to match treated group’s X distribution\n\n\nDoubly robust\nCombines regression and weighting — consistent if either model is correct\n\n\n\nAll of these rely on the same fundamental assumption. They differ in how they use X to make the comparison fair.\n\n\n\n\nConditional independence (CIA): all confounders are observed and included in X. If an unobserved variable affects both treatment and outcome, every method above is biased. This is untestable — you argue it based on institutional knowledge.\nOverlap (common support): for every value of X, there are both treated and untreated units — \\(0 &lt; P(D = 1 \\mid X) &lt; 1\\). If some covariate profiles always get treated, you can’t estimate the counterfactual for them.\nSUTVA: one unit’s treatment doesn’t affect another’s outcome.\n\n\n\n\nWhen there are unobserved confounders — variables that affect both treatment and outcome but aren’t in your data. No amount of regression, matching, or weighting can fix this.\nExamples:\n\nReturns to education: ability is unobserved. More able people get more education and earn more. Controlling for test scores helps but doesn’t fully capture ability. → You need IV.\nEffect of a new policy: states that adopt the policy may differ in unobservable ways (political will, citizen preferences). → You need DID or synthetic control.\nEffect of a drug: patients who take the drug may be sicker in ways the chart doesn’t capture. → You need an RCT.\n\nThe simulation below shows what happens when the CIA holds vs when it doesn’t.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_so\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate_so\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"obs_conf\", \"Observed confounding (X):\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      sliderInput(\"unobs_conf\", \"Unobserved confounding (U):\",\n                  min = 0, max = 3, value = 0, step = 0.25),\n\n      actionButton(\"go_so\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_so\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"so_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go_so\n    n   &lt;- input$n_so\n    ate &lt;- input$ate_so\n    gx  &lt;- input$obs_conf\n    gu  &lt;- input$unobs_conf\n\n    # Observed confounder\n    x &lt;- rnorm(n)\n\n    # Unobserved confounder\n    u &lt;- rnorm(n)\n\n    # Treatment depends on both\n    p &lt;- pnorm(gx * x + gu * u)\n    treat &lt;- rbinom(n, 1, p)\n\n    # Outcome depends on both\n    y &lt;- 1 + 2 * x + 1.5 * u + ate * treat + rnorm(n)\n\n    # Naive (no controls)\n    naive &lt;- coef(lm(y ~ treat))[2]\n\n    # Controlling for X only\n    ctrl_x &lt;- coef(lm(y ~ treat + x))[2]\n\n    # Oracle: controlling for X and U\n    oracle &lt;- coef(lm(y ~ treat + x + u))[2]\n\n    list(x = x, u = u, treat = treat, y = y,\n         naive = naive, ctrl_x = ctrl_x, oracle = oracle,\n         ate = ate, gx = gx, gu = gu)\n  })\n\n  output$so_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    estimates &lt;- c(d$naive, d$ctrl_x, d$oracle)\n    biases &lt;- estimates - d$ate\n    labels &lt;- c(\"Naive\\n(no controls)\", \"Control for X\\n(observed)\", \"Control for X + U\\n(oracle)\")\n    cols &lt;- c(\"#e74c3c\", ifelse(abs(biases[2]) &lt; 0.3, \"#27ae60\", \"#f39c12\"), \"#27ae60\")\n\n    bp &lt;- barplot(estimates, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.85,\n                  main = \"Estimated Treatment Effect by Method\",\n                  ylab = \"Estimate\", ylim = c(0, max(estimates) * 1.4))\n\n    # True ATE line\n    abline(h = d$ate, lty = 2, col = \"gray40\", lwd = 2)\n    text(0.2, d$ate + 0.15, paste0(\"True ATE = \", d$ate),\n         col = \"gray40\", cex = 0.85, adj = 0)\n\n    # Bias labels\n    text(bp, estimates + 0.15,\n         paste0(round(estimates, 2), \"\\n(bias: \", round(biases, 2), \")\"),\n         cex = 0.8)\n  })\n\n  output$results_so &lt;- renderUI({\n    d &lt;- dat()\n    bias_naive &lt;- d$naive - d$ate\n    bias_x &lt;- d$ctrl_x - d$ate\n    bias_oracle &lt;- d$oracle - d$ate\n    cia_holds &lt;- d$gu == 0\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; \", round(d$naive, 2),\n        \" &lt;span class='bad'&gt;(bias: \", round(bias_naive, 2), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Control X:&lt;/b&gt; \", round(d$ctrl_x, 2),\n        \" &lt;span class='\", ifelse(cia_holds, \"good\", \"bad\"), \"'&gt;(bias: \",\n        round(bias_x, 2), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Oracle (X+U):&lt;/b&gt; \", round(d$oracle, 2),\n        \" &lt;span class='good'&gt;(bias: \", round(bias_oracle, 2), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        if (cia_holds)\n          \"&lt;small&gt;CIA holds: controlling for X is enough.&lt;/small&gt;\"\n        else\n          \"&lt;small&gt;CIA violated: U confounds treatment. Controlling for X alone leaves residual bias.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nUnobserved confounding = 0: the CIA holds. Controlling for X eliminates all bias — the green and oracle bars match. This is the world where selection on observables works.\nUnobserved confounding = 1.5: now there’s a confounder you can’t see. Controlling for X helps (reduces bias vs naive) but doesn’t eliminate it. Only the oracle, who controls for both X and U, gets the right answer.\nUnobserved confounding = 3: controlling for X barely helps. The bias is large. No amount of regression, matching, or weighting on X can fix this — you need a different identification strategy.\nSet observed confounding = 0, unobserved = 2: all the confounding is unobserved. Naive and “control for X” give the same (biased) answer because X isn’t a confounder here.",
    "crumbs": [
      "Methods",
      "Selection on Observables"
    ]
  },
  {
    "objectID": "selection-observables.html#estimation-tools-not-just-for-selection-on-observables",
    "href": "selection-observables.html#estimation-tools-not-just-for-selection-on-observables",
    "title": "Selection on Observables",
    "section": "Estimation tools (not just for selection on observables)",
    "text": "Estimation tools (not just for selection on observables)\nOnce you have an identification strategy, you need a way to implement it. The tools below are often associated with selection on observables, but they’re general-purpose — they show up in other strategies too.\n\n\n\nTool\nUsed in selection on observables\nAlso used in\n\n\n\n\nRegression adjustment\nControl for X in a regression\nDID with covariates, RDD with covariates\n\n\nMatching\nPair treated/control on X\nDID matching estimators\n\n\nIPW\nReweight by propensity score\nIPW-DID (Abadie 2005, Callaway & Sant’Anna 2021)\n\n\nEntropy Balancing\nBalance covariates with weights\nWeighted DID\n\n\nDoubly robust\nCombine regression + weighting\nDR-DID (Sant’Anna & Zhao 2020)\n\n\n\nThe identification strategy tells you why your comparison is valid. The estimation tool tells you how to make the comparison. Don’t confuse the two — IPW is a tool, not a strategy.",
    "crumbs": [
      "Methods",
      "Selection on Observables"
    ]
  },
  {
    "objectID": "selection-observables.html#in-stata",
    "href": "selection-observables.html#in-stata",
    "title": "Selection on Observables",
    "section": "In Stata",
    "text": "In Stata\nAll SOO estimators live under teffects:\n* Regression adjustment\nteffects ra (outcome x1 x2) (treatment)\n\n* Inverse probability weighting\nteffects ipw (outcome) (treatment x1 x2)\n\n* Nearest-neighbor matching\nteffects nnmatch (outcome x1 x2) (treatment), nneighbor(1)\n\n* Doubly robust (AIPW)\nteffects aipw (outcome x1 x2) (treatment x1 x2)\n\n* Check overlap after any teffects command\nteffects overlap\nSame identification assumption (CIA) behind all of them — different ways to use \\(X\\) to make the comparison fair. See each page for details.",
    "crumbs": [
      "Methods",
      "Selection on Observables"
    ]
  },
  {
    "objectID": "selection-observables.html#did-you-know",
    "href": "selection-observables.html#did-you-know",
    "title": "Selection on Observables",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe conditional independence assumption was formalized by Rosenbaum & Rubin (1983) in their foundational paper on propensity scores. They showed that conditioning on a scalar propensity score is sufficient — you don’t need to match on every covariate separately.\nThe term “selection on observables” is economics jargon. In statistics it’s called ignorability or no unmeasured confounding. In epidemiology it’s the exchangeability assumption. Same idea, different fields, different names.\nAltonji, Elder & Taber (2005) proposed a practical check: compare how much the estimate changes when you add observed controls. If adding strong predictors of the outcome barely moves the estimate, it’s less likely that unobservables would change it much either. Not a proof — but a useful heuristic.",
    "crumbs": [
      "Methods",
      "Selection on Observables"
    ]
  },
  {
    "objectID": "rdd.html",
    "href": "rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "Some treatments are assigned by a cutoff rule: you get a scholarship if your test score is above 80, you qualify for a program if your income is below $30k, you win an election if your vote share exceeds 50%.\nRight around the cutoff, people just above and just below are nearly identical — they differ by a fraction of a point. The cutoff creates near-random assignment in a narrow window. That’s the identifying variation.\n\\[\\tau_{RDD} = \\lim_{x \\downarrow c} E[Y \\mid X = x] - \\lim_{x \\uparrow c} E[Y \\mid X = x]\\]\nThe treatment effect is the jump in the outcome at the cutoff. If the outcome is smooth everywhere except at the cutoff, that discontinuity is the causal effect.\n\n\n\nSharp RDD: treatment is a deterministic function of the running variable. Score \\(\\geq\\) 80 → treated, period. Everyone complies.\nFuzzy RDD: the cutoff changes the probability of treatment, but not perfectly. Some people above the cutoff don’t take treatment, some below do. This is essentially an IV problem — the cutoff is the instrument.\n\n\n\n\n\n\n\nWhy fuzzy RDD is IV. Map it directly: the instrument \\(Z\\) is “above the cutoff” (yes/no), the endogenous variable \\(D\\) is actually receiving treatment, and \\(Y\\) is your outcome. The cutoff satisfies the IV assumptions — it’s relevant (shifts treatment probability), excludable (scoring 80 vs 79 doesn’t directly change outcomes), and monotone (crossing the cutoff doesn’t make anyone less likely to be treated). The fuzzy RDD estimate is the Wald estimator: \\(\\tau = \\frac{\\text{jump in outcome at cutoff}}{\\text{jump in treatment probability at cutoff}}\\) — reduced form divided by first stage. Like IV, this estimates a LATE for compliers (people whose treatment status actually changes at the cutoff).\n\n\n\n\n\n\n\nContinuity: the potential outcomes \\(E[Y(0) \\mid X = x]\\) and \\(E[Y(1) \\mid X = x]\\) are continuous at the cutoff — no other jump happens at exactly that point\nNo manipulation: units cannot precisely control their running variable to sort across the cutoff. If they can, the “as good as random” logic breaks.\nLocal randomization: units just above and just below the cutoff are comparable on all observed and unobserved characteristics\nCorrect functional form: the relationship between the running variable and outcome is correctly modeled within the bandwidth window\n\n\n\n\n\nManipulation: if people can precisely control their score to land just above or below the cutoff, the “as good as random” logic breaks. Check for bunching at the cutoff (McCrary test).\nWrong functional form: if you fit a straight line but the true relationship is curved, you might mistake curvature for a jump. Use local polynomials and check sensitivity to bandwidth.\n\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"tau\", \"True treatment effect:\",\n                  min = 0, max = 5, value = 2, step = 0.25),\n\n      sliderInput(\"sigma\", \"Noise (SD):\",\n                  min = 0.5, max = 4, value = 1.5, step = 0.25),\n\n      sliderInput(\"bw\", \"Bandwidth around cutoff:\",\n                  min = 0.05, max = 0.5, value = 0.2, step = 0.05),\n\n      selectInput(\"curve\", \"True relationship:\",\n                  choices = c(\"Linear\" = \"linear\",\n                              \"Quadratic\" = \"quad\",\n                              \"Flat\" = \"flat\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"rdd_plot\", height = \"480px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n\n    tau   &lt;- input$tau\n    sigma &lt;- input$sigma\n    bw    &lt;- input$bw\n    curve &lt;- input$curve\n\n    # Running variable: uniform on [0, 1], cutoff at 0.5\n    x &lt;- runif(n)\n    cutoff &lt;- 0.5\n    treat &lt;- as.numeric(x &gt;= cutoff)\n\n    # Potential outcome (smooth function of x)\n    if (curve == \"linear\") {\n      mu &lt;- 2 + 1.5 * x\n    } else if (curve == \"quad\") {\n      mu &lt;- 2 + 3 * (x - 0.5)^2\n    } else {\n      mu &lt;- rep(3, n)\n    }\n\n    y &lt;- mu + tau * treat + rnorm(n, sd = sigma)\n\n    # Local linear regression within bandwidth\n    in_bw &lt;- abs(x - cutoff) &lt;= bw\n    x_bw &lt;- x[in_bw]\n    y_bw &lt;- y[in_bw]\n    t_bw &lt;- treat[in_bw]\n\n    # Separate regressions left and right\n    left  &lt;- x_bw &lt; cutoff\n    right &lt;- x_bw &gt;= cutoff\n\n    if (sum(left) &gt; 2 && sum(right) &gt; 2) {\n      fit_l &lt;- lm(y_bw[left] ~ x_bw[left])\n      fit_r &lt;- lm(y_bw[right] ~ x_bw[right])\n\n      # Predicted values at cutoff\n      pred_l &lt;- coef(fit_l)[1] + coef(fit_l)[2] * cutoff\n      pred_r &lt;- coef(fit_r)[1] + coef(fit_r)[2] * cutoff\n\n      rdd_est &lt;- pred_r - pred_l\n\n      # Fitted lines for plotting\n      xseq_l &lt;- seq(cutoff - bw, cutoff, length.out = 100)\n      xseq_r &lt;- seq(cutoff, cutoff + bw, length.out = 100)\n      yhat_l &lt;- coef(fit_l)[1] + coef(fit_l)[2] * xseq_l\n      yhat_r &lt;- coef(fit_r)[1] + coef(fit_r)[2] * xseq_r\n    } else {\n      rdd_est &lt;- NA\n      xseq_l &lt;- xseq_r &lt;- yhat_l &lt;- yhat_r &lt;- NULL\n      pred_l &lt;- pred_r &lt;- NA\n    }\n\n    list(x = x, y = y, treat = treat, cutoff = cutoff,\n         bw = bw, in_bw = in_bw, rdd_est = rdd_est,\n         tau = tau, sigma = sigma,\n         xseq_l = xseq_l, xseq_r = xseq_r,\n         yhat_l = yhat_l, yhat_r = yhat_r,\n         pred_l = pred_l, pred_r = pred_r)\n  })\n\n  output$rdd_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Color by treatment\n    cols &lt;- ifelse(d$treat == 1, adjustcolor(\"#3498db\", 0.25),\n                   adjustcolor(\"#e74c3c\", 0.25))\n\n    # Dim points outside bandwidth\n    cols[!d$in_bw] &lt;- adjustcolor(\"gray70\", 0.15)\n\n    plot(d$x, d$y, pch = 16, cex = 0.5, col = cols,\n         xlab = \"Running variable (X)\", ylab = \"Outcome (Y)\",\n         main = \"Regression Discontinuity Design\")\n\n    # Cutoff line\n    abline(v = d$cutoff, lty = 2, col = \"gray40\", lwd = 1.5)\n\n    # Bandwidth shading\n    rect(d$cutoff - d$bw, par(\"usr\")[3],\n         d$cutoff + d$bw, par(\"usr\")[4],\n         col = adjustcolor(\"#f39c12\", 0.08), border = NA)\n\n    # Local linear fits\n    if (!is.null(d$xseq_l)) {\n      lines(d$xseq_l, d$yhat_l, col = \"#e74c3c\", lwd = 3)\n      lines(d$xseq_r, d$yhat_r, col = \"#3498db\", lwd = 3)\n\n      # Jump arrow\n      arrows(d$cutoff + 0.01, d$pred_l, d$cutoff + 0.01, d$pred_r,\n             code = 3, lwd = 2.5, col = \"#27ae60\", length = 0.1)\n\n      text(d$cutoff + 0.03,\n           (d$pred_l + d$pred_r) / 2,\n           paste0(\"Jump = \", round(d$rdd_est, 2)),\n           col = \"#27ae60\", cex = 0.95, adj = 0, font = 2)\n    }\n\n    text(d$cutoff, par(\"usr\")[4] * 0.98, \"Cutoff\",\n         col = \"gray40\", cex = 0.8, pos = 4)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Control (below cutoff)\", \"Treated (above cutoff)\",\n                      \"Estimation window\"),\n           pch = c(16, 16, 15),\n           col = c(\"#e74c3c\", \"#3498db\", adjustcolor(\"#f39c12\", 0.3)))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    if (is.na(d$rdd_est)) {\n      return(tags$div(class = \"stats-box\",\n        HTML(\"&lt;b&gt;Not enough observations in bandwidth.&lt;/b&gt; Widen it.\")))\n    }\n\n    bias &lt;- d$rdd_est - d$tau\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;RDD estimate:&lt;/b&gt; \", round(d$rdd_est, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(abs(bias) &lt; 0.3, \"good\", \"bad\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;small&gt;Bandwidth: &plusmn;\", d$bw, \" around cutoff&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDefault settings: the jump at the cutoff is clear. The RDD estimate is close to the true effect.\nNarrow the bandwidth (0.05): fewer observations, noisier estimate — but less bias from misspecification. Widen it (0.5): more data, more precise, but you’re using observations far from the cutoff.\nSwitch to quadratic: with a narrow bandwidth, the local linear fit still works fine. But widen the bandwidth with a quadratic DGP and the estimate gets biased — the line can’t capture the curve.\nSet true effect = 0: there should be no visible jump. If you see one, it’s noise (or a bad bandwidth choice).\nCrank up noise: the jump gets harder to see, and you need more data or a wider bandwidth to detect it. This is the bias-variance tradeoff of bandwidth selection.",
    "crumbs": [
      "Methods",
      "Regression Discontinuity"
    ]
  },
  {
    "objectID": "rdd.html#the-idea",
    "href": "rdd.html#the-idea",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "Some treatments are assigned by a cutoff rule: you get a scholarship if your test score is above 80, you qualify for a program if your income is below $30k, you win an election if your vote share exceeds 50%.\nRight around the cutoff, people just above and just below are nearly identical — they differ by a fraction of a point. The cutoff creates near-random assignment in a narrow window. That’s the identifying variation.\n\\[\\tau_{RDD} = \\lim_{x \\downarrow c} E[Y \\mid X = x] - \\lim_{x \\uparrow c} E[Y \\mid X = x]\\]\nThe treatment effect is the jump in the outcome at the cutoff. If the outcome is smooth everywhere except at the cutoff, that discontinuity is the causal effect.\n\n\n\nSharp RDD: treatment is a deterministic function of the running variable. Score \\(\\geq\\) 80 → treated, period. Everyone complies.\nFuzzy RDD: the cutoff changes the probability of treatment, but not perfectly. Some people above the cutoff don’t take treatment, some below do. This is essentially an IV problem — the cutoff is the instrument.\n\n\n\n\n\n\n\nWhy fuzzy RDD is IV. Map it directly: the instrument \\(Z\\) is “above the cutoff” (yes/no), the endogenous variable \\(D\\) is actually receiving treatment, and \\(Y\\) is your outcome. The cutoff satisfies the IV assumptions — it’s relevant (shifts treatment probability), excludable (scoring 80 vs 79 doesn’t directly change outcomes), and monotone (crossing the cutoff doesn’t make anyone less likely to be treated). The fuzzy RDD estimate is the Wald estimator: \\(\\tau = \\frac{\\text{jump in outcome at cutoff}}{\\text{jump in treatment probability at cutoff}}\\) — reduced form divided by first stage. Like IV, this estimates a LATE for compliers (people whose treatment status actually changes at the cutoff).\n\n\n\n\n\n\n\nContinuity: the potential outcomes \\(E[Y(0) \\mid X = x]\\) and \\(E[Y(1) \\mid X = x]\\) are continuous at the cutoff — no other jump happens at exactly that point\nNo manipulation: units cannot precisely control their running variable to sort across the cutoff. If they can, the “as good as random” logic breaks.\nLocal randomization: units just above and just below the cutoff are comparable on all observed and unobserved characteristics\nCorrect functional form: the relationship between the running variable and outcome is correctly modeled within the bandwidth window\n\n\n\n\n\nManipulation: if people can precisely control their score to land just above or below the cutoff, the “as good as random” logic breaks. Check for bunching at the cutoff (McCrary test).\nWrong functional form: if you fit a straight line but the true relationship is curved, you might mistake curvature for a jump. Use local polynomials and check sensitivity to bandwidth.\n\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"tau\", \"True treatment effect:\",\n                  min = 0, max = 5, value = 2, step = 0.25),\n\n      sliderInput(\"sigma\", \"Noise (SD):\",\n                  min = 0.5, max = 4, value = 1.5, step = 0.25),\n\n      sliderInput(\"bw\", \"Bandwidth around cutoff:\",\n                  min = 0.05, max = 0.5, value = 0.2, step = 0.05),\n\n      selectInput(\"curve\", \"True relationship:\",\n                  choices = c(\"Linear\" = \"linear\",\n                              \"Quadratic\" = \"quad\",\n                              \"Flat\" = \"flat\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"rdd_plot\", height = \"480px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n\n    tau   &lt;- input$tau\n    sigma &lt;- input$sigma\n    bw    &lt;- input$bw\n    curve &lt;- input$curve\n\n    # Running variable: uniform on [0, 1], cutoff at 0.5\n    x &lt;- runif(n)\n    cutoff &lt;- 0.5\n    treat &lt;- as.numeric(x &gt;= cutoff)\n\n    # Potential outcome (smooth function of x)\n    if (curve == \"linear\") {\n      mu &lt;- 2 + 1.5 * x\n    } else if (curve == \"quad\") {\n      mu &lt;- 2 + 3 * (x - 0.5)^2\n    } else {\n      mu &lt;- rep(3, n)\n    }\n\n    y &lt;- mu + tau * treat + rnorm(n, sd = sigma)\n\n    # Local linear regression within bandwidth\n    in_bw &lt;- abs(x - cutoff) &lt;= bw\n    x_bw &lt;- x[in_bw]\n    y_bw &lt;- y[in_bw]\n    t_bw &lt;- treat[in_bw]\n\n    # Separate regressions left and right\n    left  &lt;- x_bw &lt; cutoff\n    right &lt;- x_bw &gt;= cutoff\n\n    if (sum(left) &gt; 2 && sum(right) &gt; 2) {\n      fit_l &lt;- lm(y_bw[left] ~ x_bw[left])\n      fit_r &lt;- lm(y_bw[right] ~ x_bw[right])\n\n      # Predicted values at cutoff\n      pred_l &lt;- coef(fit_l)[1] + coef(fit_l)[2] * cutoff\n      pred_r &lt;- coef(fit_r)[1] + coef(fit_r)[2] * cutoff\n\n      rdd_est &lt;- pred_r - pred_l\n\n      # Fitted lines for plotting\n      xseq_l &lt;- seq(cutoff - bw, cutoff, length.out = 100)\n      xseq_r &lt;- seq(cutoff, cutoff + bw, length.out = 100)\n      yhat_l &lt;- coef(fit_l)[1] + coef(fit_l)[2] * xseq_l\n      yhat_r &lt;- coef(fit_r)[1] + coef(fit_r)[2] * xseq_r\n    } else {\n      rdd_est &lt;- NA\n      xseq_l &lt;- xseq_r &lt;- yhat_l &lt;- yhat_r &lt;- NULL\n      pred_l &lt;- pred_r &lt;- NA\n    }\n\n    list(x = x, y = y, treat = treat, cutoff = cutoff,\n         bw = bw, in_bw = in_bw, rdd_est = rdd_est,\n         tau = tau, sigma = sigma,\n         xseq_l = xseq_l, xseq_r = xseq_r,\n         yhat_l = yhat_l, yhat_r = yhat_r,\n         pred_l = pred_l, pred_r = pred_r)\n  })\n\n  output$rdd_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Color by treatment\n    cols &lt;- ifelse(d$treat == 1, adjustcolor(\"#3498db\", 0.25),\n                   adjustcolor(\"#e74c3c\", 0.25))\n\n    # Dim points outside bandwidth\n    cols[!d$in_bw] &lt;- adjustcolor(\"gray70\", 0.15)\n\n    plot(d$x, d$y, pch = 16, cex = 0.5, col = cols,\n         xlab = \"Running variable (X)\", ylab = \"Outcome (Y)\",\n         main = \"Regression Discontinuity Design\")\n\n    # Cutoff line\n    abline(v = d$cutoff, lty = 2, col = \"gray40\", lwd = 1.5)\n\n    # Bandwidth shading\n    rect(d$cutoff - d$bw, par(\"usr\")[3],\n         d$cutoff + d$bw, par(\"usr\")[4],\n         col = adjustcolor(\"#f39c12\", 0.08), border = NA)\n\n    # Local linear fits\n    if (!is.null(d$xseq_l)) {\n      lines(d$xseq_l, d$yhat_l, col = \"#e74c3c\", lwd = 3)\n      lines(d$xseq_r, d$yhat_r, col = \"#3498db\", lwd = 3)\n\n      # Jump arrow\n      arrows(d$cutoff + 0.01, d$pred_l, d$cutoff + 0.01, d$pred_r,\n             code = 3, lwd = 2.5, col = \"#27ae60\", length = 0.1)\n\n      text(d$cutoff + 0.03,\n           (d$pred_l + d$pred_r) / 2,\n           paste0(\"Jump = \", round(d$rdd_est, 2)),\n           col = \"#27ae60\", cex = 0.95, adj = 0, font = 2)\n    }\n\n    text(d$cutoff, par(\"usr\")[4] * 0.98, \"Cutoff\",\n         col = \"gray40\", cex = 0.8, pos = 4)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Control (below cutoff)\", \"Treated (above cutoff)\",\n                      \"Estimation window\"),\n           pch = c(16, 16, 15),\n           col = c(\"#e74c3c\", \"#3498db\", adjustcolor(\"#f39c12\", 0.3)))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    if (is.na(d$rdd_est)) {\n      return(tags$div(class = \"stats-box\",\n        HTML(\"&lt;b&gt;Not enough observations in bandwidth.&lt;/b&gt; Widen it.\")))\n    }\n\n    bias &lt;- d$rdd_est - d$tau\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;RDD estimate:&lt;/b&gt; \", round(d$rdd_est, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(abs(bias) &lt; 0.3, \"good\", \"bad\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;small&gt;Bandwidth: &plusmn;\", d$bw, \" around cutoff&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDefault settings: the jump at the cutoff is clear. The RDD estimate is close to the true effect.\nNarrow the bandwidth (0.05): fewer observations, noisier estimate — but less bias from misspecification. Widen it (0.5): more data, more precise, but you’re using observations far from the cutoff.\nSwitch to quadratic: with a narrow bandwidth, the local linear fit still works fine. But widen the bandwidth with a quadratic DGP and the estimate gets biased — the line can’t capture the curve.\nSet true effect = 0: there should be no visible jump. If you see one, it’s noise (or a bad bandwidth choice).\nCrank up noise: the jump gets harder to see, and you need more data or a wider bandwidth to detect it. This is the bias-variance tradeoff of bandwidth selection.",
    "crumbs": [
      "Methods",
      "Regression Discontinuity"
    ]
  },
  {
    "objectID": "rdd.html#the-bandwidth-tradeoff",
    "href": "rdd.html#the-bandwidth-tradeoff",
    "title": "Regression Discontinuity",
    "section": "The bandwidth tradeoff",
    "text": "The bandwidth tradeoff\nBandwidth is the central tuning parameter in RDD:\n\n\n\n\n\n\n\n\n\nNarrow bandwidth\nWide bandwidth\n\n\n\n\nBias\nLow — only using near-identical units\nHigh — far-away units may differ systematically\n\n\nVariance\nHigh — few observations\nLow — more data\n\n\nRisk\nNoisy, imprecise estimate\nPrecise but potentially wrong\n\n\n\nIn practice, researchers use data-driven bandwidth selectors (Imbens & Kalyanaraman 2012, Calonico, Cattaneo & Titiunik 2014) that optimize this tradeoff. You should always check that your results are robust to different bandwidth choices.",
    "crumbs": [
      "Methods",
      "Regression Discontinuity"
    ]
  },
  {
    "objectID": "rdd.html#in-stata",
    "href": "rdd.html#in-stata",
    "title": "Regression Discontinuity",
    "section": "In Stata",
    "text": "In Stata\n* Install rdrobust (Cattaneo, Idrobo & Titiunik)\n* ssc install rdrobust\n\n* Sharp RDD with optimal bandwidth\nrdrobust outcome running_var, c(0)\n\n* RDD plot\nrdplot outcome running_var, c(0)\n\n* Check for manipulation of the running variable\nrddensity running_var, c(0)\n\n* Fuzzy RDD (treatment is probabilistic at cutoff)\nrdrobust outcome running_var, c(0) fuzzy(treatment)\nrdrobust handles bandwidth selection, bias correction, and robust standard errors automatically. Always show the rdplot — if you can’t see the jump visually, be skeptical of the estimate.",
    "crumbs": [
      "Methods",
      "Regression Discontinuity"
    ]
  },
  {
    "objectID": "rdd.html#did-you-know",
    "href": "rdd.html#did-you-know",
    "title": "Regression Discontinuity",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe RDD idea goes back to Thistlethwaite & Campbell (1960), who studied the effect of merit scholarships on career outcomes using the score cutoff. It was largely ignored for decades until economists rediscovered it in the late 1990s.\nLee (2008) used RDD to study the incumbency advantage in US elections — candidates who barely win vs barely lose. This paper helped establish RDD as a workhorse method in political economy.\nCattaneo, Idrobo & Titiunik wrote an excellent practical guide: A Practical Introduction to Regression Discontinuity Designs. It’s freely available and covers everything from basic plots to formal inference.",
    "crumbs": [
      "Methods",
      "Regression Discontinuity"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Causal Inference",
    "section": "",
    "text": "Causal inference methods — each topic pairs explanation with an interactive simulation you can run in the browser.\nBuilds on: Statistical Inference",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#framework",
    "href": "index.html#framework",
    "title": "Applied Causal Inference",
    "section": "Framework",
    "text": "Framework\n\nPotential Outcomes & ATE — The framework behind all causal questions\nIdentification vs Estimation — The distinction that organizes everything\nWhen ML Helps and When It Doesn’t — The boundary between estimation and identification",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#methods",
    "href": "index.html#methods",
    "title": "Applied Causal Inference",
    "section": "Methods",
    "text": "Methods\n\nSelection on Observables — When conditioning on X is enough\nDifference-in-Differences — Parallel trends & treatment effects\nInstrumental Variables — Exogenous variation to isolate causal effects\nRegression Discontinuity — Cutoff rules as natural experiments\nSynthetic Control — Building a counterfactual from weighted donors\nSample Selection & Heckman — When outcomes are observed only for a non-random subsample",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#panel-data",
    "href": "index.html#panel-data",
    "title": "Applied Causal Inference",
    "section": "Panel Data",
    "text": "Panel Data\n\nFixed vs Random Effects — Within vs between variation, the Hausman test, and when each estimator works",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#estimation-tools",
    "href": "index.html#estimation-tools",
    "title": "Applied Causal Inference",
    "section": "Estimation Tools",
    "text": "Estimation Tools\nTools that can be paired with different research designs.\n\nRegression Adjustment — Model the outcome, adjust for confounders\nMatching — Pair treated and control units on covariates\nInverse Probability Weighting — Reweighting to balance treated and control\nEntropy Balancing — Exact moment balancing without a propensity score model\nDoubly Robust — Combine outcome model + propensity score for double protection\nDouble/Debiased ML — Doubly robust + cross-fitting + ML nuisance estimation",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#beyond-averages",
    "href": "index.html#beyond-averages",
    "title": "Applied Causal Inference",
    "section": "Beyond Averages",
    "text": "Beyond Averages\nSo far we have focused on average causal effects. But policies often affect individuals differently. This section moves from estimating a single number to estimating a function.\n\nHeterogeneous Treatment Effects — From \\(\\tau\\) to \\(\\tau(x)\\): why averages can mislead and what CATE is\nEstimating Heterogeneous Effects — Causal forests, meta-learners, and practical cautions",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "sample-selection.html",
    "href": "sample-selection.html",
    "title": "Sample Selection & Heckman Correction",
    "section": "",
    "text": "Sometimes the outcome you want to study is only observed for a non-random subset of the population:\n\nWages are only observed for people who work. If people with low potential wages choose not to work, OLS on workers overestimates the average wage.\nTest scores are only observed for students who stay enrolled. If struggling students drop out, average scores look artificially high.\nStock returns are only observed for companies that survive (survivorship bias).\n\nWhen selection into the sample correlates with the outcome, estimates on the selected subsample are biased. This isn’t a standard omitted variable — the problem is that your sample itself is non-random.",
    "crumbs": [
      "Methods",
      "Sample Selection & Heckman"
    ]
  },
  {
    "objectID": "sample-selection.html#the-problem",
    "href": "sample-selection.html#the-problem",
    "title": "Sample Selection & Heckman Correction",
    "section": "",
    "text": "Sometimes the outcome you want to study is only observed for a non-random subset of the population:\n\nWages are only observed for people who work. If people with low potential wages choose not to work, OLS on workers overestimates the average wage.\nTest scores are only observed for students who stay enrolled. If struggling students drop out, average scores look artificially high.\nStock returns are only observed for companies that survive (survivorship bias).\n\nWhen selection into the sample correlates with the outcome, estimates on the selected subsample are biased. This isn’t a standard omitted variable — the problem is that your sample itself is non-random.",
    "crumbs": [
      "Methods",
      "Sample Selection & Heckman"
    ]
  },
  {
    "objectID": "sample-selection.html#the-heckman-model",
    "href": "sample-selection.html#the-heckman-model",
    "title": "Sample Selection & Heckman Correction",
    "section": "The Heckman model",
    "text": "The Heckman model\nJames Heckman (Nobel 2000) formalized this as a two-equation system:\nSelection equation (who is observed): \\[S_i^* = Z_i'\\gamma + u_i, \\qquad S_i = \\mathbf{1}(S_i^* &gt; 0)\\]\nOutcome equation (what we want to estimate): \\[Y_i = X_i'\\beta + \\varepsilon_i, \\qquad \\text{observed only if } S_i = 1\\]\nIf the errors \\((u_i, \\varepsilon_i)\\) are correlated — i.e., the unobservables that drive selection also affect the outcome — then \\(E[\\varepsilon_i \\mid S_i = 1] \\neq 0\\), and OLS on the selected sample is biased.",
    "crumbs": [
      "Methods",
      "Sample Selection & Heckman"
    ]
  },
  {
    "objectID": "sample-selection.html#two-step-correction",
    "href": "sample-selection.html#two-step-correction",
    "title": "Sample Selection & Heckman Correction",
    "section": "Two-step correction",
    "text": "Two-step correction\nStep 1: Probit. Estimate the selection equation by probit to get \\(\\hat{\\gamma}\\). Compute the inverse Mills ratio for each observation:\n\\[\\hat{\\lambda}_i = \\frac{\\phi(Z_i'\\hat{\\gamma})}{\\Phi(Z_i'\\hat{\\gamma})}\\]\nwhere \\(\\phi\\) is the normal density and \\(\\Phi\\) is the normal CDF. The Mills ratio captures the expected value of the truncated error — how “surprising” it is that observation \\(i\\) was selected.\nStep 2: OLS with correction. Include \\(\\hat{\\lambda}_i\\) in the outcome regression:\n\\[Y_i = X_i'\\beta + \\rho\\sigma_\\varepsilon \\hat{\\lambda}_i + \\eta_i\\]\nThe coefficient on the Mills ratio estimates the selection bias \\(\\rho\\sigma_\\varepsilon\\). If it’s significantly different from zero, selection bias is present.",
    "crumbs": [
      "Methods",
      "Sample Selection & Heckman"
    ]
  },
  {
    "objectID": "sample-selection.html#the-exclusion-restriction",
    "href": "sample-selection.html#the-exclusion-restriction",
    "title": "Sample Selection & Heckman Correction",
    "section": "The exclusion restriction",
    "text": "The exclusion restriction\nFor the model to be well-identified, you need at least one variable in \\(Z\\) (selection equation) that is not in \\(X\\) (outcome equation) — a variable that affects whether you’re observed but not the outcome itself.\nThis is conceptually identical to the exclusion restriction in IV. Without it, identification relies solely on the nonlinearity of the probit — which is fragile.\nGood examples: distance to a job center (affects employment but not wages directly), having young children (affects labor force participation), draft lottery number.\nBad examples: variables that directly affect both selection and the outcome.\n\n\n\n\n\n\nThe Oracle View. In the simulation below, we observe the full population — including those who are “selected out.” This lets us compare: (1) OLS on everyone (the truth we can’t normally see), (2) OLS on the selected sample (biased), and (3) Heckman-corrected estimates. In practice, you only see the selected sample.",
    "crumbs": [
      "Methods",
      "Sample Selection & Heckman"
    ]
  },
  {
    "objectID": "sample-selection.html#simulation",
    "href": "sample-selection.html#simulation",
    "title": "Sample Selection & Heckman Correction",
    "section": "Simulation",
    "text": "Simulation\nWages are observed only for workers. Selection into work correlates with unobserved ability (which also affects wages). Compare OLS on selected sample (biased) vs Heckman correction vs the population truth.\n#| standalone: true\n#| viewerHeight: 750\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .eq-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-bottom: 14px; font-size: 14px; line-height: 1.9;\n    }\n    .eq-box b { color: #2c3e50; }\n    .match  { color: #27ae60; font-weight: bold; }\n    .coef   { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 4,\n\n      sliderInput(\"n\", \"Population size:\",\n                  min = 500, max = 3000, value = 1000, step = 250),\n\n      sliderInput(\"rho\", HTML(\"Error correlation (&rho;):\"),\n                  min = -0.9, max = 0.9, value = 0.6, step = 0.1),\n\n      sliderInput(\"gamma_z\", \"Selection instrument strength:\",\n                  min = 0.5, max = 3, value = 1.5, step = 0.25),\n\n      sliderInput(\"beta\", HTML(\"True return to education (&beta;):\"),\n                  min = 0.5, max = 3, value = 1, step = 0.25),\n\n      actionButton(\"resim\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_box\")\n    ),\n\n    mainPanel(\n      width = 8,\n      plotOutput(\"main_plot\", height = \"450px\"),\n      uiOutput(\"explain_box\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$resim\n    n       &lt;- input$n\n    rho     &lt;- input$rho\n    gamma_z &lt;- input$gamma_z\n    beta    &lt;- input$beta\n\n    # Education (observed)\n    educ &lt;- rnorm(n, mean = 12, sd = 3)\n\n    # Exclusion restriction: distance to job center\n    # Affects selection but not wages\n    distance &lt;- rnorm(n)\n\n    # Correlated errors\n    e1 &lt;- rnorm(n)\n    e2 &lt;- rnorm(n)\n    u   &lt;- e1  # selection error\n    eps &lt;- rho * e1 + sqrt(1 - rho^2) * e2  # outcome error\n\n    # Latent selection\n    s_star &lt;- 0.5 + 0.3 * educ - gamma_z * distance + u\n    selected &lt;- s_star &gt; 0\n\n    # Wages (latent for everyone)\n    wage &lt;- 5 + beta * educ + 2 * eps\n\n    # Observed wages\n    wage_obs &lt;- ifelse(selected, wage, NA)\n\n    # OLS on full population (oracle)\n    ols_full &lt;- lm(wage ~ educ)\n\n    # OLS on selected sample (biased)\n    ols_sel &lt;- lm(wage[selected] ~ educ[selected])\n\n    # Heckman two-step\n    # Step 1: probit for selection\n    probit_fit &lt;- glm(selected ~ educ + distance,\n                      family = binomial(link = \"probit\"))\n    # Inverse Mills ratio for selected observations\n    xg &lt;- predict(probit_fit, type = \"link\")\n    mills &lt;- dnorm(xg) / pnorm(xg)\n    mills_sel &lt;- mills[selected]\n\n    # Step 2: OLS with Mills ratio\n    heckman_fit &lt;- lm(wage[selected] ~ educ[selected] + mills_sel)\n\n    # Prediction grids\n    educ_grid &lt;- seq(min(educ), max(educ), length.out = 100)\n\n    pred_full &lt;- coef(ols_full)[1] + coef(ols_full)[2] * educ_grid\n    pred_sel  &lt;- coef(ols_sel)[1] + coef(ols_sel)[2] * educ_grid\n    pred_heck &lt;- coef(heckman_fit)[1] + coef(heckman_fit)[2] * educ_grid\n\n    list(educ = educ, wage = wage, selected = selected,\n         educ_grid = educ_grid,\n         pred_full = pred_full, pred_sel = pred_sel,\n         pred_heck = pred_heck,\n         coef_full = coef(ols_full),\n         coef_sel = coef(ols_sel),\n         coef_heck = coef(heckman_fit),\n         pct_selected = round(100 * mean(selected), 1),\n         beta = beta, rho = rho)\n  })\n\n  output$main_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(5, 5, 4, 2))\n\n    # Plot unselected (faded)\n    plot(d$educ[!d$selected], d$wage[!d$selected],\n         pch = 16, col = adjustcolor(\"#e74c3c\", 0.15), cex = 0.5,\n         xlab = \"Education (years)\", ylab = \"Wage\",\n         main = \"Sample Selection Bias\",\n         xlim = range(d$educ),\n         ylim = quantile(d$wage, c(0.01, 0.99)))\n\n    # Plot selected\n    points(d$educ[d$selected], d$wage[d$selected],\n           pch = 16, col = adjustcolor(\"#3498db\", 0.25), cex = 0.5)\n\n    # Regression lines\n    lines(d$educ_grid, d$pred_full, col = \"#2c3e50\", lwd = 2.5, lty = 2)\n    lines(d$educ_grid, d$pred_sel, col = \"#e74c3c\", lwd = 2.5)\n    lines(d$educ_grid, d$pred_heck, col = \"#27ae60\", lwd = 2.5)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\n             \"Not selected (unobserved)\",\n             \"Selected (observed)\",\n             paste0(\"OLS on all (true, slope=\",\n                    round(d$coef_full[2], 2), \")\"),\n             paste0(\"OLS on selected (biased, slope=\",\n                    round(d$coef_sel[2], 2), \")\"),\n             paste0(\"Heckman corrected (slope=\",\n                    round(d$coef_heck[2], 2), \")\")\n           ),\n           col = c(adjustcolor(\"#e74c3c\", 0.3),\n                   adjustcolor(\"#3498db\", 0.5),\n                   \"#2c3e50\", \"#e74c3c\", \"#27ae60\"),\n           pch = c(16, 16, NA, NA, NA),\n           lwd = c(NA, NA, 2.5, 2.5, 2.5),\n           lty = c(NA, NA, 2, 1, 1))\n  })\n\n  output$results_box &lt;- renderUI({\n    d &lt;- dat()\n\n    mills_coef &lt;- round(d$coef_heck[3], 3)\n    bias_sign &lt;- if (d$rho &gt; 0) \"upward\" else if (d$rho &lt; 0) \"downward\" else \"none\"\n\n    tags$div(class = \"eq-box\", style = \"margin-top: 16px;\",\n      HTML(paste0(\n        \"&lt;b&gt;True &beta;:&lt;/b&gt; \", d$beta, \"&lt;br&gt;\",\n        \"&lt;b&gt;OLS (full population):&lt;/b&gt; \", round(d$coef_full[2], 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;OLS (selected only):&lt;/b&gt; &lt;span class='coef'&gt;\",\n        round(d$coef_sel[2], 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Heckman corrected:&lt;/b&gt; &lt;span class='match'&gt;\",\n        round(d$coef_heck[2], 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Mills ratio coef:&lt;/b&gt; \", mills_coef, \"&lt;br&gt;\",\n        \"&lt;b&gt;Selected:&lt;/b&gt; \", d$pct_selected, \"% of population&lt;br&gt;\",\n        \"&lt;small&gt;Selection bias direction: \", bias_sign, \"&lt;/small&gt;\"\n      ))\n    )\n  })\n\n  output$explain_box &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"eq-box\", style = \"margin-top: 8px;\",\n      HTML(paste0(\n        \"&lt;b&gt;Faded red dots&lt;/b&gt; = people not in the sample (would be invisible in practice). \",\n        \"&lt;b&gt;Blue dots&lt;/b&gt; = observed sample. \",\n        \"When &rho; &gt; 0, people who select in tend to have positive unobservable shocks \",\n        \"(higher ability), biasing the wage regression &lt;b&gt;upward&lt;/b&gt;. \",\n        \"The Heckman correction (green) recovers the population slope.\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\n\\(\\rho = 0\\): no correlation between selection and outcome errors. OLS on the selected sample is unbiased — no correction needed.\n\\(\\rho = 0.6\\): strong positive selection. Workers have higher unobserved ability than non-workers. The naive slope is biased upward. Heckman pulls it back.\n\\(\\rho = -0.5\\): negative selection (e.g., people with high outside options don’t work). The naive slope is biased downward.\nWeak instrument (\\(\\gamma_z = 0.5\\)): the exclusion restriction is weak. Heckman correction becomes unstable.\nLarge \\(n\\) (2000+): the Heckman-corrected estimate tightens around the true value.\n\n\n\nThe bottom line\n\nSample selection bias arises when who you observe is correlated with what you’re measuring. It’s a form of endogeneity.\nThe Heckman correction models selection explicitly using a probit and corrects the outcome equation with the inverse Mills ratio.\nLike IV, the method requires an exclusion restriction — a variable that shifts selection without directly affecting the outcome. Without it, identification is fragile.\nThe coefficient on the Mills ratio tells you whether (and how much) selection bias exists.\n\n\n\n\nConnections\n\nSelection on Observables — When selection depends on observables, you can condition on them directly. Heckman is for when selection depends on unobservables.\nInstrumental Variables — The exclusion restriction in Heckman is conceptually identical to the IV exclusion restriction.\nLimited Dependent Variables — The selection equation is a probit; the Tobit model is a special case where selection is mechanical.\n\n\n\n\nDid you know?\n\nJames Heckman shared the 2000 Nobel Prize in Economics for developing methods to deal with sample selection bias. His 1979 paper “Sample Selection Bias as a Specification Error” is one of the most cited papers in economics.\nThe inverse Mills ratio is named after John P. Mills, who tabulated the ratio of the normal density to the survival function in the 1920s. It shows up in any problem involving truncated normal distributions.\nHeckman’s original application was to female labor supply. He asked: what is the wage offer for women who choose not to work? Since we never observe their wages, naive estimation on working women overstates the average wage offer.\nThe two-step estimator is consistent but inefficient. Full maximum likelihood estimation of both equations jointly is more efficient but computationally harder. In practice, the two-step version dominates because it’s simpler and robust enough.",
    "crumbs": [
      "Methods",
      "Sample Selection & Heckman"
    ]
  },
  {
    "objectID": "potential-outcomes.html",
    "href": "potential-outcomes.html",
    "title": "Potential Outcomes & ATE",
    "section": "",
    "text": "For every person, there are two potential outcomes:\n\n\\(Y_i(1)\\): what happens if they get the treatment\n\\(Y_i(0)\\): what happens if they don’t\n\nThe individual treatment effect is \\(\\tau_i = Y_i(1) - Y_i(0)\\). The problem? We only ever observe one of these. A person is either treated or not — never both. The unobserved outcome is the counterfactual.\nThe Average Treatment Effect (ATE) is:\n\\[\\text{ATE} = E[Y(1) - Y(0)]\\]\nSince we can’t observe both for anyone, we need assumptions (like random assignment) to estimate it.\n\n\n\nSUTVA (Stable Unit Treatment Value Assumption): one person’s treatment doesn’t affect another person’s outcome — no interference between units\nConsistency: the observed outcome for a treated person equals their potential outcome under treatment, \\(Y_i = Y_i(1)\\) if treated\nRandom assignment (for unbiased estimation): treatment is independent of potential outcomes — \\(Y(0), Y(1) \\perp D\\)\n\n\n\n\nStart with the only thing we can compute from data — the difference in observed group means:\n\\[E[Y \\mid D=1] - E[Y \\mid D=0]\\]\nBy consistency (\\(Y_i = D_i Y_i(1) + (1 - D_i) Y_i(0)\\)), this equals:\n\\[E[Y(1) \\mid D=1] - E[Y(0) \\mid D=0]\\]\nNow add and subtract \\(E[Y(0) \\mid D=1]\\):\n\\[\\underbrace{E[Y(1) \\mid D=1] - E[Y(0) \\mid D=1]}_{\\text{ATT}} + \\underbrace{E[Y(0) \\mid D=1] - E[Y(0) \\mid D=0]}_{\\text{Selection bias}}\\]\nThis is the fundamental decomposition:\n\\[\\text{Difference in means} = \\text{ATT} + \\text{Selection bias}\\]\nThe selection bias term asks: would the treated group have had different outcomes even without treatment? If sicker people seek treatment, then \\(E[Y(0) \\mid D=1] &lt; E[Y(0) \\mid D=0]\\) — the treated group would have done worse anyway — and the naive comparison underestimates the effect.\n\n\n\n\n\n\nExample: the college earnings premium. People who go to college earn more than those who don’t. But is that because college causes higher earnings, or because the same people who go to college — smart, motivated, from wealthier families — would have earned more anyway? That’s selection bias: \\(E[Y(0) \\mid D=1] &gt; E[Y(0) \\mid D=0]\\). College-goers would have out-earned non-college-goers even without college. The naive earnings gap overestimates the causal effect of college because it bundles the true effect with the selection bias.\n\n\n\n\n\n\nIf \\(D\\) is independent of potential outcomes — \\(Y(0), Y(1) \\perp D\\) — then conditioning on \\(D\\) doesn’t matter:\n\\[E[Y(0) \\mid D=1] = E[Y(0) \\mid D=0] = E[Y(0)]\\]\nThe selection bias term is exactly zero. And the ATT simplifies:\n\\[\\text{ATT} = E[Y(1) \\mid D=1] - E[Y(0) \\mid D=1] = E[Y(1)] - E[Y(0)] = \\text{ATE}\\]\nSo under random assignment:\n\\[\\boxed{\\text{Difference in means} = \\text{ATT} = \\text{ATE}}\\]\nThe naive estimator — just comparing group averages — gives you the causal effect. No modeling, no assumptions about functional form. That’s why randomization is the gold standard.\nWithout randomization, the selection bias term is generally nonzero, and the difference in means ≠ ATE. Every method in this course is a strategy for eliminating or working around that selection bias.\nThe simulation below lets you see both potential outcomes (which you never get in real life), watch selection bias appear, and see how randomization kills it.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Population size:\",\n                  min = 100, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = -2, max = 5, value = 2, step = 0.5),\n\n      selectInput(\"assign\", \"Treatment assignment:\",\n                  choices = c(\"Random (coin flip)\",\n                              \"Self-selection (high Y0 seek treatment)\",\n                              \"Self-selection (low Y0 seek treatment)\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"po_plot\", height = \"400px\")),\n        column(6, plotOutput(\"obs_plot\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    ate &lt;- input$ate\n\n    # Potential outcomes\n    y0 &lt;- rnorm(n, mean = 5, sd = 2)\n    y1 &lt;- y0 + ate + rnorm(n, sd = 0.5)\n\n    # Assignment\n    if (input$assign == \"Random (coin flip)\") {\n      treat &lt;- rbinom(n, 1, 0.5)\n    } else if (input$assign == \"Self-selection (high Y0 seek treatment)\") {\n      prob &lt;- pnorm(y0, mean = mean(y0), sd = sd(y0))\n      treat &lt;- rbinom(n, 1, prob)\n    } else {\n      prob &lt;- 1 - pnorm(y0, mean = mean(y0), sd = sd(y0))\n      treat &lt;- rbinom(n, 1, prob)\n    }\n\n    # Observed outcome\n    y_obs &lt;- ifelse(treat == 1, y1, y0)\n\n    # Estimates\n    naive &lt;- mean(y_obs[treat == 1]) - mean(y_obs[treat == 0])\n    true_ate &lt;- mean(y1 - y0)\n\n    list(y0 = y0, y1 = y1, treat = treat, y_obs = y_obs,\n         naive = naive, true_ate = true_ate, ate = ate)\n  })\n\n  output$po_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$y0, d$y1, pch = 16, cex = 0.6,\n         col = ifelse(d$treat == 1, \"#3498db80\", \"#e74c3c80\"),\n         xlab = \"Y(0) — outcome without treatment\",\n         ylab = \"Y(1) — outcome with treatment\",\n         main = \"Both Potential Outcomes (God's view)\")\n    abline(0, 1, lty = 2, col = \"gray40\", lwd = 1.5)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\", \"45° line (no effect)\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray40\"),\n           pch = c(16, 16, NA), lty = c(NA, NA, 2), lwd = c(NA, NA, 1.5))\n  })\n\n  output$obs_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    grp &lt;- factor(d$treat, labels = c(\"Control\", \"Treated\"))\n    boxplot(d$y_obs ~ grp,\n            col = c(\"#e74c3c40\", \"#3498db40\"),\n            border = c(\"#e74c3c\", \"#3498db\"),\n            main = \"What we actually observe\",\n            ylab = \"Observed Y\", xlab = \"\")\n\n    m0 &lt;- mean(d$y_obs[d$treat == 0])\n    m1 &lt;- mean(d$y_obs[d$treat == 1])\n    points(1:2, c(m0, m1), pch = 18, cex = 2.5, col = c(\"#e74c3c\", \"#3498db\"))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$naive - d$true_ate\n    biased &lt;- abs(bias) &gt; 0.3\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", round(d$true_ate, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive estimate:&lt;/b&gt; \", round(d$naive, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(biased, \"bad\", \"good\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        if (biased) \"&lt;br&gt;&lt;small&gt;Selection bias: treated & control groups aren't comparable.&lt;/small&gt;\"\n        else \"&lt;br&gt;&lt;small&gt;Random assignment makes groups comparable.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nStart with random assignment: the naive estimate is close to the true ATE.\nSwitch to self-selection (high Y₀ seek treatment): people who would have done well anyway are the ones getting treated. The naive estimate is too high — that’s positive selection bias.\nSwitch to self-selection (low Y₀ seek treatment): now the opposite. Sicker people seek treatment, making it look less effective than it is.\nThe left plot shows both potential outcomes — something you never see in real data. That’s the fundamental problem.",
    "crumbs": [
      "Framework",
      "Potential Outcomes & ATE"
    ]
  },
  {
    "objectID": "potential-outcomes.html#the-fundamental-problem-of-causal-inference",
    "href": "potential-outcomes.html#the-fundamental-problem-of-causal-inference",
    "title": "Potential Outcomes & ATE",
    "section": "",
    "text": "For every person, there are two potential outcomes:\n\n\\(Y_i(1)\\): what happens if they get the treatment\n\\(Y_i(0)\\): what happens if they don’t\n\nThe individual treatment effect is \\(\\tau_i = Y_i(1) - Y_i(0)\\). The problem? We only ever observe one of these. A person is either treated or not — never both. The unobserved outcome is the counterfactual.\nThe Average Treatment Effect (ATE) is:\n\\[\\text{ATE} = E[Y(1) - Y(0)]\\]\nSince we can’t observe both for anyone, we need assumptions (like random assignment) to estimate it.\n\n\n\nSUTVA (Stable Unit Treatment Value Assumption): one person’s treatment doesn’t affect another person’s outcome — no interference between units\nConsistency: the observed outcome for a treated person equals their potential outcome under treatment, \\(Y_i = Y_i(1)\\) if treated\nRandom assignment (for unbiased estimation): treatment is independent of potential outcomes — \\(Y(0), Y(1) \\perp D\\)\n\n\n\n\nStart with the only thing we can compute from data — the difference in observed group means:\n\\[E[Y \\mid D=1] - E[Y \\mid D=0]\\]\nBy consistency (\\(Y_i = D_i Y_i(1) + (1 - D_i) Y_i(0)\\)), this equals:\n\\[E[Y(1) \\mid D=1] - E[Y(0) \\mid D=0]\\]\nNow add and subtract \\(E[Y(0) \\mid D=1]\\):\n\\[\\underbrace{E[Y(1) \\mid D=1] - E[Y(0) \\mid D=1]}_{\\text{ATT}} + \\underbrace{E[Y(0) \\mid D=1] - E[Y(0) \\mid D=0]}_{\\text{Selection bias}}\\]\nThis is the fundamental decomposition:\n\\[\\text{Difference in means} = \\text{ATT} + \\text{Selection bias}\\]\nThe selection bias term asks: would the treated group have had different outcomes even without treatment? If sicker people seek treatment, then \\(E[Y(0) \\mid D=1] &lt; E[Y(0) \\mid D=0]\\) — the treated group would have done worse anyway — and the naive comparison underestimates the effect.\n\n\n\n\n\n\nExample: the college earnings premium. People who go to college earn more than those who don’t. But is that because college causes higher earnings, or because the same people who go to college — smart, motivated, from wealthier families — would have earned more anyway? That’s selection bias: \\(E[Y(0) \\mid D=1] &gt; E[Y(0) \\mid D=0]\\). College-goers would have out-earned non-college-goers even without college. The naive earnings gap overestimates the causal effect of college because it bundles the true effect with the selection bias.\n\n\n\n\n\n\nIf \\(D\\) is independent of potential outcomes — \\(Y(0), Y(1) \\perp D\\) — then conditioning on \\(D\\) doesn’t matter:\n\\[E[Y(0) \\mid D=1] = E[Y(0) \\mid D=0] = E[Y(0)]\\]\nThe selection bias term is exactly zero. And the ATT simplifies:\n\\[\\text{ATT} = E[Y(1) \\mid D=1] - E[Y(0) \\mid D=1] = E[Y(1)] - E[Y(0)] = \\text{ATE}\\]\nSo under random assignment:\n\\[\\boxed{\\text{Difference in means} = \\text{ATT} = \\text{ATE}}\\]\nThe naive estimator — just comparing group averages — gives you the causal effect. No modeling, no assumptions about functional form. That’s why randomization is the gold standard.\nWithout randomization, the selection bias term is generally nonzero, and the difference in means ≠ ATE. Every method in this course is a strategy for eliminating or working around that selection bias.\nThe simulation below lets you see both potential outcomes (which you never get in real life), watch selection bias appear, and see how randomization kills it.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Population size:\",\n                  min = 100, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = -2, max = 5, value = 2, step = 0.5),\n\n      selectInput(\"assign\", \"Treatment assignment:\",\n                  choices = c(\"Random (coin flip)\",\n                              \"Self-selection (high Y0 seek treatment)\",\n                              \"Self-selection (low Y0 seek treatment)\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"po_plot\", height = \"400px\")),\n        column(6, plotOutput(\"obs_plot\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    ate &lt;- input$ate\n\n    # Potential outcomes\n    y0 &lt;- rnorm(n, mean = 5, sd = 2)\n    y1 &lt;- y0 + ate + rnorm(n, sd = 0.5)\n\n    # Assignment\n    if (input$assign == \"Random (coin flip)\") {\n      treat &lt;- rbinom(n, 1, 0.5)\n    } else if (input$assign == \"Self-selection (high Y0 seek treatment)\") {\n      prob &lt;- pnorm(y0, mean = mean(y0), sd = sd(y0))\n      treat &lt;- rbinom(n, 1, prob)\n    } else {\n      prob &lt;- 1 - pnorm(y0, mean = mean(y0), sd = sd(y0))\n      treat &lt;- rbinom(n, 1, prob)\n    }\n\n    # Observed outcome\n    y_obs &lt;- ifelse(treat == 1, y1, y0)\n\n    # Estimates\n    naive &lt;- mean(y_obs[treat == 1]) - mean(y_obs[treat == 0])\n    true_ate &lt;- mean(y1 - y0)\n\n    list(y0 = y0, y1 = y1, treat = treat, y_obs = y_obs,\n         naive = naive, true_ate = true_ate, ate = ate)\n  })\n\n  output$po_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$y0, d$y1, pch = 16, cex = 0.6,\n         col = ifelse(d$treat == 1, \"#3498db80\", \"#e74c3c80\"),\n         xlab = \"Y(0) — outcome without treatment\",\n         ylab = \"Y(1) — outcome with treatment\",\n         main = \"Both Potential Outcomes (God's view)\")\n    abline(0, 1, lty = 2, col = \"gray40\", lwd = 1.5)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\", \"45° line (no effect)\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray40\"),\n           pch = c(16, 16, NA), lty = c(NA, NA, 2), lwd = c(NA, NA, 1.5))\n  })\n\n  output$obs_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    grp &lt;- factor(d$treat, labels = c(\"Control\", \"Treated\"))\n    boxplot(d$y_obs ~ grp,\n            col = c(\"#e74c3c40\", \"#3498db40\"),\n            border = c(\"#e74c3c\", \"#3498db\"),\n            main = \"What we actually observe\",\n            ylab = \"Observed Y\", xlab = \"\")\n\n    m0 &lt;- mean(d$y_obs[d$treat == 0])\n    m1 &lt;- mean(d$y_obs[d$treat == 1])\n    points(1:2, c(m0, m1), pch = 18, cex = 2.5, col = c(\"#e74c3c\", \"#3498db\"))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$naive - d$true_ate\n    biased &lt;- abs(bias) &gt; 0.3\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", round(d$true_ate, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive estimate:&lt;/b&gt; \", round(d$naive, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(biased, \"bad\", \"good\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        if (biased) \"&lt;br&gt;&lt;small&gt;Selection bias: treated & control groups aren't comparable.&lt;/small&gt;\"\n        else \"&lt;br&gt;&lt;small&gt;Random assignment makes groups comparable.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nStart with random assignment: the naive estimate is close to the true ATE.\nSwitch to self-selection (high Y₀ seek treatment): people who would have done well anyway are the ones getting treated. The naive estimate is too high — that’s positive selection bias.\nSwitch to self-selection (low Y₀ seek treatment): now the opposite. Sicker people seek treatment, making it look less effective than it is.\nThe left plot shows both potential outcomes — something you never see in real data. That’s the fundamental problem.",
    "crumbs": [
      "Framework",
      "Potential Outcomes & ATE"
    ]
  },
  {
    "objectID": "potential-outcomes.html#what-if-you-cant-randomize",
    "href": "potential-outcomes.html#what-if-you-cant-randomize",
    "title": "Potential Outcomes & ATE",
    "section": "What if you can’t randomize?",
    "text": "What if you can’t randomize?\nIn a true experiment (RCT):\n\nYou randomly assign people to treatment vs control\nBecause it’s random, the two groups are identical on average — so any difference in outcomes must be caused by the treatment\n\nBut most questions in economics, policy, and social science can’t be answered with an RCT. You can’t randomly assign poverty, or force some cities to build highways and others not to. So how do you estimate causal effects?\n\nNatural experiments\nA natural experiment is when something in the real world — a policy change, a rule, a geographic boundary, a disaster — creates treatment and control groups that are as-if randomly assigned. Nobody designed it as an experiment, but the logic is the same.\nExample: Say the government announced “all tracts in counties with food desert score &gt; X get a healthy food program.” Tracts at X+1 vs X−1 didn’t choose to be on different sides of that line — the cutoff did it for them. So comparing those tracts is like comparing treatment and control in an experiment.\n\nThe word “natural” = it happened in the real world, not in a lab.\nThe word “experiment” = it created as-if random variation in who got treated.\n\nThe whole point is to get around the selection bias problem the simulation above shows. If people (or firms, or cities) choose their treatment status, the naive comparison is biased. A natural experiment gives you variation that the units didn’t choose.\n\n\nThe rest of this course\nEvery method in this course is a strategy for exploiting natural experiments or otherwise correcting for selection bias:\n\n\n\nMethod\nThe idea\n\n\n\n\nSelection on Observables\nCondition on everything that drives both treatment and outcome\n\n\nIPW\nReweight observations so treated and control look similar on observables\n\n\nEntropy Balancing\nDirectly balance covariates between groups without modeling the propensity score\n\n\nDifference-in-Differences\nCompare changes over time between treated and control groups\n\n\nInstrumental Variables\nUse exogenous variation to isolate the causal effect\n\n\nRegression Discontinuity\nExploit a cutoff rule as a natural experiment\n\n\nSynthetic Control\nBuild a weighted counterfactual from donor units\n\n\n\nEach one makes a different assumption about why the comparison is valid. The art of causal inference is choosing the right method for your setting — and being honest about when the assumptions fail.",
    "crumbs": [
      "Framework",
      "Potential Outcomes & ATE"
    ]
  },
  {
    "objectID": "potential-outcomes.html#in-stata",
    "href": "potential-outcomes.html#in-stata",
    "title": "Potential Outcomes & ATE",
    "section": "In Stata",
    "text": "In Stata\nThe simplest causal estimate — a difference in means under random assignment:\n* Difference in means (t-test)\nttest outcome, by(treatment)\n\n* Same thing as a regression (identical point estimate)\nreg outcome treatment\n\n* With controls (gains precision in an RCT, required under SOO)\nreg outcome treatment x1 x2 x3\nWith random assignment, reg outcome treatment gives you an unbiased estimate of the ATE. The coefficient on treatment is the causal effect. Adding controls doesn’t change the estimate (in expectation) but can shrink the standard errors.",
    "crumbs": [
      "Framework",
      "Potential Outcomes & ATE"
    ]
  },
  {
    "objectID": "when-ml-helps.html",
    "href": "when-ml-helps.html",
    "title": "When ML Helps and When It Doesn’t",
    "section": "",
    "text": "Machine learning is transforming parts of empirical research. But it is not transforming all parts equally, and understanding where ML helps is as important as knowing the methods. This page draws the boundary.",
    "crumbs": [
      "Framework",
      "When ML Helps and When It Doesn't"
    ]
  },
  {
    "objectID": "when-ml-helps.html#the-separation-principle",
    "href": "when-ml-helps.html#the-separation-principle",
    "title": "When ML Helps and When It Doesn’t",
    "section": "The separation principle",
    "text": "The separation principle\nIdentification vs Estimation established that every causal study has two components:\n\nIdentification: the argument for why a comparison is causal\nEstimation: the procedure for computing the causal parameter from data\n\nMachine learning helps with estimation. It does not help with identification. This is the separation principle, and it is the single most important idea in causal ML.\n\n\n\n\n\n\n\n\n\nWhat ML improves\nWhat ML does not improve\n\n\n\n\nNuisance estimation\nPropensity scores, outcome models\nThe argument for unconfoundedness\n\n\nFunctional form\nNonlinear, high-dimensional adjustment\nWhether the adjustment variables are sufficient\n\n\nHeterogeneity\nFlexible CATE estimation\nWhether the ATE is identified in the first place\n\n\nPrediction\nOut-of-sample forecasting\nWhether the forecast has a causal interpretation\n\n\n\n\n\n\n\n\n\nThe Manski principle, restated. No estimator — OLS, random forest, neural network, or transformer — can identify a causal parameter that the research design does not identify. Identification is a property of the data-generating process and the assumptions you’re willing to make, not the statistical method you apply. ML makes estimation more flexible; it does not make identification less necessary.",
    "crumbs": [
      "Framework",
      "When ML Helps and When It Doesn't"
    ]
  },
  {
    "objectID": "when-ml-helps.html#where-ml-helps",
    "href": "when-ml-helps.html#where-ml-helps",
    "title": "When ML Helps and When It Doesn’t",
    "section": "Where ML helps",
    "text": "Where ML helps\n\n1. High-dimensional confounders\nWhen the set of potential confounders is large — dozens or hundreds of variables — traditional methods struggle. You can’t include 200 variables in a logistic regression for the propensity score without overfitting. ML methods (lasso, random forests, boosting) handle high-dimensional covariate adjustment naturally.\nDML provides the framework: use ML for the nuisance functions (propensity score, outcome model), cross-fit to avoid overfitting bias, and use Neyman orthogonal scores for valid inference on the causal parameter.\n\n\n2. Nonlinear relationships\nIf the outcome depends on covariates in complex, nonlinear ways, a linear outcome model is misspecified. ML methods can capture these nonlinearities without the researcher having to specify them in advance (which polynomial terms, which interactions).\nThis is where doubly robust estimation and DML intersect: the DR structure protects against misspecification of either model, and ML provides flexible estimators that reduce the risk of misspecification in the first place.\n\n\n3. Discovering treatment effect heterogeneity\nHeterogeneous treatment effects showed that causal forests and meta-learners can estimate \\(\\tau(x)\\) — revealing which subgroups benefit most from treatment. This is genuinely new: traditional subgroup analysis requires pre-specifying which subgroups to examine, while ML-based methods can search over many covariates simultaneously.\n\n\n4. Improving precision\nEven when a simple estimator is consistent, ML-based covariate adjustment can reduce variance. In randomized experiments, the ATE is identified by randomization — you don’t need covariate adjustment for consistency. But adjusting for predictive covariates reduces the residual variance and tightens confidence intervals. ML can identify which covariates are most predictive without the researcher specifying a functional form.",
    "crumbs": [
      "Framework",
      "When ML Helps and When It Doesn't"
    ]
  },
  {
    "objectID": "when-ml-helps.html#where-ml-does-not-help",
    "href": "when-ml-helps.html#where-ml-does-not-help",
    "title": "When ML Helps and When It Doesn’t",
    "section": "Where ML does not help",
    "text": "Where ML does not help\n\n1. Unobserved confounding\nIf treatment assignment depends on an unobserved variable \\(U\\):\n\\[\nY(0), Y(1) \\not\\perp D \\mid X\n\\]\nthen no method — parametric or nonparametric — can estimate the ATE consistently from observational data on \\((Y, D, X)\\) alone. ML estimates \\(E[Y \\mid X, D]\\) more flexibly, but a more flexible approximation to a confounded conditional expectation is still confounded.\nThis is the failure mode that identification guards against. The solution is not a better estimator — it is a better research design (an instrument, a discontinuity, a natural experiment).\n\n\n2. Providing identification\nML can estimate \\(P(Y \\mid X)\\) but not \\(P(Y \\mid do(X))\\) — the distinction from the stats course. The interventional distribution requires knowledge of the causal structure (a DAG, an exclusion restriction, a parallel trends argument). ML methods are agnostic about causal structure by design — they optimize prediction, not identification.\n\n\n3. Small samples\nML methods generally require large samples to outperform simple parametric models. With \\(n = 200\\), a lasso propensity score is unlikely to improve on a carefully specified logit. DML’s cross-fitting further reduces effective sample size. In small samples, domain knowledge and parametric modeling often dominate data-driven flexibility.\n\n\n4. Interpretability of nuisance functions\nWhen a random forest estimates the propensity score, you lose the ability to inspect and interpret the model. You can’t point to specific coefficients and say “age increases treatment probability by X percentage points.” This matters when the propensity score model is of substantive interest (understanding selection into treatment) rather than just a statistical tool.",
    "crumbs": [
      "Framework",
      "When ML Helps and When It Doesn't"
    ]
  },
  {
    "objectID": "when-ml-helps.html#a-decision-framework",
    "href": "when-ml-helps.html#a-decision-framework",
    "title": "When ML Helps and When It Doesn’t",
    "section": "A decision framework",
    "text": "A decision framework\nWhen deciding whether to use ML in a causal study:\nIs the causal parameter identified?\n├── No → Fix the research design. ML cannot help.\n└── Yes → Are nuisance functions well-approximated by simple models?\n    ├── Yes → Standard parametric methods are fine. ML adds complexity\n    │         without clear benefit.\n    └── No → ML-based estimation (DML, causal forests) can help.\n        ├── Is the sample large enough? (n &gt; 1000 as rough guide)\n        │   ├── Yes → Proceed with DML / causal forests\n        │   └── No → Consider regularized parametric models (lasso)\n        └── Do you need heterogeneous effects?\n            ├── Yes → Causal forests, meta-learners\n            └── No → DML for ATE\n\n\n\n\n\n\nThe honest summary. ML is a powerful tool for the estimation half of causal inference. It handles high-dimensional confounders, nonlinear relationships, and heterogeneous effects better than traditional parametric methods. But it is entirely silent on the identification half — the question of whether the causal parameter can be recovered at all. The most sophisticated ML pipeline applied to a confounded comparison produces a precise, confident, wrong answer.",
    "crumbs": [
      "Framework",
      "When ML Helps and When It Doesn't"
    ]
  },
  {
    "objectID": "when-ml-helps.html#connecting-to-the-course",
    "href": "when-ml-helps.html#connecting-to-the-course",
    "title": "When ML Helps and When It Doesn’t",
    "section": "Connecting to the course",
    "text": "Connecting to the course\nThis page synthesizes the entire causal ML section:\n\nIdentification vs Estimation: the foundational distinction that structures everything above\nDML: the primary framework for using ML in causal estimation\nHeterogeneous Effects: the main setting where ML provides genuinely new capabilities\nDoubly Robust: the estimation structure that DML builds on\nPrediction vs Causation: the distinction between \\(P(Y \\mid X)\\) and \\(P(Y \\mid do(X))\\), applied to foundation models",
    "crumbs": [
      "Framework",
      "When ML Helps and When It Doesn't"
    ]
  },
  {
    "objectID": "heterogeneous-effects.html",
    "href": "heterogeneous-effects.html",
    "title": "Heterogeneous Treatment Effects",
    "section": "",
    "text": "Everything so far has focused on average treatment effects — the ATE or ATT from Potential Outcomes. But averages can hide enormous variation. A drug that helps most patients and harms a few has a positive ATE, but the few who are harmed would like to know that. A policy that benefits one demographic and hurts another looks mediocre on average.\nThis page introduces a new estimand: the treatment effect as a function of covariates. The next page covers the estimation machinery — causal forests, meta-learners — for recovering it from data.",
    "crumbs": [
      "Beyond Averages",
      "Heterogeneous Treatment Effects"
    ]
  },
  {
    "objectID": "heterogeneous-effects.html#from-tau-to-taux",
    "href": "heterogeneous-effects.html#from-tau-to-taux",
    "title": "Heterogeneous Treatment Effects",
    "section": "From \\(\\tau\\) to \\(\\tau(x)\\)",
    "text": "From \\(\\tau\\) to \\(\\tau(x)\\)\nThe ATE is a single number:\n\\[\n\\tau = E[Y(1) - Y(0)]\n\\]\nThe Conditional Average Treatment Effect (CATE) is a function:\n\\[\n\\tau(x) = E[Y(1) - Y(0) \\mid X = x]\n\\]\nThis answers: “What is the treatment effect for individuals with characteristics \\(X = x\\)?” The ATE is the average of the CATE across the population:\n\\[\n\\tau = E[\\tau(X)]\n\\]\nThe CATE sits alongside the other estimands in the course:\n\n\n\n\n\n\n\n\nEstimand\nDefinition\nQuestion it answers\n\n\n\n\nATE\n\\(E[Y(1) - Y(0)]\\)\nWhat is the average effect?\n\n\nATT\n\\(E[Y(1) - Y(0) \\mid D = 1]\\)\nWhat is the effect on the treated?\n\n\nLATE\nEffect for compliers (IV)\nWhat is the effect for those moved by the instrument?\n\n\nCATE\n\\(E[Y(1) - Y(0) \\mid X = x]\\)\nHow does the effect vary with \\(x\\)?\n\n\n\nThe first three are numbers. The CATE is a function — and that changes the estimation problem fundamentally.",
    "crumbs": [
      "Beyond Averages",
      "Heterogeneous Treatment Effects"
    ]
  },
  {
    "objectID": "heterogeneous-effects.html#why-average-effects-can-mislead",
    "href": "heterogeneous-effects.html#why-average-effects-can-mislead",
    "title": "Heterogeneous Treatment Effects",
    "section": "Why average effects can mislead",
    "text": "Why average effects can mislead\n\nSign reversal\nThe ATE can be positive even if the treatment harms a substantial subgroup. Suppose a job training program increases earnings by $5,000 for workers without a college degree but decreases earnings by $1,000 for workers with a degree (perhaps by diverting them from better opportunities). If 80% of participants lack a degree:\n\\[\n\\tau = 0.8 \\times 5000 + 0.2 \\times (-1000) = 3800\n\\]\nThe ATE is $3,800 — positive and “significant.” But the policy is actively harmful for 20% of participants. Without estimating \\(\\tau(x)\\), you would never know.\n\n\nOptimal targeting\nIf a treatment has heterogeneous effects and resources are limited, you want to treat the people who benefit most. This requires knowing \\(\\tau(x)\\), not just \\(\\tau\\). The optimal treatment rule is:\n\\[\nD^*(x) = \\mathbf{1}\\{\\tau(x) &gt; 0\\}\n\\]\nTreat if and only if the expected effect is positive. This is policy targeting — and it requires the CATE.\n\n\nExternal validity\nAn RCT in one population gives you \\(\\tau\\) for that population. If you want to know what would happen in a different population with different \\(X\\) distributions, you need \\(\\tau(x)\\). The CATE is transportable across populations in a way that the ATE is not (provided the effect heterogeneity is stable).",
    "crumbs": [
      "Beyond Averages",
      "Heterogeneous Treatment Effects"
    ]
  },
  {
    "objectID": "heterogeneous-effects.html#identification-of-the-cate",
    "href": "heterogeneous-effects.html#identification-of-the-cate",
    "title": "Heterogeneous Treatment Effects",
    "section": "Identification of the CATE",
    "text": "Identification of the CATE\nThe CATE requires the same identification assumptions as the ATE, but applied conditionally:\n\\[\nY(0), Y(1) \\perp D \\mid X = x \\qquad \\text{(conditional unconfoundedness)}\n\\]\n\\[\n0 &lt; P(D = 1 \\mid X = x) &lt; 1 \\qquad \\text{(overlap)}\n\\]\nThese are the selection on observables conditions, now required at every value of \\(x\\). This is a stronger requirement than for the ATE: the ATE can be identified even if overlap fails in some regions (the failures cancel out in expectation), but the CATE at \\(x\\) requires overlap at that specific \\(x\\).\n\n\n\n\n\n\nThe identification warning. If the ATE is not identified (unobserved confounders), the CATE is not identified either — and the problem is worse, not better. A biased ATE is one wrong number. A biased CATE is a wrong function — it tells you confidently that subgroup A benefits and subgroup B doesn’t, when in reality the entire comparison is confounded. Identification comes before estimation, and this is even more true for the CATE than for the ATE.",
    "crumbs": [
      "Beyond Averages",
      "Heterogeneous Treatment Effects"
    ]
  },
  {
    "objectID": "heterogeneous-effects.html#subgroup-analysis-the-traditional-approach",
    "href": "heterogeneous-effects.html#subgroup-analysis-the-traditional-approach",
    "title": "Heterogeneous Treatment Effects",
    "section": "Subgroup analysis: the traditional approach",
    "text": "Subgroup analysis: the traditional approach\nThe simplest way to look for heterogeneity: split the sample by a covariate and estimate the ATE within each subgroup. Run the regression separately for men and women, for young and old, for high-income and low-income.\nThis works when:\n\nYou have a small number of pre-specified subgroups\nYou have large samples within each subgroup\nThe heterogeneity is along a single, known dimension\n\nIt breaks down when:\n\nYou have many covariates and don’t know which ones drive heterogeneity\nInteractions matter (the effect differs for young women vs old men, not just by age or gender separately)\nYou test many subgroups without correction — a multiple testing problem\n\n\n\n\n\n\n\nThe garden of forking paths. If you test 20 subgroups at the 5% level, you expect one “significant” interaction by chance. Subgroup analysis that is not pre-specified is exploratory, not confirmatory. This is the lesson from Multiple Testing, applied to heterogeneity.\n\n\n\nThe limitations of subgroup analysis motivate data-driven methods — causal forests and meta-learners — covered in the next page. These methods search over many covariates simultaneously while controlling for overfitting, providing a principled way to discover heterogeneity that traditional subgroup analysis cannot.",
    "crumbs": [
      "Beyond Averages",
      "Heterogeneous Treatment Effects"
    ]
  },
  {
    "objectID": "heterogeneous-effects.html#connecting-to-the-course",
    "href": "heterogeneous-effects.html#connecting-to-the-course",
    "title": "Heterogeneous Treatment Effects",
    "section": "Connecting to the course",
    "text": "Connecting to the course\n\nPotential Outcomes: the CATE is defined within the potential outcomes framework — \\(\\tau(x) = E[Y(1) - Y(0) \\mid X = x]\\)\nSelection on Observables: conditional unconfoundedness is the identification assumption for the CATE\nIdentification vs Estimation: the CATE is a new estimand, not a new identification strategy — the research design must still justify the causal interpretation\nCausal Forests: the estimation machinery for recovering \\(\\tau(x)\\) from data",
    "crumbs": [
      "Beyond Averages",
      "Heterogeneous Treatment Effects"
    ]
  },
  {
    "objectID": "synth.html",
    "href": "synth.html",
    "title": "Synthetic Control",
    "section": "",
    "text": "You have one treated unit — a state that passed a law, a country hit by a crisis, a company that changed policy. You want to know what would have happened without the treatment. But there’s no single control unit that’s a good comparison.\nThe synthetic control method builds a weighted combination of untreated units that matches the treated unit’s pre-treatment trajectory. That weighted combination — the “synthetic” version — serves as the counterfactual.\n\\[\\hat{Y}_{1t}^{N} = \\sum_{j=2}^{J+1} w_j \\, Y_{jt}\\]\nwhere \\(w_j \\geq 0\\) and \\(\\sum w_j = 1\\). The weights are chosen so that the synthetic unit tracks the treated unit closely before treatment. After treatment, the gap between the treated unit and its synthetic version is the estimated effect.\n\n\nAbadie, Diamond & Hainmueller (2010): California passed Proposition 99 in 1988, a major tobacco control program. No single state is a good comparison — some are too urban, some too rural, some already had anti-smoking laws. The synthetic California is a weighted mix of states (Utah, Nevada, Colorado, Connecticut, Montana…) that together match California’s pre-1988 smoking trend almost exactly. After 1988, actual California diverges sharply below its synthetic version — that gap is the treatment effect.\n\n\n\n\nNo interference / SUTVA: the treatment of the treated unit doesn’t affect the donor units (no spillovers)\nConvex hull: the treated unit’s pre-treatment outcomes can be expressed as a weighted average of the donors — the treated unit isn’t an outlier that no combination of donors can match\nNo anticipation: the treated unit doesn’t change behavior before the treatment date\nCommon factors: treated and donor units are driven by the same underlying factors, just with different loadings — the weights that work pre-treatment continue to work post-treatment\n\n\n\n\n\nBad pre-treatment fit: if the synthetic unit can’t track the treated unit before treatment, you can’t trust the post-treatment gap. There’s no magic — if no combination of donors resembles the treated unit, the method doesn’t work.\nSpillovers: if the treatment affects the donor units too (e.g., smokers move from California to Nevada), the synthetic control is contaminated.\nToo few donors: with very few comparison units, the weights are forced and the match may be poor.\n\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n    .wt-table { font-size: 12px; margin-top: 6px; }\n    .wt-table td { padding: 1px 6px; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"tau_sc\", \"True treatment effect:\",\n                  min = -5, max = 5, value = -3, step = 0.5),\n\n      sliderInput(\"n_donors\", \"Number of donor units:\",\n                  min = 3, max = 15, value = 8, step = 1),\n\n      sliderInput(\"sigma_sc\", \"Noise (SD):\",\n                  min = 0.1, max = 2, value = 0.5, step = 0.1),\n\n      sliderInput(\"treat_time\", \"Treatment period:\",\n                  min = 8, max = 18, value = 12, step = 1),\n\n      actionButton(\"go_sc\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_sc\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"synth_plot\", height = \"500px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go_sc\n    tau   &lt;- input$tau_sc\n    J     &lt;- input$n_donors\n    sigma &lt;- input$sigma_sc\n    T0    &lt;- input$treat_time\n    TT    &lt;- 25\n\n    # Generate donor unit trajectories\n    # Each donor has its own intercept and slope\n    set.seed(NULL)\n    intercepts &lt;- rnorm(J, mean = 10, sd = 2)\n    slopes     &lt;- rnorm(J, mean = 0.3, sd = 0.15)\n    donors &lt;- matrix(NA, nrow = TT, ncol = J)\n    for (j in 1:J) {\n      donors[, j] &lt;- intercepts[j] + slopes[j] * (1:TT) + rnorm(TT, sd = sigma)\n    }\n\n    # True weights (sparse: pick 3-4 donors that matter)\n    n_active &lt;- min(4, J)\n    active &lt;- sample(1:J, n_active)\n    true_w &lt;- rep(0, J)\n    raw &lt;- runif(n_active, 0.1, 1)\n    true_w[active] &lt;- raw / sum(raw)\n\n    # Treated unit = weighted combo of donors + treatment effect after T0\n    treated &lt;- donors %*% true_w + rnorm(TT, sd = sigma * 0.5)\n    treated[(T0 + 1):TT] &lt;- treated[(T0 + 1):TT] + tau\n\n    # Estimate synthetic control weights (OLS on pre-period, constrained to sum to 1)\n    # Simple approach: non-negative least squares via iterative projection\n    pre &lt;- 1:T0\n    Y1_pre &lt;- treated[pre]\n    Y0_pre &lt;- donors[pre, ]\n\n    # Use a simple regression + normalize approach\n    # Unconstrained OLS, then clip negatives and renormalize\n    if (J &lt;= T0) {\n      fit &lt;- lm(Y1_pre ~ Y0_pre - 1)\n      w_hat &lt;- coef(fit)\n    } else {\n      # More donors than periods: use ridge-like approach\n      lambda &lt;- 0.01\n      w_hat &lt;- solve(t(Y0_pre) %*% Y0_pre + lambda * diag(J),\n                     t(Y0_pre) %*% Y1_pre)\n    }\n    w_hat[w_hat &lt; 0] &lt;- 0\n    if (sum(w_hat) &gt; 0) w_hat &lt;- w_hat / sum(w_hat) else w_hat &lt;- rep(1/J, J)\n\n    # Synthetic control trajectory\n    synth &lt;- donors %*% w_hat\n\n    # Estimated effect (post-treatment gap)\n    post &lt;- (T0 + 1):TT\n    gaps &lt;- treated[post] - synth[post]\n    avg_effect &lt;- mean(gaps)\n\n    # Pre-treatment fit (RMSPE)\n    pre_rmspe &lt;- sqrt(mean((treated[pre] - synth[pre])^2))\n\n    list(time = 1:TT, treated = as.numeric(treated),\n         synth = as.numeric(synth), donors = donors,\n         w_hat = w_hat, T0 = T0, tau = tau,\n         avg_effect = avg_effect, pre_rmspe = pre_rmspe, J = J)\n  })\n\n  output$synth_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ylim &lt;- range(c(d$treated, d$synth, d$donors)) + c(-1, 1)\n\n    # Donor units (gray background)\n    plot(d$time, d$donors[, 1], type = \"n\",\n         xlab = \"Time\", ylab = \"Outcome\",\n         main = \"Synthetic Control Method\",\n         ylim = ylim)\n\n    for (j in 1:d$J) {\n      lines(d$time, d$donors[, j], col = adjustcolor(\"gray70\", 0.4), lwd = 0.8)\n    }\n\n    # Synthetic control\n    lines(d$time, d$synth, col = \"#e74c3c\", lwd = 3, lty = 2)\n\n    # Treated unit\n    lines(d$time, d$treated, col = \"#3498db\", lwd = 3)\n\n    # Treatment line\n    abline(v = d$T0 + 0.5, lty = 3, col = \"gray40\", lwd = 1.5)\n    text(d$T0 + 0.5, ylim[2], \"Treatment\", pos = 4, cex = 0.85, col = \"gray40\")\n\n    # Gap shading in post period\n    post &lt;- (d$T0 + 1):length(d$time)\n    polygon(c(d$time[post], rev(d$time[post])),\n            c(d$treated[post], rev(d$synth[post])),\n            col = adjustcolor(\"#27ae60\", 0.15), border = NA)\n\n    # Gap label\n    mid_post &lt;- d$time[round(median(post))]\n    mid_gap &lt;- (d$treated[round(median(post))] + d$synth[round(median(post))]) / 2\n    text(mid_post, mid_gap,\n         paste0(\"Avg gap = \", round(d$avg_effect, 2)),\n         col = \"#27ae60\", font = 2, cex = 0.9)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated unit\", \"Synthetic control\", \"Donor units\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray70\"),\n           lwd = c(3, 3, 1), lty = c(1, 2, 1))\n  })\n\n  output$results_sc &lt;- renderUI({\n    d &lt;- dat()\n\n    # Top weights\n    ord &lt;- order(d$w_hat, decreasing = TRUE)\n    top &lt;- ord[d$w_hat[ord] &gt; 0.01]\n    wt_rows &lt;- paste0(\n      sapply(top, function(j) {\n        paste0(\"&lt;tr&gt;&lt;td&gt;Donor \", j, \"&lt;/td&gt;&lt;td&gt;&lt;b&gt;\",\n               round(d$w_hat[j] * 100), \"%&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;\")\n      }),\n      collapse = \"\"\n    )\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;Avg post-treatment gap:&lt;/b&gt; \",\n        \"&lt;span class='\", ifelse(abs(d$avg_effect - d$tau) &lt; 1, \"good\", \"bad\"), \"'&gt;\",\n        round(d$avg_effect, 2), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Pre-treatment RMSPE:&lt;/b&gt; \", round(d$pre_rmspe, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Weights:&lt;/b&gt;\",\n        \"&lt;table class='wt-table'&gt;\", wt_rows, \"&lt;/table&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDefault settings (effect = -3): the synthetic control (red dashed) tracks the treated unit closely before treatment, then diverges. The green-shaded gap is the estimated effect.\nSet true effect = 0: the two lines should stay close after treatment too. If you see a big gap, it’s noise — this is why pre-treatment fit matters.\nReduce donors to 3: fewer building blocks means a worse pre-treatment fit. The estimate gets noisier.\nIncrease donors to 15: more building blocks, better fit. But watch the weights — most donors get zero weight. The method is naturally sparse.\nMove treatment period later (18): short post-period, harder to judge whether the gap is real or just noise.\nCrank up noise: the pre-treatment fit deteriorates and the post-treatment gap becomes unreliable.",
    "crumbs": [
      "Methods",
      "Synthetic Control"
    ]
  },
  {
    "objectID": "synth.html#the-idea",
    "href": "synth.html#the-idea",
    "title": "Synthetic Control",
    "section": "",
    "text": "You have one treated unit — a state that passed a law, a country hit by a crisis, a company that changed policy. You want to know what would have happened without the treatment. But there’s no single control unit that’s a good comparison.\nThe synthetic control method builds a weighted combination of untreated units that matches the treated unit’s pre-treatment trajectory. That weighted combination — the “synthetic” version — serves as the counterfactual.\n\\[\\hat{Y}_{1t}^{N} = \\sum_{j=2}^{J+1} w_j \\, Y_{jt}\\]\nwhere \\(w_j \\geq 0\\) and \\(\\sum w_j = 1\\). The weights are chosen so that the synthetic unit tracks the treated unit closely before treatment. After treatment, the gap between the treated unit and its synthetic version is the estimated effect.\n\n\nAbadie, Diamond & Hainmueller (2010): California passed Proposition 99 in 1988, a major tobacco control program. No single state is a good comparison — some are too urban, some too rural, some already had anti-smoking laws. The synthetic California is a weighted mix of states (Utah, Nevada, Colorado, Connecticut, Montana…) that together match California’s pre-1988 smoking trend almost exactly. After 1988, actual California diverges sharply below its synthetic version — that gap is the treatment effect.\n\n\n\n\nNo interference / SUTVA: the treatment of the treated unit doesn’t affect the donor units (no spillovers)\nConvex hull: the treated unit’s pre-treatment outcomes can be expressed as a weighted average of the donors — the treated unit isn’t an outlier that no combination of donors can match\nNo anticipation: the treated unit doesn’t change behavior before the treatment date\nCommon factors: treated and donor units are driven by the same underlying factors, just with different loadings — the weights that work pre-treatment continue to work post-treatment\n\n\n\n\n\nBad pre-treatment fit: if the synthetic unit can’t track the treated unit before treatment, you can’t trust the post-treatment gap. There’s no magic — if no combination of donors resembles the treated unit, the method doesn’t work.\nSpillovers: if the treatment affects the donor units too (e.g., smokers move from California to Nevada), the synthetic control is contaminated.\nToo few donors: with very few comparison units, the weights are forced and the match may be poor.\n\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n    .wt-table { font-size: 12px; margin-top: 6px; }\n    .wt-table td { padding: 1px 6px; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"tau_sc\", \"True treatment effect:\",\n                  min = -5, max = 5, value = -3, step = 0.5),\n\n      sliderInput(\"n_donors\", \"Number of donor units:\",\n                  min = 3, max = 15, value = 8, step = 1),\n\n      sliderInput(\"sigma_sc\", \"Noise (SD):\",\n                  min = 0.1, max = 2, value = 0.5, step = 0.1),\n\n      sliderInput(\"treat_time\", \"Treatment period:\",\n                  min = 8, max = 18, value = 12, step = 1),\n\n      actionButton(\"go_sc\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_sc\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"synth_plot\", height = \"500px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go_sc\n    tau   &lt;- input$tau_sc\n    J     &lt;- input$n_donors\n    sigma &lt;- input$sigma_sc\n    T0    &lt;- input$treat_time\n    TT    &lt;- 25\n\n    # Generate donor unit trajectories\n    # Each donor has its own intercept and slope\n    set.seed(NULL)\n    intercepts &lt;- rnorm(J, mean = 10, sd = 2)\n    slopes     &lt;- rnorm(J, mean = 0.3, sd = 0.15)\n    donors &lt;- matrix(NA, nrow = TT, ncol = J)\n    for (j in 1:J) {\n      donors[, j] &lt;- intercepts[j] + slopes[j] * (1:TT) + rnorm(TT, sd = sigma)\n    }\n\n    # True weights (sparse: pick 3-4 donors that matter)\n    n_active &lt;- min(4, J)\n    active &lt;- sample(1:J, n_active)\n    true_w &lt;- rep(0, J)\n    raw &lt;- runif(n_active, 0.1, 1)\n    true_w[active] &lt;- raw / sum(raw)\n\n    # Treated unit = weighted combo of donors + treatment effect after T0\n    treated &lt;- donors %*% true_w + rnorm(TT, sd = sigma * 0.5)\n    treated[(T0 + 1):TT] &lt;- treated[(T0 + 1):TT] + tau\n\n    # Estimate synthetic control weights (OLS on pre-period, constrained to sum to 1)\n    # Simple approach: non-negative least squares via iterative projection\n    pre &lt;- 1:T0\n    Y1_pre &lt;- treated[pre]\n    Y0_pre &lt;- donors[pre, ]\n\n    # Use a simple regression + normalize approach\n    # Unconstrained OLS, then clip negatives and renormalize\n    if (J &lt;= T0) {\n      fit &lt;- lm(Y1_pre ~ Y0_pre - 1)\n      w_hat &lt;- coef(fit)\n    } else {\n      # More donors than periods: use ridge-like approach\n      lambda &lt;- 0.01\n      w_hat &lt;- solve(t(Y0_pre) %*% Y0_pre + lambda * diag(J),\n                     t(Y0_pre) %*% Y1_pre)\n    }\n    w_hat[w_hat &lt; 0] &lt;- 0\n    if (sum(w_hat) &gt; 0) w_hat &lt;- w_hat / sum(w_hat) else w_hat &lt;- rep(1/J, J)\n\n    # Synthetic control trajectory\n    synth &lt;- donors %*% w_hat\n\n    # Estimated effect (post-treatment gap)\n    post &lt;- (T0 + 1):TT\n    gaps &lt;- treated[post] - synth[post]\n    avg_effect &lt;- mean(gaps)\n\n    # Pre-treatment fit (RMSPE)\n    pre_rmspe &lt;- sqrt(mean((treated[pre] - synth[pre])^2))\n\n    list(time = 1:TT, treated = as.numeric(treated),\n         synth = as.numeric(synth), donors = donors,\n         w_hat = w_hat, T0 = T0, tau = tau,\n         avg_effect = avg_effect, pre_rmspe = pre_rmspe, J = J)\n  })\n\n  output$synth_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ylim &lt;- range(c(d$treated, d$synth, d$donors)) + c(-1, 1)\n\n    # Donor units (gray background)\n    plot(d$time, d$donors[, 1], type = \"n\",\n         xlab = \"Time\", ylab = \"Outcome\",\n         main = \"Synthetic Control Method\",\n         ylim = ylim)\n\n    for (j in 1:d$J) {\n      lines(d$time, d$donors[, j], col = adjustcolor(\"gray70\", 0.4), lwd = 0.8)\n    }\n\n    # Synthetic control\n    lines(d$time, d$synth, col = \"#e74c3c\", lwd = 3, lty = 2)\n\n    # Treated unit\n    lines(d$time, d$treated, col = \"#3498db\", lwd = 3)\n\n    # Treatment line\n    abline(v = d$T0 + 0.5, lty = 3, col = \"gray40\", lwd = 1.5)\n    text(d$T0 + 0.5, ylim[2], \"Treatment\", pos = 4, cex = 0.85, col = \"gray40\")\n\n    # Gap shading in post period\n    post &lt;- (d$T0 + 1):length(d$time)\n    polygon(c(d$time[post], rev(d$time[post])),\n            c(d$treated[post], rev(d$synth[post])),\n            col = adjustcolor(\"#27ae60\", 0.15), border = NA)\n\n    # Gap label\n    mid_post &lt;- d$time[round(median(post))]\n    mid_gap &lt;- (d$treated[round(median(post))] + d$synth[round(median(post))]) / 2\n    text(mid_post, mid_gap,\n         paste0(\"Avg gap = \", round(d$avg_effect, 2)),\n         col = \"#27ae60\", font = 2, cex = 0.9)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated unit\", \"Synthetic control\", \"Donor units\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray70\"),\n           lwd = c(3, 3, 1), lty = c(1, 2, 1))\n  })\n\n  output$results_sc &lt;- renderUI({\n    d &lt;- dat()\n\n    # Top weights\n    ord &lt;- order(d$w_hat, decreasing = TRUE)\n    top &lt;- ord[d$w_hat[ord] &gt; 0.01]\n    wt_rows &lt;- paste0(\n      sapply(top, function(j) {\n        paste0(\"&lt;tr&gt;&lt;td&gt;Donor \", j, \"&lt;/td&gt;&lt;td&gt;&lt;b&gt;\",\n               round(d$w_hat[j] * 100), \"%&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;\")\n      }),\n      collapse = \"\"\n    )\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;Avg post-treatment gap:&lt;/b&gt; \",\n        \"&lt;span class='\", ifelse(abs(d$avg_effect - d$tau) &lt; 1, \"good\", \"bad\"), \"'&gt;\",\n        round(d$avg_effect, 2), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Pre-treatment RMSPE:&lt;/b&gt; \", round(d$pre_rmspe, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Weights:&lt;/b&gt;\",\n        \"&lt;table class='wt-table'&gt;\", wt_rows, \"&lt;/table&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDefault settings (effect = -3): the synthetic control (red dashed) tracks the treated unit closely before treatment, then diverges. The green-shaded gap is the estimated effect.\nSet true effect = 0: the two lines should stay close after treatment too. If you see a big gap, it’s noise — this is why pre-treatment fit matters.\nReduce donors to 3: fewer building blocks means a worse pre-treatment fit. The estimate gets noisier.\nIncrease donors to 15: more building blocks, better fit. But watch the weights — most donors get zero weight. The method is naturally sparse.\nMove treatment period later (18): short post-period, harder to judge whether the gap is real or just noise.\nCrank up noise: the pre-treatment fit deteriorates and the post-treatment gap becomes unreliable.",
    "crumbs": [
      "Methods",
      "Synthetic Control"
    ]
  },
  {
    "objectID": "synth.html#inference-placebo-tests",
    "href": "synth.html#inference-placebo-tests",
    "title": "Synthetic Control",
    "section": "Inference: placebo tests",
    "text": "Inference: placebo tests\nWith one treated unit, you can’t do standard inference. Instead, you run placebo tests:\nIn-space placebos. Apply the synthetic control method to each donor unit — pretend it was treated and build a synthetic version from the remaining donors. If the treated unit’s gap is much larger than the placebo gaps, the effect is likely real.\nIn-time placebos. Move the treatment date earlier (to a period when no treatment occurred). If you find a gap in the placebo period, your method is picking up something other than the treatment.\nThese aren’t formal p-values, but they give you a sense of whether the effect is distinguishable from noise.",
    "crumbs": [
      "Methods",
      "Synthetic Control"
    ]
  },
  {
    "objectID": "synth.html#in-stata",
    "href": "synth.html#in-stata",
    "title": "Synthetic Control",
    "section": "In Stata",
    "text": "In Stata\n* Install synthetic control\n* ssc install synth\n\n* Set up panel data\ntsset unit_id year\n\n* Synthetic control\nsynth outcome x1 x2 outcome(1985) outcome(1988) outcome(1990), ///\n      trunit(3) trperiod(1994) fig\n\n* Placebo tests (permute treatment across donors)\n* ssc install synth_runner\nsynth_runner outcome x1 x2, trunit(3) trperiod(1994) gen_vars\nsynth finds weights on donor units that match the treated unit’s pre-treatment trajectory. Always run placebo tests — if the treated unit’s gap isn’t larger than the placebos, the effect isn’t credible.",
    "crumbs": [
      "Methods",
      "Synthetic Control"
    ]
  },
  {
    "objectID": "synth.html#did-you-know",
    "href": "synth.html#did-you-know",
    "title": "Synthetic Control",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe synthetic control method was developed by Abadie & Gardeazabal (2003) to study the economic impact of terrorism in the Basque Country, and formalized by Abadie, Diamond & Hainmueller (2010) in the California tobacco study. It has since become one of the most widely used methods in policy evaluation.\nAthey & Imbens (2017) called synthetic control “arguably the most important innovation in the policy evaluation literature in the last 15 years.”\nThe method works best when you have long pre-treatment panels (many time periods before treatment) and a moderate number of donor units. It struggles with short panels or when no combination of donors can approximate the treated unit.",
    "crumbs": [
      "Methods",
      "Synthetic Control"
    ]
  },
  {
    "objectID": "regression-adjustment.html",
    "href": "regression-adjustment.html",
    "title": "Regression Adjustment",
    "section": "",
    "text": "The simplest way to estimate a causal effect from observational data: run a regression of the outcome on the treatment indicator and the confounders.\n\\[Y_i = \\alpha + \\tau D_i + \\beta X_i + \\varepsilon_i\\]\nThe coefficient \\(\\hat{\\tau}\\) is your treatment effect estimate, “adjusted” for \\(X\\). OLS compares treated and control units within levels of X — holding the confounder constant to isolate the effect of treatment.\nThe gap between the two regression lines (treated vs control) is \\(\\hat{\\tau}\\).\n\n\n\nSelection on observables (CIA): all confounders are included in \\(X\\). If an unobserved variable drives both treatment and outcome, the estimate is biased — same as every other tool under SOO.\nCorrect functional form: the relationship between \\(X\\) and \\(Y\\) is correctly specified. If you fit a linear model but the truth is quadratic, the adjustment is wrong and \\(\\hat{\\tau}\\) is biased. This is the key vulnerability.\nOverlap: treated and control groups share common support in \\(X\\).\n\n\n\n\nRegression adjustment models the outcome — it writes down a specific equation for how \\(Y\\) relates to \\(X\\) and \\(D\\). That equation is the functional form:\n\nLinear: \\(Y = \\alpha + \\tau D + \\beta X\\) — assumes \\(Y\\) changes at a constant rate with \\(X\\). A straight line.\nQuadratic: \\(Y = \\alpha + \\tau D + \\beta_1 X + \\beta_2 X^2\\) — allows curvature.\nLog: \\(Y = \\alpha + \\tau D + \\beta \\log(X)\\) — assumes diminishing returns.\n\nWhen you run lm(y ~ treat + x), you’ve chosen the linear form. If the truth is quadratic but you fit a line, the line can’t capture the curvature — and that misfit contaminates your estimate of \\(\\hat{\\tau}\\). The model is wrong, so the estimate is biased, even when all confounders are observed. This is an estimation bias, not an identification failure.\nCompare this to IPW, which models the treatment assignment instead — it writes down an equation for \\(P(D = 1 \\mid X)\\), not for \\(Y\\). IPW never touches the outcome. It just reweights the raw \\(Y\\) values using the propensity score. So if the outcome-\\(X\\) relationship is nonlinear, IPW doesn’t care — as long as the propensity score model is right.\n\n\n\n\nWhat it models\nWhat can go wrong\n\n\n\n\nRegression adjustment\n\\(E[Y \\mid X, D]\\) (the outcome)\nWrong functional form for \\(Y\\)\n\n\nIPW\n\\(P(D=1 \\mid X)\\) (the treatment)\nWrong propensity score model\n\n\nDoubly robust\nBoth\nFails only if both are wrong\n\n\n\nEach method bets on getting one model right. Doubly robust estimation hedges by combining both — it only needs one to be correct.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"confounding\", \"Confounding strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      selectInput(\"shape\", \"True outcome model:\",\n                  choices = c(\"Linear (Y ~ X)\"      = \"linear\",\n                              \"Quadratic (Y ~ X\\u00b2)\" = \"quadratic\")),\n\n      actionButton(\"go\", \"New draw\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"scatter_plot\", height = \"400px\")),\n        column(6, plotOutput(\"compare_plot\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    ate  &lt;- input$ate\n    conf &lt;- input$confounding\n    is_linear &lt;- input$shape == \"linear\"\n\n    # Confounder\n    x &lt;- rnorm(n)\n\n    # Treatment depends on x\n    p_true &lt;- pnorm(conf * x)\n    treat  &lt;- rbinom(n, 1, p_true)\n\n    # Outcome depends on x (linearly or quadratically) + treatment\n    if (is_linear) {\n      y &lt;- 1 + 2 * x + ate * treat + rnorm(n)\n    } else {\n      y &lt;- 1 + 2 * x^2 + ate * treat + rnorm(n)\n    }\n\n    # Naive (no controls)\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # Regression adjustment (always linear: Y ~ D + X)\n    fit &lt;- lm(y ~ treat + x)\n    reg_est &lt;- coef(fit)[\"treat\"]\n\n    # IPW for comparison\n    ps &lt;- fitted(glm(treat ~ x, family = binomial))\n    w &lt;- ifelse(treat == 1, 1 / ps, 1 / (1 - ps))\n    ipw_est &lt;- weighted.mean(y[treat == 1], w[treat == 1]) -\n               weighted.mean(y[treat == 0], w[treat == 0])\n\n    list(x = x, treat = treat, y = y, fit = fit,\n         naive = naive, reg_est = reg_est, ipw_est = ipw_est,\n         ate = ate, is_linear = is_linear)\n  })\n\n  output$scatter_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$y, pch = 16, cex = 0.5,\n         col = ifelse(d$treat == 1, \"#3498db50\", \"#e74c3c50\"),\n         xlab = \"X (confounder)\", ylab = \"Y (outcome)\",\n         main = \"Data + Regression Fit\")\n\n    # Fitted regression lines (always linear)\n    x_seq &lt;- seq(min(d$x), max(d$x), length.out = 200)\n    pred_t &lt;- predict(d$fit,\n      newdata = data.frame(treat = 1, x = x_seq))\n    pred_c &lt;- predict(d$fit,\n      newdata = data.frame(treat = 0, x = x_seq))\n    lines(x_seq, pred_t, col = \"#3498db\", lwd = 2.5)\n    lines(x_seq, pred_c, col = \"#e74c3c\", lwd = 2.5)\n\n    # True conditional means (dashed)\n    if (d$is_linear) {\n      true_t &lt;- 1 + 2 * x_seq + d$ate\n      true_c &lt;- 1 + 2 * x_seq\n    } else {\n      true_t &lt;- 1 + 2 * x_seq^2 + d$ate\n      true_c &lt;- 1 + 2 * x_seq^2\n    }\n    lines(x_seq, true_t, col = \"#3498db\", lwd = 1.5, lty = 3)\n    lines(x_seq, true_c, col = \"#e74c3c\", lwd = 1.5, lty = 3)\n\n    # Gap arrow at x = 0\n    mid_t &lt;- predict(d$fit,\n      newdata = data.frame(treat = 1, x = 0))\n    mid_c &lt;- predict(d$fit,\n      newdata = data.frame(treat = 0, x = 0))\n    arrows(0, mid_c, 0, mid_t, code = 3,\n           col = \"#2c3e50\", lwd = 2, length = 0.08)\n    text(0.3, (mid_t + mid_c) / 2,\n         bquote(hat(tau) == .(round(coef(d$fit)[\"treat\"], 2))),\n         cex = 0.95, font = 2)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.8,\n           legend = c(\"Treated\", \"Control\",\n                      \"Regression fit\", \"True E[Y|X,D]\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray40\", \"gray40\"),\n           pch = c(16, 16, NA, NA),\n           lwd = c(NA, NA, 2.5, 1.5),\n           lty = c(NA, NA, 1, 3))\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 9, 3, 2))\n\n    ests &lt;- c(d$ipw_est, d$reg_est, d$naive)\n    labels &lt;- c(\"IPW\", \"Regression adj.\", \"Naive\")\n    cols &lt;- c(\"#27ae60\", \"#3498db\", \"#e74c3c\")\n\n    xlim &lt;- range(c(ests, d$ate))\n    pad  &lt;- max(diff(xlim) * 0.4, 0.5)\n    xlim &lt;- xlim + c(-pad, pad)\n\n    plot(ests, 1:3, pch = 19, cex = 2, col = cols,\n         xlim = xlim, ylim = c(0.5, 3.5),\n         yaxt = \"n\", xlab = \"Estimated treatment effect\",\n         ylab = \"\", main = \"Estimator Comparison\")\n    axis(2, at = 1:3, labels = labels,\n         las = 1, cex.axis = 0.9)\n\n    abline(v = d$ate, lty = 2, col = \"#2c3e50\", lwd = 2)\n    text(d$ate, 3.45,\n         paste0(\"True ATE = \", d$ate),\n         cex = 0.85, font = 2, col = \"#2c3e50\")\n\n    # Lines from true to estimate\n    segments(d$ate, 1:3, ests, 1:3,\n             col = cols, lwd = 2, lty = 2)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    b_naive &lt;- d$naive - d$ate\n    b_reg   &lt;- d$reg_est - d$ate\n    b_ipw   &lt;- d$ipw_est - d$ate\n    reg_ok  &lt;- abs(b_reg) &lt; abs(b_naive) * 0.5\n    ipw_ok  &lt;- abs(b_ipw) &lt; abs(b_naive) * 0.5\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; &lt;span class='bad'&gt;\",\n        round(d$naive, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_naive, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;Reg. adj:&lt;/b&gt; &lt;span class='\",\n        ifelse(reg_ok, \"good\", \"bad\"), \"'&gt;\",\n        round(d$reg_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_reg, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;IPW:&lt;/b&gt; &lt;span class='\",\n        ifelse(ipw_ok, \"good\", \"bad\"), \"'&gt;\",\n        round(d$ipw_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_ipw, 3), \")\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nLinear outcome, confounding = 1.5: regression adjustment works well. The fitted lines (solid) match the true curves (dotted). The estimate is close to the true ATE.\nSwitch to Quadratic: now the true outcome is \\(Y = 1 + 2X^2 + \\tau D\\), but regression still fits a line. The solid lines miss the curvature (compare to the dotted true curves). The regression estimate is biased — it’s a functional form problem, not a confounding problem.\nQuadratic outcome, compare to IPW: IPW doesn’t model the outcome, so it still works. The green dot (IPW) is close to the true ATE; the blue dot (regression) is not.\nConfounding = 0: treatment is random. All three estimators agree, and functional form doesn’t matter (no confounding to adjust for).\n\n\n\n\nRegression adjustment is the simplest and most common tool. It works great when the outcome model is correctly specified. But it’s fragile to functional form misspecification — fitting a line when the truth is curved introduces bias even when all confounders are observed.\nThis is why alternatives like IPW and entropy balancing exist: they avoid modeling the outcome entirely. And doubly robust methods hedge both bets.",
    "crumbs": [
      "Estimation Tools",
      "Regression Adjustment"
    ]
  },
  {
    "objectID": "regression-adjustment.html#the-idea",
    "href": "regression-adjustment.html#the-idea",
    "title": "Regression Adjustment",
    "section": "",
    "text": "The simplest way to estimate a causal effect from observational data: run a regression of the outcome on the treatment indicator and the confounders.\n\\[Y_i = \\alpha + \\tau D_i + \\beta X_i + \\varepsilon_i\\]\nThe coefficient \\(\\hat{\\tau}\\) is your treatment effect estimate, “adjusted” for \\(X\\). OLS compares treated and control units within levels of X — holding the confounder constant to isolate the effect of treatment.\nThe gap between the two regression lines (treated vs control) is \\(\\hat{\\tau}\\).\n\n\n\nSelection on observables (CIA): all confounders are included in \\(X\\). If an unobserved variable drives both treatment and outcome, the estimate is biased — same as every other tool under SOO.\nCorrect functional form: the relationship between \\(X\\) and \\(Y\\) is correctly specified. If you fit a linear model but the truth is quadratic, the adjustment is wrong and \\(\\hat{\\tau}\\) is biased. This is the key vulnerability.\nOverlap: treated and control groups share common support in \\(X\\).\n\n\n\n\nRegression adjustment models the outcome — it writes down a specific equation for how \\(Y\\) relates to \\(X\\) and \\(D\\). That equation is the functional form:\n\nLinear: \\(Y = \\alpha + \\tau D + \\beta X\\) — assumes \\(Y\\) changes at a constant rate with \\(X\\). A straight line.\nQuadratic: \\(Y = \\alpha + \\tau D + \\beta_1 X + \\beta_2 X^2\\) — allows curvature.\nLog: \\(Y = \\alpha + \\tau D + \\beta \\log(X)\\) — assumes diminishing returns.\n\nWhen you run lm(y ~ treat + x), you’ve chosen the linear form. If the truth is quadratic but you fit a line, the line can’t capture the curvature — and that misfit contaminates your estimate of \\(\\hat{\\tau}\\). The model is wrong, so the estimate is biased, even when all confounders are observed. This is an estimation bias, not an identification failure.\nCompare this to IPW, which models the treatment assignment instead — it writes down an equation for \\(P(D = 1 \\mid X)\\), not for \\(Y\\). IPW never touches the outcome. It just reweights the raw \\(Y\\) values using the propensity score. So if the outcome-\\(X\\) relationship is nonlinear, IPW doesn’t care — as long as the propensity score model is right.\n\n\n\n\nWhat it models\nWhat can go wrong\n\n\n\n\nRegression adjustment\n\\(E[Y \\mid X, D]\\) (the outcome)\nWrong functional form for \\(Y\\)\n\n\nIPW\n\\(P(D=1 \\mid X)\\) (the treatment)\nWrong propensity score model\n\n\nDoubly robust\nBoth\nFails only if both are wrong\n\n\n\nEach method bets on getting one model right. Doubly robust estimation hedges by combining both — it only needs one to be correct.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"confounding\", \"Confounding strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      selectInput(\"shape\", \"True outcome model:\",\n                  choices = c(\"Linear (Y ~ X)\"      = \"linear\",\n                              \"Quadratic (Y ~ X\\u00b2)\" = \"quadratic\")),\n\n      actionButton(\"go\", \"New draw\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"scatter_plot\", height = \"400px\")),\n        column(6, plotOutput(\"compare_plot\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    ate  &lt;- input$ate\n    conf &lt;- input$confounding\n    is_linear &lt;- input$shape == \"linear\"\n\n    # Confounder\n    x &lt;- rnorm(n)\n\n    # Treatment depends on x\n    p_true &lt;- pnorm(conf * x)\n    treat  &lt;- rbinom(n, 1, p_true)\n\n    # Outcome depends on x (linearly or quadratically) + treatment\n    if (is_linear) {\n      y &lt;- 1 + 2 * x + ate * treat + rnorm(n)\n    } else {\n      y &lt;- 1 + 2 * x^2 + ate * treat + rnorm(n)\n    }\n\n    # Naive (no controls)\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # Regression adjustment (always linear: Y ~ D + X)\n    fit &lt;- lm(y ~ treat + x)\n    reg_est &lt;- coef(fit)[\"treat\"]\n\n    # IPW for comparison\n    ps &lt;- fitted(glm(treat ~ x, family = binomial))\n    w &lt;- ifelse(treat == 1, 1 / ps, 1 / (1 - ps))\n    ipw_est &lt;- weighted.mean(y[treat == 1], w[treat == 1]) -\n               weighted.mean(y[treat == 0], w[treat == 0])\n\n    list(x = x, treat = treat, y = y, fit = fit,\n         naive = naive, reg_est = reg_est, ipw_est = ipw_est,\n         ate = ate, is_linear = is_linear)\n  })\n\n  output$scatter_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$y, pch = 16, cex = 0.5,\n         col = ifelse(d$treat == 1, \"#3498db50\", \"#e74c3c50\"),\n         xlab = \"X (confounder)\", ylab = \"Y (outcome)\",\n         main = \"Data + Regression Fit\")\n\n    # Fitted regression lines (always linear)\n    x_seq &lt;- seq(min(d$x), max(d$x), length.out = 200)\n    pred_t &lt;- predict(d$fit,\n      newdata = data.frame(treat = 1, x = x_seq))\n    pred_c &lt;- predict(d$fit,\n      newdata = data.frame(treat = 0, x = x_seq))\n    lines(x_seq, pred_t, col = \"#3498db\", lwd = 2.5)\n    lines(x_seq, pred_c, col = \"#e74c3c\", lwd = 2.5)\n\n    # True conditional means (dashed)\n    if (d$is_linear) {\n      true_t &lt;- 1 + 2 * x_seq + d$ate\n      true_c &lt;- 1 + 2 * x_seq\n    } else {\n      true_t &lt;- 1 + 2 * x_seq^2 + d$ate\n      true_c &lt;- 1 + 2 * x_seq^2\n    }\n    lines(x_seq, true_t, col = \"#3498db\", lwd = 1.5, lty = 3)\n    lines(x_seq, true_c, col = \"#e74c3c\", lwd = 1.5, lty = 3)\n\n    # Gap arrow at x = 0\n    mid_t &lt;- predict(d$fit,\n      newdata = data.frame(treat = 1, x = 0))\n    mid_c &lt;- predict(d$fit,\n      newdata = data.frame(treat = 0, x = 0))\n    arrows(0, mid_c, 0, mid_t, code = 3,\n           col = \"#2c3e50\", lwd = 2, length = 0.08)\n    text(0.3, (mid_t + mid_c) / 2,\n         bquote(hat(tau) == .(round(coef(d$fit)[\"treat\"], 2))),\n         cex = 0.95, font = 2)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.8,\n           legend = c(\"Treated\", \"Control\",\n                      \"Regression fit\", \"True E[Y|X,D]\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray40\", \"gray40\"),\n           pch = c(16, 16, NA, NA),\n           lwd = c(NA, NA, 2.5, 1.5),\n           lty = c(NA, NA, 1, 3))\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 9, 3, 2))\n\n    ests &lt;- c(d$ipw_est, d$reg_est, d$naive)\n    labels &lt;- c(\"IPW\", \"Regression adj.\", \"Naive\")\n    cols &lt;- c(\"#27ae60\", \"#3498db\", \"#e74c3c\")\n\n    xlim &lt;- range(c(ests, d$ate))\n    pad  &lt;- max(diff(xlim) * 0.4, 0.5)\n    xlim &lt;- xlim + c(-pad, pad)\n\n    plot(ests, 1:3, pch = 19, cex = 2, col = cols,\n         xlim = xlim, ylim = c(0.5, 3.5),\n         yaxt = \"n\", xlab = \"Estimated treatment effect\",\n         ylab = \"\", main = \"Estimator Comparison\")\n    axis(2, at = 1:3, labels = labels,\n         las = 1, cex.axis = 0.9)\n\n    abline(v = d$ate, lty = 2, col = \"#2c3e50\", lwd = 2)\n    text(d$ate, 3.45,\n         paste0(\"True ATE = \", d$ate),\n         cex = 0.85, font = 2, col = \"#2c3e50\")\n\n    # Lines from true to estimate\n    segments(d$ate, 1:3, ests, 1:3,\n             col = cols, lwd = 2, lty = 2)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    b_naive &lt;- d$naive - d$ate\n    b_reg   &lt;- d$reg_est - d$ate\n    b_ipw   &lt;- d$ipw_est - d$ate\n    reg_ok  &lt;- abs(b_reg) &lt; abs(b_naive) * 0.5\n    ipw_ok  &lt;- abs(b_ipw) &lt; abs(b_naive) * 0.5\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; &lt;span class='bad'&gt;\",\n        round(d$naive, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_naive, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;Reg. adj:&lt;/b&gt; &lt;span class='\",\n        ifelse(reg_ok, \"good\", \"bad\"), \"'&gt;\",\n        round(d$reg_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_reg, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;IPW:&lt;/b&gt; &lt;span class='\",\n        ifelse(ipw_ok, \"good\", \"bad\"), \"'&gt;\",\n        round(d$ipw_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_ipw, 3), \")\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nLinear outcome, confounding = 1.5: regression adjustment works well. The fitted lines (solid) match the true curves (dotted). The estimate is close to the true ATE.\nSwitch to Quadratic: now the true outcome is \\(Y = 1 + 2X^2 + \\tau D\\), but regression still fits a line. The solid lines miss the curvature (compare to the dotted true curves). The regression estimate is biased — it’s a functional form problem, not a confounding problem.\nQuadratic outcome, compare to IPW: IPW doesn’t model the outcome, so it still works. The green dot (IPW) is close to the true ATE; the blue dot (regression) is not.\nConfounding = 0: treatment is random. All three estimators agree, and functional form doesn’t matter (no confounding to adjust for).\n\n\n\n\nRegression adjustment is the simplest and most common tool. It works great when the outcome model is correctly specified. But it’s fragile to functional form misspecification — fitting a line when the truth is curved introduces bias even when all confounders are observed.\nThis is why alternatives like IPW and entropy balancing exist: they avoid modeling the outcome entirely. And doubly robust methods hedge both bets.",
    "crumbs": [
      "Estimation Tools",
      "Regression Adjustment"
    ]
  },
  {
    "objectID": "regression-adjustment.html#in-stata",
    "href": "regression-adjustment.html#in-stata",
    "title": "Regression Adjustment",
    "section": "In Stata",
    "text": "In Stata\n* Basic regression adjustment\nreg outcome treatment x1 x2\n\n* Stata's teffects version (potential outcomes framework)\nteffects ra (outcome x1 x2) (treatment)\n\n* Lin (2013) fix: interact treatment with demeaned covariates\n* (robust to functional form misspecification)\nforeach var of varlist x1 x2 {\n    sum `var', meanonly\n    gen `var'_dm = `var' - r(mean)\n}\nreg outcome treatment c.treatment#c.(x1_dm x2_dm) x1 x2\nPlain reg outcome treatment x1 x2 and teffects ra give the same point estimate under linearity. The Lin (2013) interaction approach is safer when you’re unsure about functional form.",
    "crumbs": [
      "Estimation Tools",
      "Regression Adjustment"
    ]
  },
  {
    "objectID": "regression-adjustment.html#did-you-know",
    "href": "regression-adjustment.html#did-you-know",
    "title": "Regression Adjustment",
    "section": "Did you know?",
    "text": "Did you know?\n\nRegression adjustment is so natural that most applied researchers use it without thinking of it as a “method.” But Freedman (2008) showed that even in randomized experiments, regression adjustment can be biased in finite samples if the model is wrong — the linear adjustment can introduce bias that wasn’t there. Lin (2013) showed how to fix this: interact the treatment indicator with all covariates, and the resulting estimator is asymptotically unbiased regardless of the true functional form.\nThe vulnerability of regression to functional form is a key motivation for semiparametric and nonparametric methods. Instead of assuming \\(E[Y \\mid X] = \\alpha + \\beta X\\) (which might be wrong), methods like kernel regression or series estimation let the data determine the shape.",
    "crumbs": [
      "Estimation Tools",
      "Regression Adjustment"
    ]
  },
  {
    "objectID": "did.html",
    "href": "did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "You have two groups: one gets treated at some point, the other never does. You observe both before and after treatment. The key assumption: absent treatment, both groups would have followed parallel trends.\n\\[\\hat{\\tau}_{DID} = (\\bar{Y}_{treat,post} - \\bar{Y}_{treat,pre}) - (\\bar{Y}_{ctrl,post} - \\bar{Y}_{ctrl,pre})\\]\nThe first difference removes time-invariant group characteristics. The second difference removes common time trends. What’s left is the treatment effect.\n\n\n\nParallel trends: absent treatment, the treated and control groups would have followed the same trajectory over time — the key assumption\nNo anticipation: treated units don’t change behavior before the treatment date\nSUTVA: treatment of one group doesn’t spill over to the control group\nStable composition: the groups don’t change membership over time (no differential attrition)\n\n\n\n\nWhen the parallel trends assumption is violated — if the treated group was already on a different trajectory before treatment. The simulation below lets you break this assumption and see the bias that results.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_units\", \"Units per group:\",\n                  min = 20, max = 200, value = 50, step = 10),\n\n      sliderInput(\"true_effect\", \"True treatment effect:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"trend_diff\", \"Differential pre-trend\\n(violation of parallel trends):\",\n                  min = -1, max = 1, value = 0, step = 0.1),\n\n      sliderInput(\"sigma\", \"Noise (SD):\",\n                  min = 0.5, max = 3, value = 1, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"did_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n_units\n    tau   &lt;- input$true_effect\n    delta &lt;- input$trend_diff\n    sigma &lt;- input$sigma\n\n    periods &lt;- -4:4\n    treat_time &lt;- 1  # treatment at t = 1\n\n    # Group means over time\n    ctrl_mean &lt;- 3 + 0.3 * periods\n    treat_mean &lt;- 5 + (0.3 + delta) * periods\n\n    # Add treatment effect post\n    treat_mean[periods &gt;= treat_time] &lt;- treat_mean[periods &gt;= treat_time] + tau\n\n    # Generate unit-level data\n    ctrl_data &lt;- sapply(ctrl_mean, function(m) m + rnorm(n, sd = sigma))\n    treat_data &lt;- sapply(treat_mean, function(m) m + rnorm(n, sd = sigma))\n\n    ctrl_means_obs &lt;- colMeans(ctrl_data)\n    treat_means_obs &lt;- colMeans(treat_data)\n\n    # DID estimate (using t=0 as pre, t=1 as post)\n    pre_idx  &lt;- which(periods == 0)\n    post_idx &lt;- which(periods == 1)\n\n    did_est &lt;- (treat_means_obs[post_idx] - treat_means_obs[pre_idx]) -\n               (ctrl_means_obs[post_idx] - ctrl_means_obs[pre_idx])\n\n    # Counterfactual for treated (parallel to control from t=0)\n    cf &lt;- treat_means_obs[pre_idx] + (ctrl_means_obs - ctrl_means_obs[pre_idx])\n\n    list(periods = periods, ctrl = ctrl_means_obs, treat = treat_means_obs,\n         cf = cf, did_est = did_est, tau = tau, delta = delta,\n         treat_time = treat_time)\n  })\n\n  output$did_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    ylim &lt;- range(c(d$ctrl, d$treat, d$cf)) + c(-0.5, 0.5)\n\n    plot(d$periods, d$treat, type = \"b\", pch = 19, lwd = 2.5, col = \"#3498db\",\n         xlab = \"Time period\", ylab = \"Mean outcome\",\n         main = \"Difference-in-Differences\",\n         ylim = ylim, xaxt = \"n\")\n    axis(1, at = d$periods)\n\n    lines(d$periods, d$ctrl, type = \"b\", pch = 19, lwd = 2.5, col = \"#e74c3c\")\n\n    # Counterfactual (dashed, post only)\n    post &lt;- d$periods &gt;= d$treat_time\n    lines(d$periods[post], d$cf[post], type = \"b\", pch = 1, lwd = 2, lty = 2,\n          col = \"#3498db80\")\n\n    # Treatment onset\n    abline(v = d$treat_time - 0.5, lty = 3, col = \"gray50\", lwd = 1.5)\n    text(d$treat_time - 0.5, ylim[2], \"Treatment\", pos = 4, cex = 0.85, col = \"gray40\")\n\n    # DID bracket\n    pre_idx  &lt;- which(d$periods == 0)\n    post_idx &lt;- which(d$periods == 1)\n    arrows(max(d$periods) - 0.3, d$cf[post_idx],\n           max(d$periods) - 0.3, d$treat[post_idx],\n           code = 3, lwd = 2, col = \"#27ae60\", length = 0.1)\n    text(max(d$periods) - 0.1, (d$cf[post_idx] + d$treat[post_idx]) / 2,\n         paste0(\"DID = \", round(d$did_est, 2)),\n         col = \"#27ae60\", cex = 0.9, adj = 0)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\", \"Counterfactual (parallel trends)\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"#3498db80\"),\n           pch = c(19, 19, 1), lty = c(1, 1, 2), lwd = c(2.5, 2.5, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$did_est - d$tau\n    biased &lt;- abs(d$delta) &gt; 0.05\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;DID estimate:&lt;/b&gt; \", round(d$did_est, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(biased, \"bad\", \"good\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        if (biased) \"&lt;br&gt;&lt;small&gt;Parallel trends violated &mdash; DID is biased.&lt;/small&gt;\"\n        else \"&lt;br&gt;&lt;small&gt;Parallel trends hold &mdash; DID is unbiased.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDifferential pre-trend = 0: parallel trends hold, DID nails the true effect.\nSlide the differential pre-trend to +0.5: the treated group was already rising faster. DID attributes some of that trend to the treatment — the estimate is biased upward.\nSet true effect = 0 with a differential trend: DID “finds” an effect that doesn’t exist. That’s how pre-trend violations create false positives.\nLook at the pre-treatment periods — if the lines aren’t parallel before treatment, you should worry.",
    "crumbs": [
      "Methods",
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "did.html#the-idea",
    "href": "did.html#the-idea",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "You have two groups: one gets treated at some point, the other never does. You observe both before and after treatment. The key assumption: absent treatment, both groups would have followed parallel trends.\n\\[\\hat{\\tau}_{DID} = (\\bar{Y}_{treat,post} - \\bar{Y}_{treat,pre}) - (\\bar{Y}_{ctrl,post} - \\bar{Y}_{ctrl,pre})\\]\nThe first difference removes time-invariant group characteristics. The second difference removes common time trends. What’s left is the treatment effect.\n\n\n\nParallel trends: absent treatment, the treated and control groups would have followed the same trajectory over time — the key assumption\nNo anticipation: treated units don’t change behavior before the treatment date\nSUTVA: treatment of one group doesn’t spill over to the control group\nStable composition: the groups don’t change membership over time (no differential attrition)\n\n\n\n\nWhen the parallel trends assumption is violated — if the treated group was already on a different trajectory before treatment. The simulation below lets you break this assumption and see the bias that results.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_units\", \"Units per group:\",\n                  min = 20, max = 200, value = 50, step = 10),\n\n      sliderInput(\"true_effect\", \"True treatment effect:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"trend_diff\", \"Differential pre-trend\\n(violation of parallel trends):\",\n                  min = -1, max = 1, value = 0, step = 0.1),\n\n      sliderInput(\"sigma\", \"Noise (SD):\",\n                  min = 0.5, max = 3, value = 1, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"did_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n_units\n    tau   &lt;- input$true_effect\n    delta &lt;- input$trend_diff\n    sigma &lt;- input$sigma\n\n    periods &lt;- -4:4\n    treat_time &lt;- 1  # treatment at t = 1\n\n    # Group means over time\n    ctrl_mean &lt;- 3 + 0.3 * periods\n    treat_mean &lt;- 5 + (0.3 + delta) * periods\n\n    # Add treatment effect post\n    treat_mean[periods &gt;= treat_time] &lt;- treat_mean[periods &gt;= treat_time] + tau\n\n    # Generate unit-level data\n    ctrl_data &lt;- sapply(ctrl_mean, function(m) m + rnorm(n, sd = sigma))\n    treat_data &lt;- sapply(treat_mean, function(m) m + rnorm(n, sd = sigma))\n\n    ctrl_means_obs &lt;- colMeans(ctrl_data)\n    treat_means_obs &lt;- colMeans(treat_data)\n\n    # DID estimate (using t=0 as pre, t=1 as post)\n    pre_idx  &lt;- which(periods == 0)\n    post_idx &lt;- which(periods == 1)\n\n    did_est &lt;- (treat_means_obs[post_idx] - treat_means_obs[pre_idx]) -\n               (ctrl_means_obs[post_idx] - ctrl_means_obs[pre_idx])\n\n    # Counterfactual for treated (parallel to control from t=0)\n    cf &lt;- treat_means_obs[pre_idx] + (ctrl_means_obs - ctrl_means_obs[pre_idx])\n\n    list(periods = periods, ctrl = ctrl_means_obs, treat = treat_means_obs,\n         cf = cf, did_est = did_est, tau = tau, delta = delta,\n         treat_time = treat_time)\n  })\n\n  output$did_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    ylim &lt;- range(c(d$ctrl, d$treat, d$cf)) + c(-0.5, 0.5)\n\n    plot(d$periods, d$treat, type = \"b\", pch = 19, lwd = 2.5, col = \"#3498db\",\n         xlab = \"Time period\", ylab = \"Mean outcome\",\n         main = \"Difference-in-Differences\",\n         ylim = ylim, xaxt = \"n\")\n    axis(1, at = d$periods)\n\n    lines(d$periods, d$ctrl, type = \"b\", pch = 19, lwd = 2.5, col = \"#e74c3c\")\n\n    # Counterfactual (dashed, post only)\n    post &lt;- d$periods &gt;= d$treat_time\n    lines(d$periods[post], d$cf[post], type = \"b\", pch = 1, lwd = 2, lty = 2,\n          col = \"#3498db80\")\n\n    # Treatment onset\n    abline(v = d$treat_time - 0.5, lty = 3, col = \"gray50\", lwd = 1.5)\n    text(d$treat_time - 0.5, ylim[2], \"Treatment\", pos = 4, cex = 0.85, col = \"gray40\")\n\n    # DID bracket\n    pre_idx  &lt;- which(d$periods == 0)\n    post_idx &lt;- which(d$periods == 1)\n    arrows(max(d$periods) - 0.3, d$cf[post_idx],\n           max(d$periods) - 0.3, d$treat[post_idx],\n           code = 3, lwd = 2, col = \"#27ae60\", length = 0.1)\n    text(max(d$periods) - 0.1, (d$cf[post_idx] + d$treat[post_idx]) / 2,\n         paste0(\"DID = \", round(d$did_est, 2)),\n         col = \"#27ae60\", cex = 0.9, adj = 0)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\", \"Counterfactual (parallel trends)\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"#3498db80\"),\n           pch = c(19, 19, 1), lty = c(1, 1, 2), lwd = c(2.5, 2.5, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$did_est - d$tau\n    biased &lt;- abs(d$delta) &gt; 0.05\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True effect:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;DID estimate:&lt;/b&gt; \", round(d$did_est, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; &lt;span class='\", ifelse(biased, \"bad\", \"good\"), \"'&gt;\",\n        round(bias, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        if (biased) \"&lt;br&gt;&lt;small&gt;Parallel trends violated &mdash; DID is biased.&lt;/small&gt;\"\n        else \"&lt;br&gt;&lt;small&gt;Parallel trends hold &mdash; DID is unbiased.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nDifferential pre-trend = 0: parallel trends hold, DID nails the true effect.\nSlide the differential pre-trend to +0.5: the treated group was already rising faster. DID attributes some of that trend to the treatment — the estimate is biased upward.\nSet true effect = 0 with a differential trend: DID “finds” an effect that doesn’t exist. That’s how pre-trend violations create false positives.\nLook at the pre-treatment periods — if the lines aren’t parallel before treatment, you should worry.",
    "crumbs": [
      "Methods",
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "did.html#in-stata",
    "href": "did.html#in-stata",
    "title": "Difference-in-Differences",
    "section": "In Stata",
    "text": "In Stata\n* Classic 2x2 DID with interaction\nreg outcome treated##post\n\n* With controls\nreg outcome treated##post x1 x2, cluster(state)\n\n* Event study (test parallel trends visually)\nreg outcome i.treated#i.year i.year i.treated, cluster(state)\n\n* Modern staggered DID (Callaway & Sant'Anna 2021)\n* ssc install csdid\ncsdid outcome x1 x2, ivar(id) time(year) gvar(first_treated)\nThe coefficient on treated#post (or 1.treated#1.post) is the DID estimate. Clustering standard errors at the group level (e.g., state) is standard practice.",
    "crumbs": [
      "Methods",
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "causal-forests.html",
    "href": "causal-forests.html",
    "title": "Estimating Heterogeneous Effects",
    "section": "",
    "text": "The previous page defined the estimand: \\(\\tau(x) = E[Y(1) - Y(0) \\mid X = x]\\). This page covers the estimation machinery — how to recover \\(\\tau(x)\\) from data when you don’t know in advance which covariates drive heterogeneity or what functional form it takes.",
    "crumbs": [
      "Beyond Averages",
      "Estimating Heterogeneous Effects"
    ]
  },
  {
    "objectID": "causal-forests.html#causal-forests",
    "href": "causal-forests.html#causal-forests",
    "title": "Estimating Heterogeneous Effects",
    "section": "Causal forests",
    "text": "Causal forests\nCausal forests (Wager and Athey, 2018) adapt random forests to estimate \\(\\tau(x)\\) directly. The key idea: instead of splitting to predict the outcome \\(Y\\), split to maximize the heterogeneity in treatment effects across the resulting subgroups.\n\nHow it works\n\nFor each tree, draw a subsample (honesty splitting — see below)\nAt each node, find the covariate split that maximizes the difference in treatment effects between the two child nodes\nWithin each terminal leaf, estimate the treatment effect by comparing treated and control outcomes (a local difference in means)\nAverage predictions across all trees in the forest\n\nThe forest produces an estimate \\(\\hat{\\tau}(x)\\) for any covariate vector \\(x\\) — a personalized treatment effect prediction.\n\n\nEffect-splitting\nIn a standard random forest, the splitting criterion is variance reduction in \\(Y\\): find the split that makes \\(Y\\) most predictable within each child node. In a causal forest, the splitting criterion is treatment effect heterogeneity: find the split such that the treatment effect in the left child differs maximally from the treatment effect in the right child.\nThis is what makes it a causal forest rather than a predictive one. The algorithm is optimized for finding the covariates that best explain variation in \\(\\tau(x)\\), not variation in \\(Y\\).\n\n\nHonesty\nA critical innovation. Each tree uses one subsample to determine the splits (the tree structure) and a separate subsample to estimate the treatment effects within leaves. This prevents overfitting: the splits are chosen to find heterogeneity, but the effect estimates are uncontaminated by this search.\nThis is the same logic as cross-fitting in DML — separate the model selection step from the estimation step. If you use the same data for both, the effect estimates are biased toward finding heterogeneity even when none exists.\n\n\nInference\nCausal forests come with valid, asymptotic confidence intervals for \\(\\hat{\\tau}(x)\\). The variance is estimated via the jackknife (related to the bootstrap):\n\\[\n\\hat{\\tau}(x) \\pm z_{\\alpha/2} \\cdot \\hat{\\sigma}(x)\n\\]\nThis is remarkable for a machine learning method: valid frequentist inference on a nonparametric estimand. The key is that the forest targets a local parameter (the CATE at a point), and the honesty + subsampling structure ensures regularity conditions hold.\n\n\n\n\n\n\nWhat causal forests assume. The same identification assumptions as any selection-on-observables estimator: unconfoundedness and overlap. The forest does not weaken these requirements — it only provides a flexible estimator for \\(\\tau(x)\\) given that identification holds. A causal forest applied to a confounded comparison produces a detailed, confident, wrong function.\n\n\n\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 200, max = 2000, value = 500, step = 200),\n\n      selectInput(\"pattern\", \"Heterogeneity pattern:\",\n                  choices = c(\n                    \"Linear: \\u03C4(x) = 2 + 3x\"       = \"linear\",\n                    \"Step: \\u03C4(x) = 1 if x&lt;0, 4 if x\\u22650\" = \"step\",\n                    \"No heterogeneity: \\u03C4(x) = 2\"   = \"constant\"\n                  )),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"scatter_plot\", height = \"430px\")),\n        column(6, plotOutput(\"cate_plot\", height = \"430px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n       &lt;- input$n\n    pattern &lt;- input$pattern\n\n    # Generate data (randomized experiment)\n    X &lt;- rnorm(n)\n    D &lt;- rbinom(n, 1, 0.5)\n\n    # True CATE function\n    tau_fn &lt;- switch(pattern,\n      linear   = function(x) 2 + 3 * x,\n      step     = function(x) ifelse(x &lt; 0, 1, 4),\n      constant = function(x) rep(2, length(x))\n    )\n\n    tau_x &lt;- tau_fn(X)\n    Y &lt;- tau_x * D + X + rnorm(n)\n\n    # Binned CATE estimation\n    n_bins &lt;- 10\n    breaks &lt;- quantile(X, probs = seq(0, 1, length.out = n_bins + 1))\n    breaks[1] &lt;- breaks[1] - 0.01\n    breaks[n_bins + 1] &lt;- breaks[n_bins + 1] + 0.01\n    bins &lt;- cut(X, breaks = breaks, labels = FALSE)\n\n    bin_mids  &lt;- numeric(n_bins)\n    bin_ests  &lt;- numeric(n_bins)\n    bin_valid &lt;- logical(n_bins)\n\n    for (j in 1:n_bins) {\n      idx &lt;- bins == j\n      bin_mids[j] &lt;- mean(X[idx])\n      t_idx &lt;- idx & D == 1\n      c_idx &lt;- idx & D == 0\n      if (sum(t_idx) &gt; 1 && sum(c_idx) &gt; 1) {\n        bin_ests[j] &lt;- mean(Y[t_idx]) - mean(Y[c_idx])\n        bin_valid[j] &lt;- TRUE\n      } else {\n        bin_ests[j] &lt;- NA\n        bin_valid[j] &lt;- FALSE\n      }\n    }\n\n    # ATE: true and estimated\n    true_ate &lt;- mean(tau_x)\n    est_ate  &lt;- mean(Y[D == 1]) - mean(Y[D == 0])\n\n    # For plotting the true curve\n    x_seq &lt;- seq(min(X), max(X), length.out = 200)\n    tau_seq &lt;- tau_fn(x_seq)\n\n    list(X = X, Y = Y, D = D, tau_x = tau_x,\n         x_seq = x_seq, tau_seq = tau_seq,\n         bin_mids = bin_mids, bin_ests = bin_ests, bin_valid = bin_valid,\n         true_ate = true_ate, est_ate = est_ate,\n         pattern = pattern, n = n, tau_fn = tau_fn)\n  })\n\n  output$scatter_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$X, d$Y, pch = 16, cex = 0.4,\n         col = ifelse(d$D == 1, \"#3498db40\", \"#e74c3c40\"),\n         xlab = \"X (covariate)\", ylab = \"Y (outcome)\",\n         main = \"Observed Data\")\n\n    # True tau(x) curve\n    lines(d$x_seq, d$tau_seq, col = \"#2c3e50\", lwd = 2.5)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.8,\n           legend = c(\"Treated\", \"Control\", expression(\"True \" * tau * \"(x)\")),\n           col = c(\"#3498db\", \"#e74c3c\", \"#2c3e50\"),\n           pch = c(16, 16, NA), lwd = c(NA, NA, 2.5))\n  })\n\n  output$cate_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    valid &lt;- d$bin_valid\n    ylim &lt;- range(c(d$tau_seq, d$bin_ests[valid]), na.rm = TRUE)\n    pad  &lt;- diff(ylim) * 0.15\n    ylim &lt;- ylim + c(-pad, pad)\n\n    plot(NULL, xlim = range(d$x_seq), ylim = ylim,\n         xlab = \"X (covariate)\",\n         ylab = expression(\"Treatment effect \" * tau * \"(x)\"),\n         main = \"CATE: Binned Estimates vs Truth\")\n\n    # True curve\n    lines(d$x_seq, d$tau_seq, col = \"#2c3e50\", lwd = 2.5)\n\n    # Binned estimates\n    points(d$bin_mids[valid], d$bin_ests[valid],\n           pch = 19, cex = 1.6, col = \"#9b59b6\")\n\n    # Connect with segments for clarity\n    segments(d$bin_mids[valid], d$tau_fn(d$bin_mids[valid]),\n             d$bin_mids[valid], d$bin_ests[valid],\n             col = \"#9b59b680\", lty = 2, lwd = 1.5)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.8,\n           legend = c(expression(\"True \" * tau * \"(x)\"), \"Binned estimate\"),\n           col = c(\"#2c3e50\", \"#9b59b6\"),\n           lwd = c(2.5, NA), pch = c(NA, 19))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    pat_label &lt;- switch(d$pattern,\n      linear   = \"Linear: \\u03C4(x) = 2 + 3x\",\n      step     = \"Step: \\u03C4(x) = 1 if x&lt;0, 4 if x\\u22650\",\n      constant = \"No heterogeneity: \\u03C4(x) = 2\"\n    )\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Pattern:&lt;/b&gt; \", pat_label, \"&lt;br&gt;\",\n        \"&lt;b&gt;n:&lt;/b&gt; \", d$n, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", round(d$true_ate, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Estimated ATE:&lt;/b&gt; \", round(d$est_ate, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bias:&lt;/b&gt; \", round(d$est_ate - d$true_ate, 3)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nLinear heterogeneity: the treatment effect increases with \\(X\\). The binned estimates trace out the linear relationship. With larger \\(n\\), the estimates get tighter around the truth.\nStep function: a sharp change at \\(X = 0\\). The binned estimator smooths over this discontinuity — bins near the cutoff mix units from both regimes. More data and finer bins would help.\nNo heterogeneity: all bins should estimate roughly the same effect. If they look scattered, that’s sampling noise — there is no real heterogeneity to find.",
    "crumbs": [
      "Beyond Averages",
      "Estimating Heterogeneous Effects"
    ]
  },
  {
    "objectID": "causal-forests.html#meta-learners",
    "href": "causal-forests.html#meta-learners",
    "title": "Estimating Heterogeneous Effects",
    "section": "Meta-learners",
    "text": "Meta-learners\nCausal forests are one approach to CATE estimation. A broader framework organizes methods by how they decompose the problem into standard supervised learning tasks:\n\nT-learner\nEstimate two separate outcome models — one for treated, one for control — and take the difference:\n\\[\n\\hat{\\tau}(x) = \\hat{\\mu}_1(x) - \\hat{\\mu}_0(x)\n\\]\nwhere \\(\\hat{\\mu}_1(x) = \\hat{E}[Y \\mid X = x, D = 1]\\) and \\(\\hat{\\mu}_0(x) = \\hat{E}[Y \\mid X = x, D = 0]\\).\nSimple and works with any ML method. But the two models are fit independently — each optimizes prediction of \\(Y\\), not estimation of the difference. Small treatment effects can be lost in the noise of predicting outcomes.\n\n\nS-learner\nEstimate a single model with treatment as a feature:\n\\[\n\\hat{\\mu}(x, d) = \\hat{E}[Y \\mid X = x, D = d]\n\\]\nThen \\(\\hat{\\tau}(x) = \\hat{\\mu}(x, 1) - \\hat{\\mu}(x, 0)\\). The model can capture treatment effect heterogeneity through interactions between \\(D\\) and \\(X\\).\nRisk: the model may not prioritize learning the treatment effect if the main effects of \\(X\\) dominate. With many strong predictors and a small treatment effect, the model may effectively ignore \\(D\\).\n\n\nR-learner (Robinson, 1988; Nie and Wager, 2021)\nResidualize both the outcome and treatment on \\(X\\) (as in the partially linear model from DML):\n\\[\n\\tilde{Y}_i = Y_i - \\hat{m}(X_i), \\qquad \\tilde{D}_i = D_i - \\hat{e}(X_i)\n\\]\nThen estimate \\(\\tau(x)\\) by minimizing:\n\\[\n\\hat{\\tau}(\\cdot) = \\arg\\min_\\tau \\sum_{i=1}^n \\left(\\tilde{Y}_i - \\tau(X_i)\\tilde{D}_i\\right)^2\n\\]\nThis is Frisch-Waugh-Lovell generalized to heterogeneous effects. By partialling out the main effects of \\(X\\) on both \\(Y\\) and \\(D\\), the R-learner isolates the treatment effect signal. It directly targets \\(\\tau(x)\\) rather than backing it out from outcome predictions.\n\n\nComparison\n\n\n\n\n\n\n\n\n\nMethod\nModels to fit\nStrengths\nWeaknesses\n\n\n\n\nT-learner\nTwo outcome models\nSimple, any ML method\nDoesn’t optimize for \\(\\tau\\)\n\n\nS-learner\nOne outcome model\nUses all data jointly\nMay underweight treatment heterogeneity\n\n\nR-learner\nTwo nuisance + CATE model\nDirectly targets \\(\\tau(x)\\)\nMore complex to implement\n\n\nCausal forest\nForest with effect-splitting\nValid inference, adaptive\nRequires unconfoundedness\n\n\n\n\n\n\n\n\n\nChoosing a method. If you need confidence intervals for \\(\\hat{\\tau}(x)\\), use causal forests — they’re the only method above with built-in inference. If you want flexibility in the ML method (boosting, neural nets), use the R-learner. If you want simplicity and a quick baseline, start with the T-learner.",
    "crumbs": [
      "Beyond Averages",
      "Estimating Heterogeneous Effects"
    ]
  },
  {
    "objectID": "causal-forests.html#practical-cautions",
    "href": "causal-forests.html#practical-cautions",
    "title": "Estimating Heterogeneous Effects",
    "section": "Practical cautions",
    "text": "Practical cautions\nDiscovery vs confirmation. These methods are powerful for discovering heterogeneity — finding subgroups where effects differ. But discovery is not confirmation. Best practice: use causal forests or meta-learners to identify potential heterogeneity in one sample, then confirm it in a held-out sample or a pre-registered replication. Discovery and confirmation should use different data.\nMultiple testing. Reporting that “the effect is significant for subgroup A but not subgroup B” is a comparison, not a test. To claim the effects differ, you need to test the interaction — and if you searched over many subgroups to find A, you have a multiple testing problem.\nSample size. Estimating \\(\\tau(x)\\) requires more data than estimating \\(\\tau\\). You’re estimating a function, not a number, and you need enough treated and control units at each value of \\(x\\). With small samples, the CATE estimates will be noisy and shrunk toward the ATE.\nOverlap. The CATE at \\(x\\) requires both treated and control units near \\(x\\). If overlap fails in some region — few treated units with \\(X\\) near \\(x\\) — the CATE estimate there is unreliable. Causal forests handle this gracefully (they simply don’t split in sparse regions), but the conceptual limitation remains.",
    "crumbs": [
      "Beyond Averages",
      "Estimating Heterogeneous Effects"
    ]
  },
  {
    "objectID": "causal-forests.html#connecting-to-the-course",
    "href": "causal-forests.html#connecting-to-the-course",
    "title": "Estimating Heterogeneous Effects",
    "section": "Connecting to the course",
    "text": "Connecting to the course\n\nHeterogeneous Treatment Effects: defines the estimand \\(\\tau(x)\\) — the conceptual foundation this page builds on\nDML: the R-learner uses DML-style residualization; causal forests use similar honesty/cross-fitting logic\nDoubly Robust: the AIPW score can be extended to target CATE instead of ATE — doubly robust CATE estimation\nFWL: the R-learner is FWL generalized to heterogeneous effects\nMultiple Testing: subgroup analysis without correction is a multiple testing problem\nBootstrap: the jackknife variance estimator in causal forests is related to bootstrap inference",
    "crumbs": [
      "Beyond Averages",
      "Estimating Heterogeneous Effects"
    ]
  },
  {
    "objectID": "ipw.html",
    "href": "ipw.html",
    "title": "Inverse Probability Weighting",
    "section": "",
    "text": "In observational data, treatment isn’t random. People who get treated differ from those who don’t — they may be older, sicker, richer, etc. A naive comparison of outcomes is biased by these confounders.",
    "crumbs": [
      "Estimation Tools",
      "IPW"
    ]
  },
  {
    "objectID": "ipw.html#the-problem",
    "href": "ipw.html#the-problem",
    "title": "Inverse Probability Weighting",
    "section": "",
    "text": "In observational data, treatment isn’t random. People who get treated differ from those who don’t — they may be older, sicker, richer, etc. A naive comparison of outcomes is biased by these confounders.",
    "crumbs": [
      "Estimation Tools",
      "IPW"
    ]
  },
  {
    "objectID": "ipw.html#the-idea",
    "href": "ipw.html#the-idea",
    "title": "Inverse Probability Weighting",
    "section": "The idea",
    "text": "The idea\nInverse Probability Weighting (IPW) reweights observations so that the treated and control groups look alike on observed covariates. The steps:\n\nEstimate the propensity score \\(e(X) = P(\\text{treated} \\mid X)\\) — the probability of treatment given covariates.\nWeight each observation inversely by its probability of receiving the treatment it actually got:\n\nTreated units get weight \\(1 / e(X)\\)\nControl units get weight \\(1 / (1 - e(X))\\)\n\nCompute the weighted difference in means.\n\nIntuition: if a treated person had only a 20% chance of being treated (based on their X), they’re “surprising” — they represent 5 similar people who weren’t treated. So they get upweighted. This creates a pseudo-population where treatment is independent of X.\n\nAssumptions\n\nUnconfoundedness (selection on observables): conditional on observed covariates \\(X\\), treatment is independent of potential outcomes — \\(Y(0), Y(1) \\perp D \\mid X\\). All confounders are observed and included.\nOverlap (positivity): every unit has a non-zero probability of being in either group — \\(0 &lt; e(X) &lt; 1\\) for all \\(X\\). No one is deterministically treated or untreated.\nCorrect propensity score model: the functional form of \\(e(X)\\) is correctly specified. If you use a logit and the true relationship is nonlinear, the weights are wrong.\n\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"confounding\", \"Confounding strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"balance_plot\", height = \"380px\")),\n        column(6, plotOutput(\"ps_plot\", height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    ate  &lt;- input$ate\n    conf &lt;- input$confounding\n\n    # Confounder\n    x &lt;- rnorm(n)\n\n    # Treatment depends on x (confounding)\n    p_true &lt;- pnorm(conf * x)\n    treat &lt;- rbinom(n, 1, p_true)\n\n    # Outcome depends on x and treatment\n    y &lt;- 1 + 2 * x + ate * treat + rnorm(n)\n\n    # Naive estimate\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # IPW estimate\n    ps &lt;- fitted(glm(treat ~ x, family = binomial))\n    w &lt;- ifelse(treat == 1, 1 / ps, 1 / (1 - ps))\n    ipw_est &lt;- weighted.mean(y[treat == 1], w[treat == 1]) -\n               weighted.mean(y[treat == 0], w[treat == 0])\n\n    list(x = x, treat = treat, y = y, ps = ps, w = w,\n         naive = naive, ipw_est = ipw_est, ate = ate)\n  })\n\n  output$balance_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Unweighted densities\n    x_t &lt;- d$x[d$treat == 1]\n    x_c &lt;- d$x[d$treat == 0]\n\n    rng &lt;- range(d$x)\n    dens_t &lt;- density(x_t, from = rng[1], to = rng[2])\n    dens_c &lt;- density(x_c, from = rng[1], to = rng[2])\n\n    ylim &lt;- c(0, max(dens_t$y, dens_c$y) * 1.2)\n\n    plot(dens_t, col = \"#3498db\", lwd = 2.5, main = \"Covariate Balance (X)\",\n         xlab = \"X (confounder)\", ylab = \"Density\", ylim = ylim)\n    lines(dens_c, col = \"#e74c3c\", lwd = 2.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\"),\n           col = c(\"#3498db\", \"#e74c3c\"), lwd = 2.5)\n  })\n\n  output$ps_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$ps, pch = 16, cex = 0.5,\n         col = ifelse(d$treat == 1, \"#3498db80\", \"#e74c3c80\"),\n         xlab = \"X (confounder)\", ylab = \"Propensity score e(X)\",\n         main = \"Propensity Score vs Confounder\")\n    abline(h = 0.5, lty = 2, col = \"gray50\")\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Treated\", \"Control\"),\n           col = c(\"#3498db\", \"#e74c3c\"), pch = 16)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive estimate:&lt;/b&gt; &lt;span class='bad'&gt;\", round(d$naive, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;IPW estimate:&lt;/b&gt; &lt;span class='good'&gt;\", round(d$ipw_est, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Naive bias:&lt;/b&gt; \", round(d$naive - d$ate, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;IPW bias:&lt;/b&gt; \", round(d$ipw_est - d$ate, 3)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nConfounding = 0: treatment is random. Naive and IPW give the same answer.\nConfounding = 1.5: the covariate distributions for treated and control diverge (left plot). Naive is biased, IPW corrects it.\nConfounding = 3: extreme selection. The propensity scores are near 0 or 1 (right plot), meaning some units get huge weights. IPW becomes noisy — this is the extreme weights problem.",
    "crumbs": [
      "Estimation Tools",
      "IPW"
    ]
  },
  {
    "objectID": "ipw.html#in-stata",
    "href": "ipw.html#in-stata",
    "title": "Inverse Probability Weighting",
    "section": "In Stata",
    "text": "In Stata\n* IPW with Stata's teffects\nteffects ipw (outcome) (treatment x1 x2)\n\n* Check propensity score overlap\nteffects overlap\n\n* Manual approach: estimate propensity score, then weight\nlogit treatment x1 x2\npredict pscore, pr\ngen ipw = treatment/pscore + (1-treatment)/(1-pscore)\nreg outcome treatment [pw=ipw]\nteffects ipw handles everything — propensity score estimation, weighting, and correct standard errors. The manual approach is useful for understanding what’s happening under the hood.",
    "crumbs": [
      "Estimation Tools",
      "IPW"
    ]
  },
  {
    "objectID": "doubly-robust.html",
    "href": "doubly-robust.html",
    "title": "Doubly Robust Estimation",
    "section": "",
    "text": "Every estimation tool under selection on observables relies on getting a model right:\n\nRegression adjustment needs the outcome model \\(E[Y \\mid X, D]\\) to be correct.\nIPW needs the propensity score model \\(P(D = 1 \\mid X)\\) to be correct.\n\nWhat if you’re not sure which one you got right? Doubly robust (DR) estimation combines both models and gives you a consistent estimate if either one is correctly specified.\n\n\n\n\n\n\nExample: vaccine effectiveness. You want to know if a vaccine reduces hospitalization. You model who gets vaccinated (propensity score: older people and healthcare workers vaccinate more) and you model hospitalization risk (outcome model: depends on age, comorbidities, obesity). DR combines both. If your outcome model misses a nonlinear age effect but your propensity score is right — you’re fine. If your propensity score ignores rural vs urban but your outcome model captures it — also fine. You only fail if both models are wrong simultaneously.\n\n\n\n\n\nThe most common DR estimator is Augmented Inverse Probability Weighting (AIPW):\n\\[\\hat{\\tau}_{DR} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\hat{\\mu}_1(X_i) - \\hat{\\mu}_0(X_i) + \\frac{D_i (Y_i - \\hat{\\mu}_1(X_i))}{\\hat{e}(X_i)} - \\frac{(1 - D_i)(Y_i - \\hat{\\mu}_0(X_i))}{1 - \\hat{e}(X_i)} \\right]\\]\nwhere \\(\\hat{\\mu}_d(X)\\) is the estimated outcome under treatment \\(d\\), and \\(\\hat{e}(X)\\) is the estimated propensity score.\nIntuition: start with the regression prediction (\\(\\hat{\\mu}_1 - \\hat{\\mu}_0\\)), then correct it using the IPW-weighted residuals. If the outcome model is perfect, the residuals are zero and the correction vanishes. If the outcome model is wrong but the propensity score is right, the correction fixes the bias.\n\n\n\n\n\n\nOutcome model\nPropensity score\nDR consistent?\n\n\n\n\nCorrect\nCorrect\nYes\n\n\nCorrect\nWrong\nYes\n\n\nWrong\nCorrect\nYes\n\n\nWrong\nWrong\nNo\n\n\n\nThree out of four — you get two shots at a consistent estimate. This is the key advantage over methods that rely on a single model.\n\n\n\nTo see DR in action, we need a setting where models can be wrong:\n\nTrue outcome: \\(Y = 1 + 2X^2 + \\tau D + \\varepsilon\\) (quadratic in \\(X\\))\nTrue treatment: \\(P(D=1 \\mid X) = \\Phi(1.5 \\cdot X)\\) (probit)\n“Correct” outcome model: includes \\(X^2\\)\n“Wrong” outcome model: linear only (\\(X\\), no \\(X^2\\))\n“Correct” PS model: logit on \\(X\\) (close enough to probit)\n“Wrong” PS model: constant 0.5 (ignores \\(X\\) entirely)\n\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 500, max = 3000, value = 1000, step = 250),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      selectInput(\"scenario\", \"Scenario:\",\n                  choices = c(\n                    \"Both correct\"         = \"both\",\n                    \"Only outcome correct\"  = \"outcome_only\",\n                    \"Only PS correct\"       = \"ps_only\",\n                    \"Neither correct\"       = \"neither\"\n                  )),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"fit_plot\", height = \"430px\")),\n        column(6, plotOutput(\"compare_plot\", height = \"430px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    ate &lt;- input$ate\n    sc  &lt;- input$scenario\n\n    # Data generating process\n    x &lt;- rnorm(n)\n\n    # True PS: probit\n    p_true &lt;- pnorm(1.5 * x)\n    treat &lt;- rbinom(n, 1, p_true)\n\n    # True outcome: quadratic\n    y &lt;- 1 + 2 * x^2 + ate * treat + rnorm(n)\n\n    # --- Outcome models ---\n    outcome_correct &lt;- sc %in% c(\"both\", \"outcome_only\")\n    if (outcome_correct) {\n      x2 &lt;- x^2\n      fit1 &lt;- lm(y ~ treat + x + x2)\n      mu1 &lt;- predict(fit1, newdata = data.frame(treat = 1, x = x, x2 = x^2))\n      mu0 &lt;- predict(fit1, newdata = data.frame(treat = 0, x = x, x2 = x^2))\n    } else {\n      fit1 &lt;- lm(y ~ treat + x)\n      mu1 &lt;- predict(fit1, newdata = data.frame(treat = 1, x = x))\n      mu0 &lt;- predict(fit1, newdata = data.frame(treat = 0, x = x))\n    }\n\n    # Regression adjustment estimate\n    reg_est &lt;- mean(mu1 - mu0)\n\n    # --- PS models ---\n    ps_correct &lt;- sc %in% c(\"both\", \"ps_only\")\n    if (ps_correct) {\n      ps &lt;- fitted(glm(treat ~ x, family = binomial))\n    } else {\n      ps &lt;- rep(0.5, n)\n    }\n\n    # IPW estimate\n    w &lt;- ifelse(treat == 1, 1 / ps, 1 / (1 - ps))\n    ipw_est &lt;- weighted.mean(y[treat == 1], w[treat == 1]) -\n               weighted.mean(y[treat == 0], w[treat == 0])\n\n    # Doubly robust (AIPW)\n    dr_est &lt;- mean(mu1 - mu0 +\n      treat * (y - mu1) / ps -\n      (1 - treat) * (y - mu0) / (1 - ps))\n\n    # Naive\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # For plotting: outcome model fit curves\n    x_seq &lt;- seq(min(x), max(x), length.out = 200)\n    if (outcome_correct) {\n      pred_t &lt;- predict(fit1, newdata = data.frame(treat = 1, x = x_seq, x2 = x_seq^2))\n      pred_c &lt;- predict(fit1, newdata = data.frame(treat = 0, x = x_seq, x2 = x_seq^2))\n    } else {\n      pred_t &lt;- predict(fit1, newdata = data.frame(treat = 1, x = x_seq))\n      pred_c &lt;- predict(fit1, newdata = data.frame(treat = 0, x = x_seq))\n    }\n    true_t &lt;- 1 + 2 * x_seq^2 + ate\n    true_c &lt;- 1 + 2 * x_seq^2\n\n    list(x = x, y = y, treat = treat,\n         x_seq = x_seq, pred_t = pred_t, pred_c = pred_c,\n         true_t = true_t, true_c = true_c,\n         naive = naive, reg_est = reg_est, ipw_est = ipw_est, dr_est = dr_est,\n         ate = ate, sc = sc,\n         outcome_correct = outcome_correct, ps_correct = ps_correct)\n  })\n\n  output$fit_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$y, pch = 16, cex = 0.3,\n         col = ifelse(d$treat == 1, \"#3498db30\", \"#e74c3c30\"),\n         xlab = \"X (confounder)\", ylab = \"Y (outcome)\",\n         main = \"Outcome Model Fit vs Truth\")\n\n    # Fitted curves (solid)\n    lines(d$x_seq, d$pred_t, col = \"#3498db\", lwd = 2.5)\n    lines(d$x_seq, d$pred_c, col = \"#e74c3c\", lwd = 2.5)\n\n    # True curves (dashed)\n    lines(d$x_seq, d$true_t, col = \"#3498db\", lwd = 1.5, lty = 3)\n    lines(d$x_seq, d$true_c, col = \"#e74c3c\", lwd = 1.5, lty = 3)\n\n    outcome_label &lt;- if (d$outcome_correct) \"Correct (quadratic)\" else \"Wrong (linear)\"\n    ps_label &lt;- if (d$ps_correct) \"Correct (logit)\" else \"Wrong (constant)\"\n\n    legend(\"topleft\", bty = \"n\", cex = 0.75,\n           legend = c(\"Treated data\", \"Control data\",\n                      \"Model fit (solid)\", \"True E[Y|X,D] (dotted)\",\n                      paste0(\"Outcome: \", outcome_label),\n                      paste0(\"PS: \", ps_label)),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray40\", \"gray40\", NA, NA),\n           pch = c(16, 16, NA, NA, NA, NA),\n           lwd = c(NA, NA, 2.5, 1.5, NA, NA),\n           lty = c(NA, NA, 1, 3, NA, NA))\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 9, 3, 2))\n\n    ests &lt;- c(d$dr_est, d$ipw_est, d$reg_est, d$naive)\n    labels &lt;- c(\"Doubly Robust\", \"IPW\", \"Regression adj.\", \"Naive\")\n    cols &lt;- c(\"#9b59b6\", \"#27ae60\", \"#3498db\", \"#e74c3c\")\n\n    xlim &lt;- range(c(ests, d$ate))\n    pad  &lt;- max(diff(xlim) * 0.4, 0.5)\n    xlim &lt;- xlim + c(-pad, pad)\n\n    plot(ests, 1:4, pch = 19, cex = 2, col = cols,\n         xlim = xlim, ylim = c(0.5, 4.5),\n         yaxt = \"n\", xlab = \"Estimated treatment effect\",\n         ylab = \"\", main = \"Estimator Comparison\")\n    axis(2, at = 1:4, labels = labels, las = 1, cex.axis = 0.9)\n\n    abline(v = d$ate, lty = 2, col = \"#2c3e50\", lwd = 2)\n    text(d$ate, 4.45, paste0(\"True ATE = \", d$ate),\n         cex = 0.85, font = 2, col = \"#2c3e50\")\n\n    segments(d$ate, 1:4, ests, 1:4, col = cols, lwd = 2, lty = 2)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    b_naive &lt;- d$naive - d$ate\n    b_reg   &lt;- d$reg_est - d$ate\n    b_ipw   &lt;- d$ipw_est - d$ate\n    b_dr    &lt;- d$dr_est - d$ate\n\n    scenario_label &lt;- switch(d$sc,\n      both         = \"Both models correct\",\n      outcome_only = \"Only outcome model correct\",\n      ps_only      = \"Only propensity score correct\",\n      neither      = \"Neither model correct\")\n\n    dr_ok &lt;- abs(b_dr) &lt; abs(b_naive) * 0.5\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Scenario:&lt;/b&gt; \", scenario_label, \"&lt;br&gt;\",\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; &lt;span class='bad'&gt;\", round(d$naive, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_naive, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;Reg. adj:&lt;/b&gt; \", round(d$reg_est, 3),\n        \" (bias: \", round(b_reg, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;IPW:&lt;/b&gt; \", round(d$ipw_est, 3),\n        \" (bias: \", round(b_ipw, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;DR:&lt;/b&gt; &lt;span class='\", ifelse(dr_ok, \"good\", \"bad\"), \"'&gt;\",\n        round(d$dr_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_dr, 3), \")\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nWalk through all four scenarios:\n\nBoth correct: all three adjusted estimators (regression, IPW, DR) are close to the truth. The left plot shows the fitted curves matching the true curves. DR works.\nOnly outcome correct: the PS is constant (ignores X), so IPW is just the naive difference — biased. But regression gets the outcome right, and DR inherits that. DR works.\nOnly PS correct: the outcome model fits a line when the truth is quadratic — you can see the misfit on the left plot. Regression is biased. But the PS is right, so the IPW correction rescues DR. DR works.\nNeither correct: both models are wrong. The left plot shows a bad fit, and the PS can’t correct it. DR fails — garbage in, garbage out.\n\nKey insight: DR doesn’t require both models to be right. It requires at least one to be right. That’s the “double robustness” — two chances instead of one.",
    "crumbs": [
      "Estimation Tools",
      "Doubly Robust"
    ]
  },
  {
    "objectID": "doubly-robust.html#the-idea",
    "href": "doubly-robust.html#the-idea",
    "title": "Doubly Robust Estimation",
    "section": "",
    "text": "Every estimation tool under selection on observables relies on getting a model right:\n\nRegression adjustment needs the outcome model \\(E[Y \\mid X, D]\\) to be correct.\nIPW needs the propensity score model \\(P(D = 1 \\mid X)\\) to be correct.\n\nWhat if you’re not sure which one you got right? Doubly robust (DR) estimation combines both models and gives you a consistent estimate if either one is correctly specified.\n\n\n\n\n\n\nExample: vaccine effectiveness. You want to know if a vaccine reduces hospitalization. You model who gets vaccinated (propensity score: older people and healthcare workers vaccinate more) and you model hospitalization risk (outcome model: depends on age, comorbidities, obesity). DR combines both. If your outcome model misses a nonlinear age effect but your propensity score is right — you’re fine. If your propensity score ignores rural vs urban but your outcome model captures it — also fine. You only fail if both models are wrong simultaneously.\n\n\n\n\n\nThe most common DR estimator is Augmented Inverse Probability Weighting (AIPW):\n\\[\\hat{\\tau}_{DR} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\hat{\\mu}_1(X_i) - \\hat{\\mu}_0(X_i) + \\frac{D_i (Y_i - \\hat{\\mu}_1(X_i))}{\\hat{e}(X_i)} - \\frac{(1 - D_i)(Y_i - \\hat{\\mu}_0(X_i))}{1 - \\hat{e}(X_i)} \\right]\\]\nwhere \\(\\hat{\\mu}_d(X)\\) is the estimated outcome under treatment \\(d\\), and \\(\\hat{e}(X)\\) is the estimated propensity score.\nIntuition: start with the regression prediction (\\(\\hat{\\mu}_1 - \\hat{\\mu}_0\\)), then correct it using the IPW-weighted residuals. If the outcome model is perfect, the residuals are zero and the correction vanishes. If the outcome model is wrong but the propensity score is right, the correction fixes the bias.\n\n\n\n\n\n\nOutcome model\nPropensity score\nDR consistent?\n\n\n\n\nCorrect\nCorrect\nYes\n\n\nCorrect\nWrong\nYes\n\n\nWrong\nCorrect\nYes\n\n\nWrong\nWrong\nNo\n\n\n\nThree out of four — you get two shots at a consistent estimate. This is the key advantage over methods that rely on a single model.\n\n\n\nTo see DR in action, we need a setting where models can be wrong:\n\nTrue outcome: \\(Y = 1 + 2X^2 + \\tau D + \\varepsilon\\) (quadratic in \\(X\\))\nTrue treatment: \\(P(D=1 \\mid X) = \\Phi(1.5 \\cdot X)\\) (probit)\n“Correct” outcome model: includes \\(X^2\\)\n“Wrong” outcome model: linear only (\\(X\\), no \\(X^2\\))\n“Correct” PS model: logit on \\(X\\) (close enough to probit)\n“Wrong” PS model: constant 0.5 (ignores \\(X\\) entirely)\n\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 500, max = 3000, value = 1000, step = 250),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      selectInput(\"scenario\", \"Scenario:\",\n                  choices = c(\n                    \"Both correct\"         = \"both\",\n                    \"Only outcome correct\"  = \"outcome_only\",\n                    \"Only PS correct\"       = \"ps_only\",\n                    \"Neither correct\"       = \"neither\"\n                  )),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"fit_plot\", height = \"430px\")),\n        column(6, plotOutput(\"compare_plot\", height = \"430px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    ate &lt;- input$ate\n    sc  &lt;- input$scenario\n\n    # Data generating process\n    x &lt;- rnorm(n)\n\n    # True PS: probit\n    p_true &lt;- pnorm(1.5 * x)\n    treat &lt;- rbinom(n, 1, p_true)\n\n    # True outcome: quadratic\n    y &lt;- 1 + 2 * x^2 + ate * treat + rnorm(n)\n\n    # --- Outcome models ---\n    outcome_correct &lt;- sc %in% c(\"both\", \"outcome_only\")\n    if (outcome_correct) {\n      x2 &lt;- x^2\n      fit1 &lt;- lm(y ~ treat + x + x2)\n      mu1 &lt;- predict(fit1, newdata = data.frame(treat = 1, x = x, x2 = x^2))\n      mu0 &lt;- predict(fit1, newdata = data.frame(treat = 0, x = x, x2 = x^2))\n    } else {\n      fit1 &lt;- lm(y ~ treat + x)\n      mu1 &lt;- predict(fit1, newdata = data.frame(treat = 1, x = x))\n      mu0 &lt;- predict(fit1, newdata = data.frame(treat = 0, x = x))\n    }\n\n    # Regression adjustment estimate\n    reg_est &lt;- mean(mu1 - mu0)\n\n    # --- PS models ---\n    ps_correct &lt;- sc %in% c(\"both\", \"ps_only\")\n    if (ps_correct) {\n      ps &lt;- fitted(glm(treat ~ x, family = binomial))\n    } else {\n      ps &lt;- rep(0.5, n)\n    }\n\n    # IPW estimate\n    w &lt;- ifelse(treat == 1, 1 / ps, 1 / (1 - ps))\n    ipw_est &lt;- weighted.mean(y[treat == 1], w[treat == 1]) -\n               weighted.mean(y[treat == 0], w[treat == 0])\n\n    # Doubly robust (AIPW)\n    dr_est &lt;- mean(mu1 - mu0 +\n      treat * (y - mu1) / ps -\n      (1 - treat) * (y - mu0) / (1 - ps))\n\n    # Naive\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # For plotting: outcome model fit curves\n    x_seq &lt;- seq(min(x), max(x), length.out = 200)\n    if (outcome_correct) {\n      pred_t &lt;- predict(fit1, newdata = data.frame(treat = 1, x = x_seq, x2 = x_seq^2))\n      pred_c &lt;- predict(fit1, newdata = data.frame(treat = 0, x = x_seq, x2 = x_seq^2))\n    } else {\n      pred_t &lt;- predict(fit1, newdata = data.frame(treat = 1, x = x_seq))\n      pred_c &lt;- predict(fit1, newdata = data.frame(treat = 0, x = x_seq))\n    }\n    true_t &lt;- 1 + 2 * x_seq^2 + ate\n    true_c &lt;- 1 + 2 * x_seq^2\n\n    list(x = x, y = y, treat = treat,\n         x_seq = x_seq, pred_t = pred_t, pred_c = pred_c,\n         true_t = true_t, true_c = true_c,\n         naive = naive, reg_est = reg_est, ipw_est = ipw_est, dr_est = dr_est,\n         ate = ate, sc = sc,\n         outcome_correct = outcome_correct, ps_correct = ps_correct)\n  })\n\n  output$fit_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$y, pch = 16, cex = 0.3,\n         col = ifelse(d$treat == 1, \"#3498db30\", \"#e74c3c30\"),\n         xlab = \"X (confounder)\", ylab = \"Y (outcome)\",\n         main = \"Outcome Model Fit vs Truth\")\n\n    # Fitted curves (solid)\n    lines(d$x_seq, d$pred_t, col = \"#3498db\", lwd = 2.5)\n    lines(d$x_seq, d$pred_c, col = \"#e74c3c\", lwd = 2.5)\n\n    # True curves (dashed)\n    lines(d$x_seq, d$true_t, col = \"#3498db\", lwd = 1.5, lty = 3)\n    lines(d$x_seq, d$true_c, col = \"#e74c3c\", lwd = 1.5, lty = 3)\n\n    outcome_label &lt;- if (d$outcome_correct) \"Correct (quadratic)\" else \"Wrong (linear)\"\n    ps_label &lt;- if (d$ps_correct) \"Correct (logit)\" else \"Wrong (constant)\"\n\n    legend(\"topleft\", bty = \"n\", cex = 0.75,\n           legend = c(\"Treated data\", \"Control data\",\n                      \"Model fit (solid)\", \"True E[Y|X,D] (dotted)\",\n                      paste0(\"Outcome: \", outcome_label),\n                      paste0(\"PS: \", ps_label)),\n           col = c(\"#3498db\", \"#e74c3c\", \"gray40\", \"gray40\", NA, NA),\n           pch = c(16, 16, NA, NA, NA, NA),\n           lwd = c(NA, NA, 2.5, 1.5, NA, NA),\n           lty = c(NA, NA, 1, 3, NA, NA))\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 9, 3, 2))\n\n    ests &lt;- c(d$dr_est, d$ipw_est, d$reg_est, d$naive)\n    labels &lt;- c(\"Doubly Robust\", \"IPW\", \"Regression adj.\", \"Naive\")\n    cols &lt;- c(\"#9b59b6\", \"#27ae60\", \"#3498db\", \"#e74c3c\")\n\n    xlim &lt;- range(c(ests, d$ate))\n    pad  &lt;- max(diff(xlim) * 0.4, 0.5)\n    xlim &lt;- xlim + c(-pad, pad)\n\n    plot(ests, 1:4, pch = 19, cex = 2, col = cols,\n         xlim = xlim, ylim = c(0.5, 4.5),\n         yaxt = \"n\", xlab = \"Estimated treatment effect\",\n         ylab = \"\", main = \"Estimator Comparison\")\n    axis(2, at = 1:4, labels = labels, las = 1, cex.axis = 0.9)\n\n    abline(v = d$ate, lty = 2, col = \"#2c3e50\", lwd = 2)\n    text(d$ate, 4.45, paste0(\"True ATE = \", d$ate),\n         cex = 0.85, font = 2, col = \"#2c3e50\")\n\n    segments(d$ate, 1:4, ests, 1:4, col = cols, lwd = 2, lty = 2)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    b_naive &lt;- d$naive - d$ate\n    b_reg   &lt;- d$reg_est - d$ate\n    b_ipw   &lt;- d$ipw_est - d$ate\n    b_dr    &lt;- d$dr_est - d$ate\n\n    scenario_label &lt;- switch(d$sc,\n      both         = \"Both models correct\",\n      outcome_only = \"Only outcome model correct\",\n      ps_only      = \"Only propensity score correct\",\n      neither      = \"Neither model correct\")\n\n    dr_ok &lt;- abs(b_dr) &lt; abs(b_naive) * 0.5\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Scenario:&lt;/b&gt; \", scenario_label, \"&lt;br&gt;\",\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; &lt;span class='bad'&gt;\", round(d$naive, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_naive, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;Reg. adj:&lt;/b&gt; \", round(d$reg_est, 3),\n        \" (bias: \", round(b_reg, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;IPW:&lt;/b&gt; \", round(d$ipw_est, 3),\n        \" (bias: \", round(b_ipw, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;DR:&lt;/b&gt; &lt;span class='\", ifelse(dr_ok, \"good\", \"bad\"), \"'&gt;\",\n        round(d$dr_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_dr, 3), \")\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nWalk through all four scenarios:\n\nBoth correct: all three adjusted estimators (regression, IPW, DR) are close to the truth. The left plot shows the fitted curves matching the true curves. DR works.\nOnly outcome correct: the PS is constant (ignores X), so IPW is just the naive difference — biased. But regression gets the outcome right, and DR inherits that. DR works.\nOnly PS correct: the outcome model fits a line when the truth is quadratic — you can see the misfit on the left plot. Regression is biased. But the PS is right, so the IPW correction rescues DR. DR works.\nNeither correct: both models are wrong. The left plot shows a bad fit, and the PS can’t correct it. DR fails — garbage in, garbage out.\n\nKey insight: DR doesn’t require both models to be right. It requires at least one to be right. That’s the “double robustness” — two chances instead of one.",
    "crumbs": [
      "Estimation Tools",
      "Doubly Robust"
    ]
  },
  {
    "objectID": "doubly-robust.html#why-not-always-use-dr",
    "href": "doubly-robust.html#why-not-always-use-dr",
    "title": "Doubly Robust Estimation",
    "section": "Why not always use DR?",
    "text": "Why not always use DR?\nIf DR is better than regression alone and IPW alone, why not always use it?\n\nYou still need at least one model right. DR isn’t magic — it fails when both models are misspecified. In the “neither correct” scenario above, DR is just as biased as the others.\nVariance. DR can have higher variance than a correctly specified single model. The extra robustness comes at the cost of efficiency. In the “both correct” scenario, regression alone is often more precise.\nComplexity. Two models to specify, diagnose, and justify instead of one.\n\nIn practice, DR is most valuable when you’re uncertain about your models — which is most of the time.",
    "crumbs": [
      "Estimation Tools",
      "Doubly Robust"
    ]
  },
  {
    "objectID": "doubly-robust.html#in-stata",
    "href": "doubly-robust.html#in-stata",
    "title": "Doubly Robust Estimation",
    "section": "In Stata",
    "text": "In Stata\n* Augmented IPW (doubly robust)\nteffects aipw (outcome x1 x2) (treatment x1 x2)\n\n* Check overlap\nteffects overlap\n\n* Doubly robust DID (Sant'Anna & Zhao 2020)\n* ssc install drdid\ndrdid outcome x1 x2, ivar(id) time(year) treatment(treated)\nteffects aipw specifies two models in one command: the outcome model (first parentheses) and the treatment model (second parentheses). You get double robustness — consistent if either model is correct.",
    "crumbs": [
      "Estimation Tools",
      "Doubly Robust"
    ]
  },
  {
    "objectID": "doubly-robust.html#did-you-know",
    "href": "doubly-robust.html#did-you-know",
    "title": "Doubly Robust Estimation",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe doubly robust property was established by Scharfstein, Rotnitzky & Robins (1999) and Bang & Robins (2005). The key insight is that AIPW belongs to a class of semiparametrically efficient estimators — it achieves the lowest possible variance among regular estimators when both models are correct, and remains consistent when either is correct.\nDoubly robust DID (DR-DID) extends the idea to difference-in-differences. Sant’Anna & Zhao (2020) showed how to combine outcome regression and propensity score weighting in the DID setting, getting double robustness for treatment effect estimation under parallel trends. This is now standard in the staggered DID literature.\nThe DR estimator is connected to targeted learning (van der Laan & Rose,\n\nand debiased machine learning (Chernozhukov et al., 2018). These frameworks use machine learning for the nuisance models (\\(\\hat{\\mu}\\) and \\(\\hat{e}\\)) while maintaining valid inference for the causal parameter — DR structure is what makes this possible.",
    "crumbs": [
      "Estimation Tools",
      "Doubly Robust"
    ]
  },
  {
    "objectID": "double-debiased-ml.html",
    "href": "double-debiased-ml.html",
    "title": "Double/Debiased Machine Learning",
    "section": "",
    "text": "Doubly robust estimation showed that combining an outcome model with a propensity score model gives you two shots at consistency. But both models were specified parametrically — linear regression for the outcome, logit for the propensity score. What if you want to use machine learning for these nuisance functions? The answer is not as simple as “plug in a random forest.”",
    "crumbs": [
      "Estimation Tools",
      "Double/Debiased ML"
    ]
  },
  {
    "objectID": "double-debiased-ml.html#the-problem-with-naively-plugging-in-ml",
    "href": "double-debiased-ml.html#the-problem-with-naively-plugging-in-ml",
    "title": "Double/Debiased Machine Learning",
    "section": "The problem with naively plugging in ML",
    "text": "The problem with naively plugging in ML\nSuppose you want to estimate the ATE under selection on observables:\n\\[\n\\tau = E[Y(1) - Y(0)]\n\\]\nYou need two nuisance functions: the outcome model \\(\\mu_d(X) = E[Y \\mid X, D=d]\\) and the propensity score \\(e(X) = P(D=1 \\mid X)\\). The AIPW estimator from the doubly robust page uses both:\n\\[\n\\hat{\\tau}_{DR} = \\frac{1}{n}\\sum_{i=1}^n \\left[\\hat{\\mu}_1(X_i) - \\hat{\\mu}_0(X_i) + \\frac{D_i(Y_i - \\hat{\\mu}_1(X_i))}{\\hat{e}(X_i)} - \\frac{(1-D_i)(Y_i - \\hat{\\mu}_0(X_i))}{1 - \\hat{e}(X_i)}\\right]\n\\]\nIf you estimate \\(\\hat{\\mu}\\) and \\(\\hat{e}\\) with OLS and logit, standard asymptotic theory gives you \\(\\sqrt{n}\\)-consistent inference on \\(\\hat{\\tau}\\). But if you estimate them with a random forest, lasso, or neural network, the standard theory breaks down. The problem is regularization bias: ML estimators are biased (they trade bias for variance — the bias-variance tradeoff), and this bias contaminates the estimate of \\(\\tau\\).\nConcretely, the bias of \\(\\hat{\\tau}\\) depends on the product of the biases of \\(\\hat{\\mu}\\) and \\(\\hat{e}\\):\n\\[\n\\text{Bias}(\\hat{\\tau}) \\approx \\text{Bias}(\\hat{\\mu}) \\times \\text{Bias}(\\hat{e})\n\\]\nFor parametric models with \\(\\sqrt{n}\\) convergence, both biases are \\(O(1/\\sqrt{n})\\), so the product is \\(O(1/n)\\) — negligible. But ML estimators typically converge slower than \\(\\sqrt{n}\\), and the bias product can be large enough to distort inference.",
    "crumbs": [
      "Estimation Tools",
      "Double/Debiased ML"
    ]
  },
  {
    "objectID": "double-debiased-ml.html#the-dml-solution-neyman-orthogonality-cross-fitting",
    "href": "double-debiased-ml.html#the-dml-solution-neyman-orthogonality-cross-fitting",
    "title": "Double/Debiased Machine Learning",
    "section": "The DML solution: Neyman orthogonality + cross-fitting",
    "text": "The DML solution: Neyman orthogonality + cross-fitting\nChernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, and Robins (2018) showed that two ingredients fix this problem.\n\nIngredient 1: Neyman orthogonal scores\nUse an estimating equation (a “score”) whose expectation is insensitive to small perturbations in the nuisance functions. The AIPW score from doubly robust estimation has this property — it is Neyman orthogonal:\n\\[\n\\psi(Y, D, X; \\tau, \\mu, e) = \\hat{\\mu}_1(X) - \\hat{\\mu}_0(X) + \\frac{D(Y - \\hat{\\mu}_1(X))}{e(X)} - \\frac{(1-D)(Y - \\hat{\\mu}_0(X))}{1 - e(X)} - \\tau\n\\]\nThe Neyman orthogonality condition means that the derivative of \\(E[\\psi]\\) with respect to the nuisance functions \\((\\mu, e)\\) is zero at the true values. Intuitively, the score is “locally robust” to errors in the nuisance estimates — first-order mistakes in \\(\\hat{\\mu}\\) and \\(\\hat{e}\\) don’t affect \\(\\hat{\\tau}\\).\nThis is what “debiased” means. The AIPW structure removes the first-order bias from the nuisance estimation. What remains is the product of biases — which is second-order.\n\n\nIngredient 2: Cross-fitting\nEven with an orthogonal score, using the same data to estimate nuisance functions and the causal parameter creates an overfitting bias. ML estimators can overfit in subtle ways that parametric models don’t, and this leaks into \\(\\hat{\\tau}\\).\nThe fix is cross-fitting — a form of sample splitting:\n\nSplit the data into \\(K\\) folds (typically \\(K = 5\\))\nFor each fold \\(k\\):\n\nEstimate \\(\\hat{\\mu}\\) and \\(\\hat{e}\\) using data outside fold \\(k\\)\nCompute the score \\(\\psi_i\\) for observations inside fold \\(k\\)\n\nAverage all scores: \\(\\hat{\\tau} = \\frac{1}{n}\\sum_{i=1}^n \\psi_i\\)\n\nThis ensures the nuisance estimates are never evaluated on the same data used to construct them. Cross-fitting eliminates the overfitting bias while using all data for both nuisance estimation and causal estimation (unlike simple sample splitting, which wastes half the data).\n\n\n\n\n\n\nThe DML recipe. (1) Choose a Neyman orthogonal score — AIPW for the ATE, the partially linear model score for a coefficient. (2) Estimate nuisance functions with any ML method, using cross-fitting. (3) Compute the causal parameter from the averaged scores. (4) Standard errors from the variance of the scores: \\(\\widehat{\\text{Var}}(\\hat{\\tau}) = \\frac{1}{n^2}\\sum_i \\hat{\\psi}_i^2\\).",
    "crumbs": [
      "Estimation Tools",
      "Double/Debiased ML"
    ]
  },
  {
    "objectID": "double-debiased-ml.html#the-partially-linear-model",
    "href": "double-debiased-ml.html#the-partially-linear-model",
    "title": "Double/Debiased Machine Learning",
    "section": "The partially linear model",
    "text": "The partially linear model\nA common DML application. Suppose the model is:\n\\[\nY_i = \\tau D_i + g(X_i) + \\varepsilon_i\n\\]\nwhere \\(g(X_i)\\) is a potentially complex function of high-dimensional controls. You want \\(\\tau\\) — the treatment effect — but \\(g\\) is a nuisance function.\nThe DML approach:\n\nUse ML to estimate \\(\\hat{g}(X) \\approx E[Y \\mid X]\\) and \\(\\hat{m}(X) \\approx E[D \\mid X]\\) via cross-fitting\nCompute residuals: \\(\\tilde{Y}_i = Y_i - \\hat{g}(X_i)\\) and \\(\\tilde{D}_i = D_i - \\hat{m}(X_i)\\)\nRegress \\(\\tilde{Y}\\) on \\(\\tilde{D}\\) to get \\(\\hat{\\tau}\\)\n\nThis is Frisch-Waugh-Lovell with ML residualization. Instead of controlling for \\(X\\) linearly, you partial it out flexibly. The treatment effect \\(\\tau\\) remains a simple, interpretable parameter — only the nuisance part uses ML.\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 200, max = 2000, value = 500, step = 200),\n\n      sliderInput(\"tau\", \"True treatment effect (\\u03C4):\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"p\", \"Number of confounders (p):\",\n                  min = 2, max = 20, value = 5, step = 1),\n\n      actionButton(\"go\", \"Run simulation\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"density_plot\", height = \"500px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  sim &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    tau &lt;- input$tau\n    p   &lt;- input$p\n    B   &lt;- 200\n\n    naive_ests &lt;- numeric(B)\n    ols_ests   &lt;- numeric(B)\n    dml_ests   &lt;- numeric(B)\n\n    for (b in 1:B) {\n      # Generate confounders\n      X &lt;- matrix(rnorm(n * p), n, p)\n\n      # Propensity and treatment\n      e_x &lt;- plogis(X[, 1] + 0.5 * X[, 2])\n      D   &lt;- rbinom(n, 1, e_x)\n\n      # Outcome\n      Y &lt;- tau * D + sin(X[, 1]) + X[, 2]^2 + rnorm(n)\n\n      # 1. Naive: unadjusted difference in means\n      naive_ests[b] &lt;- mean(Y[D == 1]) - mean(Y[D == 0])\n\n      # 2. OLS: lm(Y ~ D + X)\n      dat_ols  &lt;- data.frame(Y = Y, D = D, X)\n      ols_fit  &lt;- lm(Y ~ D + ., data = dat_ols)\n      ols_ests[b] &lt;- coef(ols_fit)[\"D\"]\n\n      # 3. DML-style: residualize Y and D on X with cross-fitting\n      fold &lt;- rep(1:2, length.out = n)\n      fold &lt;- sample(fold)\n\n      Y_resid &lt;- numeric(n)\n      D_resid &lt;- numeric(n)\n\n      for (k in 1:2) {\n        train &lt;- fold != k\n        test  &lt;- fold == k\n\n        X_train &lt;- X[train, , drop = FALSE]\n        X_test  &lt;- X[test, , drop = FALSE]\n\n        # Fit nuisance models on training fold\n        dat_y &lt;- data.frame(Y = Y[train], X_train)\n        dat_d &lt;- data.frame(D = D[train], X_train)\n\n        fit_y &lt;- lm(Y ~ ., data = dat_y)\n        fit_d &lt;- lm(D ~ ., data = dat_d)\n\n        # Predict on test fold\n        newdat &lt;- data.frame(X_test)\n        names(newdat) &lt;- names(dat_y)[-1]\n\n        Y_resid[test] &lt;- Y[test] - predict(fit_y, newdata = newdat)\n        D_resid[test] &lt;- D[test] - predict(fit_d, newdata = newdat)\n      }\n\n      # Regress residualized Y on residualized D\n      dml_ests[b] &lt;- coef(lm(Y_resid ~ D_resid - 1))\n    }\n\n    list(naive = naive_ests, ols = ols_ests, dml = dml_ests, tau = tau)\n  })\n\n  output$density_plot &lt;- renderPlot({\n    s &lt;- sim()\n    par(mar = c(5, 4.5, 3, 1))\n\n    # Compute densities\n    d_naive &lt;- density(s$naive, adjust = 1.2)\n    d_ols   &lt;- density(s$ols,   adjust = 1.2)\n    d_dml   &lt;- density(s$dml,   adjust = 1.2)\n\n    xlim &lt;- range(c(d_naive$x, d_ols$x, d_dml$x))\n    ylim &lt;- c(0, max(c(d_naive$y, d_ols$y, d_dml$y)) * 1.15)\n\n    plot(NULL, xlim = xlim, ylim = ylim,\n         xlab = expression(\"Estimated \" * tau),\n         ylab = \"Density\",\n         main = \"Sampling Distributions (200 Monte Carlo Reps)\")\n\n    # Density curves\n    polygon(d_naive$x, d_naive$y, col = adjustcolor(\"#e74c3c\", 0.25), border = \"#e74c3c\", lwd = 2)\n    polygon(d_ols$x,   d_ols$y,   col = adjustcolor(\"#3498db\", 0.25), border = \"#3498db\", lwd = 2)\n    polygon(d_dml$x,   d_dml$y,   col = adjustcolor(\"#27ae60\", 0.25), border = \"#27ae60\", lwd = 2)\n\n    # True tau\n    abline(v = s$tau, lty = 2, lwd = 2.5, col = \"#2c3e50\")\n    text(s$tau, ylim[2] * 0.95,\n         bquote(\"True \" * tau == .(s$tau)),\n         cex = 0.9, font = 2, col = \"#2c3e50\", pos = 4)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Naive (diff. in means)\", \"OLS (with controls)\", \"DML-style (cross-fitted)\"),\n           col = c(\"#e74c3c\", \"#3498db\", \"#27ae60\"),\n           lwd = 2, fill = adjustcolor(c(\"#e74c3c\", \"#3498db\", \"#27ae60\"), 0.25),\n           border = c(\"#e74c3c\", \"#3498db\", \"#27ae60\"))\n  })\n\n  output$results &lt;- renderUI({\n    s &lt;- sim()\n    tau &lt;- s$tau\n\n    mn &lt;- c(mean(s$naive), mean(s$ols), mean(s$dml))\n    sd &lt;- c(sd(s$naive), sd(s$ols), sd(s$dml))\n    bias &lt;- mn - tau\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True \\u03C4:&lt;/b&gt; \", tau, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; mean = &lt;span class='bad'&gt;\", round(mn[1], 3), \"&lt;/span&gt;\",\n        \" (SD: \", round(sd[1], 3), \", bias: \", round(bias[1], 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;OLS:&lt;/b&gt; mean = \", round(mn[2], 3),\n        \" (SD: \", round(sd[2], 3), \", bias: \", round(bias[2], 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;DML:&lt;/b&gt; mean = &lt;span class='good'&gt;\", round(mn[3], 3), \"&lt;/span&gt;\",\n        \" (SD: \", round(sd[3], 3), \", bias: \", round(bias[3], 3), \")\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nIncrease confounders: with more confounders \\(p\\), the naive estimator gets worse because there is more confounding. OLS and DML both adjust, but DML’s cross-fitting avoids overfitting bias that can creep in when \\(p\\) is large relative to \\(n\\).\nSet \\(\\tau = 0\\): all estimators should center near zero if unbiased. The naive estimator will still be biased because treatment is confounded.\nSmall \\(n\\), large \\(p\\): with \\(n = 200\\) and \\(p = 20\\), OLS starts to overfit the confounders. The cross-fitted DML approach is more robust because it never evaluates predictions on the same data used to fit the nuisance models.",
    "crumbs": [
      "Estimation Tools",
      "Double/Debiased ML"
    ]
  },
  {
    "objectID": "double-debiased-ml.html#what-dml-assumes",
    "href": "double-debiased-ml.html#what-dml-assumes",
    "title": "Double/Debiased Machine Learning",
    "section": "What DML assumes",
    "text": "What DML assumes\nDML does not weaken the identification assumptions. You still need:\n\nSelection on observables: \\(Y(0), Y(1) \\perp D \\mid X\\)\nOverlap: \\(0 &lt; P(D=1 \\mid X) &lt; 1\\) for all \\(X\\)\nSUTVA and consistency from Potential Outcomes\n\nWhat DML relaxes are the estimation assumptions:\n\nThe nuisance functions \\(\\mu(X)\\) and \\(e(X)\\) can be nonparametric — estimated by lasso, random forest, boosting, neural network, or any method\nThe nuisance estimators need to converge at rate \\(n^{-1/4}\\) (not \\(n^{-1/2}\\)) — a much weaker requirement that many ML methods satisfy\nNo need to correctly specify a parametric model for \\(\\mu\\) or \\(e\\)\n\n\n\n\n\n\n\nThe key distinction. DML separates the causal problem (identification) from the statistical problem (nuisance estimation). Identification comes from the research design — the argument for why treatment is as-good-as-random conditional on \\(X\\). ML handles the high-dimensional, potentially nonlinear adjustment for \\(X\\). These are complementary, not substitutes. As noted in Identification vs Estimation: identification comes first, and no estimator — parametric or ML — can fix a failure of identification.",
    "crumbs": [
      "Estimation Tools",
      "Double/Debiased ML"
    ]
  },
  {
    "objectID": "double-debiased-ml.html#continuous-treatment-and-heterogeneous-effects",
    "href": "double-debiased-ml.html#continuous-treatment-and-heterogeneous-effects",
    "title": "Double/Debiased Machine Learning",
    "section": "Continuous treatment and heterogeneous effects",
    "text": "Continuous treatment and heterogeneous effects\nEverything above assumes a binary treatment \\(D \\in \\{0,1\\}\\) and estimates a single average effect \\(\\tau\\). But DML extends naturally to two important generalizations.\n\nContinuous treatment\nWhen treatment is a dose rather than a switch — dollar store exposure within a radius, pollution concentration, hours of tutoring — the partially linear model already handles it:\n\\[Y_i = \\tau D_i + g(X_i) + \\varepsilon_i\\]\nHere \\(D_i\\) is continuous and \\(\\tau\\) is the marginal effect of a one-unit increase in \\(D\\). The DML recipe is identical: residualize \\(Y\\) and \\(D\\) on \\(X\\) via cross-fitting, then regress \\(\\tilde{Y}\\) on \\(\\tilde{D}\\). No propensity score is needed — you estimate \\(E[D \\mid X]\\) (the conditional mean of treatment) and \\(E[Y \\mid X]\\) as the two nuisance functions.\nThe orthogonalization ensures \\(\\sqrt{n}\\)-consistency of \\(\\hat{\\tau}\\) even when the nuisance models converge at slower nonparametric rates — this is the same guarantee as in the binary case.\n\n\nConditional average treatment effects (CATE)\nTo go from a single \\(\\tau\\) to a function \\(\\tau(x)\\) — how the effect varies across individuals — DML combines with causal forests or other CATE estimators. The idea:\n\nResidualize \\(Y\\) and \\(D\\) on \\(X\\) via cross-fitting (the DML step)\nEstimate heterogeneity in \\(\\tilde{Y} = \\tau(X)\\tilde{D} + \\eta\\) using a causal forest or kernel method on the residualized data\n\nThis is called the R-learner (Nie & Wager, 2021). It solves the loss:\n\\[\\hat{\\tau}(x) = \\arg\\min_{\\tau} \\sum_{i=1}^{n} \\left(\\tilde{Y}_i - \\tau(X_i)\\tilde{D}_i\\right)^2\\]\nThe residualization removes confounding; the second stage finds heterogeneity. This works for both binary and continuous treatments — with continuous \\(D\\), \\(\\tau(x)\\) is the heterogeneous marginal effect, not a binary treatment/control comparison.\n\n\n\n\n\n\nWhy orthogonalize first? Without residualization, a CATE estimator conflates two sources of variation: (1) differences in treatment effects across \\(X\\) and (2) differences in confounding across \\(X\\). By partialling out the nuisance via DML, the second stage only sees treatment effect heterogeneity. The \\(\\sqrt{n}\\)-consistency of the average effect carries over, and the CATE estimates inherit the robustness to nuisance estimation errors.\n\n\n\n\n\nWhen to use this\n\n\n\nSetting\nMethod\n\n\n\n\nBinary \\(D\\), want ATE\nStandard DML (this page)\n\n\nBinary \\(D\\), want \\(\\tau(x)\\)\nDML + causal forest (R-learner)\n\n\nContinuous \\(D\\), want average marginal effect\nPartially linear model with DML\n\n\nContinuous \\(D\\), want \\(\\tau(x)\\)\nR-learner with continuous \\(D\\)",
    "crumbs": [
      "Estimation Tools",
      "Double/Debiased ML"
    ]
  },
  {
    "objectID": "double-debiased-ml.html#connecting-to-the-course",
    "href": "double-debiased-ml.html#connecting-to-the-course",
    "title": "Double/Debiased Machine Learning",
    "section": "Connecting to the course",
    "text": "Connecting to the course\nDML builds directly on several pages:\n\nDoubly robust estimation: the AIPW estimator is the score function that DML uses. DML adds cross-fitting and ML nuisance estimation.\nIPW: the propensity score \\(e(X)\\) can now be estimated flexibly instead of by logit.\nFWL: the partially linear model is FWL with ML residualization.\nIdentification vs Estimation: DML is the clearest example of this separation — ML for estimation, research design for identification.",
    "crumbs": [
      "Estimation Tools",
      "Double/Debiased ML"
    ]
  },
  {
    "objectID": "entropy-balancing.html",
    "href": "entropy-balancing.html",
    "title": "Entropy Balancing",
    "section": "",
    "text": "IPW relies on correctly specifying the propensity score model. If you get the model wrong, the weights are wrong, and the estimate is biased. Even if the model is right, extreme propensity scores create extreme weights and noisy estimates.",
    "crumbs": [
      "Estimation Tools",
      "Entropy Balancing"
    ]
  },
  {
    "objectID": "entropy-balancing.html#the-problem-with-ipw",
    "href": "entropy-balancing.html#the-problem-with-ipw",
    "title": "Entropy Balancing",
    "section": "",
    "text": "IPW relies on correctly specifying the propensity score model. If you get the model wrong, the weights are wrong, and the estimate is biased. Even if the model is right, extreme propensity scores create extreme weights and noisy estimates.",
    "crumbs": [
      "Estimation Tools",
      "Entropy Balancing"
    ]
  },
  {
    "objectID": "entropy-balancing.html#entropy-balancing-a-different-approach",
    "href": "entropy-balancing.html#entropy-balancing-a-different-approach",
    "title": "Entropy Balancing",
    "section": "Entropy balancing: a different approach",
    "text": "Entropy balancing: a different approach\nEntropy balancing (Hainmueller, 2012) skips the propensity score entirely. Instead, it directly finds weights for the control group that make the covariate distributions exactly match the treated group on specified moments (mean, variance, skewness).\nThe weights are chosen to be as close to uniform as possible (maximum entropy) subject to the balance constraints. This guarantees:\n\nExact balance on the moments you specify\nSmooth weights (no extreme values like IPW can produce)\n\n\nIPW vs Entropy Balancing\n\n\n\n\nIPW\nEntropy Balancing\n\n\n\n\nRequires a propensity score model\nYes\nNo\n\n\nBalance is…\nApproximate (check after)\nExact (by construction)\n\n\nExtreme weights?\nCan be severe\nControlled\n\n\nSensitive to misspecification?\nYes\nLess so\n\n\n\n\n\nAssumptions\n\nSelection on observables: conditional on the covariates you balance on, treatment is independent of potential outcomes. Same as IPW — if you’re missing a confounder, balancing the observed ones doesn’t help.\nOverlap: treated and control groups share common support in covariate space. You can’t reweight controls to look like treated units if no controls exist in that part of the distribution.\nCorrect moments: you need to balance the right moments. If the outcome depends on \\(X^2\\) but you only balance on \\(E[X]\\), you’ll miss the confounding.\n\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"confounding\", \"Confounding strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"balance_plot\", height = \"380px\")),\n        column(6, plotOutput(\"weight_plot\",  height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  # Simple entropy balancing: find weights for control group\n  # that match treated group mean of X\n  ebal &lt;- function(x_ctrl, target_mean, max_iter = 200, tol = 1e-6) {\n    n &lt;- length(x_ctrl)\n    lambda &lt;- 0\n    for (i in seq_len(max_iter)) {\n      w &lt;- exp(lambda * x_ctrl)\n      w &lt;- w / sum(w) * n\n      current &lt;- weighted.mean(x_ctrl, w)\n      grad &lt;- current - target_mean\n      if (abs(grad) &lt; tol) break\n      lambda &lt;- lambda - 0.5 * grad\n    }\n    w / sum(w) * n\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    ate  &lt;- input$ate\n    conf &lt;- input$confounding\n\n    x &lt;- rnorm(n)\n    p_true &lt;- pnorm(conf * x)\n    treat &lt;- rbinom(n, 1, p_true)\n\n    y &lt;- 1 + 2 * x + ate * treat + rnorm(n)\n\n    # Naive\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # IPW\n    ps &lt;- fitted(glm(treat ~ x, family = binomial))\n    w_ipw &lt;- ifelse(treat == 1, 1 / ps, 1 / (1 - ps))\n    ipw_est &lt;- weighted.mean(y[treat == 1], w_ipw[treat == 1]) -\n               weighted.mean(y[treat == 0], w_ipw[treat == 0])\n\n    # Entropy balancing (balance control to match treated mean of X)\n    x_ctrl &lt;- x[treat == 0]\n    x_treat_mean &lt;- mean(x[treat == 1])\n    w_eb &lt;- ebal(x_ctrl, x_treat_mean)\n\n    eb_est &lt;- mean(y[treat == 1]) - weighted.mean(y[treat == 0], w_eb)\n\n    # Balance diagnostics\n    ctrl_mean_raw &lt;- mean(x[treat == 0])\n    ctrl_mean_eb  &lt;- weighted.mean(x[treat == 0], w_eb)\n    treat_mean_x  &lt;- x_treat_mean\n\n    list(x = x, treat = treat, y = y,\n         w_ipw = w_ipw, w_eb = w_eb,\n         naive = naive, ipw_est = ipw_est, eb_est = eb_est,\n         ate = ate,\n         ctrl_mean_raw = ctrl_mean_raw,\n         ctrl_mean_eb = ctrl_mean_eb,\n         treat_mean_x = treat_mean_x)\n  })\n\n  output$balance_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 1, 3, 1))\n\n    means &lt;- c(d$treat_mean_x, d$ctrl_mean_raw, d$ctrl_mean_eb)\n    cols &lt;- c(\"#3498db\", \"#e74c3c\", \"#27ae60\")\n    labels &lt;- c(\"Treated\", \"Control\\n(unweighted)\", \"Control\\n(EB weighted)\")\n\n    bp &lt;- barplot(means, col = cols, border = NA,\n                  names.arg = labels, cex.names = 0.85,\n                  main = \"Mean of X: Balance Check\",\n                  ylab = \"\", ylim = range(means) * c(0.8, 1.3))\n    text(bp, means + 0.05, round(means, 3), cex = 0.9)\n  })\n\n  output$weight_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ctrl_idx &lt;- which(d$treat == 0)\n\n    w_ipw_ctrl &lt;- d$w_ipw[ctrl_idx]\n    w_eb_ctrl  &lt;- d$w_eb\n\n    xlim &lt;- c(0, max(c(w_ipw_ctrl, w_eb_ctrl)) * 1.1)\n\n    d_ipw &lt;- density(w_ipw_ctrl, from = 0)\n    d_eb  &lt;- density(w_eb_ctrl, from = 0)\n    ylim &lt;- c(0, max(d_ipw$y, d_eb$y) * 1.2)\n\n    plot(d_ipw, col = \"#e74c3c\", lwd = 2.5,\n         main = \"Weight Distributions (Control Units)\",\n         xlab = \"Weight\", ylab = \"Density\",\n         xlim = xlim, ylim = ylim)\n    lines(d_eb, col = \"#27ae60\", lwd = 2.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"IPW weights\", \"Entropy balancing weights\"),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = 2.5)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; &lt;span class='bad'&gt;\", round(d$naive, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(d$naive - d$ate, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;IPW:&lt;/b&gt; \", round(d$ipw_est, 3),\n        \" (bias: \", round(d$ipw_est - d$ate, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;Entropy Bal:&lt;/b&gt; &lt;span class='good'&gt;\", round(d$eb_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(d$eb_est - d$ate, 3), \")\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nConfounding = 1.5: look at the right plot. IPW weights have a long tail (some control units get huge weight). Entropy balancing weights are much smoother.\nConfounding = 3: IPW weights become extreme. EB stays stable.\nLeft plot: the green bar (EB-weighted control mean) exactly matches the blue bar (treated mean). That’s the guarantee — exact balance by construction.\nCompare the bias numbers in the sidebar: EB is typically closer to the true ATE, especially under strong confounding.",
    "crumbs": [
      "Estimation Tools",
      "Entropy Balancing"
    ]
  },
  {
    "objectID": "entropy-balancing.html#in-stata",
    "href": "entropy-balancing.html#in-stata",
    "title": "Entropy Balancing",
    "section": "In Stata",
    "text": "In Stata\n* Install entropy balancing\n* ssc install ebalance\n\n* Generate weights that balance x1, x2, x3 between treated and control\nebalance treatment x1 x2 x3, gen(eb_weight)\n\n* Use the weights in your outcome regression\nreg outcome treatment x1 x2 x3 [pw=eb_weight]\nebalance finds the smoothest set of control-group weights that exactly matches the treated group on the moments you specify (means by default; add targets(2) to also match variances). No propensity score model needed.",
    "crumbs": [
      "Estimation Tools",
      "Entropy Balancing"
    ]
  },
  {
    "objectID": "matching.html",
    "href": "matching.html",
    "title": "Matching",
    "section": "",
    "text": "You want the causal effect of a treatment, and you believe selection on observables holds — all confounders are observed. Matching implements this by finding, for each treated unit, a control unit that looks similar on covariates \\(X\\).\nThe logic is simple: if two people have the same \\(X\\), and one got treated while the other didn’t, the difference in their outcomes estimates the treatment effect for that value of \\(X\\). Average across all matched pairs and you get the ATT.\n\n\n\n\n\n\nExample: job training programs. The government runs a job training program. People who enroll earn more afterward — but is that the program, or did motivated people self-select? Matching says: for each enrollee, find a non-enrollee with the same age, education, prior income, and industry. Compare their earnings. If the enrollee still earns more after matching on everything observable, that’s the program’s effect. This is exactly what LaLonde (1986) studied — and found that naive comparisons badly overestimated the program’s impact.\n\n\n\n\\[\\hat{\\tau}_{match} = \\frac{1}{N_1} \\sum_{i: D_i=1} \\Big[ Y_i - Y_{j(i)} \\Big]\\]\nwhere \\(j(i)\\) is the control unit matched to treated unit \\(i\\).\n\n\n\n\n\n\n\n\n\n\n\nMethod\nHow it matches\nPros\nCons\n\n\n\n\nExact\nIdentical X values\nNo approximation error\nInfeasible with continuous or many covariates\n\n\nNearest neighbor (on X)\nClosest \\(\\|X_i - X_j\\|\\)\nSimple, intuitive\nCurse of dimensionality with many covariates\n\n\nNearest neighbor (on PS)\nClosest \\(\\|e(X_i) - e(X_j)\\|\\)\nReduces to 1D; Rosenbaum & Rubin (1983)\nInherits PS model risk\n\n\nCaliper\nNN, but reject if distance \\(&gt; c\\)\nDrops bad matches\nLoses observations\n\n\nMahalanobis\nDistance accounts for correlations\nBetter than Euclidean for correlated X\nStill suffers in high dimensions\n\n\n\n\n\n\nSame as all selection on observables methods:\n\nConditional independence (CIA): \\(Y(0), Y(1) \\perp D \\mid X\\). All confounders are observed.\nOverlap (common support): \\(0 &lt; P(D = 1 \\mid X) &lt; 1\\). For every treated unit, a comparable control exists.\nSUTVA: no interference between units.\n\n\n\n\nMatching finds comparisons rather than modeling outcomes. But the quality of those comparisons depends on:\n\nOverlap: if treated and control units live in different parts of the covariate space, the “nearest” neighbor may be far away. The match is bad and the estimate is biased.\nDimensionality: with many covariates, even the nearest neighbor can be quite distant (“curse of dimensionality”). This is why propensity score matching collapses \\(X\\) to a single dimension.\n\nCompare this to regression adjustment, which extrapolates using a model, and IPW, which reweights instead of pairing. Each has different failure modes.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"confounding\", \"Confounding strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      selectInput(\"match_type\", \"Matching method:\",\n                  choices = c(\"NN on X\" = \"nn_x\",\n                              \"NN on propensity score\" = \"nn_ps\",\n                              \"Caliper (on X)\" = \"caliper\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"match_plot\", height = \"420px\")),\n        column(6, plotOutput(\"compare_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    ate  &lt;- input$ate\n    conf &lt;- input$confounding\n    mtype &lt;- input$match_type\n\n    # Confounder\n    x &lt;- rnorm(n)\n\n    # Treatment depends on x\n    p_true &lt;- pnorm(conf * x)\n    treat &lt;- rbinom(n, 1, p_true)\n\n    # Outcome\n    y &lt;- 1 + 2 * x + ate * treat + rnorm(n)\n\n    idx_t &lt;- which(treat == 1)\n    idx_c &lt;- which(treat == 0)\n\n    # Propensity score (for PS matching)\n    ps &lt;- fitted(glm(treat ~ x, family = binomial))\n\n    # Matching\n    matched_j &lt;- integer(length(idx_t))\n    match_dist &lt;- numeric(length(idx_t))\n\n    if (mtype == \"nn_x\") {\n      for (k in seq_along(idx_t)) {\n        dists &lt;- abs(x[idx_t[k]] - x[idx_c])\n        best &lt;- which.min(dists)\n        matched_j[k] &lt;- idx_c[best]\n        match_dist[k] &lt;- dists[best]\n      }\n    } else if (mtype == \"nn_ps\") {\n      for (k in seq_along(idx_t)) {\n        dists &lt;- abs(ps[idx_t[k]] - ps[idx_c])\n        best &lt;- which.min(dists)\n        matched_j[k] &lt;- idx_c[best]\n        match_dist[k] &lt;- abs(x[idx_t[k]] - x[idx_c[best]])\n      }\n    } else {\n      # Caliper on X (caliper = 0.25 * sd(x))\n      cal &lt;- 0.25 * sd(x)\n      for (k in seq_along(idx_t)) {\n        dists &lt;- abs(x[idx_t[k]] - x[idx_c])\n        best &lt;- which.min(dists)\n        if (dists[best] &lt;= cal) {\n          matched_j[k] &lt;- idx_c[best]\n          match_dist[k] &lt;- dists[best]\n        } else {\n          matched_j[k] &lt;- NA\n          match_dist[k] &lt;- NA\n        }\n      }\n    }\n\n    # Matching estimate\n    valid &lt;- !is.na(matched_j)\n    n_matched &lt;- sum(valid)\n    if (n_matched &gt; 0) {\n      match_est &lt;- mean(y[idx_t[valid]] - y[matched_j[valid]])\n    } else {\n      match_est &lt;- NA\n    }\n    avg_dist &lt;- mean(match_dist[valid])\n\n    # Naive\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # Regression adjustment\n    reg_est &lt;- coef(lm(y ~ treat + x))[\"treat\"]\n\n    list(x = x, y = y, treat = treat, ps = ps,\n         idx_t = idx_t, idx_c = idx_c,\n         matched_j = matched_j, valid = valid,\n         match_est = match_est, naive = naive, reg_est = reg_est,\n         ate = ate, n_matched = n_matched, avg_dist = avg_dist,\n         mtype = mtype)\n  })\n\n  output$match_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$y, pch = 16, cex = 0.5,\n         col = ifelse(d$treat == 1, \"#3498db40\", \"#e74c3c40\"),\n         xlab = \"X (confounder)\", ylab = \"Y (outcome)\",\n         main = \"Matched Pairs\")\n\n    # Draw match lines for a subset (first 30 valid matches)\n    valid_idx &lt;- which(d$valid)\n    show &lt;- valid_idx[seq_len(min(30, length(valid_idx)))]\n\n    for (k in show) {\n      i &lt;- d$idx_t[k]\n      j &lt;- d$matched_j[k]\n      segments(d$x[i], d$y[i], d$x[j], d$y[j],\n               col = \"#9b59b680\", lwd = 1)\n    }\n\n    # Re-draw matched points on top\n    for (k in show) {\n      i &lt;- d$idx_t[k]\n      j &lt;- d$matched_j[k]\n      points(d$x[i], d$y[i], pch = 16, cex = 0.7, col = \"#3498db\")\n      points(d$x[j], d$y[j], pch = 16, cex = 0.7, col = \"#e74c3c\")\n    }\n\n    n_show &lt;- length(show)\n    legend(\"topleft\", bty = \"n\", cex = 0.8,\n           legend = c(\"Treated\", \"Control\",\n                      paste0(\"Match link (\", n_show, \" shown)\")),\n           col = c(\"#3498db\", \"#e74c3c\", \"#9b59b6\"),\n           pch = c(16, 16, NA), lwd = c(NA, NA, 1.5))\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 9, 3, 2))\n\n    ests &lt;- c(d$reg_est, d$match_est, d$naive)\n    labels &lt;- c(\"Regression adj.\", \"Matching\", \"Naive\")\n    cols &lt;- c(\"#3498db\", \"#9b59b6\", \"#e74c3c\")\n\n    valid_ests &lt;- !is.na(ests)\n    xlim &lt;- range(c(ests[valid_ests], d$ate))\n    pad  &lt;- max(diff(xlim) * 0.4, 0.5)\n    xlim &lt;- xlim + c(-pad, pad)\n\n    plot(ests, 1:3, pch = 19, cex = 2, col = cols,\n         xlim = xlim, ylim = c(0.5, 3.5),\n         yaxt = \"n\", xlab = \"Estimated treatment effect\",\n         ylab = \"\", main = \"Estimator Comparison\")\n    axis(2, at = 1:3, labels = labels, las = 1, cex.axis = 0.9)\n\n    abline(v = d$ate, lty = 2, col = \"#2c3e50\", lwd = 2)\n    text(d$ate, 3.45, paste0(\"True ATE = \", d$ate),\n         cex = 0.85, font = 2, col = \"#2c3e50\")\n\n    segments(d$ate, 1:3, ests, 1:3, col = cols, lwd = 2, lty = 2)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    b_naive &lt;- d$naive - d$ate\n    b_match &lt;- if (!is.na(d$match_est)) d$match_est - d$ate else NA\n    b_reg   &lt;- d$reg_est - d$ate\n\n    match_class &lt;- if (!is.na(b_match) && abs(b_match) &lt; abs(b_naive) * 0.5) \"good\" else \"bad\"\n    reg_class   &lt;- if (abs(b_reg) &lt; abs(b_naive) * 0.5) \"good\" else \"bad\"\n\n    method_label &lt;- switch(d$mtype,\n      nn_x    = \"NN on X\",\n      nn_ps   = \"NN on propensity score\",\n      caliper = \"Caliper on X\")\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Method:&lt;/b&gt; \", method_label, \"&lt;br&gt;\",\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; &lt;span class='bad'&gt;\", round(d$naive, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_naive, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;Matching:&lt;/b&gt; &lt;span class='\", match_class, \"'&gt;\",\n        if (!is.na(d$match_est)) round(d$match_est, 3) else \"N/A\",\n        \"&lt;/span&gt;\",\n        if (!is.na(b_match)) paste0(\" (bias: \", round(b_match, 3), \")\") else \"\", \"&lt;br&gt;\",\n        \"&lt;b&gt;Reg. adj:&lt;/b&gt; &lt;span class='\", reg_class, \"'&gt;\",\n        round(d$reg_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_reg, 3), \")&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Pairs matched:&lt;/b&gt; \", d$n_matched, \" / \", length(d$idx_t), \"&lt;br&gt;\",\n        \"&lt;b&gt;Avg match distance:&lt;/b&gt; \",\n        if (!is.na(d$avg_dist)) round(d$avg_dist, 3) else \"N/A\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nNN on X, confounding = 1.5: matching works well. The purple lines connecting matched pairs are short — each treated unit finds a similar control. The matching estimate is close to the true ATE.\nSwitch to caliper: some treated units in the tails can’t find a close match and get dropped. “Pairs matched” drops below the total. The remaining matches are better quality (shorter distances).\nConfounding = 3: treated and control units are far apart in X space. Match distances increase, match quality degrades, and the estimate gets noisier. This is the overlap problem — matching can’t fix it.\nSmall sample (n = 200) with high confounding: matching struggles because there aren’t enough control units in the right part of the covariate space. Regression adjustment extrapolates, which helps here but can hurt elsewhere (see the regression adjustment page).\nNN on propensity score: matches on the estimated \\(e(X)\\) instead of \\(X\\) directly. With one confounder, results are similar. The advantage appears with multiple covariates (not shown here).",
    "crumbs": [
      "Estimation Tools",
      "Matching"
    ]
  },
  {
    "objectID": "matching.html#the-idea",
    "href": "matching.html#the-idea",
    "title": "Matching",
    "section": "",
    "text": "You want the causal effect of a treatment, and you believe selection on observables holds — all confounders are observed. Matching implements this by finding, for each treated unit, a control unit that looks similar on covariates \\(X\\).\nThe logic is simple: if two people have the same \\(X\\), and one got treated while the other didn’t, the difference in their outcomes estimates the treatment effect for that value of \\(X\\). Average across all matched pairs and you get the ATT.\n\n\n\n\n\n\nExample: job training programs. The government runs a job training program. People who enroll earn more afterward — but is that the program, or did motivated people self-select? Matching says: for each enrollee, find a non-enrollee with the same age, education, prior income, and industry. Compare their earnings. If the enrollee still earns more after matching on everything observable, that’s the program’s effect. This is exactly what LaLonde (1986) studied — and found that naive comparisons badly overestimated the program’s impact.\n\n\n\n\\[\\hat{\\tau}_{match} = \\frac{1}{N_1} \\sum_{i: D_i=1} \\Big[ Y_i - Y_{j(i)} \\Big]\\]\nwhere \\(j(i)\\) is the control unit matched to treated unit \\(i\\).\n\n\n\n\n\n\n\n\n\n\n\nMethod\nHow it matches\nPros\nCons\n\n\n\n\nExact\nIdentical X values\nNo approximation error\nInfeasible with continuous or many covariates\n\n\nNearest neighbor (on X)\nClosest \\(\\|X_i - X_j\\|\\)\nSimple, intuitive\nCurse of dimensionality with many covariates\n\n\nNearest neighbor (on PS)\nClosest \\(\\|e(X_i) - e(X_j)\\|\\)\nReduces to 1D; Rosenbaum & Rubin (1983)\nInherits PS model risk\n\n\nCaliper\nNN, but reject if distance \\(&gt; c\\)\nDrops bad matches\nLoses observations\n\n\nMahalanobis\nDistance accounts for correlations\nBetter than Euclidean for correlated X\nStill suffers in high dimensions\n\n\n\n\n\n\nSame as all selection on observables methods:\n\nConditional independence (CIA): \\(Y(0), Y(1) \\perp D \\mid X\\). All confounders are observed.\nOverlap (common support): \\(0 &lt; P(D = 1 \\mid X) &lt; 1\\). For every treated unit, a comparable control exists.\nSUTVA: no interference between units.\n\n\n\n\nMatching finds comparisons rather than modeling outcomes. But the quality of those comparisons depends on:\n\nOverlap: if treated and control units live in different parts of the covariate space, the “nearest” neighbor may be far away. The match is bad and the estimate is biased.\nDimensionality: with many covariates, even the nearest neighbor can be quite distant (“curse of dimensionality”). This is why propensity score matching collapses \\(X\\) to a single dimension.\n\nCompare this to regression adjustment, which extrapolates using a model, and IPW, which reweights instead of pairing. Each has different failure modes.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 200, max = 2000, value = 500, step = 100),\n\n      sliderInput(\"ate\", \"True ATE:\",\n                  min = 0, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"confounding\", \"Confounding strength:\",\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      selectInput(\"match_type\", \"Matching method:\",\n                  choices = c(\"NN on X\" = \"nn_x\",\n                              \"NN on propensity score\" = \"nn_ps\",\n                              \"Caliper (on X)\" = \"caliper\")),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"match_plot\", height = \"420px\")),\n        column(6, plotOutput(\"compare_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    ate  &lt;- input$ate\n    conf &lt;- input$confounding\n    mtype &lt;- input$match_type\n\n    # Confounder\n    x &lt;- rnorm(n)\n\n    # Treatment depends on x\n    p_true &lt;- pnorm(conf * x)\n    treat &lt;- rbinom(n, 1, p_true)\n\n    # Outcome\n    y &lt;- 1 + 2 * x + ate * treat + rnorm(n)\n\n    idx_t &lt;- which(treat == 1)\n    idx_c &lt;- which(treat == 0)\n\n    # Propensity score (for PS matching)\n    ps &lt;- fitted(glm(treat ~ x, family = binomial))\n\n    # Matching\n    matched_j &lt;- integer(length(idx_t))\n    match_dist &lt;- numeric(length(idx_t))\n\n    if (mtype == \"nn_x\") {\n      for (k in seq_along(idx_t)) {\n        dists &lt;- abs(x[idx_t[k]] - x[idx_c])\n        best &lt;- which.min(dists)\n        matched_j[k] &lt;- idx_c[best]\n        match_dist[k] &lt;- dists[best]\n      }\n    } else if (mtype == \"nn_ps\") {\n      for (k in seq_along(idx_t)) {\n        dists &lt;- abs(ps[idx_t[k]] - ps[idx_c])\n        best &lt;- which.min(dists)\n        matched_j[k] &lt;- idx_c[best]\n        match_dist[k] &lt;- abs(x[idx_t[k]] - x[idx_c[best]])\n      }\n    } else {\n      # Caliper on X (caliper = 0.25 * sd(x))\n      cal &lt;- 0.25 * sd(x)\n      for (k in seq_along(idx_t)) {\n        dists &lt;- abs(x[idx_t[k]] - x[idx_c])\n        best &lt;- which.min(dists)\n        if (dists[best] &lt;= cal) {\n          matched_j[k] &lt;- idx_c[best]\n          match_dist[k] &lt;- dists[best]\n        } else {\n          matched_j[k] &lt;- NA\n          match_dist[k] &lt;- NA\n        }\n      }\n    }\n\n    # Matching estimate\n    valid &lt;- !is.na(matched_j)\n    n_matched &lt;- sum(valid)\n    if (n_matched &gt; 0) {\n      match_est &lt;- mean(y[idx_t[valid]] - y[matched_j[valid]])\n    } else {\n      match_est &lt;- NA\n    }\n    avg_dist &lt;- mean(match_dist[valid])\n\n    # Naive\n    naive &lt;- mean(y[treat == 1]) - mean(y[treat == 0])\n\n    # Regression adjustment\n    reg_est &lt;- coef(lm(y ~ treat + x))[\"treat\"]\n\n    list(x = x, y = y, treat = treat, ps = ps,\n         idx_t = idx_t, idx_c = idx_c,\n         matched_j = matched_j, valid = valid,\n         match_est = match_est, naive = naive, reg_est = reg_est,\n         ate = ate, n_matched = n_matched, avg_dist = avg_dist,\n         mtype = mtype)\n  })\n\n  output$match_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$y, pch = 16, cex = 0.5,\n         col = ifelse(d$treat == 1, \"#3498db40\", \"#e74c3c40\"),\n         xlab = \"X (confounder)\", ylab = \"Y (outcome)\",\n         main = \"Matched Pairs\")\n\n    # Draw match lines for a subset (first 30 valid matches)\n    valid_idx &lt;- which(d$valid)\n    show &lt;- valid_idx[seq_len(min(30, length(valid_idx)))]\n\n    for (k in show) {\n      i &lt;- d$idx_t[k]\n      j &lt;- d$matched_j[k]\n      segments(d$x[i], d$y[i], d$x[j], d$y[j],\n               col = \"#9b59b680\", lwd = 1)\n    }\n\n    # Re-draw matched points on top\n    for (k in show) {\n      i &lt;- d$idx_t[k]\n      j &lt;- d$matched_j[k]\n      points(d$x[i], d$y[i], pch = 16, cex = 0.7, col = \"#3498db\")\n      points(d$x[j], d$y[j], pch = 16, cex = 0.7, col = \"#e74c3c\")\n    }\n\n    n_show &lt;- length(show)\n    legend(\"topleft\", bty = \"n\", cex = 0.8,\n           legend = c(\"Treated\", \"Control\",\n                      paste0(\"Match link (\", n_show, \" shown)\")),\n           col = c(\"#3498db\", \"#e74c3c\", \"#9b59b6\"),\n           pch = c(16, 16, NA), lwd = c(NA, NA, 1.5))\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 9, 3, 2))\n\n    ests &lt;- c(d$reg_est, d$match_est, d$naive)\n    labels &lt;- c(\"Regression adj.\", \"Matching\", \"Naive\")\n    cols &lt;- c(\"#3498db\", \"#9b59b6\", \"#e74c3c\")\n\n    valid_ests &lt;- !is.na(ests)\n    xlim &lt;- range(c(ests[valid_ests], d$ate))\n    pad  &lt;- max(diff(xlim) * 0.4, 0.5)\n    xlim &lt;- xlim + c(-pad, pad)\n\n    plot(ests, 1:3, pch = 19, cex = 2, col = cols,\n         xlim = xlim, ylim = c(0.5, 3.5),\n         yaxt = \"n\", xlab = \"Estimated treatment effect\",\n         ylab = \"\", main = \"Estimator Comparison\")\n    axis(2, at = 1:3, labels = labels, las = 1, cex.axis = 0.9)\n\n    abline(v = d$ate, lty = 2, col = \"#2c3e50\", lwd = 2)\n    text(d$ate, 3.45, paste0(\"True ATE = \", d$ate),\n         cex = 0.85, font = 2, col = \"#2c3e50\")\n\n    segments(d$ate, 1:3, ests, 1:3, col = cols, lwd = 2, lty = 2)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    b_naive &lt;- d$naive - d$ate\n    b_match &lt;- if (!is.na(d$match_est)) d$match_est - d$ate else NA\n    b_reg   &lt;- d$reg_est - d$ate\n\n    match_class &lt;- if (!is.na(b_match) && abs(b_match) &lt; abs(b_naive) * 0.5) \"good\" else \"bad\"\n    reg_class   &lt;- if (abs(b_reg) &lt; abs(b_naive) * 0.5) \"good\" else \"bad\"\n\n    method_label &lt;- switch(d$mtype,\n      nn_x    = \"NN on X\",\n      nn_ps   = \"NN on propensity score\",\n      caliper = \"Caliper on X\")\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Method:&lt;/b&gt; \", method_label, \"&lt;br&gt;\",\n        \"&lt;b&gt;True ATE:&lt;/b&gt; \", d$ate, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Naive:&lt;/b&gt; &lt;span class='bad'&gt;\", round(d$naive, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_naive, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;Matching:&lt;/b&gt; &lt;span class='\", match_class, \"'&gt;\",\n        if (!is.na(d$match_est)) round(d$match_est, 3) else \"N/A\",\n        \"&lt;/span&gt;\",\n        if (!is.na(b_match)) paste0(\" (bias: \", round(b_match, 3), \")\") else \"\", \"&lt;br&gt;\",\n        \"&lt;b&gt;Reg. adj:&lt;/b&gt; &lt;span class='\", reg_class, \"'&gt;\",\n        round(d$reg_est, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(b_reg, 3), \")&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Pairs matched:&lt;/b&gt; \", d$n_matched, \" / \", length(d$idx_t), \"&lt;br&gt;\",\n        \"&lt;b&gt;Avg match distance:&lt;/b&gt; \",\n        if (!is.na(d$avg_dist)) round(d$avg_dist, 3) else \"N/A\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nNN on X, confounding = 1.5: matching works well. The purple lines connecting matched pairs are short — each treated unit finds a similar control. The matching estimate is close to the true ATE.\nSwitch to caliper: some treated units in the tails can’t find a close match and get dropped. “Pairs matched” drops below the total. The remaining matches are better quality (shorter distances).\nConfounding = 3: treated and control units are far apart in X space. Match distances increase, match quality degrades, and the estimate gets noisier. This is the overlap problem — matching can’t fix it.\nSmall sample (n = 200) with high confounding: matching struggles because there aren’t enough control units in the right part of the covariate space. Regression adjustment extrapolates, which helps here but can hurt elsewhere (see the regression adjustment page).\nNN on propensity score: matches on the estimated \\(e(X)\\) instead of \\(X\\) directly. With one confounder, results are similar. The advantage appears with multiple covariates (not shown here).",
    "crumbs": [
      "Estimation Tools",
      "Matching"
    ]
  },
  {
    "objectID": "matching.html#how-does-matching-compare-to-other-tools",
    "href": "matching.html#how-does-matching-compare-to-other-tools",
    "title": "Matching",
    "section": "How does matching compare to other tools?",
    "text": "How does matching compare to other tools?\n\n\n\nTool\nStrategy\nVulnerability\n\n\n\n\nMatching\nFind similar units\nBad matches when overlap is poor\n\n\nRegression adj.\nModel the outcome\nWrong functional form\n\n\nIPW\nReweight by propensity score\nExtreme weights\n\n\nEntropy Balancing\nBalance moments exactly\nMissing higher-order moments\n\n\nDoubly Robust\nCombine outcome model + PS\nFails only if both models wrong\n\n\n\nAll rely on the same identification assumption (CIA). They differ in how they use \\(X\\) to make the comparison fair.",
    "crumbs": [
      "Estimation Tools",
      "Matching"
    ]
  },
  {
    "objectID": "matching.html#in-stata",
    "href": "matching.html#in-stata",
    "title": "Matching",
    "section": "In Stata",
    "text": "In Stata\n* Nearest-neighbor matching (on covariates)\nteffects nnmatch (outcome x1 x2) (treatment), nneighbor(1)\n\n* Bias-corrected matching (Abadie & Imbens)\nteffects nnmatch (outcome x1 x2) (treatment), nneighbor(1) biasadj(x1 x2)\n\n* Propensity score matching\nteffects psmatch (outcome) (treatment x1 x2)\n\n* Coarsened exact matching\n* ssc install cem\ncem x1 (#5) x2 (#3), treatment(treatment)\nreg outcome treatment x1 x2 [iw=cem_weights]\nteffects nnmatch with biasadj() corrects for the remaining covariate imbalance within matched pairs — important when matching on continuous variables.",
    "crumbs": [
      "Estimation Tools",
      "Matching"
    ]
  },
  {
    "objectID": "matching.html#did-you-know",
    "href": "matching.html#did-you-know",
    "title": "Matching",
    "section": "Did you know?",
    "text": "Did you know?\n\nAbadie & Imbens (2006) showed that nearest-neighbor matching is not \\(\\sqrt{n}\\)-consistent — its convergence rate is slower than parametric estimators when matching on more than one continuous covariate. The bias from imperfect matches doesn’t vanish fast enough. Their bias-corrected matching estimator fixes this by running a regression within each matched pair to adjust for the remaining covariate difference.\nPropensity score matching was proposed by Rosenbaum & Rubin (1983), who proved that matching on the scalar \\(e(X) = P(D=1 \\mid X)\\) is sufficient for removing confounding — you don’t need to match on every covariate separately. This was a breakthrough because it collapsed the high-dimensional matching problem to one dimension.\nKing & Nielsen (2019) argued that propensity score matching can paradoxically increase imbalance and model dependence. Their critique centers on the fact that exact PS matches don’t guarantee covariate balance — two units with the same propensity score can have very different covariate values.",
    "crumbs": [
      "Estimation Tools",
      "Matching"
    ]
  }
]