[
  {
    "objectID": "bias-variance.html",
    "href": "bias-variance.html",
    "title": "The Bias-Variance Tradeoff",
    "section": "",
    "text": "You might think: “More flexible models fit the data better, so they must be better.” But there’s a catch. A model that fits the training data perfectly will often perform terribly on new data. This is overfitting.\nThe bias-variance tradeoff explains why:\n\n\n\n\n\n\n\n\n\nSimple model (e.g., linear)\nComplex model (e.g., degree-20 polynomial)\n\n\n\n\nBias\nHigh — can’t capture the true shape\nLow — flexible enough to match any pattern\n\n\nVariance\nLow — stable across samples\nHigh — estimates change wildly with new data\n\n\nTraining error\nHigher\nLower (maybe zero)\n\n\nTest error\nU-shaped — sweet spot exists\nU-shaped — sweet spot exists\n\n\n\nThe total prediction error (MSE) decomposes as:\n\\[\\text{MSE} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible noise}\\]\nYou can’t eliminate all three. Reducing bias increases variance, and vice versa. The best model balances both.",
    "crumbs": [
      "Regression & Diagnostics",
      "Bias-Variance Tradeoff"
    ]
  },
  {
    "objectID": "bias-variance.html#why-the-best-model-isnt-the-most-complex-one",
    "href": "bias-variance.html#why-the-best-model-isnt-the-most-complex-one",
    "title": "The Bias-Variance Tradeoff",
    "section": "",
    "text": "You might think: “More flexible models fit the data better, so they must be better.” But there’s a catch. A model that fits the training data perfectly will often perform terribly on new data. This is overfitting.\nThe bias-variance tradeoff explains why:\n\n\n\n\n\n\n\n\n\nSimple model (e.g., linear)\nComplex model (e.g., degree-20 polynomial)\n\n\n\n\nBias\nHigh — can’t capture the true shape\nLow — flexible enough to match any pattern\n\n\nVariance\nLow — stable across samples\nHigh — estimates change wildly with new data\n\n\nTraining error\nHigher\nLower (maybe zero)\n\n\nTest error\nU-shaped — sweet spot exists\nU-shaped — sweet spot exists\n\n\n\nThe total prediction error (MSE) decomposes as:\n\\[\\text{MSE} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible noise}\\]\nYou can’t eliminate all three. Reducing bias increases variance, and vice versa. The best model balances both.",
    "crumbs": [
      "Regression & Diagnostics",
      "Bias-Variance Tradeoff"
    ]
  },
  {
    "objectID": "bias-variance.html#simulation-1-polynomial-regression-underfitting-vs-overfitting",
    "href": "bias-variance.html#simulation-1-polynomial-regression-underfitting-vs-overfitting",
    "title": "The Bias-Variance Tradeoff",
    "section": "Simulation 1: Polynomial regression — underfitting vs overfitting",
    "text": "Simulation 1: Polynomial regression — underfitting vs overfitting\nFit polynomials of different degrees to noisy data. Low degrees underfit (too rigid). High degrees overfit (too wiggly). Watch the training error decrease monotonically while the test error has a U-shape.\n#| standalone: true\n#| viewerHeight: 700\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"true_fn\", \"True function:\",\n                  choices = c(\"Sine wave\",\n                              \"Quadratic\",\n                              \"Step function\",\n                              \"Linear\")),\n\n      sliderInput(\"n_train\", \"Training points:\",\n                  min = 20, max = 200, value = 50, step = 10),\n\n      sliderInput(\"noise\", \"Noise level (\\u03c3):\",\n                  min = 0.1, max = 2, value = 0.5, step = 0.1),\n\n      sliderInput(\"degree\", \"Polynomial degree:\",\n                  min = 1, max = 20, value = 3, step = 1),\n\n      actionButton(\"go\", \"New data\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(7, plotOutput(\"fit_plot\", height = \"400px\")),\n        column(5, plotOutput(\"error_curve\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  f_true &lt;- function(x, fn) {\n    switch(fn,\n      \"Sine wave\"     = sin(2 * pi * x),\n      \"Quadratic\"     = 2 * (x - 0.5)^2,\n      \"Step function\" = ifelse(x &gt; 0.5, 1, 0),\n      \"Linear\"        = x\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n_train\n    sigma &lt;- input$noise\n    fn    &lt;- input$true_fn\n\n    # Training data\n    x_train &lt;- sort(runif(n, 0, 1))\n    y_train &lt;- f_true(x_train, fn) + rnorm(n, sd = sigma)\n\n    # Test data (fresh)\n    x_test &lt;- sort(runif(n, 0, 1))\n    y_test &lt;- f_true(x_test, fn) + rnorm(n, sd = sigma)\n\n    # Fit polynomials of degree 1 through 20\n    degrees &lt;- 1:20\n    train_mse &lt;- numeric(20)\n    test_mse  &lt;- numeric(20)\n\n    for (d in degrees) {\n      fit &lt;- lm(y_train ~ poly(x_train, d, raw = TRUE))\n      pred_train &lt;- predict(fit)\n      pred_test  &lt;- predict(fit, newdata = data.frame(x_train = x_test))\n      train_mse[d] &lt;- mean((y_train - pred_train)^2)\n      test_mse[d]  &lt;- mean((y_test - pred_test)^2)\n    }\n\n    # Current degree fit for plotting\n    d_now &lt;- input$degree\n    fit_now &lt;- lm(y_train ~ poly(x_train, d_now, raw = TRUE))\n    x_grid &lt;- seq(0, 1, length.out = 300)\n    pred_grid &lt;- predict(fit_now, newdata = data.frame(x_train = x_grid))\n\n    list(x_train = x_train, y_train = y_train,\n         x_test = x_test, y_test = y_test,\n         x_grid = x_grid, pred_grid = pred_grid,\n         train_mse = train_mse, test_mse = test_mse,\n         fn = fn, d_now = d_now, sigma = sigma)\n  })\n\n  output$fit_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x_train, d$y_train, pch = 16,\n         col = \"#3498db80\", cex = 0.8,\n         xlab = \"x\", ylab = \"y\",\n         main = paste0(\"Degree \", d$d_now, \" polynomial fit\"),\n         ylim = range(c(d$y_train, d$pred_grid)))\n\n    # True function\n    x_fine &lt;- seq(0, 1, length.out = 300)\n    lines(x_fine, f_true(x_fine, d$fn),\n          col = \"#27ae60\", lwd = 2, lty = 2)\n\n    # Fitted curve\n    lines(d$x_grid, d$pred_grid, col = \"#e74c3c\", lwd = 2.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Training data\", \"True function\",\n                      paste0(\"Degree \", d$d_now, \" fit\")),\n           col = c(\"#3498db80\", \"#27ae60\", \"#e74c3c\"),\n           pch = c(16, NA, NA), lwd = c(NA, 2, 2.5),\n           lty = c(NA, 2, 1))\n  })\n\n  output$error_curve &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ylim &lt;- c(0, min(max(d$test_mse) * 1.1, max(d$test_mse[1:15]) * 1.5))\n\n    plot(1:20, d$train_mse, type = \"b\", pch = 19, cex = 0.7,\n         col = \"#3498db\", lwd = 2,\n         xlab = \"Polynomial degree\",\n         ylab = \"Mean Squared Error\",\n         main = \"Train vs Test Error\",\n         ylim = ylim)\n    lines(1:20, d$test_mse, type = \"b\", pch = 17, cex = 0.7,\n          col = \"#e74c3c\", lwd = 2)\n\n    # Mark current degree\n    points(d$d_now, d$train_mse[d$d_now], pch = 19, cex = 2, col = \"#3498db\")\n    points(d$d_now, d$test_mse[d$d_now], pch = 17, cex = 2, col = \"#e74c3c\")\n\n    # Mark optimal\n    best &lt;- which.min(d$test_mse)\n    abline(v = best, lty = 3, col = \"#7f8c8d\")\n\n    abline(h = d$sigma^2, lty = 2, col = \"#95a5a6\")\n    text(15, d$sigma^2 * 1.1, expression(\"Irreducible noise (\" * sigma^2 * \")\"),\n         cex = 0.75, col = \"#95a5a6\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Training error\", \"Test error\",\n                      paste0(\"Best degree: \", best)),\n           col = c(\"#3498db\", \"#e74c3c\", \"#7f8c8d\"),\n           pch = c(19, 17, NA), lwd = c(2, 2, 1),\n           lty = c(1, 1, 3))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    best &lt;- which.min(d$test_mse)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Degree \", d$d_now, \":&lt;/b&gt;&lt;br&gt;\",\n        \"Train MSE: \", round(d$train_mse[d$d_now], 4), \"&lt;br&gt;\",\n        \"Test MSE: \", round(d$test_mse[d$d_now], 4), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Optimal degree:&lt;/b&gt; \", best, \"&lt;br&gt;\",\n        \"Test MSE: \", round(d$test_mse[best], 4), \"&lt;br&gt;\",\n        \"&lt;small&gt;Noise floor: \\u03c3\\u00b2 = \",\n        round(d$sigma^2, 3), \"&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Regression & Diagnostics",
      "Bias-Variance Tradeoff"
    ]
  },
  {
    "objectID": "bias-variance.html#simulation-2-bias²-variance-mse",
    "href": "bias-variance.html#simulation-2-bias²-variance-mse",
    "title": "The Bias-Variance Tradeoff",
    "section": "Simulation 2: Bias² + Variance = MSE",
    "text": "Simulation 2: Bias² + Variance = MSE\nRun many simulations with different training sets. For each polynomial degree, decompose the MSE into bias² and variance. Watch the U-shaped total error curve emerge.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"fn2\", \"True function:\",\n                  choices = c(\"Sine wave\",\n                              \"Quadratic\",\n                              \"Step function\",\n                              \"Wiggly sine\")),\n\n      sliderInput(\"n2\", \"Training points:\",\n                  min = 20, max = 100, value = 40, step = 10),\n\n      sliderInput(\"noise2\", \"Noise level:\",\n                  min = 0.2, max = 1.5, value = 0.5, step = 0.1),\n\n      sliderInput(\"mc_sims\", \"MC simulations:\",\n                  min = 100, max = 500, value = 200, step = 50),\n\n      actionButton(\"go2\", \"Run\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(7, plotOutput(\"bv_plot\", height = \"500px\")),\n        column(5, plotOutput(\"bar_plot\", height = \"500px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  f_true &lt;- function(x, fn) {\n    switch(fn,\n      \"Sine wave\"     = sin(2 * pi * x),\n      \"Quadratic\"     = 2 * (x - 0.5)^2,\n      \"Step function\" = ifelse(x &gt; 0.5, 1, 0),\n      \"Wiggly sine\"   = sin(4 * pi * x) + 0.5 * cos(6 * pi * x)\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go2\n    fn    &lt;- input$fn2\n    n     &lt;- input$n2\n    sigma &lt;- input$noise2\n    sims  &lt;- input$mc_sims\n\n    x_eval &lt;- seq(0.05, 0.95, length.out = 50)\n    f_eval &lt;- f_true(x_eval, fn)\n\n    degrees &lt;- 1:12\n    bias2_vec &lt;- numeric(length(degrees))\n    var_vec   &lt;- numeric(length(degrees))\n    mse_vec   &lt;- numeric(length(degrees))\n\n    for (di in seq_along(degrees)) {\n      d &lt;- degrees[di]\n      pred_mat &lt;- matrix(NA, nrow = sims, ncol = length(x_eval))\n\n      for (s in seq_len(sims)) {\n        x_train &lt;- sort(runif(n, 0, 1))\n        y_train &lt;- f_true(x_train, fn) + rnorm(n, sd = sigma)\n        fit &lt;- lm(y_train ~ poly(x_train, d, raw = TRUE))\n        pred_mat[s, ] &lt;- predict(fit,\n          newdata = data.frame(x_train = x_eval))\n      }\n\n      avg_pred &lt;- colMeans(pred_mat)\n      bias2_vec[di] &lt;- mean((avg_pred - f_eval)^2)\n      var_vec[di]   &lt;- mean(apply(pred_mat, 2, var))\n      mse_vec[di]   &lt;- bias2_vec[di] + var_vec[di]\n    }\n\n    list(degrees = degrees, bias2 = bias2_vec, variance = var_vec,\n         mse = mse_vec, sigma = sigma)\n  })\n\n  output$bv_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ylim &lt;- c(0, max(d$mse) * 1.15)\n\n    # Stacked area: variance on bottom, bias² on top\n    x &lt;- d$degrees\n    plot(x, d$mse, type = \"n\",\n         xlab = \"Polynomial degree\", ylab = \"Error\", ylim = ylim,\n         main = \"Bias\\u00b2 + Variance = MSE\")\n\n    # Fill variance (bottom layer)\n    polygon(c(x, rev(x)),\n            c(d$variance, rep(0, length(x))),\n            col = adjustcolor(\"#3498db\", 0.35), border = NA)\n\n    # Fill bias² (top layer, stacked on variance)\n    polygon(c(x, rev(x)),\n            c(d$variance + d$bias2, rev(d$variance)),\n            col = adjustcolor(\"#e74c3c\", 0.35), border = NA)\n\n    # Lines on top\n    lines(x, d$variance, col = \"#3498db\", lwd = 2)\n    lines(x, d$variance + d$bias2, col = \"#2c3e50\", lwd = 2.5)\n    lines(x, d$bias2, col = \"#e74c3c\", lwd = 2, lty = 2)\n\n    abline(h = d$sigma^2, lty = 2, col = \"#95a5a6\")\n\n    best &lt;- x[which.min(d$mse)]\n    abline(v = best, lty = 3, col = \"#7f8c8d\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"MSE (Bias\\u00b2 + Var)\",\n                      \"Bias\\u00b2\", \"Variance\",\n                      paste0(\"Optimal: degree \", best)),\n           col = c(\"#2c3e50\", \"#e74c3c\", \"#3498db\", \"#7f8c8d\"),\n           lwd = c(2.5, 2, 2, 1),\n           lty = c(1, 2, 1, 3),\n           fill = c(NA, adjustcolor(\"#e74c3c\", 0.35),\n                    adjustcolor(\"#3498db\", 0.35), NA),\n           border = c(NA, NA, NA, NA))\n  })\n\n  output$bar_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Stacked bar chart for selected degrees\n    show &lt;- c(1, 3, 5, 8, 12)\n    show &lt;- show[show &lt;= length(d$degrees)]\n\n    bar_mat &lt;- rbind(d$bias2[show], d$variance[show])\n    rownames(bar_mat) &lt;- c(\"Bias\\u00b2\", \"Variance\")\n    colnames(bar_mat) &lt;- paste0(\"d=\", d$degrees[show])\n\n    bp &lt;- barplot(bar_mat, beside = FALSE,\n            col = c(adjustcolor(\"#e74c3c\", 0.7),\n                    adjustcolor(\"#3498db\", 0.7)),\n            ylab = \"Error\",\n            main = \"MSE = Bias\\u00b2 + Variance\",\n            ylim = c(0, max(colSums(bar_mat)) * 1.2),\n            border = \"white\", las = 1)\n\n    # Add MSE total labels on top\n    totals &lt;- colSums(bar_mat)\n    text(bp, totals, labels = round(totals, 3),\n         pos = 3, cex = 0.8, font = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.9,\n           legend = c(\"Bias\\u00b2\", \"Variance\"),\n           fill = c(adjustcolor(\"#e74c3c\", 0.7),\n                    adjustcolor(\"#3498db\", 0.7)),\n           border = \"white\")\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- dat()\n    best &lt;- d$degrees[which.min(d$mse)]\n    bi &lt;- which.min(d$mse)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Optimal degree:&lt;/b&gt; \", best, \"&lt;br&gt;\",\n        \"Bias\\u00b2: \", round(d$bias2[bi], 4), \"&lt;br&gt;\",\n        \"Variance: \", round(d$variance[bi], 4), \"&lt;br&gt;\",\n        \"MSE: \", round(d$mse[bi], 4), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;MSE = Bias\\u00b2 + Variance&lt;br&gt;\",\n        \"Noise floor: \", round(d$sigma^2, 3), \"&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nSine wave, degree 1: the linear fit can’t capture the curve — high bias, low variance. Classic underfitting.\nSine wave, degree 20: the polynomial goes wild between data points — low bias, high variance. Classic overfitting.\nSine wave, degree 3–5: the sweet spot where bias and variance are balanced. The test error curve reaches its minimum.\nIncrease noise: the optimal degree shifts down. Noisier data means simpler models are better — there’s less signal to extract.\nQuadratic true function, degree 2: the model matches the DGP perfectly. Bias is essentially zero. Going beyond degree 2 only adds variance.\n\n\n\nThe bottom line\n\nSimple models are biased but stable. They miss patterns in the data but give consistent predictions across samples.\nComplex models are flexible but noisy. They capture the training data perfectly but their predictions change wildly with new data.\nThe best model balances both. The U-shaped test error curve is the signature of the bias-variance tradeoff.\nIn practice, we use cross-validation to find the sweet spot without needing access to the true function.\n\n\n\n\nDid you know?\n\nThe bias-variance decomposition was formalized by Stuart Geman and Donald Geman in their landmark 1984 paper on image processing. But the intuition goes back much further — Charles Stein showed in 1956 that the sample mean is inadmissible (not the best estimator) in dimensions ≥ 3. The James-Stein estimator reduces MSE by shrinking toward the grand mean — trading bias for variance. This result was so counterintuitive that Stein’s colleagues initially didn’t believe it.\nThe bias-variance tradeoff is the theoretical foundation of regularization methods like ridge regression, LASSO, and elastic net. These methods intentionally introduce bias (by shrinking coefficients) to dramatically reduce variance, resulting in better predictions overall.\nCross-validation — the practical tool for navigating the tradeoff — was proposed by Seymour Geisser in 1975 and popularized by Mervyn Stone in the same year. The leave-one-out version was known even earlier.",
    "crumbs": [
      "Regression & Diagnostics",
      "Bias-Variance Tradeoff"
    ]
  },
  {
    "objectID": "sampling-distribution.html",
    "href": "sampling-distribution.html",
    "title": "The Sampling Distribution",
    "section": "",
    "text": "You run an experiment and compute a statistic — say the sample mean. You get \\(\\bar{x} = 4.7\\). But if you ran the experiment again with new data, you’d get \\(\\bar{x} = 5.1\\). And again: \\(\\bar{x} = 4.3\\).\nThe sampling distribution is the distribution of all possible values your statistic could take across every possible sample you could have drawn. It tells you: how much does my estimate bounce around due to the randomness of sampling?\nYou never observe it directly — you only ran the experiment once. But it’s the foundation of everything: standard errors, confidence intervals, p-values, and hypothesis tests all depend on it.\n\n\nThis is the most common confusion in statistics:\n\n\n\n\n\n\n\n\n\nDistribution of the data\nSampling distribution\n\n\n\n\nWhat is it?\nHistogram of individual observations\nHistogram of the statistic across repeated samples\n\n\nWhat does it show?\nHow spread out individual values are\nHow much the estimate varies\n\n\nShape\nCould be anything (skewed, bimodal, etc.)\nOften approximately normal for averages, by the CLT\n\n\nWidth\nDepends on \\(\\sigma\\) (population SD)\nDepends on \\(\\sigma / \\sqrt{n}\\) (standard error)\n\n\nYou can see it?\nYes — plot your data\nNo — you only have one sample\n\n\n\nYour data might be wildly skewed. But the sampling distribution of the mean can still be normal, because averaging smooths things out. That’s the CLT.\nThe simulation below makes this concrete. The left panel shows your data (one sample). The right panel shows what happens when you repeat the experiment 1,000 times — the sampling distribution.\n#| standalone: true\n#| viewerHeight: 1050\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"pop\", \"Population shape:\",\n                  choices = c(\"Normal\",\n                              \"Uniform\",\n                              \"Skewed (exponential)\",\n                              \"Heavily skewed (chi-sq 2)\",\n                              \"Bimodal\",\n                              \"Heavy-tailed (t, df=3)\")),\n\n      selectInput(\"stat\", \"Statistic:\",\n                  choices = c(\"Mean\", \"Median\", \"SD\",\n                              \"Max\", \"IQR\")),\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 2, max = 200, value = 5, step = 1),\n\n      sliderInput(\"reps\", \"Repeated experiments:\",\n                  min = 100, max = 5000, value = 1000, step = 100),\n\n      actionButton(\"go\", \"Run experiments\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"data_plot\", height = \"350px\")),\n        column(6, plotOutput(\"samp_dist_plot\", height = \"350px\"))\n      ),\n      fluidRow(\n        column(12, plotOutput(\"convergence_plot\", height = \"450px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  draw_pop &lt;- function(n, pop) {\n    switch(pop,\n      \"Normal\" = rnorm(n, mean = 5, sd = 2),\n      \"Uniform\" = runif(n, 0, 10),\n      \"Skewed (exponential)\" = rexp(n, rate = 0.5),\n      \"Heavily skewed (chi-sq 2)\" = rchisq(n, df = 2),\n      \"Bimodal\" = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, 2, 0.7) + (1 - k) * rnorm(n, 8, 0.7)\n      },\n      \"Heavy-tailed (t, df=3)\" = rt(n, df = 3) * 2 + 5\n    )\n  }\n\n  compute_stat &lt;- function(x, stat) {\n    switch(stat,\n      \"Mean\"   = mean(x),\n      \"Median\" = median(x),\n      \"SD\"     = sd(x),\n      \"Max\"    = max(x),\n      \"IQR\"    = IQR(x)\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    reps &lt;- input$reps\n    pop  &lt;- input$pop\n    stat &lt;- input$stat\n\n    # One sample (for display)\n    one_sample &lt;- draw_pop(n, pop)\n    one_stat   &lt;- compute_stat(one_sample, stat)\n\n    # Big population draw (to show true shape)\n    big_pop &lt;- draw_pop(10000, pop)\n\n    # Repeated experiments\n    samp_stats &lt;- replicate(reps, compute_stat(draw_pop(n, pop), stat))\n\n    # Check normality (Shapiro on subset)\n    shap_sub &lt;- samp_stats[seq_len(min(5000, length(samp_stats)))]\n    shap_p &lt;- tryCatch(shapiro.test(shap_sub)$p.value, error = function(e) NA)\n\n    list(one_sample = one_sample, one_stat = one_stat,\n         big_pop = big_pop, samp_stats = samp_stats,\n         n = n, reps = reps, stat = stat, pop = pop,\n         shap_p = shap_p)\n  })\n\n  # --- Left: data distribution ---\n  output$data_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$big_pop, breaks = 60, probability = TRUE,\n         col = \"#e74c3c20\", border = \"#e74c3c60\",\n         main = paste(\"Population:\", d$pop),\n         xlab = \"X\", ylab = \"Density\")\n\n    # Overlay one sample as rug\n    rug(d$one_sample, col = \"#2c3e50\", lwd = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Population shape\",\n                      paste0(\"Your sample (n = \", d$n, \")\")),\n           col = c(\"#e74c3c60\", \"#2c3e50\"),\n           lwd = c(8, 2), lty = c(1, 1))\n  })\n\n  # --- Right: sampling distribution ---\n  output$samp_dist_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$samp_stats, breaks = 50, probability = TRUE,\n         col = \"#3498db30\", border = \"#3498db80\",\n         main = paste0(\"Sampling dist. of \", d$stat, \" (n = \", d$n, \")\"),\n         xlab = paste0(\"Sample \", tolower(d$stat)),\n         ylab = \"Density\")\n\n    # Normal overlay\n    x_seq &lt;- seq(min(d$samp_stats), max(d$samp_stats), length.out = 300)\n    lines(x_seq,\n          dnorm(x_seq, mean = mean(d$samp_stats), sd = sd(d$samp_stats)),\n          col = \"#e74c3c\", lwd = 2, lty = 2)\n\n    # Mark your one estimate\n    abline(v = d$one_stat, col = \"#2c3e50\", lwd = 2.5, lty = 1)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Sampling distribution\",\n                      \"Normal approximation\",\n                      paste0(\"Your estimate: \", round(d$one_stat, 3))),\n           col = c(\"#3498db80\", \"#e74c3c\", \"#2c3e50\"),\n           lwd = c(8, 2, 2.5), lty = c(1, 2, 1))\n  })\n\n  # --- Bottom: how normality improves with n ---\n  output$convergence_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ns &lt;- c(2, 5, 10, 30, 50, 100)\n    ns &lt;- ns[ns &lt;= 200]\n\n    colors &lt;- colorRampPalette(c(\"#e74c3c\", \"#3498db\"))(length(ns))\n\n    # First pass to get x range\n    all_stats &lt;- list()\n    for (i in seq_along(ns)) {\n      all_stats[[i]] &lt;- replicate(500, compute_stat(draw_pop(ns[i], d$pop), d$stat))\n    }\n    xlim &lt;- range(unlist(all_stats))\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, length(ns) * 1.5 + 1),\n         yaxt = \"n\", ylab = \"\", xlab = paste0(\"Sample \", tolower(d$stat)),\n         main = paste0(\"Sampling distribution of \", d$stat, \" at different n\"))\n    positions &lt;- seq_along(ns) * 1.5\n    axis(2, at = positions, labels = paste0(\"n=\", ns), las = 1, cex.axis = 0.9)\n\n    for (i in seq_along(ns)) {\n      dens &lt;- density(all_stats[[i]])\n      # Scale density to fit in a strip\n      dens$y &lt;- dens$y / max(dens$y) * 0.7\n      polygon(dens$x, dens$y + positions[i], col = adjustcolor(colors[i], 0.4),\n              border = colors[i], lwd = 1.5)\n    }\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    se &lt;- sd(d$samp_stats)\n    normal_enough &lt;- !is.na(d$shap_p) && d$shap_p &gt; 0.05\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Your estimate:&lt;/b&gt; \", round(d$one_stat, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;SE (from sampling dist):&lt;/b&gt; \", round(se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Sampling dist SD:&lt;/b&gt; \", round(sd(d$samp_stats), 4), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Looks normal?&lt;/b&gt; \",\n        if (normal_enough)\n          \"&lt;span style='color:#27ae60;'&gt;Yes (Shapiro p = \" else\n          \"&lt;span style='color:#e74c3c;'&gt;No (Shapiro p = \",\n        round(d$shap_p, 4), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;small&gt;Shapiro-Wilk tests whether the&lt;br&gt;\",\n        \"sampling distribution is normal.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nThe red dashed line on the right panel is the normal approximation. When it fits the histogram well, you can safely use normal-based inference (z-tests, t-tests, standard CIs).\nRules of thumb:\n\n\n\nPopulation\nStatistic\nn needed for normality\n\n\n\n\nSymmetric (normal, uniform)\nMean\n~10–15\n\n\nModerately skewed (exponential)\nMean\n~30\n\n\nHeavily skewed (chi-sq 2)\nMean\n~50–100\n\n\nAny shape\nMean\n~30 is the textbook answer, but check\n\n\nAny shape\nMedian\nSlower — needs larger n\n\n\nAny shape\nMax\nVery slow — often never normal\n\n\nAny shape\nSD\nModerate — ~30–50\n\n\n\n\n\n\n\nExponential, Mean, n = 5: the sampling distribution is visibly skewed. Normal approximation is poor. Shapiro test says “No.”\nSame but n = 50: now it’s nearly bell-shaped. CLT has kicked in.\nAny population, Max, n = 100: the sampling distribution of the maximum is not normal even with large n. It has its own distribution (extreme value theory). Normal approximation fails.\nBimodal, Mean, n = 2: the sampling distribution itself is bimodal! With only 2 observations, averaging doesn’t smooth enough.\nBottom panel (ridgeline): watch the sampling distribution narrow and become more normal as n increases, all on one plot.\n\n\n\n\nThe sampling distribution is not your data. It’s the distribution of your estimate. Whether it’s normal depends on three things:\n\nThe population shape — the uglier it is, the more data you need\nThe sample size — larger n → CLT → normality\nThe statistic — means converge fast, medians slower, maxima barely at all\n\nWhen in doubt, don’t assume — bootstrap it. The bootstrap page shows you how to build the sampling distribution empirically.\n\n\n\n\n\nWilliam Sealy Gosset, a chemist at the Guinness brewery, invented the t-distribution in 1908 because he was working with tiny samples of barley. Guinness didn’t let employees publish under their own names, so he used the pen name “Student” — hence “Student’s t-test.”\nGosset’s problem: with only 3–4 observations, you can’t assume the sampling distribution is normal. The t-distribution has heavier tails to account for the extra uncertainty when \\(n\\) is small. As \\(n\\) grows, the t-distribution converges to the normal — because with enough data, the sampling distribution is normal (CLT).\nThe concept of a sampling distribution was so confusing to early statisticians that R.A. Fisher reportedly said it was the most difficult idea in all of statistics to teach clearly.",
    "crumbs": [
      "Probability & Uncertainty",
      "The Sampling Distribution"
    ]
  },
  {
    "objectID": "sampling-distribution.html#what-is-a-sampling-distribution",
    "href": "sampling-distribution.html#what-is-a-sampling-distribution",
    "title": "The Sampling Distribution",
    "section": "",
    "text": "You run an experiment and compute a statistic — say the sample mean. You get \\(\\bar{x} = 4.7\\). But if you ran the experiment again with new data, you’d get \\(\\bar{x} = 5.1\\). And again: \\(\\bar{x} = 4.3\\).\nThe sampling distribution is the distribution of all possible values your statistic could take across every possible sample you could have drawn. It tells you: how much does my estimate bounce around due to the randomness of sampling?\nYou never observe it directly — you only ran the experiment once. But it’s the foundation of everything: standard errors, confidence intervals, p-values, and hypothesis tests all depend on it.\n\n\nThis is the most common confusion in statistics:\n\n\n\n\n\n\n\n\n\nDistribution of the data\nSampling distribution\n\n\n\n\nWhat is it?\nHistogram of individual observations\nHistogram of the statistic across repeated samples\n\n\nWhat does it show?\nHow spread out individual values are\nHow much the estimate varies\n\n\nShape\nCould be anything (skewed, bimodal, etc.)\nOften approximately normal for averages, by the CLT\n\n\nWidth\nDepends on \\(\\sigma\\) (population SD)\nDepends on \\(\\sigma / \\sqrt{n}\\) (standard error)\n\n\nYou can see it?\nYes — plot your data\nNo — you only have one sample\n\n\n\nYour data might be wildly skewed. But the sampling distribution of the mean can still be normal, because averaging smooths things out. That’s the CLT.\nThe simulation below makes this concrete. The left panel shows your data (one sample). The right panel shows what happens when you repeat the experiment 1,000 times — the sampling distribution.\n#| standalone: true\n#| viewerHeight: 1050\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"pop\", \"Population shape:\",\n                  choices = c(\"Normal\",\n                              \"Uniform\",\n                              \"Skewed (exponential)\",\n                              \"Heavily skewed (chi-sq 2)\",\n                              \"Bimodal\",\n                              \"Heavy-tailed (t, df=3)\")),\n\n      selectInput(\"stat\", \"Statistic:\",\n                  choices = c(\"Mean\", \"Median\", \"SD\",\n                              \"Max\", \"IQR\")),\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 2, max = 200, value = 5, step = 1),\n\n      sliderInput(\"reps\", \"Repeated experiments:\",\n                  min = 100, max = 5000, value = 1000, step = 100),\n\n      actionButton(\"go\", \"Run experiments\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"data_plot\", height = \"350px\")),\n        column(6, plotOutput(\"samp_dist_plot\", height = \"350px\"))\n      ),\n      fluidRow(\n        column(12, plotOutput(\"convergence_plot\", height = \"450px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  draw_pop &lt;- function(n, pop) {\n    switch(pop,\n      \"Normal\" = rnorm(n, mean = 5, sd = 2),\n      \"Uniform\" = runif(n, 0, 10),\n      \"Skewed (exponential)\" = rexp(n, rate = 0.5),\n      \"Heavily skewed (chi-sq 2)\" = rchisq(n, df = 2),\n      \"Bimodal\" = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, 2, 0.7) + (1 - k) * rnorm(n, 8, 0.7)\n      },\n      \"Heavy-tailed (t, df=3)\" = rt(n, df = 3) * 2 + 5\n    )\n  }\n\n  compute_stat &lt;- function(x, stat) {\n    switch(stat,\n      \"Mean\"   = mean(x),\n      \"Median\" = median(x),\n      \"SD\"     = sd(x),\n      \"Max\"    = max(x),\n      \"IQR\"    = IQR(x)\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    reps &lt;- input$reps\n    pop  &lt;- input$pop\n    stat &lt;- input$stat\n\n    # One sample (for display)\n    one_sample &lt;- draw_pop(n, pop)\n    one_stat   &lt;- compute_stat(one_sample, stat)\n\n    # Big population draw (to show true shape)\n    big_pop &lt;- draw_pop(10000, pop)\n\n    # Repeated experiments\n    samp_stats &lt;- replicate(reps, compute_stat(draw_pop(n, pop), stat))\n\n    # Check normality (Shapiro on subset)\n    shap_sub &lt;- samp_stats[seq_len(min(5000, length(samp_stats)))]\n    shap_p &lt;- tryCatch(shapiro.test(shap_sub)$p.value, error = function(e) NA)\n\n    list(one_sample = one_sample, one_stat = one_stat,\n         big_pop = big_pop, samp_stats = samp_stats,\n         n = n, reps = reps, stat = stat, pop = pop,\n         shap_p = shap_p)\n  })\n\n  # --- Left: data distribution ---\n  output$data_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$big_pop, breaks = 60, probability = TRUE,\n         col = \"#e74c3c20\", border = \"#e74c3c60\",\n         main = paste(\"Population:\", d$pop),\n         xlab = \"X\", ylab = \"Density\")\n\n    # Overlay one sample as rug\n    rug(d$one_sample, col = \"#2c3e50\", lwd = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Population shape\",\n                      paste0(\"Your sample (n = \", d$n, \")\")),\n           col = c(\"#e74c3c60\", \"#2c3e50\"),\n           lwd = c(8, 2), lty = c(1, 1))\n  })\n\n  # --- Right: sampling distribution ---\n  output$samp_dist_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$samp_stats, breaks = 50, probability = TRUE,\n         col = \"#3498db30\", border = \"#3498db80\",\n         main = paste0(\"Sampling dist. of \", d$stat, \" (n = \", d$n, \")\"),\n         xlab = paste0(\"Sample \", tolower(d$stat)),\n         ylab = \"Density\")\n\n    # Normal overlay\n    x_seq &lt;- seq(min(d$samp_stats), max(d$samp_stats), length.out = 300)\n    lines(x_seq,\n          dnorm(x_seq, mean = mean(d$samp_stats), sd = sd(d$samp_stats)),\n          col = \"#e74c3c\", lwd = 2, lty = 2)\n\n    # Mark your one estimate\n    abline(v = d$one_stat, col = \"#2c3e50\", lwd = 2.5, lty = 1)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Sampling distribution\",\n                      \"Normal approximation\",\n                      paste0(\"Your estimate: \", round(d$one_stat, 3))),\n           col = c(\"#3498db80\", \"#e74c3c\", \"#2c3e50\"),\n           lwd = c(8, 2, 2.5), lty = c(1, 2, 1))\n  })\n\n  # --- Bottom: how normality improves with n ---\n  output$convergence_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ns &lt;- c(2, 5, 10, 30, 50, 100)\n    ns &lt;- ns[ns &lt;= 200]\n\n    colors &lt;- colorRampPalette(c(\"#e74c3c\", \"#3498db\"))(length(ns))\n\n    # First pass to get x range\n    all_stats &lt;- list()\n    for (i in seq_along(ns)) {\n      all_stats[[i]] &lt;- replicate(500, compute_stat(draw_pop(ns[i], d$pop), d$stat))\n    }\n    xlim &lt;- range(unlist(all_stats))\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, length(ns) * 1.5 + 1),\n         yaxt = \"n\", ylab = \"\", xlab = paste0(\"Sample \", tolower(d$stat)),\n         main = paste0(\"Sampling distribution of \", d$stat, \" at different n\"))\n    positions &lt;- seq_along(ns) * 1.5\n    axis(2, at = positions, labels = paste0(\"n=\", ns), las = 1, cex.axis = 0.9)\n\n    for (i in seq_along(ns)) {\n      dens &lt;- density(all_stats[[i]])\n      # Scale density to fit in a strip\n      dens$y &lt;- dens$y / max(dens$y) * 0.7\n      polygon(dens$x, dens$y + positions[i], col = adjustcolor(colors[i], 0.4),\n              border = colors[i], lwd = 1.5)\n    }\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    se &lt;- sd(d$samp_stats)\n    normal_enough &lt;- !is.na(d$shap_p) && d$shap_p &gt; 0.05\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Your estimate:&lt;/b&gt; \", round(d$one_stat, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;SE (from sampling dist):&lt;/b&gt; \", round(se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Sampling dist SD:&lt;/b&gt; \", round(sd(d$samp_stats), 4), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Looks normal?&lt;/b&gt; \",\n        if (normal_enough)\n          \"&lt;span style='color:#27ae60;'&gt;Yes (Shapiro p = \" else\n          \"&lt;span style='color:#e74c3c;'&gt;No (Shapiro p = \",\n        round(d$shap_p, 4), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;small&gt;Shapiro-Wilk tests whether the&lt;br&gt;\",\n        \"sampling distribution is normal.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nThe red dashed line on the right panel is the normal approximation. When it fits the histogram well, you can safely use normal-based inference (z-tests, t-tests, standard CIs).\nRules of thumb:\n\n\n\nPopulation\nStatistic\nn needed for normality\n\n\n\n\nSymmetric (normal, uniform)\nMean\n~10–15\n\n\nModerately skewed (exponential)\nMean\n~30\n\n\nHeavily skewed (chi-sq 2)\nMean\n~50–100\n\n\nAny shape\nMean\n~30 is the textbook answer, but check\n\n\nAny shape\nMedian\nSlower — needs larger n\n\n\nAny shape\nMax\nVery slow — often never normal\n\n\nAny shape\nSD\nModerate — ~30–50\n\n\n\n\n\n\n\nExponential, Mean, n = 5: the sampling distribution is visibly skewed. Normal approximation is poor. Shapiro test says “No.”\nSame but n = 50: now it’s nearly bell-shaped. CLT has kicked in.\nAny population, Max, n = 100: the sampling distribution of the maximum is not normal even with large n. It has its own distribution (extreme value theory). Normal approximation fails.\nBimodal, Mean, n = 2: the sampling distribution itself is bimodal! With only 2 observations, averaging doesn’t smooth enough.\nBottom panel (ridgeline): watch the sampling distribution narrow and become more normal as n increases, all on one plot.\n\n\n\n\nThe sampling distribution is not your data. It’s the distribution of your estimate. Whether it’s normal depends on three things:\n\nThe population shape — the uglier it is, the more data you need\nThe sample size — larger n → CLT → normality\nThe statistic — means converge fast, medians slower, maxima barely at all\n\nWhen in doubt, don’t assume — bootstrap it. The bootstrap page shows you how to build the sampling distribution empirically.\n\n\n\n\n\nWilliam Sealy Gosset, a chemist at the Guinness brewery, invented the t-distribution in 1908 because he was working with tiny samples of barley. Guinness didn’t let employees publish under their own names, so he used the pen name “Student” — hence “Student’s t-test.”\nGosset’s problem: with only 3–4 observations, you can’t assume the sampling distribution is normal. The t-distribution has heavier tails to account for the extra uncertainty when \\(n\\) is small. As \\(n\\) grows, the t-distribution converges to the normal — because with enough data, the sampling distribution is normal (CLT).\nThe concept of a sampling distribution was so confusing to early statisticians that R.A. Fisher reportedly said it was the most difficult idea in all of statistics to teach clearly.",
    "crumbs": [
      "Probability & Uncertainty",
      "The Sampling Distribution"
    ]
  },
  {
    "objectID": "measurement-error.html",
    "href": "measurement-error.html",
    "title": "Measurement Error & Attenuation Bias",
    "section": "",
    "text": "You want to estimate the effect of \\(X^*\\) on \\(Y\\):\n\\[Y_i = \\alpha + \\beta X_i^* + u_i\\]\nBut you don’t observe \\(X^*\\) perfectly. Instead you observe \\(X\\) with noise:\n\\[X_i = X_i^* + \\eta_i, \\qquad \\eta_i \\sim (0, \\sigma_\\eta^2)\\]\nwhere \\(\\eta\\) is measurement error — independent of \\(X^*\\) and \\(u\\).\nWhen you run OLS on the mismeasured \\(X\\), you don’t get \\(\\beta\\). You get:\n\\[\\hat{\\beta}_{OLS} \\xrightarrow{p} \\beta \\times \\underbrace{\\frac{\\text{Var}(X^*)}{\\text{Var}(X^*) + \\sigma_\\eta^2}}_{\\lambda}\\]\nThat fraction \\(\\lambda\\) is always between 0 and 1. So the estimate is biased toward zero. This is attenuation bias.\n\n\nThink of it this way. The measurement error adds random noise to \\(X\\). From OLS’s perspective, some of the variation in \\(X\\) is real signal (correlated with \\(Y\\)) and some is pure noise (uncorrelated with \\(Y\\)). OLS can’t tell which is which, so it averages over both — diluting the estimated slope.\nMore noise → more dilution → flatter slope.",
    "crumbs": [
      "Regression & Diagnostics",
      "Measurement Error & Attenuation Bias"
    ]
  },
  {
    "objectID": "measurement-error.html#the-problem",
    "href": "measurement-error.html#the-problem",
    "title": "Measurement Error & Attenuation Bias",
    "section": "",
    "text": "You want to estimate the effect of \\(X^*\\) on \\(Y\\):\n\\[Y_i = \\alpha + \\beta X_i^* + u_i\\]\nBut you don’t observe \\(X^*\\) perfectly. Instead you observe \\(X\\) with noise:\n\\[X_i = X_i^* + \\eta_i, \\qquad \\eta_i \\sim (0, \\sigma_\\eta^2)\\]\nwhere \\(\\eta\\) is measurement error — independent of \\(X^*\\) and \\(u\\).\nWhen you run OLS on the mismeasured \\(X\\), you don’t get \\(\\beta\\). You get:\n\\[\\hat{\\beta}_{OLS} \\xrightarrow{p} \\beta \\times \\underbrace{\\frac{\\text{Var}(X^*)}{\\text{Var}(X^*) + \\sigma_\\eta^2}}_{\\lambda}\\]\nThat fraction \\(\\lambda\\) is always between 0 and 1. So the estimate is biased toward zero. This is attenuation bias.\n\n\nThink of it this way. The measurement error adds random noise to \\(X\\). From OLS’s perspective, some of the variation in \\(X\\) is real signal (correlated with \\(Y\\)) and some is pure noise (uncorrelated with \\(Y\\)). OLS can’t tell which is which, so it averages over both — diluting the estimated slope.\nMore noise → more dilution → flatter slope.",
    "crumbs": [
      "Regression & Diagnostics",
      "Measurement Error & Attenuation Bias"
    ]
  },
  {
    "objectID": "measurement-error.html#simulation-1-watch-the-slope-attenuate",
    "href": "measurement-error.html#simulation-1-watch-the-slope-attenuate",
    "title": "Measurement Error & Attenuation Bias",
    "section": "Simulation 1: Watch the slope attenuate",
    "text": "Simulation 1: Watch the slope attenuate\nIncrease the measurement error and watch the estimated slope shrink toward zero. The true relationship stays the same — only the noise changes.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"beta\", \"True slope (\\u03b2):\",\n                  min = 0.5, max = 3, value = 1.5, step = 0.1),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 50, max = 500, value = 200, step = 50),\n\n      sliderInput(\"sigma_x\", \"SD of true X*:\",\n                  min = 1, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"sigma_u\", \"SD of outcome noise (u):\",\n                  min = 0.5, max = 3, value = 1, step = 0.25),\n\n      sliderInput(\"sigma_eta\", \"Measurement error (\\u03c3\\u03b7):\",\n                  min = 0, max = 5, value = 0, step = 0.25),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"scatter\", height = \"550px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  # True data — only regenerates when beta, n, sigma_x, sigma_u change\n  base &lt;- reactive({\n    n   &lt;- input$n\n    b   &lt;- input$beta\n    sx  &lt;- input$sigma_x\n    su  &lt;- input$sigma_u\n\n    x_star &lt;- rnorm(n, 0, sx)\n    u      &lt;- rnorm(n, su)\n    y      &lt;- 2 + b * x_star + u\n\n    list(x_star = x_star, y = y, beta = b, sx = sx)\n  })\n\n  # Measurement error applied on top — changes when eta slider moves\n  sim &lt;- reactive({\n    d   &lt;- base()\n    se  &lt;- input$sigma_eta\n\n    eta   &lt;- rnorm(length(d$x_star), 0, se)\n    x_obs &lt;- d$x_star + eta\n\n    fit_true &lt;- lm(d$y ~ d$x_star)\n    fit_obs  &lt;- lm(d$y ~ x_obs)\n\n    lambda &lt;- d$sx^2 / (d$sx^2 + se^2)\n\n    list(x_star = d$x_star, x_obs = x_obs, y = d$y,\n         b_true = coef(fit_true)[2], b_obs = coef(fit_obs)[2],\n         beta = d$beta, lambda = lambda, sigma_eta = se)\n  })\n\n  output$scatter &lt;- renderPlot({\n    d &lt;- sim()\n\n    par(mfrow = c(1, 2), mar = c(4.5, 4.5, 3.5, 1))\n\n    # Left: true X*\n    plot(d$x_star, d$y, pch = 16, cex = 0.6,\n         col = adjustcolor(\"#3498db\", 0.5),\n         xlab = \"True X*\", ylab = \"Y\",\n         main = \"Regression on true X*\")\n    abline(lm(d$y ~ d$x_star), col = \"#27ae60\", lwd = 3)\n    mtext(paste0(\"Slope = \", round(d$b_true, 3)),\n          side = 3, line = 0, cex = 1.1, font = 2, col = \"#27ae60\")\n\n    # Right: observed X with error\n    plot(d$x_obs, d$y, pch = 16, cex = 0.6,\n         col = adjustcolor(\"#e74c3c\", 0.4),\n         xlab = \"Observed X (with error)\", ylab = \"Y\",\n         main = paste0(\"Regression on mismeasured X\"))\n    abline(lm(d$y ~ d$x_obs), col = \"#e74c3c\", lwd = 3)\n    abline(a = coef(lm(d$y ~ d$x_star))[1],\n           b = d$beta, col = \"#27ae60\", lwd = 2, lty = 2)\n    mtext(paste0(\"Slope = \", round(d$b_obs, 3),\n                 \"  (true = \", d$beta, \")\"),\n          side = 3, line = 0, cex = 1.1, font = 2, col = \"#e74c3c\")\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"OLS on mismeasured X\", \"True slope\"),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = c(3, 2), lty = c(1, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- sim()\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True \\u03b2:&lt;/b&gt; \", d$beta, \"&lt;br&gt;\",\n        \"&lt;b&gt;OLS on X*:&lt;/b&gt; \", round(d$b_true, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;OLS on X:&lt;/b&gt; \", round(d$b_obs, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Attenuation factor (\\u03bb):&lt;/b&gt;&lt;br&gt;\",\n        \"Var(X*) / [Var(X*) + Var(\\u03b7)]&lt;br&gt;\",\n        \"= \", round(d$lambda, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;\\u03b2 \\u00d7 \\u03bb = &lt;/b&gt;\",\n        round(d$beta * d$lambda, 3)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nStart with \\(\\eta\\) = 0: both panels are identical. No measurement error, no bias.\nSlowly increase \\(\\eta\\): watch the right panel’s slope flatten. The cloud of points spreads horizontally (noise in X), so OLS “sees” a weaker relationship.\n\\(\\eta\\) = 5, SD of X* = 2: the attenuation factor drops to ~0.14. Your estimate is 86% too small.\nIncrease n: the slope doesn’t recover! Attenuation bias is not a small-sample problem — it persists no matter how much data you have. More data just gives you a more precise estimate of the wrong number.",
    "crumbs": [
      "Regression & Diagnostics",
      "Measurement Error & Attenuation Bias"
    ]
  },
  {
    "objectID": "measurement-error.html#simulation-2-attenuation-is-systematic-not-just-noisy",
    "href": "measurement-error.html#simulation-2-attenuation-is-systematic-not-just-noisy",
    "title": "Measurement Error & Attenuation Bias",
    "section": "Simulation 2: Attenuation is systematic, not just noisy",
    "text": "Simulation 2: Attenuation is systematic, not just noisy\nRun 500 regressions, each with fresh measurement error. The distribution of slope estimates is centered below the true \\(\\beta\\) — it’s not random noise, it’s a systematic downward bias.\n#| standalone: true\n#| viewerHeight: 550\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"beta2\", \"True slope (\\u03b2):\",\n                  min = 0.5, max = 3, value = 1.5, step = 0.1),\n\n      sliderInput(\"n2\", \"Sample size per run:\",\n                  min = 50, max = 300, value = 100, step = 50),\n\n      sliderInput(\"sigma_eta2\", \"Measurement error (\\u03c3\\u03b7):\",\n                  min = 0, max = 5, value = 2, step = 0.25),\n\n      sliderInput(\"n_sims\", \"Number of simulations:\",\n                  min = 100, max = 1000, value = 500, step = 100),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"mc_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  sim &lt;- reactive({\n    n     &lt;- input$n2\n    b     &lt;- input$beta2\n    se    &lt;- input$sigma_eta2\n    nsims &lt;- input$n_sims\n    sx    &lt;- 2\n\n    betas_true &lt;- numeric(nsims)\n    betas_obs  &lt;- numeric(nsims)\n\n    for (i in seq_len(nsims)) {\n      x_star &lt;- rnorm(n, 0, sx)\n      u      &lt;- rnorm(n)\n      y      &lt;- 2 + b * x_star + u\n\n      eta   &lt;- rnorm(n, 0, se)\n      x_obs &lt;- x_star + eta\n\n      betas_true[i] &lt;- coef(lm(y ~ x_star))[2]\n      betas_obs[i]  &lt;- coef(lm(y ~ x_obs))[2]\n    }\n\n    lambda &lt;- sx^2 / (sx^2 + se^2)\n\n    list(betas_true = betas_true, betas_obs = betas_obs,\n         beta = b, lambda = lambda)\n  })\n\n  output$mc_plot &lt;- renderPlot({\n    d &lt;- sim()\n\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    all_b &lt;- c(d$betas_true, d$betas_obs)\n    xlim  &lt;- range(all_b) + c(-0.1, 0.1)\n\n    # No-error distribution\n    hist(d$betas_true, breaks = 40,\n         col = adjustcolor(\"#27ae60\", 0.5), border = \"white\",\n         main = \"Distribution of slope estimates across simulations\",\n         xlab = expression(hat(beta)), xlim = xlim,\n         freq = FALSE, cex.main = 1.3)\n\n    # With-error distribution\n    hist(d$betas_obs, breaks = 40,\n         col = adjustcolor(\"#e74c3c\", 0.45), border = \"white\",\n         add = TRUE, freq = FALSE)\n\n    abline(v = d$beta, col = \"#2c3e50\", lwd = 2.5, lty = 2)\n    abline(v = d$beta * d$lambda, col = \"#e74c3c\", lwd = 2, lty = 3)\n    abline(v = mean(d$betas_obs), col = \"#e74c3c\", lwd = 1.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.9,\n           legend = c(\n             paste0(\"No error (centered at \\u03b2 = \", d$beta, \")\"),\n             paste0(\"With error (centered at \", round(mean(d$betas_obs), 3), \")\"),\n             paste0(\"Theory: \\u03b2\\u03bb = \", round(d$beta * d$lambda, 3))\n           ),\n           fill = c(adjustcolor(\"#27ae60\", 0.5),\n                    adjustcolor(\"#e74c3c\", 0.45), NA),\n           border = c(\"white\", \"white\", NA),\n           col = c(NA, NA, \"#e74c3c\"),\n           lwd = c(NA, NA, 2), lty = c(NA, NA, 3))\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- sim()\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True \\u03b2:&lt;/b&gt; \", d$beta, \"&lt;br&gt;\",\n        \"&lt;b&gt;Avg estimate (no error):&lt;/b&gt; \",\n        round(mean(d$betas_true), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Avg estimate (with error):&lt;/b&gt; \",\n        round(mean(d$betas_obs), 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Attenuation factor:&lt;/b&gt; \", round(d$lambda, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;\\u03b2 \\u00d7 \\u03bb:&lt;/b&gt; \",\n        round(d$beta * d$lambda, 3), \"&lt;br&gt;\",\n        \"&lt;small&gt;Bias: \",\n        round(mean(d$betas_obs) - d$beta, 3), \"&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\n\\(\\eta\\) = 0: both distributions overlap perfectly — no bias at all.\n\\(\\eta\\) = 2: the red distribution shifts left. The average slope is systematically below the truth.\nIncrease n to 300: the distributions get narrower (more precise) but the red one stays centered at the wrong value. Attenuation bias doesn’t go away with more data.\nCompare theory vs simulation: the theoretical \\(\\beta\\lambda\\) should closely match the average of the red distribution.",
    "crumbs": [
      "Regression & Diagnostics",
      "Measurement Error & Attenuation Bias"
    ]
  },
  {
    "objectID": "measurement-error.html#measurement-error-in-y-vs-x",
    "href": "measurement-error.html#measurement-error-in-y-vs-x",
    "title": "Measurement Error & Attenuation Bias",
    "section": "Measurement error in Y vs X",
    "text": "Measurement error in Y vs X\nA crucial asymmetry:\n\n\n\n\n\n\n\n\n\nError in X\nError in Y\n\n\n\n\nBias?\nYes — toward zero\nNo bias\n\n\nPrecision?\nSlightly worse\nWorse (larger SEs)\n\n\nGoes away with more data?\nNo\nSEs shrink, but that’s just precision\n\n\n\nWhy the asymmetry? When \\(Y\\) is measured with error, the noise goes into the residual — it’s just more \\(u\\). The slope is unbiased; you just estimate it less precisely. When \\(X\\) is measured with error, the noise is in the regressor, which contaminates the covariance between \\(X\\) and \\(Y\\) and biases the slope.",
    "crumbs": [
      "Regression & Diagnostics",
      "Measurement Error & Attenuation Bias"
    ]
  },
  {
    "objectID": "measurement-error.html#what-can-you-do-about-it",
    "href": "measurement-error.html#what-can-you-do-about-it",
    "title": "Measurement Error & Attenuation Bias",
    "section": "What can you do about it?",
    "text": "What can you do about it?\n\nMeasure better. The best fix is reducing \\(\\sigma_\\eta\\). Use validated instruments, multiple measurements, averages of repeated measures.\nInstrumental variables (IV). Find a variable \\(Z\\) that predicts \\(X^*\\) but isn’t contaminated by the measurement error. Two-stage least squares recovers the true \\(\\beta\\).\nReliability ratio correction. If you know (or can estimate) the reliability \\(\\lambda = \\text{Var}(X^*) / \\text{Var}(X)\\), divide your estimate by \\(\\lambda\\): \\(\\hat{\\beta}_{corrected} = \\hat{\\beta}_{OLS} / \\lambda\\).\nMultiple indicators. If you have two noisy measures of \\(X^*\\) with independent errors, their covariance identifies \\(\\text{Var}(X^*)\\), letting you compute \\(\\lambda\\) directly.",
    "crumbs": [
      "Regression & Diagnostics",
      "Measurement Error & Attenuation Bias"
    ]
  },
  {
    "objectID": "measurement-error.html#did-you-know",
    "href": "measurement-error.html#did-you-know",
    "title": "Measurement Error & Attenuation Bias",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe attenuation bias formula was derived by Karl Pearson in the early 1900s, making it one of the oldest results in regression theory. Pearson was studying the relationship between fathers’ and sons’ heights and realized that imprecise measurements of height would make the hereditary correlation look weaker than it really was.\nIn economics, Jerry Hausman (2001) showed that measurement error in survey data on income and consumption can attenuate elasticity estimates by 30–50%. Studies using administrative tax records (with near-zero measurement error) consistently find larger effects than survey-based studies — exactly what attenuation bias predicts.\nThe errors-in-variables literature distinguishes between classical measurement error (what we covered: \\(X = X^* + \\eta\\) with \\(\\eta\\) independent of \\(X^*\\)) and non-classical error (where the error depends on the true value). Non-classical error can bias in either direction, not just toward zero. Mean-reverting error in test scores is a common example.",
    "crumbs": [
      "Regression & Diagnostics",
      "Measurement Error & Attenuation Bias"
    ]
  },
  {
    "objectID": "simulations.html",
    "href": "simulations.html",
    "title": "CLT Simulator",
    "section": "",
    "text": "Drag the sample size slider to watch the Central Limit Theorem in action. The left plot shows the true population; the right shows the sampling distribution of the mean — notice how it becomes normal as n grows, regardless of the population shape.\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\n\n# ---------------------------------------------------------------------------\n# Helper: draw a single random sample from the chosen distribution\n# ---------------------------------------------------------------------------\ndraw_sample &lt;- function(n, dist) {\n  switch(dist,\n    \"Uniform(0, 1)\"      = runif(n),\n    \"Exponential(1)\"     = rexp(n, rate = 1),\n    \"Right-skewed\"       = rchisq(n, df = 3),\n    \"Bimodal\"            = {\n      k &lt;- rbinom(n, 1, 0.5)\n      k * rnorm(n, mean = -2, sd = 0.6) + (1 - k) * rnorm(n, mean = 2, sd = 0.6)\n    },\n    \"Bernoulli(0.3)\"     = rbinom(n, size = 1, prob = 0.3),\n    runif(n)\n  )\n}\n\n# Theoretical mean & sd of each population distribution\npop_params &lt;- list(\n  \"Uniform(0, 1)\"  = list(mu = 0.5, sigma = sqrt(1 / 12)),\n  \"Exponential(1)\" = list(mu = 1,   sigma = 1),\n  \"Right-skewed\"   = list(mu = 3,   sigma = sqrt(6)),\n  \"Bimodal\"        = list(mu = 0,   sigma = sqrt(0.6^2 + 4)),\n  \"Bernoulli(0.3)\" = list(mu = 0.3, sigma = sqrt(0.3 * 0.7))\n)\n\n# ---------------------------------------------------------------------------\n# UI\n# ---------------------------------------------------------------------------\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #eaf2f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.8;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"dist\", \"Population distribution:\",\n                  choices = names(pop_params)),\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 1, max = 200, value = 5, step = 1),\n\n      sliderInput(\"reps\", \"Number of samples:\",\n                  min = 100, max = 3000, value = 1000, step = 100),\n\n      actionButton(\"resample\", \"Draw new samples\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"stats_box\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"parent_plot\", height = \"380px\")),\n        column(6, plotOutput(\"sampling_plot\", height = \"380px\"))\n      )\n    )\n  )\n)\n\n# ---------------------------------------------------------------------------\n# Server\n# ---------------------------------------------------------------------------\nserver &lt;- function(input, output, session) {\n\n  sim &lt;- reactive({\n    input$resample\n    n    &lt;- input$n\n    reps &lt;- input$reps\n    dist &lt;- input$dist\n\n    means &lt;- replicate(reps, mean(draw_sample(n, dist)))\n\n    params &lt;- pop_params[[dist]]\n    theo_mu &lt;- params$mu\n    theo_se &lt;- params$sigma / sqrt(n)\n\n    list(means = means, dist = dist, n = n, reps = reps,\n         theo_mu = theo_mu, theo_se = theo_se)\n  })\n\n  output$parent_plot &lt;- renderPlot({\n    dist &lt;- input$dist\n    big  &lt;- draw_sample(10000, dist)\n\n    par(mar = c(4.5, 4, 3, 1))\n    hist(big, breaks = 60, probability = TRUE,\n         col = \"#d5e8d4\", border = \"#82b366\",\n         main = paste(\"True Population:\", dist),\n         xlab = \"x\", ylab = \"Density\")\n  })\n\n  output$sampling_plot &lt;- renderPlot({\n    s &lt;- sim()\n\n    par(mar = c(4.5, 4, 3, 1))\n    hist(s$means, breaks = 40, probability = TRUE,\n         col = \"#dae8fc\", border = \"#6c8ebf\",\n         main = paste0(\"Sampling Distribution of the Mean (n = \", s$n, \")\"),\n         xlab = \"Sample mean\", ylab = \"Density\")\n\n    x_seq &lt;- seq(min(s$means), max(s$means), length.out = 300)\n    lines(x_seq, dnorm(x_seq, mean = s$theo_mu, sd = s$theo_se),\n          col = \"#e74c3c\", lwd = 2.5)\n\n    abline(v = s$theo_mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\",\n           legend = c(\"Normal approximation\", \"Theoretical mean\"),\n           col    = c(\"#e74c3c\", \"#2c3e50\"),\n           lwd    = c(2.5, 2),\n           lty    = c(1, 2),\n           bty    = \"n\", cex = 0.9)\n  })\n\n  output$stats_box &lt;- renderUI({\n    s &lt;- sim()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Theoretical mean:&lt;/b&gt; \",   round(s$theo_mu, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Observed mean:&lt;/b&gt; \",      round(mean(s$means), 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Theoretical SE:&lt;/b&gt; \",     round(s$theo_se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Observed SD:&lt;/b&gt; \",        round(sd(s$means), 4)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nDid you know?\n\nThe CLT was first glimpsed by Abraham de Moivre in 1733, who showed that the binomial distribution approaches a bell curve. Laplace generalized it in 1812. But the rigorous proof for arbitrary distributions came from Aleksandr Lyapunov in 1901 — over 150 years after de Moivre’s insight.\nThe normal distribution is sometimes called the “Gaussian” distribution after Carl Friedrich Gauss, but Gauss wasn’t the first to describe it — de Moivre was. Gauss just got better publicity.\nThe CLT explains why so many things in nature look bell-shaped: human heights, blood pressure, measurement errors, IQ scores. Whenever an outcome is the sum of many small independent factors, the CLT kicks in.",
    "crumbs": [
      "The Central Limit Theorem",
      "CLT Simulator"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundations of Statistical Inference",
    "section": "",
    "text": "Welcome 👋\nThis course builds intuition for:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#start-here",
    "href": "index.html#start-here",
    "title": "Foundations of Statistical Inference",
    "section": "Start Here",
    "text": "Start Here\n\nProbability & Uncertainty\n\nFoundations — Distributions, Sampling & Confidence Intervals\nVariance, SD & Standard Error — Population vs Sample, SD vs SE & Why n − 1\nThe Sampling Distribution — Data vs Sampling Distribution & When to Assume Normality\n\n\n\nThe Central Limit Theorem\n\nCLT Simulator — Interactive CLT Simulator\nLLN vs CLT — Why Averages Stabilize Before They Become Normal\n\n\n\nInference\n\np-values & Confidence Intervals — What They Actually Mean\nThe Bootstrap — Resampling-Based Inference\nPower, Alpha, Beta & MDE — Hypothesis Testing & Experiment Design\nMonte Carlo Experiments — How We Understand Estimators\nMultiple Testing — False Discoveries & the Replication Crisis\n\n\n\nRegression & Diagnostics\n\nRegression & the CEF — OLS and the Conditional Expectation Function\nResiduals & Controls — Diagnostics, Partialling Out & Omitted Variable Bias\nHomo- & Heteroskedasticity — Constant vs Non-Constant Variance & Robust SEs\nClustered SEs — When Observations Aren’t Independent\nFrisch-Waugh-Lovell — Partialling Out & OVB\nMeasurement Error & Attenuation Bias — Why Noisy Regressors Bias You Toward Zero\nBias-Variance Tradeoff — Underfitting, Overfitting & MSE Decomposition\n\n\n\nBayesian Thinking\n\nBayesian Updating — One Gentle Introduction",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "cef.html",
    "href": "cef.html",
    "title": "Regression & the CEF",
    "section": "",
    "text": "What is the CEF?\nThe conditional expectation function (CEF) answers a simple question: “What do you expect Y to be, given that you know X?”\nSay you’re looking at income (\\(Y\\)) vs. years of education (\\(X\\)). The CEF answers: among everyone with exactly 12 years of education, what’s their average income? What about 16 years? 20 years? If you plot those averages, you get a curve — that’s the CEF. It could be a straight line, or it could bend, flatten out, jump — whatever the data actually does.\n\n\nHow does regression fit in?\nOLS draws a straight line through that. Two cases:\n\nCEF is already a straight line — OLS gets it exactly right. Each extra unit of \\(X\\) adds the same bump to expected \\(Y\\). The line is the CEF.\nCEF is curved — maybe the first few years of education matter a lot, but returns flatten after a PhD. OLS can’t bend, so it draws the best straight line it can through that curve. It’s a useful summary, but it misses the shape.\n\nRegression doesn’t assume the world is linear. It gives you the best linear approximation to whatever the true relationship is. The simulator below lets you see exactly where that approximation works and where it breaks down.\nIn the plots: the red dots show the conditional mean of \\(Y\\) in each bin of \\(X\\) (the empirical CEF), the green curve is the true CEF, and the blue line is OLS. Switch DGPs to see when they agree and when they diverge.\n\n\nReading the residual plot\nThe right panel shows residuals vs. fitted values with a LOESS smoother (red curve). LOESS fits a tiny weighted regression at each point using only nearby observations, producing a flexible curve that follows local patterns. If OLS is correctly specified, the LOESS line should be flat at zero. If it curves, that’s visual evidence of nonlinearity that OLS is missing.\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\ndgp_choices &lt;- c(\n  \"Linear\",\n  \"Quadratic\",\n  \"Log (diminishing returns)\",\n  \"Step function\",\n  \"Sine wave\"\n)\n\n# True CEF for each DGP\ncef_fun &lt;- function(x, dgp) {\n  switch(dgp,\n    \"Linear\"                   = 2 + 1.5 * x,\n    \"Quadratic\"                = 1 + 0.8 * x - 0.15 * x^2,\n    \"Log (diminishing returns)\" = 3 * log(x + 1),\n    \"Step function\"            = ifelse(x &lt; 0, -1, 2),\n    \"Sine wave\"                = 2 * sin(x),\n    1.5 * x\n  )\n}\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .info-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.8;\n    }\n    .info-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"dgp\", \"True DGP:\",\n                  choices = dgp_choices),\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 50, max = 1000, value = 300, step = 50),\n\n      sliderInput(\"sigma\", HTML(\"Error SD (&sigma;):\"),\n                  min = 0.5, max = 4, value = 1.5, step = 0.5),\n\n      sliderInput(\"nbins\", \"Bins for CEF:\",\n                  min = 5, max = 30, value = 15, step = 1),\n\n      actionButton(\"redraw\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"info_box\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"scatter_plot\", height = \"420px\")),\n        column(6, plotOutput(\"resid_plot\",   height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$redraw\n    n     &lt;- input$n\n    sigma &lt;- input$sigma\n    dgp   &lt;- input$dgp\n\n    # X range depends on DGP\n    if (dgp == \"Log (diminishing returns)\") {\n      x &lt;- runif(n, 0, 6)\n    } else if (dgp == \"Step function\") {\n      x &lt;- runif(n, -3, 3)\n    } else if (dgp == \"Sine wave\") {\n      x &lt;- runif(n, -pi, 2 * pi)\n    } else {\n      x &lt;- runif(n, -3, 5)\n    }\n\n    mu &lt;- cef_fun(x, dgp)\n    y  &lt;- mu + rnorm(n, sd = sigma)\n\n    ols &lt;- lm(y ~ x)\n\n    # Bin X and compute conditional means\n    nbins &lt;- input$nbins\n    breaks &lt;- seq(min(x), max(x), length.out = nbins + 1)\n    bin    &lt;- cut(x, breaks, include.lowest = TRUE)\n    bin_mid &lt;- tapply(x, bin, mean)\n    bin_cef &lt;- tapply(y, bin, mean)\n    bin_n   &lt;- tapply(y, bin, length)\n\n    list(x = x, y = y, mu = mu, ols = ols, dgp = dgp,\n         bin_mid = bin_mid, bin_cef = bin_cef, bin_n = bin_n)\n  })\n\n  # --- Main scatter plot ---\n  output$scatter_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$y, pch = 16, col = \"#bdc3c780\", cex = 0.6,\n         xlab = \"X\", ylab = \"Y\",\n         main = paste(\"DGP:\", d$dgp))\n\n    # True CEF curve\n    xo &lt;- sort(d$x)\n    lines(xo, cef_fun(xo, d$dgp), col = \"#2ecc71\", lwd = 2.5)\n\n    # OLS line\n    abline(d$ols, col = \"#3498db\", lwd = 2.5)\n\n    # Binned conditional means (empirical CEF)\n    keep &lt;- !is.na(d$bin_cef) & !is.na(d$bin_mid)\n    points(d$bin_mid[keep], d$bin_cef[keep],\n           pch = 19, col = \"#e74c3c\", cex = 1.6)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(expression(\"True \" * E * \"[Y|X]\"),\n                      \"OLS regression\",\n                      expression(\"Binned \" * bar(Y) * \" (empirical CEF)\")),\n           col = c(\"#2ecc71\", \"#3498db\", \"#e74c3c\"),\n           lwd = c(2.5, 2.5, NA),\n           pch = c(NA, NA, 19),\n           pt.cex = c(NA, NA, 1.4))\n  })\n\n  # --- Residual plot ---\n  output$resid_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    r &lt;- resid(d$ols)\n    fv &lt;- fitted(d$ols)\n\n    plot(fv, r, pch = 16, col = \"#9b59b680\", cex = 0.6,\n         xlab = \"Fitted values\", ylab = \"Residuals\",\n         main = \"Residuals vs Fitted\")\n    abline(h = 0, lty = 2, col = \"gray40\", lwd = 1.5)\n\n    # Loess smoother to reveal nonlinearity\n    lo &lt;- loess(r ~ fv)\n    ox &lt;- order(fv)\n    lines(fv[ox], predict(lo)[ox], col = \"#e74c3c\", lwd = 2)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Loess smoother\", \"Zero line\"),\n           col = c(\"#e74c3c\", \"gray40\"),\n           lwd = c(2, 1.5),\n           lty = c(1, 2))\n  })\n\n  # --- Info box ---\n  output$info_box &lt;- renderUI({\n    d &lt;- dat()\n    b0 &lt;- round(coef(d$ols)[1], 3)\n    b1 &lt;- round(coef(d$ols)[2], 3)\n    r2 &lt;- round(summary(d$ols)$r.squared, 3)\n\n    linear &lt;- d$dgp == \"Linear\"\n\n    tags$div(class = \"info-box\",\n      HTML(paste0(\n        \"&lt;b&gt;OLS:&lt;/b&gt; Y = \", b0, \" + \", b1, \"X&lt;br&gt;\",\n        \"&lt;b&gt;R&sup2;:&lt;/b&gt; \", r2, \"&lt;br&gt;&lt;br&gt;\",\n        if (linear) {\n          \"&lt;span style='color:#27ae60;'&gt;&lt;b&gt;&#10003; CEF is linear &mdash; OLS recovers it exactly.&lt;/b&gt;&lt;/span&gt;\"\n        } else {\n          \"&lt;span style='color:#e67e22;'&gt;&lt;b&gt;CEF is nonlinear &mdash; OLS is the best linear approximation.&lt;/b&gt;&lt;br&gt;Check the residual plot for the pattern.&lt;/span&gt;\"\n        }\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nDid you know?\n\nThe word “regression” comes from Francis Galton’s 1886 study of heights. He noticed that tall parents tended to have children who were tall — but not as tall. Short parents had children who were short — but not as short. Children’s heights “regressed toward the mean.” The statistical technique kept the name, even though modern regression has nothing to do with reverting to averages.\nGalton was also Charles Darwin’s half-cousin. He applied statistical thinking to heredity, fingerprints, and even the optimal way to brew tea.\nThe conditional expectation function is sometimes called the “regression function” — which makes sense once you know Galton’s story. OLS literally estimates the function that tells you the expected value of \\(Y\\) given \\(X\\).",
    "crumbs": [
      "Regression & Diagnostics",
      "Regression & the CEF"
    ]
  },
  {
    "objectID": "bayesian-updating.html",
    "href": "bayesian-updating.html",
    "title": "Bayesian Updating: One Gentle Introduction",
    "section": "",
    "text": "Bayesian inference has exactly one formula:\n\\[\\text{Posterior} \\propto \\text{Prior} \\times \\text{Likelihood}\\]\nThat’s it. Everything else is details.\n\nPrior: what you believed before seeing data\nLikelihood: how probable the data is under each possible parameter value\nPosterior: your updated belief after seeing data\n\nThe key insight: your belief gets updated as data arrives. Start with a prior (maybe vague, maybe informed), observe data, and the posterior combines both. With enough data, the prior gets overwhelmed — the data speaks for itself.\n\n\n\n\n\n\n\n\n\n\n\nFrequentist\nBayesian\n\n\n\n\nParameters\nFixed but unknown\nRandom variables with distributions\n\n\nProbability\nLong-run frequency\nDegree of belief\n\n\nCan say\n“If H₀ is true, data this extreme has p = 0.03”\n“Given the data, P(parameter &gt; 0) = 0.97”\n\n\nCI / CrI\n“95% of intervals from this procedure contain \\(\\theta\\)”\n“There’s a 95% probability \\(\\theta\\) is in this interval”\n\n\n\nThe Bayesian interpretation is often what people think a confidence interval means — but it requires a prior.",
    "crumbs": [
      "Bayesian Thinking",
      "Bayesian Updating"
    ]
  },
  {
    "objectID": "bayesian-updating.html#the-bayesian-recipe",
    "href": "bayesian-updating.html#the-bayesian-recipe",
    "title": "Bayesian Updating: One Gentle Introduction",
    "section": "",
    "text": "Bayesian inference has exactly one formula:\n\\[\\text{Posterior} \\propto \\text{Prior} \\times \\text{Likelihood}\\]\nThat’s it. Everything else is details.\n\nPrior: what you believed before seeing data\nLikelihood: how probable the data is under each possible parameter value\nPosterior: your updated belief after seeing data\n\nThe key insight: your belief gets updated as data arrives. Start with a prior (maybe vague, maybe informed), observe data, and the posterior combines both. With enough data, the prior gets overwhelmed — the data speaks for itself.\n\n\n\n\n\n\n\n\n\n\n\nFrequentist\nBayesian\n\n\n\n\nParameters\nFixed but unknown\nRandom variables with distributions\n\n\nProbability\nLong-run frequency\nDegree of belief\n\n\nCan say\n“If H₀ is true, data this extreme has p = 0.03”\n“Given the data, P(parameter &gt; 0) = 0.97”\n\n\nCI / CrI\n“95% of intervals from this procedure contain \\(\\theta\\)”\n“There’s a 95% probability \\(\\theta\\) is in this interval”\n\n\n\nThe Bayesian interpretation is often what people think a confidence interval means — but it requires a prior.",
    "crumbs": [
      "Bayesian Thinking",
      "Bayesian Updating"
    ]
  },
  {
    "objectID": "bayesian-updating.html#simulation-1-coin-flipping-watch-the-posterior-update",
    "href": "bayesian-updating.html#simulation-1-coin-flipping-watch-the-posterior-update",
    "title": "Bayesian Updating: One Gentle Introduction",
    "section": "Simulation 1: Coin flipping — watch the posterior update",
    "text": "Simulation 1: Coin flipping — watch the posterior update\nStart with a prior belief about a coin’s bias. Flip the coin one at a time and watch the posterior distribution update in real time. With enough flips, the posterior concentrates around the true bias — regardless of the prior.\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_p\", \"True coin bias:\",\n                  min = 0.1, max = 0.9, value = 0.7, step = 0.05),\n\n      selectInput(\"prior\", \"Prior belief:\",\n                  choices = c(\"Uniform (a=1, b=1)\" = \"uniform\",\n                              \"Skeptical of bias (a=5, b=5)\" = \"skeptical\",\n                              \"Believe heads-biased (a=8, b=2)\" = \"heads\",\n                              \"Believe tails-biased (a=2, b=8)\" = \"tails\",\n                              \"Very strong fair (a=50, b=50)\" = \"strong_fair\")),\n\n      sliderInput(\"n_flips\", \"Number of flips:\",\n                  min = 1, max = 200, value = 10, step = 1),\n\n      actionButton(\"go\", \"Flip coins\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(12, plotOutput(\"posterior_plot\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  get_prior &lt;- function(prior) {\n    switch(prior,\n      \"uniform\"      = c(a = 1, b = 1),\n      \"skeptical\"    = c(a = 5, b = 5),\n      \"heads\"        = c(a = 8, b = 2),\n      \"tails\"        = c(a = 2, b = 8),\n      \"strong_fair\"  = c(a = 50, b = 50)\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go\n    true_p  &lt;- input$true_p\n    prior   &lt;- input$prior\n    n_flips &lt;- input$n_flips\n\n    ab &lt;- get_prior(prior)\n    a0 &lt;- ab[\"a\"]; b0 &lt;- ab[\"b\"]\n\n    # Generate flips\n    flips &lt;- rbinom(n_flips, 1, true_p)\n    heads &lt;- sum(flips)\n    tails &lt;- n_flips - heads\n\n    # Posterior parameters\n    a_post &lt;- a0 + heads\n    b_post &lt;- b0 + tails\n\n    list(a0 = a0, b0 = b0, a_post = a_post, b_post = b_post,\n         true_p = true_p, n_flips = n_flips, heads = heads,\n         tails = tails, flips = flips)\n  })\n\n  output$posterior_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    x &lt;- seq(0, 1, length.out = 500)\n    y_prior &lt;- dbeta(x, d$a0, d$b0)\n    y_post  &lt;- dbeta(x, d$a_post, d$b_post)\n\n    ylim &lt;- c(0, max(c(y_prior, y_post)) * 1.1)\n\n    plot(x, y_post, type = \"l\", lwd = 3, col = \"#3498db\",\n         xlab = expression(\"Coin bias (\" * theta * \")\"),\n         ylab = \"Density\",\n         main = paste0(\"Bayesian updating after \", d$n_flips,\n                      \" flips (\", d$heads, \"H, \", d$tails, \"T)\"),\n         ylim = ylim)\n\n    # Prior\n    lines(x, y_prior, lwd = 2, col = \"#95a5a6\", lty = 2)\n\n    # Shade posterior\n    polygon(c(0, x, 1), c(0, y_post, 0),\n            col = adjustcolor(\"#3498db\", 0.2), border = NA)\n\n    # True value\n    abline(v = d$true_p, lwd = 2.5, col = \"#e74c3c\", lty = 2)\n\n    # Posterior mean\n    post_mean &lt;- d$a_post / (d$a_post + d$b_post)\n    abline(v = post_mean, lwd = 2, col = \"#27ae60\", lty = 3)\n\n    # 95% credible interval\n    ci_lo &lt;- qbeta(0.025, d$a_post, d$b_post)\n    ci_hi &lt;- qbeta(0.975, d$a_post, d$b_post)\n    x_ci &lt;- seq(ci_lo, ci_hi, length.out = 200)\n    y_ci &lt;- dbeta(x_ci, d$a_post, d$b_post)\n    polygon(c(ci_lo, x_ci, ci_hi), c(0, y_ci, 0),\n            col = adjustcolor(\"#3498db\", 0.3), border = NA)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Prior\",\n                      \"Posterior\",\n                      \"95% credible interval\",\n                      paste0(\"True \\u03b8 = \", d$true_p),\n                      paste0(\"Posterior mean = \", round(post_mean, 3))),\n           col = c(\"#95a5a6\", \"#3498db\",\n                   adjustcolor(\"#3498db\", 0.4),\n                   \"#e74c3c\", \"#27ae60\"),\n           lwd = c(2, 3, 8, 2.5, 2),\n           lty = c(2, 1, 1, 2, 3))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    post_mean &lt;- d$a_post / (d$a_post + d$b_post)\n    ci_lo &lt;- qbeta(0.025, d$a_post, d$b_post)\n    ci_hi &lt;- qbeta(0.975, d$a_post, d$b_post)\n\n    prior_mean &lt;- d$a0 / (d$a0 + d$b0)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Prior:&lt;/b&gt; Beta(\", d$a0, \", \", d$b0, \")&lt;br&gt;\",\n        \"Prior mean: \", round(prior_mean, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Data:&lt;/b&gt; \", d$heads, \"H / \", d$tails, \"T&lt;br&gt;\",\n        \"MLE: \", round(d$heads / d$n_flips, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Posterior:&lt;/b&gt; Beta(\", d$a_post, \", \", d$b_post, \")&lt;br&gt;\",\n        \"Post. mean: \", round(post_mean, 3), \"&lt;br&gt;\",\n        \"95% CrI: [\", round(ci_lo, 3), \", \", round(ci_hi, 3), \"]&lt;br&gt;\",\n        \"&lt;b&gt;True \\u03b8:&lt;/b&gt; \", d$true_p\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Bayesian Thinking",
      "Bayesian Updating"
    ]
  },
  {
    "objectID": "bayesian-updating.html#simulation-2-prior-sensitivity-different-priors-same-data",
    "href": "bayesian-updating.html#simulation-2-prior-sensitivity-different-priors-same-data",
    "title": "Bayesian Updating: One Gentle Introduction",
    "section": "Simulation 2: Prior sensitivity — different priors, same data",
    "text": "Simulation 2: Prior sensitivity — different priors, same data\nSame coin, same flips, but different starting beliefs. With little data, the prior matters a lot. With enough data, all priors converge to the same posterior. The data overwhelms the prior.\n#| standalone: true\n#| viewerHeight: 580\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_p2\", \"True coin bias:\",\n                  min = 0.1, max = 0.9, value = 0.7, step = 0.05),\n\n      sliderInput(\"n_flips2\", \"Number of flips:\",\n                  min = 1, max = 500, value = 10, step = 1),\n\n      actionButton(\"go2\", \"Flip coins\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"sensitivity_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go2\n    true_p  &lt;- input$true_p2\n    n_flips &lt;- input$n_flips2\n\n    # Generate one set of flips (shared across priors)\n    flips &lt;- rbinom(n_flips, 1, true_p)\n    heads &lt;- sum(flips)\n    tails &lt;- n_flips - heads\n\n    priors &lt;- list(\n      \"Uniform (1,1)\"        = c(1, 1),\n      \"Fair-biased (10,10)\"  = c(10, 10),\n      \"Heads-biased (8,2)\"   = c(8, 2),\n      \"Tails-biased (2,8)\"   = c(2, 8),\n      \"Strong fair (50,50)\"  = c(50, 50)\n    )\n\n    list(priors = priors, heads = heads, tails = tails,\n         true_p = true_p, n_flips = n_flips)\n  })\n\n  output$sensitivity_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    x &lt;- seq(0, 1, length.out = 500)\n    cols &lt;- c(\"#3498db\", \"#e74c3c\", \"#27ae60\", \"#9b59b6\", \"#e67e22\")\n\n    # Find y range\n    ymax &lt;- 0\n    for (i in seq_along(d$priors)) {\n      ab &lt;- d$priors[[i]]\n      y &lt;- dbeta(x, ab[1] + d$heads, ab[2] + d$tails)\n      ymax &lt;- max(ymax, max(y))\n    }\n\n    plot(NULL, xlim = c(0, 1), ylim = c(0, ymax * 1.1),\n         xlab = expression(\"Coin bias (\" * theta * \")\"),\n         ylab = \"Posterior density\",\n         main = paste0(\"5 priors, same data (\",\n                      d$heads, \"H / \", d$tails, \"T out of \",\n                      d$n_flips, \" flips)\"))\n\n    for (i in seq_along(d$priors)) {\n      ab &lt;- d$priors[[i]]\n      y &lt;- dbeta(x, ab[1] + d$heads, ab[2] + d$tails)\n      lines(x, y, lwd = 2.5, col = cols[i])\n    }\n\n    abline(v = d$true_p, lty = 2, lwd = 2.5, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(names(d$priors),\n                      paste0(\"True \\u03b8 = \", d$true_p)),\n           col = c(cols, \"#2c3e50\"),\n           lwd = c(rep(2.5, 5), 2.5),\n           lty = c(rep(1, 5), 2))\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- dat()\n\n    lines &lt;- sapply(seq_along(d$priors), function(i) {\n      ab &lt;- d$priors[[i]]\n      a_post &lt;- ab[1] + d$heads\n      b_post &lt;- ab[2] + d$tails\n      post_mean &lt;- a_post / (a_post + b_post)\n      paste0(names(d$priors)[i], \": \",\n             round(post_mean, 3))\n    })\n\n    mle &lt;- d$heads / d$n_flips\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Posterior means:&lt;/b&gt;&lt;br&gt;\",\n        paste(lines, collapse = \"&lt;br&gt;\"), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;MLE:&lt;/b&gt; \", round(mle, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;True \\u03b8:&lt;/b&gt; \", d$true_p, \"&lt;br&gt;\",\n        \"&lt;small&gt;With more flips, all posteriors&lt;br&gt;\",\n        \"converge to the MLE.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nSim 1, 5 flips: the posterior is wide and heavily influenced by the prior. The 95% credible interval is broad.\nSim 1, 100 flips: the posterior is sharp and centered near the true bias. The prior barely matters anymore.\nSim 1, strong fair prior (a=50, b=50) with true bias = 0.7: the prior pulls the posterior toward 0.5. You need many flips to overcome a strong prior.\nSim 2, 10 flips: the five posteriors are spread apart — the prior matters a lot.\nSim 2, 200 flips: all five posteriors are nearly identical and centered on the true value. The data overwhelms the prior.\nSim 2, slide from 1 to 500 flips: watch the posteriors converge in real time. This is the key Bayesian result — with enough data, the prior washes out.\n\n\n\nThe bottom line\n\nBayesian inference = prior × likelihood → posterior. That’s the entire framework.\nThe prior encodes what you know before seeing data. It can be informative (based on previous studies) or vague (letting the data speak).\nThe posterior is your updated belief. It’s a full distribution, not just a point estimate — you get a complete picture of uncertainty.\nWith enough data, the prior doesn’t matter. All reasonable priors converge to the same posterior. This is reassuring: Bayesian inference isn’t “subjective” in the long run.\nThe credible interval has the interpretation people want from a confidence interval: “There’s a 95% probability that \\(\\theta\\) is in this interval.” But this requires accepting the Bayesian framework (and a prior).\n\n\n\n\nDid you know?\n\nThomas Bayes was an English Presbyterian minister who never published his theorem. His friend Richard Price found the manuscript after Bayes’ death and published it in 1763 as “An Essay towards solving a Problem in the Doctrine of Chances.” Bayes’ original example was essentially the coin flipping problem in this simulation.\nBayesian methods were largely abandoned in the 20th century because they were computationally intractable — computing posteriors for realistic problems required integrals that couldn’t be solved analytically. The revival came with Markov Chain Monte Carlo (MCMC) algorithms in the 1990s, which made it possible to sample from posteriors instead of computing them exactly.\nPierre-Simon Laplace independently discovered Bayes’ theorem and used it extensively — including to estimate the mass of Saturn, the probability that the sun would rise tomorrow, and the bias of a coin. Laplace was arguably the first true Bayesian; Bayes himself only scratched the surface.\nThis page is a teaser. The full Bayesian course covers MCMC, hierarchical models, prior selection, and model comparison in depth.",
    "crumbs": [
      "Bayesian Thinking",
      "Bayesian Updating"
    ]
  },
  {
    "objectID": "multiple-testing.html",
    "href": "multiple-testing.html",
    "title": "Multiple Testing & the Replication Crisis",
    "section": "",
    "text": "If you test one null hypothesis at \\(\\alpha = 0.05\\), there’s a 5% chance of a false positive. But if you test 20 null hypotheses, the chance that at least one is falsely significant is:\n\\[P(\\text{at least one false positive}) = 1 - (1 - 0.05)^{20} \\approx 0.64\\]\nThat’s a 64% chance of finding something “significant” even when nothing is real. This is the multiple testing problem, and it’s at the heart of the replication crisis.\n\n\n\nA researcher tests 20 outcome variables and reports the one with p &lt; 0.05\nA genetics study tests 500,000 SNPs for association with a disease\nAn A/B test checks conversions, clicks, revenue, time on page, bounce rate…\nA psychology experiment tests the effect on 10 different mood scales\n\nIn each case, the nominal \\(\\alpha = 0.05\\) no longer controls the actual false positive rate. You need a correction.",
    "crumbs": [
      "Inference",
      "Multiple Testing"
    ]
  },
  {
    "objectID": "multiple-testing.html#the-problem-test-enough-things-and-something-will-be-significant",
    "href": "multiple-testing.html#the-problem-test-enough-things-and-something-will-be-significant",
    "title": "Multiple Testing & the Replication Crisis",
    "section": "",
    "text": "If you test one null hypothesis at \\(\\alpha = 0.05\\), there’s a 5% chance of a false positive. But if you test 20 null hypotheses, the chance that at least one is falsely significant is:\n\\[P(\\text{at least one false positive}) = 1 - (1 - 0.05)^{20} \\approx 0.64\\]\nThat’s a 64% chance of finding something “significant” even when nothing is real. This is the multiple testing problem, and it’s at the heart of the replication crisis.\n\n\n\nA researcher tests 20 outcome variables and reports the one with p &lt; 0.05\nA genetics study tests 500,000 SNPs for association with a disease\nAn A/B test checks conversions, clicks, revenue, time on page, bounce rate…\nA psychology experiment tests the effect on 10 different mood scales\n\nIn each case, the nominal \\(\\alpha = 0.05\\) no longer controls the actual false positive rate. You need a correction.",
    "crumbs": [
      "Inference",
      "Multiple Testing"
    ]
  },
  {
    "objectID": "multiple-testing.html#simulation-1-false-discoveries-from-multiple-tests",
    "href": "multiple-testing.html#simulation-1-false-discoveries-from-multiple-tests",
    "title": "Multiple Testing & the Replication Crisis",
    "section": "Simulation 1: False discoveries from multiple tests",
    "text": "Simulation 1: False discoveries from multiple tests\nTest 20 true null hypotheses at \\(\\alpha = 0.05\\). Watch how many come up “significant” purely by chance. Repeat to see the pattern.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_tests\", \"Number of hypotheses tested:\",\n                  min = 5, max = 50, value = 20, step = 5),\n\n      sliderInput(\"n_obs\", \"Sample size per test:\",\n                  min = 20, max = 200, value = 50, step = 10),\n\n      sliderInput(\"alpha\", HTML(\"&alpha; (per-test):\"),\n                  min = 0.01, max = 0.10, value = 0.05, step = 0.01),\n\n      sliderInput(\"n_real\", \"Number with real effects:\",\n                  min = 0, max = 10, value = 0, step = 1),\n\n      actionButton(\"go\", \"Run tests\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"pval_plot\", height = \"520px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    m     &lt;- input$n_tests\n    n     &lt;- input$n_obs\n    alpha &lt;- input$alpha\n    n_real &lt;- min(input$n_real, m)\n\n    p_vals &lt;- numeric(m)\n    is_real &lt;- rep(FALSE, m)\n\n    for (i in seq_len(m)) {\n      if (i &lt;= n_real) {\n        # Real effect (d = 0.5)\n        x &lt;- rnorm(n, mean = 0.5, sd = 1)\n        is_real[i] &lt;- TRUE\n      } else {\n        # Null is true\n        x &lt;- rnorm(n, mean = 0, sd = 1)\n      }\n      p_vals[i] &lt;- t.test(x, mu = 0)$p.value\n    }\n\n    # Bonferroni correction\n    bonf_alpha &lt;- alpha / m\n    bonf_reject &lt;- p_vals &lt; bonf_alpha\n\n    # BH (FDR) correction\n    sorted_idx &lt;- order(p_vals)\n    sorted_p &lt;- p_vals[sorted_idx]\n    bh_threshold &lt;- (seq_len(m) / m) * alpha\n    bh_max &lt;- max(c(0, which(sorted_p &lt;= bh_threshold)))\n    bh_reject &lt;- rep(FALSE, m)\n    if (bh_max &gt; 0) bh_reject[sorted_idx[seq_len(bh_max)]] &lt;- TRUE\n\n    list(p_vals = p_vals, is_real = is_real, m = m, n = n,\n         alpha = alpha, n_real = n_real,\n         raw_reject = p_vals &lt; alpha,\n         bonf_reject = bonf_reject,\n         bh_reject = bh_reject,\n         bonf_alpha = bonf_alpha)\n  })\n\n  output$pval_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 6, 3, 1))\n\n    ord &lt;- order(d$p_vals)\n    p_sorted &lt;- d$p_vals[ord]\n    real_sorted &lt;- d$is_real[ord]\n\n    cols &lt;- ifelse(real_sorted, \"#3498db\", \"#95a5a6\")\n    pch_vals &lt;- ifelse(real_sorted, 17, 16)\n\n    plot(p_sorted, seq_len(d$m), pch = pch_vals, cex = 1.5,\n         col = cols, xlim = c(0, 1),\n         xlab = \"p-value\", ylab = \"Test (sorted by p-value)\",\n         main = paste0(d$m, \" hypothesis tests (\", d$n_real,\n                      \" real effects)\"),\n         yaxt = \"n\")\n    axis(2, at = seq_len(d$m), labels = seq_len(d$m),\n         las = 1, cex.axis = 0.7)\n\n    # Alpha threshold\n    abline(v = d$alpha, lty = 2, lwd = 2, col = \"#e74c3c\")\n    text(d$alpha + 0.02, d$m, paste0(\"\\u03b1 = \", d$alpha),\n         col = \"#e74c3c\", cex = 0.8, adj = 0)\n\n    # Bonferroni threshold\n    abline(v = d$bonf_alpha, lty = 2, lwd = 2, col = \"#e67e22\")\n    text(d$bonf_alpha + 0.02, d$m - 1,\n         paste0(\"Bonf = \", round(d$bonf_alpha, 4)),\n         col = \"#e67e22\", cex = 0.8, adj = 0)\n\n    # BH line\n    bh_line &lt;- (seq_len(d$m) / d$m) * d$alpha\n    lines(bh_line[ord], seq_len(d$m), lty = 3, lwd = 2, col = \"#27ae60\")\n\n    # Highlight rejections\n    raw_sig &lt;- which(p_sorted &lt; d$alpha)\n    if (length(raw_sig) &gt; 0) {\n      points(p_sorted[raw_sig], raw_sig, pch = 1, cex = 2.5,\n             col = \"#e74c3c\", lwd = 2)\n    }\n\n    legend(\"bottomright\", bty = \"n\", cex = 0.8,\n           legend = c(\n             ifelse(d$n_real &gt; 0, \"Real effect\", \"\"),\n             \"Null (no effect)\",\n             paste0(\"Uncorrected \\u03b1 = \", d$alpha),\n             paste0(\"Bonferroni \\u03b1 = \", round(d$bonf_alpha, 4)),\n             \"BH (FDR) threshold\"\n           )[c(d$n_real &gt; 0, TRUE, TRUE, TRUE, TRUE)],\n           col = c(\n             if (d$n_real &gt; 0) \"#3498db\",\n             \"#95a5a6\", \"#e74c3c\", \"#e67e22\", \"#27ae60\"\n           ),\n           pch = c(\n             if (d$n_real &gt; 0) 17,\n             16, NA, NA, NA\n           ),\n           lwd = c(\n             if (d$n_real &gt; 0) NA,\n             NA, 2, 2, 2\n           ),\n           lty = c(\n             if (d$n_real &gt; 0) NA,\n             NA, 2, 2, 3\n           ))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n\n    raw_fp &lt;- sum(d$raw_reject & !d$is_real)\n    bonf_fp &lt;- sum(d$bonf_reject & !d$is_real)\n    bh_fp &lt;- sum(d$bh_reject & !d$is_real)\n\n    raw_tp &lt;- sum(d$raw_reject & d$is_real)\n    bonf_tp &lt;- sum(d$bonf_reject & d$is_real)\n    bh_tp &lt;- sum(d$bh_reject & d$is_real)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Uncorrected:&lt;/b&gt;&lt;br&gt;\",\n        \"&nbsp; Reject: \", sum(d$raw_reject),\n        \" (FP: &lt;span class='bad'&gt;\", raw_fp, \"&lt;/span&gt;\",\n        if (d$n_real &gt; 0) paste0(\", TP: \", raw_tp), \")&lt;br&gt;\",\n        \"&lt;b&gt;Bonferroni:&lt;/b&gt;&lt;br&gt;\",\n        \"&nbsp; Reject: \", sum(d$bonf_reject),\n        \" (FP: \", bonf_fp,\n        if (d$n_real &gt; 0) paste0(\", TP: \", bonf_tp), \")&lt;br&gt;\",\n        \"&lt;b&gt;BH (FDR):&lt;/b&gt;&lt;br&gt;\",\n        \"&nbsp; Reject: \", sum(d$bh_reject),\n        \" (FP: \", bh_fp,\n        if (d$n_real &gt; 0) paste0(\", TP: \", bh_tp), \")&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;FP = false positive&lt;br&gt;\",\n        \"TP = true positive&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Inference",
      "Multiple Testing"
    ]
  },
  {
    "objectID": "multiple-testing.html#simulation-2-correction-methods-compared",
    "href": "multiple-testing.html#simulation-2-correction-methods-compared",
    "title": "Multiple Testing & the Replication Crisis",
    "section": "Simulation 2: Correction methods compared",
    "text": "Simulation 2: Correction methods compared\nRun the multiple testing experiment many times to see the long-run performance of each correction method. Track the family-wise error rate (FWER: any false positive?) and false discovery rate (FDR: what fraction of discoveries are false?).\n#| standalone: true\n#| viewerHeight: 520\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"m2\", \"Number of tests:\",\n                  min = 5, max = 50, value = 20, step = 5),\n\n      sliderInput(\"n_real2\", \"Tests with real effects:\",\n                  min = 0, max = 10, value = 3, step = 1),\n\n      sliderInput(\"sims2\", \"Simulations:\",\n                  min = 200, max = 1000, value = 500, step = 100),\n\n      actionButton(\"go2\", \"Run simulations\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"compare_plot\", height = \"420px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go2\n    m      &lt;- input$m2\n    n_real &lt;- min(input$n_real2, m)\n    sims   &lt;- input$sims2\n    n      &lt;- 50\n    alpha  &lt;- 0.05\n\n    fwer &lt;- c(raw = 0, bonf = 0, bh = 0)\n    power &lt;- c(raw = 0, bonf = 0, bh = 0)\n    fdr_sum &lt;- c(raw = 0, bonf = 0, bh = 0)\n    fdr_count &lt;- c(raw = 0, bonf = 0, bh = 0)\n\n    for (s in seq_len(sims)) {\n      p_vals &lt;- numeric(m)\n      is_real &lt;- rep(FALSE, m)\n\n      for (i in seq_len(m)) {\n        if (i &lt;= n_real) {\n          x &lt;- rnorm(n, mean = 0.5, sd = 1)\n          is_real[i] &lt;- TRUE\n        } else {\n          x &lt;- rnorm(n, mean = 0, sd = 1)\n        }\n        p_vals[i] &lt;- t.test(x, mu = 0)$p.value\n      }\n\n      # Raw\n      raw_rej &lt;- p_vals &lt; alpha\n\n      # Bonferroni\n      bonf_rej &lt;- p_vals &lt; (alpha / m)\n\n      # BH\n      sorted_idx &lt;- order(p_vals)\n      sorted_p &lt;- p_vals[sorted_idx]\n      bh_thresh &lt;- (seq_len(m) / m) * alpha\n      bh_max &lt;- max(c(0, which(sorted_p &lt;= bh_thresh)))\n      bh_rej &lt;- rep(FALSE, m)\n      if (bh_max &gt; 0) bh_rej[sorted_idx[seq_len(bh_max)]] &lt;- TRUE\n\n      for (method in list(list(\"raw\", raw_rej),\n                          list(\"bonf\", bonf_rej),\n                          list(\"bh\", bh_rej))) {\n        nm &lt;- method[[1]]\n        rej &lt;- method[[2]]\n\n        # FWER\n        if (any(rej & !is_real)) fwer[nm] &lt;- fwer[nm] + 1\n\n        # Power (if any real effects)\n        if (n_real &gt; 0) {\n          power[nm] &lt;- power[nm] + sum(rej & is_real)\n        }\n\n        # FDR\n        if (sum(rej) &gt; 0) {\n          fdr_sum[nm] &lt;- fdr_sum[nm] + sum(rej & !is_real) / sum(rej)\n          fdr_count[nm] &lt;- fdr_count[nm] + 1\n        }\n      }\n    }\n\n    fwer &lt;- fwer / sims\n    if (n_real &gt; 0) power &lt;- power / (sims * n_real)\n    fdr &lt;- ifelse(fdr_count &gt; 0, fdr_sum / fdr_count, 0)\n\n    list(fwer = fwer, power = power, fdr = fdr,\n         m = m, n_real = n_real, sims = sims)\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 6), xpd = TRUE)\n\n    methods &lt;- c(\"Uncorrected\", \"Bonferroni\", \"BH (FDR)\")\n    cols &lt;- c(\"#e74c3c\", \"#e67e22\", \"#27ae60\")\n    x_pos &lt;- seq_along(methods)\n\n    if (d$n_real &gt; 0) {\n      # Show both FWER and power\n      mat &lt;- rbind(d$fwer * 100, d$power * 100)\n      bp &lt;- barplot(mat, beside = TRUE, names.arg = methods,\n                    col = c(\"#e74c3c80\", \"#3498db80\"),\n                    border = c(\"#e74c3c\", \"#3498db\"),\n                    ylim = c(0, 100),\n                    ylab = \"Rate (%)\",\n                    main = paste0(\"FWER & Power (\", d$sims,\n                                 \" simulations, \", d$n_real,\n                                 \" real effects)\"))\n\n      abline(h = 5, lty = 2, lwd = 2, col = \"#7f8c8d\")\n      text(max(bp) + 1, 7, \"\\u03b1 = 5%\", cex = 0.8, col = \"#7f8c8d\")\n\n      legend(\"topright\", inset = c(-0.18, 0), bty = \"n\", cex = 0.85,\n             legend = c(\"FWER\", \"Power\"),\n             fill = c(\"#e74c3c80\", \"#3498db80\"),\n             border = c(\"#e74c3c\", \"#3498db\"))\n    } else {\n      bp &lt;- barplot(d$fwer * 100, names.arg = methods,\n                    col = cols, border = NA,\n                    ylim = c(0, max(d$fwer * 100) * 1.3),\n                    ylab = \"Family-wise error rate (%)\",\n                    main = paste0(\"FWER under H\\u2080 (\",\n                                 d$sims, \" simulations)\"))\n\n      abline(h = 5, lty = 2, lwd = 2, col = \"#7f8c8d\")\n      text(max(bp) + 0.3, 7, \"Target: 5%\", cex = 0.8, col = \"#7f8c8d\")\n\n      text(bp, d$fwer * 100 + 2,\n           paste0(round(d$fwer * 100, 1), \"%\"), cex = 0.9)\n    }\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- dat()\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;FWER (any false positive):&lt;/b&gt;&lt;br&gt;\",\n        \"&nbsp; Uncorrected: &lt;span class='bad'&gt;\",\n        round(d$fwer[\"raw\"] * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&nbsp; Bonferroni: &lt;span class='good'&gt;\",\n        round(d$fwer[\"bonf\"] * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&nbsp; BH: \", round(d$fwer[\"bh\"] * 100, 1), \"%&lt;br&gt;\",\n        if (d$n_real &gt; 0) paste0(\n          \"&lt;hr style='margin:8px 0'&gt;\",\n          \"&lt;b&gt;Power (detect real effects):&lt;/b&gt;&lt;br&gt;\",\n          \"&nbsp; Uncorrected: \", round(d$power[\"raw\"] * 100, 1), \"%&lt;br&gt;\",\n          \"&nbsp; Bonferroni: \", round(d$power[\"bonf\"] * 100, 1), \"%&lt;br&gt;\",\n          \"&nbsp; BH: \", round(d$power[\"bh\"] * 100, 1), \"%\"\n        ) else \"\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\n20 tests, 0 real effects: uncorrected FWER is ~64%. Bonferroni brings it to ~5%. BH is somewhere in between.\n20 tests, 3 real effects: Bonferroni controls FWER tightly but has lower power. BH controls the false discovery rate (what fraction of your discoveries are wrong) while maintaining better power.\n50 tests, 0 real effects: uncorrected FWER is over 90%. The more you test, the worse it gets.\nSim 1 with 0 real effects: click “Run tests” repeatedly. Each time, a different set of null hypotheses appears “significant.” This is exactly how researchers accidentally find spurious results.\n\n\n\nWhen to use which correction\n\n\n\n\n\n\n\n\nMethod\nControls\nBest for\n\n\n\n\nNone\nPer-comparison \\(\\alpha\\)\nSingle pre-registered hypothesis\n\n\nBonferroni\nFWER (any false positive)\nSafety-critical decisions; few tests\n\n\nBH (Benjamini-Hochberg)\nFDR (false discovery proportion)\nExploratory analysis; many tests\n\n\nPre-registration\nEverything\nSpecify your hypothesis before seeing data\n\n\n\n\n\nThe bottom line\n\nIf you test enough things, something will be significant. The question is whether it’s real.\nBonferroni is conservative — it controls the probability of any false positive but sacrifices power.\nBH is more permissive — it controls the proportion of discoveries that are false, giving you better power.\nThe best correction is not testing everything. Pre-register your primary hypothesis and keep the number of tests small.\n\n\n\n\nDid you know?\n\nThe XKCD comic “Significant” perfectly illustrates the multiple testing problem: researchers test whether jelly beans cause acne, testing 20 different colors. Green jelly beans come up significant at p &lt; 0.05 — and the newspaper headline reads “Green Jelly Beans Linked to Acne!”\nJohn Ioannidis published “Why Most Published Research Findings Are False” in 2005. His argument: when studies are underpowered, when many hypotheses are tested, and when researchers have flexibility in their analysis, the majority of “significant” findings are likely false positives. The paper has been cited over 10,000 times and helped launch the replication crisis movement.\nBenjamini and Hochberg published their FDR procedure in 1995. It was initially met with skepticism — why would you allow some false discoveries? But in fields like genomics, where you test thousands of genes simultaneously, controlling the proportion of false discoveries turns out to be much more practical (and powerful) than trying to prevent any single false positive.",
    "crumbs": [
      "Inference",
      "Multiple Testing"
    ]
  },
  {
    "objectID": "residuals.html",
    "href": "residuals.html",
    "title": "Residuals & Controls",
    "section": "",
    "text": "A residual is what’s left over after your model has done its best:\n\\[e_i = Y_i - \\hat{Y}_i\\]\nIt’s the vertical distance between the actual data point and the regression line. If your model is good, residuals should look like random noise — no patterns, no structure.\nIf the residuals do have structure, your model is missing something. This is the single most important diagnostic in regression.",
    "crumbs": [
      "Regression & Diagnostics",
      "Residuals & Controls"
    ]
  },
  {
    "objectID": "residuals.html#what-is-a-residual",
    "href": "residuals.html#what-is-a-residual",
    "title": "Residuals & Controls",
    "section": "",
    "text": "A residual is what’s left over after your model has done its best:\n\\[e_i = Y_i - \\hat{Y}_i\\]\nIt’s the vertical distance between the actual data point and the regression line. If your model is good, residuals should look like random noise — no patterns, no structure.\nIf the residuals do have structure, your model is missing something. This is the single most important diagnostic in regression.",
    "crumbs": [
      "Regression & Diagnostics",
      "Residuals & Controls"
    ]
  },
  {
    "objectID": "residuals.html#residuals-and-the-cef",
    "href": "residuals.html#residuals-and-the-cef",
    "title": "Residuals & Controls",
    "section": "Residuals and the CEF",
    "text": "Residuals and the CEF\nRecall from the CEF page: OLS is the best linear approximation to \\(E[Y \\mid X]\\). When the CEF is nonlinear, the residuals absorb the nonlinearity. That’s why a curved pattern in the residual plot signals model misspecification — the residuals are doing the work the model should be doing.\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .info-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.8;\n    }\n    .info-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"dgp\", \"True relationship:\",\n                  choices = c(\"Linear (correct spec)\",\n                              \"Quadratic (misspecified)\",\n                              \"Heteroskedastic\",\n                              \"Outliers\")),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 50, max = 500, value = 200, step = 50),\n\n      sliderInput(\"sigma\", \"Noise (SD):\",\n                  min = 0.5, max = 4, value = 1.5, step = 0.5),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"info\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(4, plotOutput(\"scatter\", height = \"380px\")),\n        column(4, plotOutput(\"resid_fitted\", height = \"380px\")),\n        column(4, plotOutput(\"resid_hist\", height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n\n    sigma &lt;- input$sigma\n    dgp   &lt;- input$dgp\n\n    x &lt;- runif(n, -3, 5)\n\n    if (dgp == \"Linear (correct spec)\") {\n      y &lt;- 2 + 1.5 * x + rnorm(n, sd = sigma)\n    } else if (dgp == \"Quadratic (misspecified)\") {\n      y &lt;- 1 + 0.5 * x - 0.3 * x^2 + rnorm(n, sd = sigma)\n    } else if (dgp == \"Heteroskedastic\") {\n      y &lt;- 2 + 1.5 * x + rnorm(n, sd = sigma * (0.3 + 0.4 * abs(x)))\n    } else {\n      y &lt;- 2 + 1.5 * x + rnorm(n, sd = sigma)\n      # Add outliers\n      outlier_idx &lt;- sample(n, 5)\n      y[outlier_idx] &lt;- y[outlier_idx] + sample(c(-1, 1), 5, replace = TRUE) * 8\n    }\n\n    fit &lt;- lm(y ~ x)\n    list(x = x, y = y, fit = fit, dgp = dgp)\n  })\n\n  output$scatter &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$x, d$y, pch = 16, col = \"#3498db80\", cex = 0.7,\n         xlab = \"X\", ylab = \"Y\", main = \"Data + OLS fit\")\n    abline(d$fit, col = \"#e74c3c\", lwd = 2.5)\n  })\n\n  output$resid_fitted &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    r  &lt;- resid(d$fit)\n    fv &lt;- fitted(d$fit)\n\n    plot(fv, r, pch = 16, col = \"#9b59b680\", cex = 0.7,\n         xlab = \"Fitted values\", ylab = \"Residuals\",\n         main = \"Residuals vs Fitted\")\n    abline(h = 0, lty = 2, col = \"gray40\", lwd = 1.5)\n\n    lo &lt;- loess(r ~ fv)\n    ox &lt;- order(fv)\n    lines(fv[ox], predict(lo)[ox], col = \"#e74c3c\", lwd = 2)\n  })\n\n  output$resid_hist &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    r &lt;- resid(d$fit)\n    hist(r, breaks = 30, probability = TRUE,\n         col = \"#dae8fc\", border = \"#6c8ebf\",\n         main = \"Residual Distribution\",\n         xlab = \"Residuals\", ylab = \"Density\")\n    x_seq &lt;- seq(min(r), max(r), length.out = 200)\n    lines(x_seq, dnorm(x_seq, mean = 0, sd = sd(r)),\n          col = \"#e74c3c\", lwd = 2)\n  })\n\n  output$info &lt;- renderUI({\n    d &lt;- dat()\n    r &lt;- resid(d$fit)\n\n    msg &lt;- switch(d$dgp,\n      \"Linear (correct spec)\" =\n        \"Residuals look like random noise. No patterns in the residual plot. The model is correctly specified.\",\n      \"Quadratic (misspecified)\" =\n        \"U-shaped pattern in residuals! The LOESS curve bends, revealing the quadratic structure OLS is missing.\",\n      \"Heteroskedastic\" =\n        \"Fan shape: residuals spread out as fitted values increase. The variance isn't constant (heteroskedasticity).\",\n      \"Outliers\" =\n        \"A few points have huge residuals. Check the histogram for heavy tails. These points have outsized influence on the fit.\"\n    )\n\n    tags$div(class = \"info-box\", HTML(paste0(\"&lt;b&gt;Diagnosis:&lt;/b&gt;&lt;br&gt;\", msg)))\n  })\n}\n\nshinyApp(ui, server)\n\nReading the three panels\n\nLeft — Scatter + fit: does the line follow the data?\nMiddle — Residuals vs fitted: the key diagnostic. If you see a pattern (curve, fan, clusters), your model is missing something.\nRight — Residual histogram: should look roughly normal and centered at 0. Heavy tails or skew signal problems.\n\nSwitch between the four DGPs above and learn to recognize each pattern — you’ll see these in every applied paper you read.",
    "crumbs": [
      "Regression & Diagnostics",
      "Residuals & Controls"
    ]
  },
  {
    "objectID": "residuals.html#controls-and-residuals",
    "href": "residuals.html#controls-and-residuals",
    "title": "Residuals & Controls",
    "section": "Controls and residuals",
    "text": "Controls and residuals\nWhen you “control for” a variable in a regression, what you’re really doing is removing its influence via residuals. This is the Frisch-Waugh-Lovell theorem in action (see the FWL page).\nAdding \\(X_2\\) as a control means:\n\nRegress \\(Y\\) on \\(X_2\\) → residuals \\(\\tilde{Y}\\) (variation in \\(Y\\) not explained by \\(X_2\\))\nRegress \\(X_1\\) on \\(X_2\\) → residuals \\(\\tilde{X}_1\\) (variation in \\(X_1\\) not explained by \\(X_2\\))\nRegress \\(\\tilde{Y}\\) on \\(\\tilde{X}_1\\) → the coefficient is \\(\\hat{\\beta}_1\\)\n\n“Controlling for \\(X_2\\)” = looking at the relationship between \\(Y\\) and \\(X_1\\) after removing what \\(X_2\\) explains about each.\n#| standalone: true\n#| viewerHeight: 580\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; }\n    .bad  { color: #e74c3c; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\", min = 100, max = 500, value = 300, step = 50),\n\n      sliderInput(\"b1\", HTML(\"True &beta;&lt;sub&gt;1&lt;/sub&gt;:\"),\n                  min = -2, max = 3, value = 0.5, step = 0.25),\n\n      sliderInput(\"b2\", HTML(\"True &beta;&lt;sub&gt;2&lt;/sub&gt; (confounder effect):\"),\n                  min = -3, max = 3, value = 2, step = 0.25),\n\n      sliderInput(\"rho\", HTML(\"Corr(X&lt;sub&gt;1&lt;/sub&gt;, X&lt;sub&gt;2&lt;/sub&gt;):\"),\n                  min = -0.9, max = 0.9, value = 0.7, step = 0.1),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(4, plotOutput(\"raw_plot\",  height = \"380px\")),\n        column(4, plotOutput(\"ctrl_y\",    height = \"380px\")),\n        column(4, plotOutput(\"ctrl_both\", height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    b1  &lt;- input$b1\n    b2  &lt;- input$b2\n    rho &lt;- input$rho\n\n    z1 &lt;- rnorm(n)\n    z2 &lt;- rnorm(n)\n    x1 &lt;- z1\n    x2 &lt;- rho * z1 + sqrt(1 - rho^2) * z2\n    y  &lt;- b1 * x1 + b2 * x2 + rnorm(n)\n\n    # Without control\n    naive_fit &lt;- lm(y ~ x1)\n    naive_b1 &lt;- coef(naive_fit)[2]\n\n    # With control\n    full_fit &lt;- lm(y ~ x1 + x2)\n    full_b1 &lt;- coef(full_fit)[2]\n\n    # FWL residuals\n    ey &lt;- resid(lm(y ~ x2))\n    ex &lt;- resid(lm(x1 ~ x2))\n\n    list(x1 = x1, x2 = x2, y = y, ey = ey, ex = ex,\n         naive_b1 = naive_b1, full_b1 = full_b1,\n         b1 = b1, b2 = b2, rho = rho)\n  })\n\n  output$raw_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$x1, d$y, pch = 16, col = \"#3498db60\", cex = 0.6,\n         xlab = expression(X[1]), ylab = \"Y\",\n         main = \"No control (omit X2)\")\n    abline(lm(d$y ~ d$x1), col = \"#e74c3c\", lwd = 2.5)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = paste0(\"Slope = \", round(d$naive_b1, 3)))\n  })\n\n  output$ctrl_y &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$x1, d$ey, pch = 16, col = \"#9b59b660\", cex = 0.6,\n         xlab = expression(X[1]),\n         ylab = expression(\"Residualized Y  (\" * tilde(Y) * \")\"),\n         main = expression(\"Remove \" * X[2] * \" from Y\"))\n    abline(h = 0, lty = 2, col = \"gray60\")\n  })\n\n  output$ctrl_both &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$ex, d$ey, pch = 16, col = \"#27ae6080\", cex = 0.6,\n         xlab = expression(\"Residualized \" * X[1] * \"  (\" * tilde(X)[1] * \")\"),\n         ylab = expression(\"Residualized Y  (\" * tilde(Y) * \")\"),\n         main = \"After controlling for X2\")\n    abline(lm(d$ey ~ d$ex), col = \"#e74c3c\", lwd = 2.5)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = paste0(\"Slope = \", round(d$full_b1, 3)))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$naive_b1 - d$b1\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True &beta;&lt;sub&gt;1&lt;/sub&gt;:&lt;/b&gt; \", d$b1, \"&lt;br&gt;\",\n        \"&lt;b&gt;Without control:&lt;/b&gt; &lt;span class='bad'&gt;\",\n        round(d$naive_b1, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(bias, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;With control:&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$full_b1, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;Omitted variable bias = &beta;&lt;sub&gt;2&lt;/sub&gt; &times; \",\n        \"corr(X&lt;sub&gt;1&lt;/sub&gt;,X&lt;sub&gt;2&lt;/sub&gt;) / ... &lt;br&gt;\",\n        \"Higher confounding (&beta;&lt;sub&gt;2&lt;/sub&gt;) + higher correlation \",\n        \"= more bias when you don't control.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nTrue \\(\\beta_1\\) = 0.5, Corr = 0.7, \\(\\beta_2\\) = 2: the naive slope (left panel) is way off because \\(X_2\\) confounds the relationship. The controlled slope (right panel) recovers the truth.\nSet Corr = 0: no confounding. Both estimates are the same — controls don’t help when there’s nothing to control for.\nSet \\(\\beta_2\\) = 0: same thing. Even if \\(X_1\\) and \\(X_2\\) are correlated, \\(X_2\\) doesn’t affect \\(Y\\), so omitting it doesn’t bias \\(\\beta_1\\).\nMiddle panel: shows what \\(Y\\) looks like after removing \\(X_2\\)’s contribution. The right panel adds the second step — removing \\(X_2\\) from \\(X_1\\) too — which isolates the independent variation in \\(X_1\\).\n\n\n\n\nDid you know?\n\nCarl Friedrich Gauss invented the method of least squares at age 18 in 1795 to predict the orbit of the asteroid Ceres. When Ceres reappeared exactly where Gauss predicted, he became an overnight celebrity. The entire method is built on minimizing the sum of squared residuals.\nAdrien-Marie Legendre independently published the method in 1805, before Gauss. But Gauss claimed he’d been using it since 1795. The priority dispute was never fully resolved — but we call it “Gaussian” anyway.\nThe idea of “controlling for” a variable sounds scientific, but it’s been criticized. As Edward Leamer wrote in his famous 1983 paper “Let’s Take the Con Out of Econometrics”: adding controls is not the same as running an experiment. The choice of what to control for is often arbitrary and can introduce more bias than it removes (see: bad controls, collider bias).",
    "crumbs": [
      "Regression & Diagnostics",
      "Residuals & Controls"
    ]
  },
  {
    "objectID": "pvalues-ci.html",
    "href": "pvalues-ci.html",
    "title": "p-values, Confidence Intervals & What They Actually Mean",
    "section": "",
    "text": "These are two separate steps, and confusing them is where most misunderstanding begins.\nStep 1 — Estimation is about computing numbers from your data:\n\nYou collect a sample and calculate a point estimate — your best guess at the true parameter. For example, \\(\\hat{\\beta} = -5\\) or \\(\\bar{x} = 12.3\\).\nYou also calculate a standard error (SE) — how much that estimate would bounce around if you repeated the experiment. A small SE means your estimate is precise; a large SE means it’s noisy.\n\nEstimation gives you a number. It does not tell you what to conclude.\nStep 2 — Inference is about drawing conclusions from those numbers:\n\nIs the effect real, or could it be noise? → p-value\nWhat range of values is plausible for the true parameter? → confidence interval\nBoth are built from the same two ingredients: the estimate and its SE.\n\n\\[\\text{test statistic} = \\frac{\\text{estimate}}{\\text{SE}} \\qquad \\qquad \\text{CI} = \\text{estimate} \\pm 1.96 \\times \\text{SE}\\]\nEverything on this page is Step 2. If the estimation step is wrong (biased estimate, wrong SE), then the inference is wrong too — no matter how sophisticated the test. Good inference starts with good estimation.",
    "crumbs": [
      "Inference",
      "p-values & Confidence Intervals"
    ]
  },
  {
    "objectID": "pvalues-ci.html#first-estimation.-then-inference.",
    "href": "pvalues-ci.html#first-estimation.-then-inference.",
    "title": "p-values, Confidence Intervals & What They Actually Mean",
    "section": "",
    "text": "These are two separate steps, and confusing them is where most misunderstanding begins.\nStep 1 — Estimation is about computing numbers from your data:\n\nYou collect a sample and calculate a point estimate — your best guess at the true parameter. For example, \\(\\hat{\\beta} = -5\\) or \\(\\bar{x} = 12.3\\).\nYou also calculate a standard error (SE) — how much that estimate would bounce around if you repeated the experiment. A small SE means your estimate is precise; a large SE means it’s noisy.\n\nEstimation gives you a number. It does not tell you what to conclude.\nStep 2 — Inference is about drawing conclusions from those numbers:\n\nIs the effect real, or could it be noise? → p-value\nWhat range of values is plausible for the true parameter? → confidence interval\nBoth are built from the same two ingredients: the estimate and its SE.\n\n\\[\\text{test statistic} = \\frac{\\text{estimate}}{\\text{SE}} \\qquad \\qquad \\text{CI} = \\text{estimate} \\pm 1.96 \\times \\text{SE}\\]\nEverything on this page is Step 2. If the estimation step is wrong (biased estimate, wrong SE), then the inference is wrong too — no matter how sophisticated the test. Good inference starts with good estimation.",
    "crumbs": [
      "Inference",
      "p-values & Confidence Intervals"
    ]
  },
  {
    "objectID": "pvalues-ci.html#part-1-p-values",
    "href": "pvalues-ci.html#part-1-p-values",
    "title": "p-values, Confidence Intervals & What They Actually Mean",
    "section": "Part 1: p-values",
    "text": "Part 1: p-values\n\nWhat is a test statistic?\nBefore we can talk about p-values, we need the test statistic — a single number that measures how far your estimate is from what the null hypothesis predicts, in units of standard error.\nThe most common one is the z-statistic (or t-statistic for small samples):\n\\[z = \\frac{\\bar{x} - \\mu_0}{\\text{SE}} = \\frac{\\text{estimate} - \\text{null value}}{\\text{standard error}}\\]\nIt answers: “How many standard errors is my estimate from the null?” A \\(z\\) of 2 means your estimate is 2 SEs away from what H₀ predicts — that’s unusual enough to start doubting H₀.\n\n\nWhat a p-value actually is\nThe “p” stands for probability.\n\nThe p-value is the probability of getting a test statistic as large as (or larger than) yours, assuming the null hypothesis is true.\n\nThat’s it. Large \\(|z|\\) → small p-value → more evidence against H₀.\nIt is not:\n\nThe probability that H₀ is true\nThe probability that you made a mistake\nThe probability that the result will replicate\n\nIt’s a statement about the data, not about the hypothesis. The p-value asks: “If there really were no effect, how surprising would my data be?”\n\n\nA concrete example\nSay you’re testing whether a drug lowers blood pressure. You run a regression and get \\(\\hat{\\beta} = -5\\) (blood pressure drops 5 points).\nH₀: \\(\\beta = 0\\) — the drug does nothing.\nThe p-value asks: if the drug truly does nothing (\\(\\beta = 0\\)), what’s the probability of observing \\(\\hat{\\beta}\\) as extreme as \\(-5\\) or more?\np = 0.03 means: there’s only a 3% chance of seeing an estimate this large purely from random noise. So either:\n\nH₀ is true and you got unlucky (3% chance), or\nH₀ is false — the drug actually works\n\nYou reject H₀ because 3% feels too unlikely to be just noise.\nBut the p-value never tells you \\(\\beta\\)’s actual value. It only tells you the data would be surprising if \\(\\beta\\) were exactly zero. This matters:\n\np = 0.03 with \\(\\hat{\\beta} = -5\\) → probably a real, meaningful effect\np = 0.03 with \\(\\hat{\\beta} = -0.001\\) and \\(n = 10{,}000{,}000\\) → “statistically significant” but practically meaningless\n\nThat’s why you should always look at the estimate (\\(\\hat{\\beta}\\)) and confidence interval together with the p-value — not just whether p &lt; 0.05.\n\n\nSimulation: The p-value machine\nDraw a sample, compute a test statistic, and see where it falls on the null distribution. The shaded area is the p-value. Under H₀, p-values are uniformly distributed — every value between 0 and 1 is equally likely.\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 10, max = 200, value = 30, step = 5),\n\n      sliderInput(\"true_mu\", HTML(\"True &mu; (for data generation):\"),\n                  min = -1, max = 1, value = 0, step = 0.1),\n\n      sliderInput(\"reps\", \"Number of experiments:\",\n                  min = 100, max = 2000, value = 500, step = 100),\n\n      helpText(\"Set true \\u03bc = 0 to see p-values under H\\u2080.\n               Set it away from 0 to see p-values under H\\u2081.\"),\n\n      actionButton(\"go\", \"Run experiments\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"null_dist\", height = \"400px\")),\n        column(6, plotOutput(\"pval_hist\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n      &lt;- input$n\n    mu     &lt;- input$true_mu\n    reps   &lt;- input$reps\n\n    # Draw one sample for display\n    one_sample &lt;- rnorm(n, mean = mu, sd = 1)\n    one_z &lt;- mean(one_sample) / (1 / sqrt(n))\n    one_p &lt;- 2 * pnorm(-abs(one_z))\n\n    # Draw many samples\n    z_stats &lt;- replicate(reps, {\n      x &lt;- rnorm(n, mean = mu, sd = 1)\n      mean(x) / (1 / sqrt(n))\n    })\n    p_vals &lt;- 2 * pnorm(-abs(z_stats))\n\n    reject_rate &lt;- mean(p_vals &lt; 0.05)\n\n    list(one_z = one_z, one_p = one_p, z_stats = z_stats, p_vals = p_vals,\n         n = n, mu = mu, reps = reps, reject_rate = reject_rate)\n  })\n\n  output$null_dist &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    x &lt;- seq(-4, 4, length.out = 300)\n    y &lt;- dnorm(x)\n\n    plot(x, y, type = \"l\", lwd = 2.5, col = \"#2c3e50\",\n         xlab = \"Test statistic (z)\", ylab = \"Density\",\n         main = \"Null distribution & your test statistic\")\n\n    # Shade p-value area (two-tailed)\n    z_abs &lt;- abs(d$one_z)\n    if (z_abs &lt; 4) {\n      x_right &lt;- seq(z_abs, 4, length.out = 100)\n      polygon(c(z_abs, x_right, 4), c(0, dnorm(x_right), 0),\n              col = adjustcolor(\"#e74c3c\", 0.4), border = NA)\n      x_left &lt;- seq(-4, -z_abs, length.out = 100)\n      polygon(c(-4, x_left, -z_abs), c(0, dnorm(x_left), 0),\n              col = adjustcolor(\"#e74c3c\", 0.4), border = NA)\n    }\n\n    abline(v = d$one_z, lwd = 2.5, col = \"#3498db\", lty = 1)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Null distribution (H\\u2080)\",\n                      paste0(\"Your z = \", round(d$one_z, 3)),\n                      paste0(\"p-value = \", round(d$one_p, 4))),\n           col = c(\"#2c3e50\", \"#3498db\", adjustcolor(\"#e74c3c\", 0.6)),\n           lwd = c(2.5, 2.5, 8), lty = c(1, 1, 1))\n  })\n\n  output$pval_hist &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$p_vals, breaks = 20, probability = TRUE,\n         col = \"#3498db30\", border = \"#3498db80\",\n         main = paste0(\"Distribution of p-values (\",\n                       d$reps, \" experiments)\"),\n         xlab = \"p-value\", ylab = \"Density\",\n         xlim = c(0, 1))\n\n    if (abs(d$mu) &lt; 0.001) {\n      abline(h = 1, lty = 2, lwd = 2, col = \"#e74c3c\")\n      legend(\"topright\", bty = \"n\", cex = 0.85,\n             legend = c(\"p-value histogram\",\n                        \"Uniform(0,1) reference\"),\n             col = c(\"#3498db80\", \"#e74c3c\"),\n             lwd = c(8, 2), lty = c(1, 2))\n    } else {\n      legend(\"topright\", bty = \"n\", cex = 0.85,\n             legend = paste0(\"Under H\\u2081 (\\u03bc = \", d$mu, \"):\\n\",\n                            \"p-values pile up near 0\"))\n    }\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;This sample:&lt;/b&gt;&lt;br&gt;\",\n        \"z = \", round(d$one_z, 3), \"&lt;br&gt;\",\n        \"p = \", round(d$one_p, 4), \" \",\n        ifelse(d$one_p &lt; 0.05,\n               \"&lt;span class='bad'&gt;(&lt; 0.05)&lt;/span&gt;\",\n               \"&lt;span class='good'&gt;(\\u2265 0.05)&lt;/span&gt;\"), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Across \", d$reps, \" experiments:&lt;/b&gt;&lt;br&gt;\",\n        \"Rejection rate: \", round(d$reject_rate * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;small&gt;\",\n        ifelse(abs(d$mu) &lt; 0.001,\n               \"Under H\\u2080, this should be ~5%.\",\n               paste0(\"Under H\\u2081, this is the power.\")),\n        \"&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nTrue \\(\\mu\\) = 0: p-values are uniform — flat histogram. This is the defining property of a valid test. About 5% of p-values fall below 0.05 purely by chance.\nTrue \\(\\mu\\) = 0.5: p-values pile up near 0. The further \\(\\mu\\) is from 0, the more powerful the test, and the smaller the p-values tend to be.",
    "crumbs": [
      "Inference",
      "p-values & Confidence Intervals"
    ]
  },
  {
    "objectID": "pvalues-ci.html#part-2-confidence-intervals",
    "href": "pvalues-ci.html#part-2-confidence-intervals",
    "title": "p-values, Confidence Intervals & What They Actually Mean",
    "section": "Part 2: Confidence intervals",
    "text": "Part 2: Confidence intervals\n\nWhat a confidence interval actually is\n\nA 95% CI is constructed by a procedure that, in repeated sampling, captures the true parameter 95% of the time.\n\nIt is not:\n\nA 95% probability that the true parameter is inside this particular interval\nA range where 95% of the data falls\nA range where 95% of sample means fall\n\nOnce you compute a specific CI, the true parameter is either in it or it isn’t — there’s no probability about it. The 95% refers to the method, not to any single interval.\n\n\nHow a CI is constructed\nThe recipe is simple. You need three ingredients:\n\nA point estimate — your best guess (e.g., \\(\\bar{x}\\) or \\(\\hat{\\beta}\\))\nA standard error — how much the estimate bounces around due to sampling\nA critical value — how many SEs to go out for your desired confidence level\n\nThe formula for a 95% CI for a mean:\n\\[\\text{CI} = \\bar{x} \\pm z_{0.975} \\times \\text{SE} = \\bar{x} \\pm 1.96 \\times \\frac{s}{\\sqrt{n}}\\]\nThat’s it: estimate ± margin of error. The margin of error is just a scaled-up standard error.\n\n\n\nConfidence level\nCritical value (\\(z\\))\nMargin of error\n\n\n\n\n90%\n1.645\nNarrower CI\n\n\n95%\n1.960\nStandard\n\n\n99%\n2.576\nWider CI\n\n\n\nWhere does the 1.96 come from? The middle 95% of a standard normal distribution falls between \\(-1.96\\) and \\(+1.96\\). So if the sampling distribution of \\(\\bar{x}\\) is approximately normal (CLT), going out 1.96 SEs in each direction captures the true mean 95% of the time.\nFor small samples (roughly \\(n &lt; 30\\)), replace the \\(z\\) critical value with a \\(t\\) critical value from the \\(t\\)-distribution with \\(n - 1\\) degrees of freedom. The \\(t\\)-distribution has heavier tails, making the CI wider to account for the extra uncertainty in estimating the SE from small samples. As \\(n\\) grows, the \\(t\\) and \\(z\\) values converge.\nThe key insight: a CI is wide when (a) the data is noisy (large \\(s\\)), (b) the sample is small (small \\(n\\)), or (c) you demand more confidence (larger critical value). You can’t have precision, small samples, and high confidence all at once — pick two.\n\n\nSimulation: CI coverage\nDraw 100 confidence intervals. Each one either contains the true \\(\\mu\\) (blue) or misses it (red). Over many experiments, about 95% contain \\(\\mu\\) — but any single CI either does or doesn’t. The 95% is about the procedure, not about your interval.\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n2\", \"Sample size (n):\",\n                  min = 5, max = 100, value = 25, step = 5),\n\n      sliderInput(\"conf\", \"Confidence level:\",\n                  min = 0.80, max = 0.99, value = 0.95, step = 0.01),\n\n      sliderInput(\"n_ci\", \"Number of CIs to draw:\",\n                  min = 20, max = 200, value = 100, step = 10),\n\n      actionButton(\"go2\", \"Draw new CIs\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"ci_plot\", height = \"550px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go2\n    n     &lt;- input$n2\n    conf  &lt;- input$conf\n    n_ci  &lt;- input$n_ci\n    mu    &lt;- 5  # true mean\n    sigma &lt;- 2\n\n    z &lt;- qnorm(1 - (1 - conf) / 2)\n\n    results &lt;- t(replicate(n_ci, {\n      x &lt;- rnorm(n, mean = mu, sd = sigma)\n      xbar &lt;- mean(x)\n      se &lt;- sd(x) / sqrt(n)\n      lo &lt;- xbar - z * se\n      hi &lt;- xbar + z * se\n      c(xbar = xbar, lo = lo, hi = hi, covers = (lo &lt;= mu & mu &lt;= hi))\n    }))\n\n    list(results = results, mu = mu, n = n, conf = conf, n_ci = n_ci)\n  })\n\n  output$ci_plot &lt;- renderPlot({\n    d &lt;- dat()\n    res &lt;- d$results\n    n_ci &lt;- d$n_ci\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    covers &lt;- as.logical(res[, \"covers\"])\n    cols &lt;- ifelse(covers, \"#3498db\", \"#e74c3c\")\n\n    plot(NULL, xlim = range(res[, c(\"lo\", \"hi\")]),\n         ylim = c(0.5, n_ci + 0.5),\n         xlab = expression(\"Value of \" * mu),\n         ylab = \"Experiment number\",\n         main = paste0(n_ci, \" Confidence Intervals (\",\n                       d$conf * 100, \"% level)\"),\n         yaxt = \"n\")\n\n    # Draw CIs\n    segments(res[, \"lo\"], seq_len(n_ci), res[, \"hi\"], seq_len(n_ci),\n             col = cols, lwd = 1.5)\n    points(res[, \"xbar\"], seq_len(n_ci), pch = 16, cex = 0.5, col = cols)\n\n    # True mu\n    abline(v = d$mu, lty = 2, lwd = 2.5, col = \"#2c3e50\")\n\n    n_miss &lt;- sum(!covers)\n    legend(\"topright\", bty = \"n\", cex = 0.9,\n           legend = c(paste0(\"Contains \\u03bc (\", sum(covers), \")\"),\n                      paste0(\"Misses \\u03bc (\", n_miss, \")\"),\n                      paste0(\"True \\u03bc = \", d$mu)),\n           col = c(\"#3498db\", \"#e74c3c\", \"#2c3e50\"),\n           lwd = c(3, 3, 2.5), lty = c(1, 1, 2))\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- dat()\n    covers &lt;- as.logical(d$results[, \"covers\"])\n    cover_rate &lt;- mean(covers)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Coverage rate:&lt;/b&gt; &lt;span class='\",\n        ifelse(abs(cover_rate - d$conf) &lt; 0.05, \"good\", \"bad\"), \"'&gt;\",\n        round(cover_rate * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Target:&lt;/b&gt; \", d$conf * 100, \"%&lt;br&gt;\",\n        \"&lt;b&gt;Missed:&lt;/b&gt; \", sum(!covers), \" of \", d$n_ci, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;Each red line is a CI that&lt;br&gt;\",\n        \"does not contain the true \\u03bc.&lt;br&gt;\",\n        \"The method gets it right ~\",\n        d$conf * 100, \"% of the time.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\n95% level: roughly 5 out of 100 CIs miss \\(\\mu\\) (shown in red). Click “Draw new CIs” several times — the exact number varies, but it averages to 5%.\nChange confidence to 99%: fewer CIs miss \\(\\mu\\), but each CI is wider. There’s always a tradeoff between coverage and precision.\nSmall n (n = 5): the CIs are very wide. With little data, you can’t be precise. This is the price of honesty.",
    "crumbs": [
      "Inference",
      "p-values & Confidence Intervals"
    ]
  },
  {
    "objectID": "pvalues-ci.html#the-bottom-line",
    "href": "pvalues-ci.html#the-bottom-line",
    "title": "p-values, Confidence Intervals & What They Actually Mean",
    "section": "The bottom line",
    "text": "The bottom line\n\nA p-value is not the probability the null is true. It’s the probability of seeing data this extreme if the null were true.\nA confidence interval is not a probability statement about \\(\\mu\\). It’s a statement about the procedure: if you repeated the experiment many times, 95% of your CIs would contain \\(\\mu\\).\nBoth concepts are statements about long-run frequency, not about any single experiment. This is what makes frequentist inference subtle — and why Bayesian methods (which can make probability statements about parameters) are appealing to many.\n\n\n\nDid you know?\n\nThe p-value was popularized by R.A. Fisher in the 1920s as an informal measure of evidence against the null. But the rigid “reject if p &lt; 0.05” framework came from Jerzy Neyman and Egon Pearson in the 1930s. Fisher and Neyman-Pearson fundamentally disagreed about what statistical testing means, and the debate was never resolved — we use an awkward hybrid of both frameworks to this day.\nIn 2016, the American Statistical Association issued its first-ever formal statement on p-values, warning against common misinterpretations. Statement #2: “P-values do not measure the probability that the studied hypothesis is true.”\nConfidence intervals were invented by Neyman in 1937. He was explicit that the 95% refers to the procedure, not to any single interval. He wrote: “It is not possible to say that the probability of the true value falling in any particular interval is 95%.”",
    "crumbs": [
      "Inference",
      "p-values & Confidence Intervals"
    ]
  },
  {
    "objectID": "power.html",
    "href": "power.html",
    "title": "Power, Alpha, Beta & MDE",
    "section": "",
    "text": "You run an experiment to test whether some treatment works. There are only four things that can happen:\n\n\n\n\n\n\n\n\n\nTreatment does nothing (H₀ true)\nTreatment works (H₁ true)\n\n\n\n\nYou say “no effect”\nCorrect\nType II error (miss it) — probability = \\(\\beta\\)\n\n\nYou say “it works!”\nType I error (false alarm) — probability = \\(\\alpha\\)\nCorrect — probability = Power = \\(1 - \\beta\\)\n\n\n\nThat’s it. Everything on this page is about these four cells.",
    "crumbs": [
      "Inference",
      "Power, Alpha, Beta & MDE"
    ]
  },
  {
    "objectID": "power.html#the-big-picture",
    "href": "power.html#the-big-picture",
    "title": "Power, Alpha, Beta & MDE",
    "section": "",
    "text": "You run an experiment to test whether some treatment works. There are only four things that can happen:\n\n\n\n\n\n\n\n\n\nTreatment does nothing (H₀ true)\nTreatment works (H₁ true)\n\n\n\n\nYou say “no effect”\nCorrect\nType II error (miss it) — probability = \\(\\beta\\)\n\n\nYou say “it works!”\nType I error (false alarm) — probability = \\(\\alpha\\)\nCorrect — probability = Power = \\(1 - \\beta\\)\n\n\n\nThat’s it. Everything on this page is about these four cells.",
    "crumbs": [
      "Inference",
      "Power, Alpha, Beta & MDE"
    ]
  },
  {
    "objectID": "power.html#what-are-alpha-and-beta",
    "href": "power.html#what-are-alpha-and-beta",
    "title": "Power, Alpha, Beta & MDE",
    "section": "What are \\(\\alpha\\) and \\(\\beta\\)?",
    "text": "What are \\(\\alpha\\) and \\(\\beta\\)?\n\\(\\alpha\\) (alpha) is how often you cry wolf. You set this before the experiment — typically 0.05. It’s the false positive rate: the chance you declare “it works!” when the treatment actually does nothing.\n\\(\\beta\\) (beta) is how often you miss a real effect. If the treatment genuinely works, \\(\\beta\\) is the probability you shrug and say “no effect.” You want this to be small.\nPower = \\(1 - \\beta\\) is the flip side: the probability you correctly detect a real effect. Convention is to aim for 0.80 (80%).\n\nThe two-distribution picture\nThe key insight is that there are two worlds — one where the treatment does nothing (null), and one where it has an effect (alternative). Each world gives you a different sampling distribution for your test statistic:\n\nUnder the null, the distribution is centered at 0 (no effect).\nUnder the alternative, the distribution is shifted by the true effect size.\n\nYou pick a critical value (the cutoff). If your test statistic lands past it, you reject H₀. The simulation below shows both distributions. Drag the sliders and watch how \\(\\alpha\\), \\(\\beta\\), and power change.\n#| standalone: true\n#| viewerHeight: 580\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"effect\", \"True effect size (d):\",\n                  min = 0, max = 2, value = 0.5, step = 0.05),\n\n      sliderInput(\"n\", \"Sample size per group (n):\",\n                  min = 10, max = 500, value = 50, step = 10),\n\n      sliderInput(\"alpha\", HTML(\"&alpha; (significance level):\"),\n                  min = 0.01, max = 0.10, value = 0.05, step = 0.01),\n\n      uiOutput(\"results_box\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"dist_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  vals &lt;- reactive({\n    d     &lt;- input$effect\n    n     &lt;- input$n\n    alpha &lt;- input$alpha\n\n    se    &lt;- sqrt(2 / n)          # SE of difference in means (sigma=1 each group)\n    shift &lt;- d / se               # noncentrality (in SE units)\n    crit  &lt;- qnorm(1 - alpha)     # one-sided critical value\n\n    power &lt;- 1 - pnorm(crit - shift)\n    beta  &lt;- 1 - power\n\n    list(se = se, shift = shift, crit = crit,\n         power = power, beta = beta, alpha = alpha, d = d, n = n)\n  })\n\n  output$dist_plot &lt;- renderPlot({\n    v &lt;- vals()\n\n    xmin &lt;- min(-4, v$shift - 4)\n    xmax &lt;- max(4, v$shift + 4)\n    x &lt;- seq(xmin, xmax, length.out = 500)\n\n    y_null &lt;- dnorm(x)\n    y_alt  &lt;- dnorm(x, mean = v$shift)\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(x, y_null, type = \"l\", lwd = 2.5, col = \"#2c3e50\",\n         xlab = \"Test statistic (z)\", ylab = \"Density\",\n         main = \"Null vs Alternative Distribution\",\n         ylim = c(0, max(y_null, y_alt) * 1.15),\n         xlim = c(xmin, xmax))\n    lines(x, y_alt, lwd = 2.5, col = \"#3498db\")\n\n    # Critical value line\n    abline(v = v$crit, lty = 2, lwd = 2, col = \"#7f8c8d\")\n\n    # Shade alpha region (right tail of null beyond crit)\n    x_alpha &lt;- seq(v$crit, xmax, length.out = 200)\n    polygon(c(v$crit, x_alpha, xmax),\n            c(0, dnorm(x_alpha), 0),\n            col = adjustcolor(\"#e74c3c\", 0.35), border = NA)\n\n    # Shade beta region (left part of alternative, below crit)\n    x_beta &lt;- seq(xmin, v$crit, length.out = 200)\n    polygon(c(xmin, x_beta, v$crit),\n            c(0, dnorm(x_beta, mean = v$shift), 0),\n            col = adjustcolor(\"#f39c12\", 0.35), border = NA)\n\n    # Shade power region (right part of alternative, beyond crit)\n    x_pow &lt;- seq(v$crit, xmax, length.out = 200)\n    polygon(c(v$crit, x_pow, xmax),\n            c(0, dnorm(x_pow, mean = v$shift), 0),\n            col = adjustcolor(\"#2ecc71\", 0.35), border = NA)\n\n    # Labels\n    legend(\"topleft\", bty = \"n\", cex = 0.9,\n           legend = c(\n             expression(\"Null distribution (H\"[0]*\": no effect)\"),\n             expression(\"Alternative distribution (H\"[1]*\": effect exists)\"),\n             \"Critical value\",\n             expression(alpha * \" (false positive)\"),\n             expression(beta * \" (miss / Type II)\"),\n             \"Power (correct detection)\"\n           ),\n           col = c(\"#2c3e50\", \"#3498db\", \"#7f8c8d\",\n                   adjustcolor(\"#e74c3c\", 0.6),\n                   adjustcolor(\"#f39c12\", 0.6),\n                   adjustcolor(\"#2ecc71\", 0.6)),\n           lwd = c(2.5, 2.5, 2, 8, 8, 8),\n           lty = c(1, 1, 2, 1, 1, 1))\n  })\n\n  output$results_box &lt;- renderUI({\n    v &lt;- vals()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;&alpha;:&lt;/b&gt; \", v$alpha, \"&lt;br&gt;\",\n        \"&lt;b&gt;&beta;:&lt;/b&gt; \", round(v$beta, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Power:&lt;/b&gt; \", round(v$power, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Effect (d):&lt;/b&gt; \", v$d, \"&lt;br&gt;\",\n        \"&lt;b&gt;n per group:&lt;/b&gt; \", v$n, \"&lt;br&gt;\",\n        \"&lt;b&gt;Critical z:&lt;/b&gt; \", round(v$crit, 2)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nSet effect = 0 and watch: there is no alternative distribution to detect. Any rejection is a false positive.\nSet effect = 0.5 with n = 20: power is low. Now slide n up — power climbs. This is why sample size matters.\nSet n = 200 and shrink the effect toward 0: even large samples struggle to detect tiny effects.\nLower \\(\\alpha\\) from 0.05 to 0.01: the critical value moves right, \\(\\alpha\\) shrinks, but \\(\\beta\\) grows. There’s always a tradeoff between false positives and false negatives.",
    "crumbs": [
      "Inference",
      "Power, Alpha, Beta & MDE"
    ]
  },
  {
    "objectID": "power.html#minimum-detectable-effect-mde",
    "href": "power.html#minimum-detectable-effect-mde",
    "title": "Power, Alpha, Beta & MDE",
    "section": "Minimum Detectable Effect (MDE)",
    "text": "Minimum Detectable Effect (MDE)\nWhen planning an experiment, you often ask: “Given my sample size, what’s the smallest effect I can reliably detect?” That’s the MDE.\nIt depends on three things: sample size (\\(n\\)), significance level (\\(\\alpha\\)), and desired power (\\(1 - \\beta\\)). The formula for a two-sample test with equal groups is:\n\\[\n\\text{MDE} = (z_{1-\\alpha} + z_{1-\\beta}) \\times \\sqrt{\\frac{2}{n}}\n\\]\nNotice: that \\(\\sqrt{2/n}\\) is just the standard error of the difference in means. So MDE is really just a scaled-up SE:\n\\[MDE = (z_{1-\\alpha} + z_{1-\\beta}) \\times SE\\]\nThe critical values (~2.8 for 5% significance and 80% power) are fixed multipliers. The only thing you control is the SE — by increasing \\(n\\) or reducing \\(\\sigma\\) (through better measurement, stratification, or controls). Power analysis is really just an SE calculation in disguise. See Variance, SD & Standard Error for more on this connection.\nLarger \\(n\\) shrinks the SE, which shrinks the MDE. Higher power demands a larger MDE (or more \\(n\\)).\n#| standalone: true\n#| viewerHeight: 480\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .mde-box {\n      background: #eaf2f8; border-radius: 6px; padding: 16px;\n      margin-top: 14px; font-size: 15px; line-height: 2;\n      text-align: center;\n    }\n    .mde-box .big { font-size: 28px; color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n2\", \"Sample size per group (n):\",\n                  min = 10, max = 1000, value = 100, step = 10),\n\n      sliderInput(\"alpha2\", HTML(\"&alpha;:\"),\n                  min = 0.01, max = 0.10, value = 0.05, step = 0.01),\n\n      sliderInput(\"power2\", \"Desired power:\",\n                  min = 0.50, max = 0.95, value = 0.80, step = 0.05),\n\n      uiOutput(\"mde_box\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"mde_curve\", height = \"400px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  output$mde_curve &lt;- renderPlot({\n    alpha &lt;- input$alpha2\n    power &lt;- input$power2\n    n_now &lt;- input$n2\n\n    ns &lt;- seq(10, 1000, by = 5)\n    mdes &lt;- (qnorm(1 - alpha) + qnorm(power)) * sqrt(2 / ns)\n\n    mde_now &lt;- (qnorm(1 - alpha) + qnorm(power)) * sqrt(2 / n_now)\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(ns, mdes, type = \"l\", lwd = 2.5, col = \"#3498db\",\n         xlab = \"Sample size per group (n)\",\n         ylab = \"MDE (standardized effect size)\",\n         main = paste0(\"MDE curve (\\u03b1 = \", alpha, \", power = \", power, \")\"),\n         ylim = c(0, max(mdes)))\n\n    # Highlight current n\n    points(n_now, mde_now, pch = 19, cex = 2, col = \"#e74c3c\")\n    segments(n_now, 0, n_now, mde_now, lty = 2, col = \"#e74c3c\")\n    segments(0, mde_now, n_now, mde_now, lty = 2, col = \"#e74c3c\")\n\n    text(n_now + 30, mde_now + 0.02,\n         paste0(\"MDE = \", round(mde_now, 3)),\n         col = \"#e74c3c\", cex = 0.95, adj = 0)\n  })\n\n  output$mde_box &lt;- renderUI({\n    alpha &lt;- input$alpha2\n    power &lt;- input$power2\n    n_now &lt;- input$n2\n    mde &lt;- (qnorm(1 - alpha) + qnorm(power)) * sqrt(2 / n_now)\n\n    tags$div(class = \"mde-box\",\n      HTML(paste0(\n        \"With &lt;b&gt;n = \", n_now, \"&lt;/b&gt; per group,&lt;br&gt;\",\n        \"you can detect effects as small as:&lt;br&gt;\",\n        \"&lt;span class='big'&gt;d = \", round(mde, 3), \"&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThe intuition\n\nMDE is your experiment’s resolution. A microscope can’t see atoms; your experiment can’t see effects smaller than the MDE.\nMore data (larger \\(n\\)) = sharper microscope = smaller MDE.\nIf you need to detect a 1% lift in click-through rate but your MDE is 3%, your experiment is pointless — you’ll almost certainly miss it even if the effect is real.\nIn practice: figure out the smallest effect that would matter for your decision, then compute the \\(n\\) needed to detect it.\n\n\n\n\nDid you know?\n\nJacob Cohen, the psychologist who popularized power analysis, found in 1962 that the median power of studies in behavioral science journals was only 0.48 — meaning most studies had less than a coin-flip chance of detecting the effects they were looking for. He spent the rest of his career trying to fix this. His book Statistical Power Analysis (1969) remains a classic.\nCohen’s famous effect size conventions (small = 0.2, medium = 0.5, large = 0.8) were meant as rough guides, not rigid rules. He later regretted that people treated them as gospel: “My intent was that d = 0.5 represents a medium effect… it does not mean that 0.5 is a medium effect in your field.”\nThe replication crisis in psychology and medicine is largely a power problem. Underpowered studies that happen to find significant results are published; the many more that find nothing are filed away. This is publication bias, and it’s a direct consequence of running experiments without power calculations.",
    "crumbs": [
      "Inference",
      "Power, Alpha, Beta & MDE"
    ]
  },
  {
    "objectID": "bootstrap.html",
    "href": "bootstrap.html",
    "title": "The Bootstrap",
    "section": "",
    "text": "You want a confidence interval for some statistic (mean, median, regression coefficient), but you don’t know the sampling distribution. Maybe the formula is complicated. Maybe there is no formula.\nThe bootstrap says: pretend your sample is the population. Then simulate the sampling process by resampling with replacement from your data, over and over. Each resample gives you a new estimate. The distribution of those estimates approximates the true sampling distribution.\n\n\n\nYou have a sample of \\(n\\) observations.\nDraw \\(n\\) observations with replacement from your sample (some points get picked twice, some not at all).\nCompute your statistic on this resample.\nRepeat B times (typically 1,000–10,000).\nThe spread of those B estimates gives you a standard error and a confidence interval.\n\nThat’s it. No formulas, no distributional assumptions. Just resampling.\n\n\n\nNo — and this is a crucial distinction. There are two different distributions at play:\n\nThe distribution of the data = what individual observations look like (their histogram). This tells you about spread, skew, outliers in your raw data.\nThe sampling distribution = what your statistic (mean, median, slope) would look like if you could repeat the entire experiment many times. This is what you need for standard errors and confidence intervals.\n\nYou can always plot your data. But you cannot see the sampling distribution — you only ran the experiment once. You have one mean, not a distribution of means.\nThe bootstrap bridges that gap. It simulates the “what if I repeated this experiment?” question using only the data you have. The left panel below shows your data distribution. The right panel shows the bootstrap sampling distribution. Notice: they look completely different. Your data might be skewed and spread out, but the sampling distribution of the mean is narrow and roughly normal (thanks to the CLT). The bootstrap discovers this for you without any formulas.\n#| standalone: true\n#| viewerHeight: 900\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"stat\", \"Statistic to bootstrap:\",\n                  choices = c(\"Mean\", \"Median\", \"SD\",\n                              \"Correlation\", \"Regression slope\")),\n\n      selectInput(\"pop\", \"Population shape:\",\n                  choices = c(\"Normal\", \"Skewed (exponential)\",\n                              \"Heavy-tailed\", \"Bimodal\")),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 10, max = 200, value = 30, step = 10),\n\n      sliderInput(\"B\", \"Bootstrap resamples (B):\",\n                  min = 100, max = 5000, value = 1000, step = 100),\n\n      actionButton(\"go\", \"New sample + bootstrap\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"sample_plot\", height = \"350px\")),\n        column(6, plotOutput(\"boot_plot\", height = \"350px\"))\n      ),\n      fluidRow(\n        column(12, plotOutput(\"compare_plot\", height = \"350px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  draw_pop &lt;- function(n, pop) {\n    switch(pop,\n      \"Normal\" = rnorm(n, mean = 5, sd = 2),\n      \"Skewed (exponential)\" = rexp(n, rate = 0.5),\n      \"Heavy-tailed\" = rt(n, df = 3) * 2 + 5,\n      \"Bimodal\" = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, 2, 0.8) + (1 - k) * rnorm(n, 7, 0.8)\n      }\n    )\n  }\n\n  compute_stat &lt;- function(x, y = NULL, stat) {\n    switch(stat,\n      \"Mean\" = mean(x),\n      \"Median\" = median(x),\n      \"SD\" = sd(x),\n      \"Correlation\" = cor(x, y),\n      \"Regression slope\" = coef(lm(y ~ x))[2]\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    B    &lt;- input$B\n    stat &lt;- input$stat\n    pop  &lt;- input$pop\n\n    x &lt;- draw_pop(n, pop)\n    y &lt;- NULL\n    if (stat %in% c(\"Correlation\", \"Regression slope\")) {\n      y &lt;- 1 + 0.8 * x + rnorm(n, sd = 2)\n    }\n\n    obs_stat &lt;- compute_stat(x, y, stat)\n\n    # Bootstrap\n    boot_stats &lt;- replicate(B, {\n      idx &lt;- sample(n, n, replace = TRUE)\n      x_b &lt;- x[idx]\n      y_b &lt;- if (!is.null(y)) y[idx] else NULL\n      compute_stat(x_b, y_b, stat)\n    })\n\n    boot_se &lt;- sd(boot_stats)\n    boot_ci &lt;- quantile(boot_stats, c(0.025, 0.975))\n\n    list(x = x, y = y, obs_stat = obs_stat,\n         boot_stats = boot_stats, boot_se = boot_se,\n         boot_ci = boot_ci, stat = stat, n = n, B = B)\n  })\n\n  output$sample_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    if (d$stat %in% c(\"Correlation\", \"Regression slope\")) {\n      plot(d$x, d$y, pch = 16, col = \"#3498db80\", cex = 0.8,\n           xlab = \"X\", ylab = \"Y\", main = \"Your sample\")\n      if (d$stat == \"Regression slope\") {\n        abline(lm(d$y ~ d$x), col = \"#e74c3c\", lwd = 2)\n      }\n    } else {\n      hist(d$x, breaks = 20, col = \"#d5e8d4\", border = \"#82b366\",\n           main = \"Your sample\", xlab = \"X\", ylab = \"Frequency\")\n      abline(v = d$obs_stat, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n      legend(\"topright\", bty = \"n\", cex = 0.85,\n             legend = paste0(d$stat, \" = \", round(d$obs_stat, 3)),\n             col = \"#e74c3c\", lty = 2, lwd = 2)\n    }\n  })\n\n  output$boot_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$boot_stats, breaks = 40, probability = TRUE,\n         col = \"#dae8fc\", border = \"#6c8ebf\",\n         main = paste0(\"Bootstrap distribution (B = \", d$B, \")\"),\n         xlab = paste0(\"Bootstrap \", tolower(d$stat)),\n         ylab = \"Density\")\n\n    abline(v = d$obs_stat, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n    abline(v = d$boot_ci, col = \"#27ae60\", lwd = 2, lty = 3)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(paste0(\"Observed: \", round(d$obs_stat, 3)),\n                      paste0(\"95% CI: [\", round(d$boot_ci[1], 3),\n                             \", \", round(d$boot_ci[2], 3), \"]\")),\n           col = c(\"#e74c3c\", \"#27ae60\"),\n           lty = c(2, 3), lwd = 2)\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    if (d$stat %in% c(\"Correlation\", \"Regression slope\")) {\n      # For bivariate stats, just show bootstrap dist with CI\n      hist(d$boot_stats, breaks = 40, probability = TRUE,\n           col = \"#dae8fc\", border = \"#6c8ebf\",\n           main = \"Bootstrap sampling dist.\",\n           xlab = paste0(\"Bootstrap \", tolower(d$stat)),\n           ylab = \"Density\")\n      abline(v = d$boot_ci, col = \"#27ae60\", lwd = 2, lty = 3)\n      abline(v = d$obs_stat, col = \"#e74c3c\", lwd = 2, lty = 2)\n    } else {\n      # Overlay: data distribution (wide) vs bootstrap distribution (narrow)\n      # Rescale data density to fit on same axes\n      dens_data &lt;- density(d$x)\n      dens_boot &lt;- density(d$boot_stats)\n\n      # Normalize both to peak at 1 for visual comparison\n      dens_data$y &lt;- dens_data$y / max(dens_data$y)\n      dens_boot$y &lt;- dens_boot$y / max(dens_boot$y)\n\n      xlim &lt;- range(c(dens_data$x, dens_boot$x))\n\n      plot(dens_data, col = \"#e74c3c\", lwd = 2.5, xlim = xlim,\n           ylim = c(0, 1.25), main = \"Data vs Sampling Distribution\",\n           xlab = \"Value\", ylab = \"Scaled density\")\n      polygon(dens_data$x, dens_data$y,\n              col = adjustcolor(\"#e74c3c\", 0.15), border = NA)\n\n      lines(dens_boot, col = \"#3498db\", lwd = 2.5)\n      polygon(dens_boot$x, dens_boot$y,\n              col = adjustcolor(\"#3498db\", 0.15), border = NA)\n\n      legend(\"topright\", bty = \"n\", cex = 0.8,\n             legend = c(\"Your data (wide, raw obs.)\",\n                        paste0(\"Sampling dist. of \", tolower(d$stat),\n                               \" (narrow)\")),\n             col = c(\"#e74c3c\", \"#3498db\"), lwd = 2.5)\n    }\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Statistic:&lt;/b&gt; \", d$stat, \"&lt;br&gt;\",\n        \"&lt;b&gt;Observed:&lt;/b&gt; \", round(d$obs_stat, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bootstrap SE:&lt;/b&gt; \", round(d$boot_se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;95% CI:&lt;/b&gt; [\", round(d$boot_ci[1], 4),\n        \", \", round(d$boot_ci[2], 4), \"]&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;CI uses percentile method:&lt;br&gt;\",\n        \"2.5th and 97.5th percentiles of&lt;br&gt;\",\n        \"bootstrap distribution.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nMean with Normal population: the bootstrap distribution looks normal, similar to what CLT gives you analytically.\nMedian with skewed population: no easy formula for the SE of a median. But the bootstrap handles it effortlessly.\nSD with heavy-tailed data: the bootstrap distribution is skewed — the percentile CI is asymmetric, which is exactly right.\nRegression slope: bootstrap the slope to get a CI without assuming homoskedasticity.\nSmall n (10) vs large n (200): with small samples the bootstrap distribution is choppy (limited unique resamples). With more data it smooths out.\n\n\n\n\n\n\n\n\n\n\n\nUse bootstrap when…\nDon’t bother when…\n\n\n\n\nNo formula for the SE exists\nStandard formulas are available and valid\n\n\nThe statistic is complicated (ratios, quantiles)\nYou’re computing a simple mean\n\n\nYou suspect non-normality\nn is large and CLT applies\n\n\nYou want a quick, assumption-free CI\nYou need exact small-sample inference\n\n\n\nThe bootstrap is not magic — it can fail with very small samples or non-smooth statistics. But for most practical situations, it’s the easiest path to a valid confidence interval.\n\n\n\n\n\nBradley Efron invented the bootstrap in 1979 at Stanford. The name comes from the expression “pulling yourself up by your bootstraps” — attributed to the tall tales of Baron Munchausen, who claimed to have pulled himself out of a swamp by his own hair (later versions say bootstraps). The idea that a sample can estimate its own sampling distribution seemed equally absurd at first.\nWhen Efron first presented the bootstrap, many statisticians were skeptical. One famously asked: “You’re just sampling from your sample — how can that tell you anything new?” The answer: it tells you about the variability of your estimate, not about new data points.\nThe bootstrap is now one of the most cited statistical methods ever. Efron’s 1979 paper has over 20,000 citations. He received the International Prize in Statistics (the statistics equivalent of the Nobel) in 2019 for this work.\nBefore the bootstrap, getting standard errors for complex statistics (ratios, quantiles, eigenvalues) required either painful analytical derivations or the delta method — a Taylor expansion trick that often required heroic assumptions. The bootstrap made all of that unnecessary.",
    "crumbs": [
      "Inference",
      "The Bootstrap"
    ]
  },
  {
    "objectID": "bootstrap.html#the-idea",
    "href": "bootstrap.html#the-idea",
    "title": "The Bootstrap",
    "section": "",
    "text": "You want a confidence interval for some statistic (mean, median, regression coefficient), but you don’t know the sampling distribution. Maybe the formula is complicated. Maybe there is no formula.\nThe bootstrap says: pretend your sample is the population. Then simulate the sampling process by resampling with replacement from your data, over and over. Each resample gives you a new estimate. The distribution of those estimates approximates the true sampling distribution.\n\n\n\nYou have a sample of \\(n\\) observations.\nDraw \\(n\\) observations with replacement from your sample (some points get picked twice, some not at all).\nCompute your statistic on this resample.\nRepeat B times (typically 1,000–10,000).\nThe spread of those B estimates gives you a standard error and a confidence interval.\n\nThat’s it. No formulas, no distributional assumptions. Just resampling.\n\n\n\nNo — and this is a crucial distinction. There are two different distributions at play:\n\nThe distribution of the data = what individual observations look like (their histogram). This tells you about spread, skew, outliers in your raw data.\nThe sampling distribution = what your statistic (mean, median, slope) would look like if you could repeat the entire experiment many times. This is what you need for standard errors and confidence intervals.\n\nYou can always plot your data. But you cannot see the sampling distribution — you only ran the experiment once. You have one mean, not a distribution of means.\nThe bootstrap bridges that gap. It simulates the “what if I repeated this experiment?” question using only the data you have. The left panel below shows your data distribution. The right panel shows the bootstrap sampling distribution. Notice: they look completely different. Your data might be skewed and spread out, but the sampling distribution of the mean is narrow and roughly normal (thanks to the CLT). The bootstrap discovers this for you without any formulas.\n#| standalone: true\n#| viewerHeight: 900\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"stat\", \"Statistic to bootstrap:\",\n                  choices = c(\"Mean\", \"Median\", \"SD\",\n                              \"Correlation\", \"Regression slope\")),\n\n      selectInput(\"pop\", \"Population shape:\",\n                  choices = c(\"Normal\", \"Skewed (exponential)\",\n                              \"Heavy-tailed\", \"Bimodal\")),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 10, max = 200, value = 30, step = 10),\n\n      sliderInput(\"B\", \"Bootstrap resamples (B):\",\n                  min = 100, max = 5000, value = 1000, step = 100),\n\n      actionButton(\"go\", \"New sample + bootstrap\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"sample_plot\", height = \"350px\")),\n        column(6, plotOutput(\"boot_plot\", height = \"350px\"))\n      ),\n      fluidRow(\n        column(12, plotOutput(\"compare_plot\", height = \"350px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  draw_pop &lt;- function(n, pop) {\n    switch(pop,\n      \"Normal\" = rnorm(n, mean = 5, sd = 2),\n      \"Skewed (exponential)\" = rexp(n, rate = 0.5),\n      \"Heavy-tailed\" = rt(n, df = 3) * 2 + 5,\n      \"Bimodal\" = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, 2, 0.8) + (1 - k) * rnorm(n, 7, 0.8)\n      }\n    )\n  }\n\n  compute_stat &lt;- function(x, y = NULL, stat) {\n    switch(stat,\n      \"Mean\" = mean(x),\n      \"Median\" = median(x),\n      \"SD\" = sd(x),\n      \"Correlation\" = cor(x, y),\n      \"Regression slope\" = coef(lm(y ~ x))[2]\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    B    &lt;- input$B\n    stat &lt;- input$stat\n    pop  &lt;- input$pop\n\n    x &lt;- draw_pop(n, pop)\n    y &lt;- NULL\n    if (stat %in% c(\"Correlation\", \"Regression slope\")) {\n      y &lt;- 1 + 0.8 * x + rnorm(n, sd = 2)\n    }\n\n    obs_stat &lt;- compute_stat(x, y, stat)\n\n    # Bootstrap\n    boot_stats &lt;- replicate(B, {\n      idx &lt;- sample(n, n, replace = TRUE)\n      x_b &lt;- x[idx]\n      y_b &lt;- if (!is.null(y)) y[idx] else NULL\n      compute_stat(x_b, y_b, stat)\n    })\n\n    boot_se &lt;- sd(boot_stats)\n    boot_ci &lt;- quantile(boot_stats, c(0.025, 0.975))\n\n    list(x = x, y = y, obs_stat = obs_stat,\n         boot_stats = boot_stats, boot_se = boot_se,\n         boot_ci = boot_ci, stat = stat, n = n, B = B)\n  })\n\n  output$sample_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    if (d$stat %in% c(\"Correlation\", \"Regression slope\")) {\n      plot(d$x, d$y, pch = 16, col = \"#3498db80\", cex = 0.8,\n           xlab = \"X\", ylab = \"Y\", main = \"Your sample\")\n      if (d$stat == \"Regression slope\") {\n        abline(lm(d$y ~ d$x), col = \"#e74c3c\", lwd = 2)\n      }\n    } else {\n      hist(d$x, breaks = 20, col = \"#d5e8d4\", border = \"#82b366\",\n           main = \"Your sample\", xlab = \"X\", ylab = \"Frequency\")\n      abline(v = d$obs_stat, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n      legend(\"topright\", bty = \"n\", cex = 0.85,\n             legend = paste0(d$stat, \" = \", round(d$obs_stat, 3)),\n             col = \"#e74c3c\", lty = 2, lwd = 2)\n    }\n  })\n\n  output$boot_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$boot_stats, breaks = 40, probability = TRUE,\n         col = \"#dae8fc\", border = \"#6c8ebf\",\n         main = paste0(\"Bootstrap distribution (B = \", d$B, \")\"),\n         xlab = paste0(\"Bootstrap \", tolower(d$stat)),\n         ylab = \"Density\")\n\n    abline(v = d$obs_stat, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n    abline(v = d$boot_ci, col = \"#27ae60\", lwd = 2, lty = 3)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(paste0(\"Observed: \", round(d$obs_stat, 3)),\n                      paste0(\"95% CI: [\", round(d$boot_ci[1], 3),\n                             \", \", round(d$boot_ci[2], 3), \"]\")),\n           col = c(\"#e74c3c\", \"#27ae60\"),\n           lty = c(2, 3), lwd = 2)\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    if (d$stat %in% c(\"Correlation\", \"Regression slope\")) {\n      # For bivariate stats, just show bootstrap dist with CI\n      hist(d$boot_stats, breaks = 40, probability = TRUE,\n           col = \"#dae8fc\", border = \"#6c8ebf\",\n           main = \"Bootstrap sampling dist.\",\n           xlab = paste0(\"Bootstrap \", tolower(d$stat)),\n           ylab = \"Density\")\n      abline(v = d$boot_ci, col = \"#27ae60\", lwd = 2, lty = 3)\n      abline(v = d$obs_stat, col = \"#e74c3c\", lwd = 2, lty = 2)\n    } else {\n      # Overlay: data distribution (wide) vs bootstrap distribution (narrow)\n      # Rescale data density to fit on same axes\n      dens_data &lt;- density(d$x)\n      dens_boot &lt;- density(d$boot_stats)\n\n      # Normalize both to peak at 1 for visual comparison\n      dens_data$y &lt;- dens_data$y / max(dens_data$y)\n      dens_boot$y &lt;- dens_boot$y / max(dens_boot$y)\n\n      xlim &lt;- range(c(dens_data$x, dens_boot$x))\n\n      plot(dens_data, col = \"#e74c3c\", lwd = 2.5, xlim = xlim,\n           ylim = c(0, 1.25), main = \"Data vs Sampling Distribution\",\n           xlab = \"Value\", ylab = \"Scaled density\")\n      polygon(dens_data$x, dens_data$y,\n              col = adjustcolor(\"#e74c3c\", 0.15), border = NA)\n\n      lines(dens_boot, col = \"#3498db\", lwd = 2.5)\n      polygon(dens_boot$x, dens_boot$y,\n              col = adjustcolor(\"#3498db\", 0.15), border = NA)\n\n      legend(\"topright\", bty = \"n\", cex = 0.8,\n             legend = c(\"Your data (wide, raw obs.)\",\n                        paste0(\"Sampling dist. of \", tolower(d$stat),\n                               \" (narrow)\")),\n             col = c(\"#e74c3c\", \"#3498db\"), lwd = 2.5)\n    }\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Statistic:&lt;/b&gt; \", d$stat, \"&lt;br&gt;\",\n        \"&lt;b&gt;Observed:&lt;/b&gt; \", round(d$obs_stat, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bootstrap SE:&lt;/b&gt; \", round(d$boot_se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;95% CI:&lt;/b&gt; [\", round(d$boot_ci[1], 4),\n        \", \", round(d$boot_ci[2], 4), \"]&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;CI uses percentile method:&lt;br&gt;\",\n        \"2.5th and 97.5th percentiles of&lt;br&gt;\",\n        \"bootstrap distribution.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nMean with Normal population: the bootstrap distribution looks normal, similar to what CLT gives you analytically.\nMedian with skewed population: no easy formula for the SE of a median. But the bootstrap handles it effortlessly.\nSD with heavy-tailed data: the bootstrap distribution is skewed — the percentile CI is asymmetric, which is exactly right.\nRegression slope: bootstrap the slope to get a CI without assuming homoskedasticity.\nSmall n (10) vs large n (200): with small samples the bootstrap distribution is choppy (limited unique resamples). With more data it smooths out.\n\n\n\n\n\n\n\n\n\n\n\nUse bootstrap when…\nDon’t bother when…\n\n\n\n\nNo formula for the SE exists\nStandard formulas are available and valid\n\n\nThe statistic is complicated (ratios, quantiles)\nYou’re computing a simple mean\n\n\nYou suspect non-normality\nn is large and CLT applies\n\n\nYou want a quick, assumption-free CI\nYou need exact small-sample inference\n\n\n\nThe bootstrap is not magic — it can fail with very small samples or non-smooth statistics. But for most practical situations, it’s the easiest path to a valid confidence interval.\n\n\n\n\n\nBradley Efron invented the bootstrap in 1979 at Stanford. The name comes from the expression “pulling yourself up by your bootstraps” — attributed to the tall tales of Baron Munchausen, who claimed to have pulled himself out of a swamp by his own hair (later versions say bootstraps). The idea that a sample can estimate its own sampling distribution seemed equally absurd at first.\nWhen Efron first presented the bootstrap, many statisticians were skeptical. One famously asked: “You’re just sampling from your sample — how can that tell you anything new?” The answer: it tells you about the variability of your estimate, not about new data points.\nThe bootstrap is now one of the most cited statistical methods ever. Efron’s 1979 paper has over 20,000 citations. He received the International Prize in Statistics (the statistics equivalent of the Nobel) in 2019 for this work.\nBefore the bootstrap, getting standard errors for complex statistics (ratios, quantiles, eigenvalues) required either painful analytical derivations or the delta method — a Taylor expansion trick that often required heroic assumptions. The bootstrap made all of that unnecessary.",
    "crumbs": [
      "Inference",
      "The Bootstrap"
    ]
  },
  {
    "objectID": "clustered-se.html",
    "href": "clustered-se.html",
    "title": "Robust vs Clustered SEs: When Observations Aren’t Independent",
    "section": "",
    "text": "OLS assumes that observations are independent. But in many real datasets they aren’t:\n\nStudents within the same school share teachers and resources\nPatients within the same hospital get similar care\nPurchases by the same customer are correlated over time\nEmployees within the same firm face the same management\n\nWhen observations within a group (cluster) are correlated, OLS standard errors are too small — often dramatically so. Your t-statistics are inflated, your p-values are too small, and you reject the null far more than 5% of the time.\n\n\n\n\n\n\n\n\n\n\nSE type\nWhat it assumes\nWhen to use\n\n\n\n\nOLS (classical)\nErrors are i.i.d. (independent, constant variance)\nAlmost never in practice\n\n\nRobust (HC)\nErrors can have different variances, but are independent\nCross-sectional data with heteroskedasticity\n\n\nClustered\nErrors can be correlated within clusters\nAny grouped/panel data\n\n\n\nRobust SEs fix heteroskedasticity but not within-cluster correlation. Clustered SEs fix both. If your data has clusters, you need clustered SEs.",
    "crumbs": [
      "Regression & Diagnostics",
      "Clustered SEs"
    ]
  },
  {
    "objectID": "clustered-se.html#the-independence-assumption-you-forgot-about",
    "href": "clustered-se.html#the-independence-assumption-you-forgot-about",
    "title": "Robust vs Clustered SEs: When Observations Aren’t Independent",
    "section": "",
    "text": "OLS assumes that observations are independent. But in many real datasets they aren’t:\n\nStudents within the same school share teachers and resources\nPatients within the same hospital get similar care\nPurchases by the same customer are correlated over time\nEmployees within the same firm face the same management\n\nWhen observations within a group (cluster) are correlated, OLS standard errors are too small — often dramatically so. Your t-statistics are inflated, your p-values are too small, and you reject the null far more than 5% of the time.\n\n\n\n\n\n\n\n\n\n\nSE type\nWhat it assumes\nWhen to use\n\n\n\n\nOLS (classical)\nErrors are i.i.d. (independent, constant variance)\nAlmost never in practice\n\n\nRobust (HC)\nErrors can have different variances, but are independent\nCross-sectional data with heteroskedasticity\n\n\nClustered\nErrors can be correlated within clusters\nAny grouped/panel data\n\n\n\nRobust SEs fix heteroskedasticity but not within-cluster correlation. Clustered SEs fix both. If your data has clusters, you need clustered SEs.",
    "crumbs": [
      "Regression & Diagnostics",
      "Clustered SEs"
    ]
  },
  {
    "objectID": "clustered-se.html#simulation-1-clustered-data-and-wrong-ses",
    "href": "clustered-se.html#simulation-1-clustered-data-and-wrong-ses",
    "title": "Robust vs Clustered SEs: When Observations Aren’t Independent",
    "section": "Simulation 1: Clustered data and wrong SEs",
    "text": "Simulation 1: Clustered data and wrong SEs\nGenerate data with students nested in schools. Within each school, outcomes are correlated (shared school effect). Compare the three types of standard errors.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_clusters\", \"Number of schools:\",\n                  min = 10, max = 100, value = 30, step = 5),\n\n      sliderInput(\"cluster_size\", \"Students per school:\",\n                  min = 5, max = 50, value = 20, step = 5),\n\n      sliderInput(\"icc\", \"ICC (intra-cluster correlation):\",\n                  min = 0, max = 0.8, value = 0.3, step = 0.05),\n\n      helpText(\"ICC = share of total variance due to the\n               school-level component. Higher = more clustering.\"),\n\n      actionButton(\"go\", \"New draw\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"scatter\", height = \"400px\")),\n        column(6, plotOutput(\"se_compare\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    G  &lt;- input$n_clusters\n    m  &lt;- input$cluster_size\n    icc &lt;- input$icc\n\n    # Variance decomposition: total var = 1\n    sigma_b &lt;- sqrt(icc)        # between-cluster SD\n    sigma_w &lt;- sqrt(1 - icc)    # within-cluster SD\n\n    # Generate clustered data (no true effect: beta = 0)\n    cluster_id &lt;- rep(seq_len(G), each = m)\n    school_effect &lt;- rep(rnorm(G, sd = sigma_b), each = m)\n    x &lt;- rnorm(G * m)\n    y &lt;- 0 * x + school_effect + rnorm(G * m, sd = sigma_w)\n\n    fit &lt;- lm(y ~ x)\n    b_hat &lt;- coef(fit)[2]\n    n &lt;- G * m\n\n    # OLS SE\n    ols_se &lt;- summary(fit)$coefficients[2, 2]\n\n    # Robust SE (HC1)\n    r &lt;- resid(fit)\n    X &lt;- cbind(1, x)\n    bread &lt;- solve(t(X) %*% X)\n    meat_hc &lt;- t(X) %*% diag(r^2 * n / (n - 2)) %*% X\n    robust_se &lt;- sqrt((bread %*% meat_hc %*% bread)[2, 2])\n\n    # Clustered SE (Liang-Zeger)\n    meat_cl &lt;- matrix(0, 2, 2)\n    for (g in seq_len(G)) {\n      idx &lt;- which(cluster_id == g)\n      Xg &lt;- X[idx, , drop = FALSE]\n      rg &lt;- r[idx]\n      meat_cl &lt;- meat_cl + t(Xg) %*% (rg %*% t(rg)) %*% Xg\n    }\n    adj &lt;- G / (G - 1) * (n - 1) / (n - 2)\n    cl_vcov &lt;- adj * bread %*% meat_cl %*% bread\n    clustered_se &lt;- sqrt(cl_vcov[2, 2])\n\n    list(x = x, y = y, cluster_id = cluster_id, fit = fit,\n         b_hat = b_hat, ols_se = ols_se, robust_se = robust_se,\n         clustered_se = clustered_se, G = G, m = m, icc = icc)\n  })\n\n  output$scatter &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Color by cluster (cycle through colors)\n    palette &lt;- rep(c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"#9b59b6\",\n                     \"#e67e22\", \"#1abc9c\", \"#34495e\", \"#f39c12\"),\n                   length.out = d$G)\n    cols &lt;- palette[d$cluster_id]\n\n    plot(d$x, d$y, pch = 16, col = adjustcolor(cols, 0.5), cex = 0.6,\n         xlab = \"X\", ylab = \"Y\",\n         main = paste0(d$G, \" schools, \", d$m, \" students each\"))\n    abline(d$fit, col = \"#2c3e50\", lwd = 2.5)\n    abline(h = 0, lty = 3, col = \"gray60\")\n  })\n\n  output$se_compare &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 8, 3, 1))\n\n    ses &lt;- c(d$ols_se, d$robust_se, d$clustered_se)\n    names_se &lt;- c(\"OLS SE\", \"Robust SE\", \"Clustered SE\")\n    cols &lt;- c(\"#e74c3c\", \"#e67e22\", \"#27ae60\")\n\n    # CIs\n    lo &lt;- d$b_hat - 1.96 * ses\n    hi &lt;- d$b_hat + 1.96 * ses\n\n    xlim &lt;- range(c(lo, hi, 0)) + c(-0.3, 0.3)\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, 3.5),\n         yaxt = \"n\", ylab = \"\", xlab = expression(hat(beta)),\n         main = \"95% CIs with different SEs\")\n    axis(2, at = 1:3, labels = names_se, las = 1, cex.axis = 0.85)\n\n    for (i in 1:3) {\n      segments(lo[i], i, hi[i], i, lwd = 4, col = cols[i])\n      points(d$b_hat, i, pch = 19, cex = 1.5, col = cols[i])\n    }\n\n    abline(v = 0, lty = 2, lwd = 2, col = \"#2c3e50\")\n    text(0, 3.4, expression(\"True \" * beta * \" = 0\"), cex = 0.9)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    ratio &lt;- d$clustered_se / d$ols_se\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;\", expression(\"hat(beta)\"), \":&lt;/b&gt; \",\n        round(d$b_hat, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;OLS SE:&lt;/b&gt; &lt;span class='bad'&gt;\",\n        round(d$ols_se, 4), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Robust SE:&lt;/b&gt; \",\n        round(d$robust_se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Clustered SE:&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$clustered_se, 4), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Cluster/OLS ratio:&lt;/b&gt; \", round(ratio, 2), \"x&lt;br&gt;\",\n        \"&lt;small&gt;With ICC = \", d$icc, \", clustered SE is&lt;br&gt;\",\n        round(ratio, 1), \"x larger than OLS SE.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Regression & Diagnostics",
      "Clustered SEs"
    ]
  },
  {
    "objectID": "clustered-se.html#simulation-2-rejection-rates",
    "href": "clustered-se.html#simulation-2-rejection-rates",
    "title": "Robust vs Clustered SEs: When Observations Aren’t Independent",
    "section": "Simulation 2: Rejection rates",
    "text": "Simulation 2: Rejection rates\nThe real test: run 500 experiments where the true effect is zero. How often does each type of SE incorrectly reject H₀ at the 5% level? OLS SEs reject far too often. Clustered SEs get it right.\n#| standalone: true\n#| viewerHeight: 520\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"G2\", \"Number of clusters:\",\n                  min = 10, max = 100, value = 30, step = 5),\n\n      sliderInput(\"m2\", \"Obs per cluster:\",\n                  min = 5, max = 50, value = 20, step = 5),\n\n      sliderInput(\"icc2\", \"ICC:\",\n                  min = 0, max = 0.8, value = 0.3, step = 0.05),\n\n      sliderInput(\"sims\", \"Number of simulations:\",\n                  min = 200, max = 1000, value = 500, step = 100),\n\n      actionButton(\"go2\", \"Run simulations\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"rejection_plot\", height = \"420px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go2\n    G    &lt;- input$G2\n    m    &lt;- input$m2\n    icc  &lt;- input$icc2\n    sims &lt;- input$sims\n\n    sigma_b &lt;- sqrt(icc)\n    sigma_w &lt;- sqrt(1 - icc)\n    n &lt;- G * m\n\n    reject_ols &lt;- 0\n    reject_robust &lt;- 0\n    reject_cluster &lt;- 0\n\n    for (s in seq_len(sims)) {\n      cluster_id &lt;- rep(seq_len(G), each = m)\n      school_eff &lt;- rep(rnorm(G, sd = sigma_b), each = m)\n      x &lt;- rnorm(n)\n      y &lt;- 0 * x + school_eff + rnorm(n, sd = sigma_w)\n\n      fit &lt;- lm(y ~ x)\n      b_hat &lt;- coef(fit)[2]\n      r &lt;- resid(fit)\n      X &lt;- cbind(1, x)\n\n      # OLS\n      ols_se &lt;- summary(fit)$coefficients[2, 2]\n\n      # Robust\n      bread &lt;- solve(t(X) %*% X)\n      meat_hc &lt;- t(X) %*% diag(r^2 * n / (n - 2)) %*% X\n      robust_se &lt;- sqrt((bread %*% meat_hc %*% bread)[2, 2])\n\n      # Clustered\n      meat_cl &lt;- matrix(0, 2, 2)\n      for (g in seq_len(G)) {\n        idx &lt;- which(cluster_id == g)\n        Xg &lt;- X[idx, , drop = FALSE]\n        rg &lt;- r[idx]\n        meat_cl &lt;- meat_cl + t(Xg) %*% (rg %*% t(rg)) %*% Xg\n      }\n      adj &lt;- G / (G - 1) * (n - 1) / (n - 2)\n      cl_se &lt;- sqrt((adj * bread %*% meat_cl %*% bread)[2, 2])\n\n      if (abs(b_hat / ols_se) &gt; 1.96)    reject_ols &lt;- reject_ols + 1\n      if (abs(b_hat / robust_se) &gt; 1.96) reject_robust &lt;- reject_robust + 1\n      if (abs(b_hat / cl_se) &gt; 1.96)     reject_cluster &lt;- reject_cluster + 1\n    }\n\n    list(rej_ols = reject_ols / sims,\n         rej_robust = reject_robust / sims,\n         rej_cluster = reject_cluster / sims,\n         sims = sims, G = G, m = m, icc = icc)\n  })\n\n  output$rejection_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 8, 3, 1))\n\n    rates &lt;- c(d$rej_ols, d$rej_robust, d$rej_cluster) * 100\n    names_se &lt;- c(\"OLS SE\", \"Robust SE\", \"Clustered SE\")\n    cols &lt;- c(\"#e74c3c\", \"#e67e22\", \"#27ae60\")\n\n    bp &lt;- barplot(rates, names.arg = names_se, col = cols,\n                  horiz = TRUE, xlim = c(0, max(rates, 10) * 1.3),\n                  xlab = \"Rejection rate (%)\",\n                  main = paste0(\"False rejection rate at 5% level\\n(\",\n                               d$sims, \" simulations, ICC = \",\n                               d$icc, \")\"),\n                  las = 1, border = NA)\n\n    abline(v = 5, lty = 2, lwd = 2.5, col = \"#2c3e50\")\n    text(5, max(bp) + 0.6, \"Target: 5%\", cex = 0.85)\n\n    text(rates + 1, bp, paste0(round(rates, 1), \"%\"),\n         cex = 0.9, adj = 0)\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;OLS rejection rate:&lt;/b&gt; &lt;span class='\",\n        ifelse(d$rej_ols &gt; 0.08, \"bad\", \"good\"), \"'&gt;\",\n        round(d$rej_ols * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Robust rejection rate:&lt;/b&gt; &lt;span class='\",\n        ifelse(d$rej_robust &gt; 0.08, \"bad\", \"good\"), \"'&gt;\",\n        round(d$rej_robust * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Clustered rejection rate:&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$rej_cluster * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;True \\u03b2 = 0 in all simulations.&lt;br&gt;\",\n        \"Correct rejection rate is 5%.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nICC = 0 (no clustering): all three SEs give ~5% rejection. When there’s no within-cluster correlation, OLS SEs are fine.\nICC = 0.3: OLS rejection shoots up to ~15–25%. Robust SEs help a little but not enough. Clustered SEs stay near 5%.\nICC = 0.5: OLS can reject 30–40% of the time — catastrophic.\nIncrease cluster size, hold clusters fixed: the problem gets worse with more observations per cluster. More correlated data doesn’t help — it just gives you a false sense of precision.\nIncrease number of clusters: this does help. The effective sample size for clustered data is closer to the number of clusters than the number of observations.\n\n\n\nCode reference\n\n\n\n\n\n\n\nSoftware\nClustered SEs\n\n\n\n\nR\nlmtest::coeftest(fit, vcov = sandwich::vcovCL, cluster = ~school)\n\n\nStata\nreg y x, cluster(school)\n\n\nPython\nsm.OLS(y, X).fit(cov_type='cluster', cov_kwds={'groups': school})\n\n\n\n\n\nThe practical rule\n\nAlways cluster at the level of treatment assignment. If you randomized schools, cluster by school. If you randomized classrooms, cluster by classroom.\nWhen in doubt, cluster at the highest level that makes sense. Clustering too aggressively (too few clusters) can be a problem, but not clustering when you should is always worse.\nThe minimum number of clusters for reliable clustered SEs is roughly 30–50. With fewer clusters, consider the wild cluster bootstrap.\n\n\n\n\nDid you know?\n\nBrent Moulton (1990) published a now-classic paper showing that regressions using aggregate variables (like state-level policies) with individual-level data produce t-statistics that are wildly inflated if you ignore clustering. He called it the “Moulton problem.” Many published results in economics were likely spurious because of this.\nThe Moulton factor — the ratio of the true variance to the (too-small) OLS variance — is approximately \\(1 + (m-1) \\times ICC\\), where \\(m\\) is the average cluster size. With 50 students per school and ICC = 0.2, the Moulton factor is about 11, meaning your OLS SE is \\(\\sqrt{11} \\approx\n3.3\\) times too small. Your t-statistic is 3x too large.\nCameron, Gelbach, and Miller (2008) showed that even clustered SEs can be unreliable when the number of clusters is small (&lt; 30–50), leading to the development of the wild cluster bootstrap as a more robust alternative.",
    "crumbs": [
      "Regression & Diagnostics",
      "Clustered SEs"
    ]
  },
  {
    "objectID": "monte-carlo.html",
    "href": "monte-carlo.html",
    "title": "Monte Carlo Experiments: How We Understand Estimators",
    "section": "",
    "text": "Every claim in this course — unbiasedness, coverage, power — was verified the same way: by simulation. A Monte Carlo experiment is simple:\n\nSpecify a data-generating process (DGP): decide the true parameter, the distribution, and the sample size.\nDraw a sample from the DGP and compute your estimator.\nRepeat many times (1,000–10,000 simulations).\nSummarize: look at the distribution of your estimates. Is the estimator centered on the truth (unbiased)? How spread out is it (variance)? How often does the CI cover the truth?\n\nThis is how statisticians check their own work. Theory says OLS is unbiased? Prove it — simulate 5,000 datasets and see if the average estimate equals the true value. Theory says the CI has 95% coverage? Simulate it.",
    "crumbs": [
      "Inference",
      "Monte Carlo Experiments"
    ]
  },
  {
    "objectID": "monte-carlo.html#what-is-a-monte-carlo-experiment",
    "href": "monte-carlo.html#what-is-a-monte-carlo-experiment",
    "title": "Monte Carlo Experiments: How We Understand Estimators",
    "section": "",
    "text": "Every claim in this course — unbiasedness, coverage, power — was verified the same way: by simulation. A Monte Carlo experiment is simple:\n\nSpecify a data-generating process (DGP): decide the true parameter, the distribution, and the sample size.\nDraw a sample from the DGP and compute your estimator.\nRepeat many times (1,000–10,000 simulations).\nSummarize: look at the distribution of your estimates. Is the estimator centered on the truth (unbiased)? How spread out is it (variance)? How often does the CI cover the truth?\n\nThis is how statisticians check their own work. Theory says OLS is unbiased? Prove it — simulate 5,000 datasets and see if the average estimate equals the true value. Theory says the CI has 95% coverage? Simulate it.",
    "crumbs": [
      "Inference",
      "Monte Carlo Experiments"
    ]
  },
  {
    "objectID": "monte-carlo.html#simulation-1-compare-estimators",
    "href": "monte-carlo.html#simulation-1-compare-estimators",
    "title": "Monte Carlo Experiments: How We Understand Estimators",
    "section": "Simulation 1: Compare estimators",
    "text": "Simulation 1: Compare estimators\nPick an estimator, pick a DGP, run \\(N\\) simulations. Compare bias, variance, and MSE (mean squared error = bias² + variance) across estimators.\n#| standalone: true\n#| viewerHeight: 700\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"dgp\", \"Data-generating process:\",\n                  choices = c(\"Normal(5, 2)\",\n                              \"Exponential(rate=0.5)\",\n                              \"Contaminated normal\",\n                              \"Uniform(0, 10)\")),\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 5, max = 200, value = 20, step = 5),\n\n      sliderInput(\"sims\", \"Number of simulations:\",\n                  min = 500, max = 5000, value = 2000, step = 500),\n\n      checkboxGroupInput(\"estimators\", \"Estimators to compare:\",\n                         choices = c(\"Mean\", \"Median\",\n                                     \"10% trimmed mean\",\n                                     \"Midrange\"),\n                         selected = c(\"Mean\", \"Median\",\n                                      \"10% trimmed mean\")),\n\n      actionButton(\"go\", \"Run Monte Carlo\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(8, plotOutput(\"hist_plot\", height = \"380px\")),\n        column(4, plotOutput(\"mse_plot\", height = \"380px\"))\n      ),\n      fluidRow(\n        column(12, plotOutput(\"dgp_plot\", height = \"220px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  draw_dgp &lt;- function(n, dgp) {\n    switch(dgp,\n      \"Normal(5, 2)\"          = rnorm(n, mean = 5, sd = 2),\n      \"Exponential(rate=0.5)\" = rexp(n, rate = 0.5),\n      \"Contaminated normal\"   = {\n        k &lt;- rbinom(n, 1, 0.1)\n        (1 - k) * rnorm(n, 5, 1) + k * rnorm(n, 5, 10)\n      },\n      \"Uniform(0, 10)\"        = runif(n, 0, 10)\n    )\n  }\n\n  true_mu &lt;- function(dgp) {\n    switch(dgp,\n      \"Normal(5, 2)\"          = 5,\n      \"Exponential(rate=0.5)\" = 2,\n      \"Contaminated normal\"   = 5,\n      \"Uniform(0, 10)\"        = 5\n    )\n  }\n\n  compute_est &lt;- function(x, est) {\n    switch(est,\n      \"Mean\"              = mean(x),\n      \"Median\"            = median(x),\n      \"10% trimmed mean\"  = mean(x, trim = 0.1),\n      \"Midrange\"          = (min(x) + max(x)) / 2\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    sims &lt;- input$sims\n    dgp  &lt;- input$dgp\n    ests &lt;- input$estimators\n    mu   &lt;- true_mu(dgp)\n\n    if (length(ests) == 0) ests &lt;- \"Mean\"\n\n    results &lt;- list()\n    for (est in ests) {\n      vals &lt;- replicate(sims, compute_est(draw_dgp(n, dgp), est))\n      bias &lt;- mean(vals) - mu\n      variance &lt;- var(vals)\n      mse &lt;- bias^2 + variance\n      results[[est]] &lt;- list(vals = vals, bias = bias,\n                             variance = variance, mse = mse)\n    }\n\n    list(results = results, mu = mu, n = n, sims = sims,\n         dgp = dgp, ests = ests)\n  })\n\n  output$dgp_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(3, 4.5, 2, 1))\n\n    big &lt;- draw_dgp(10000, d$dgp)\n    hist(big, breaks = 60, probability = TRUE,\n         col = \"#e0e0e0\", border = \"#b0b0b0\",\n         main = paste(\"DGP:\", d$dgp),\n         xlab = \"\", ylab = \"Density\")\n    abline(v = d$mu, lty = 2, lwd = 2, col = \"#e74c3c\")\n  })\n\n  output$hist_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    cols &lt;- c(\"#3498db\", \"#e74c3c\", \"#27ae60\", \"#9b59b6\")\n    all_vals &lt;- unlist(lapply(d$results, function(r) r$vals))\n    xlim &lt;- quantile(all_vals, c(0.005, 0.995))\n\n    first &lt;- TRUE\n    for (i in seq_along(d$ests)) {\n      vals &lt;- d$results[[d$ests[i]]]$vals\n      if (first) {\n        hist(vals, breaks = 50, probability = TRUE,\n             col = adjustcolor(cols[i], 0.3),\n             border = adjustcolor(cols[i], 0.6),\n             main = paste0(\"Sampling distributions (\",\n                          d$sims, \" simulations, n = \", d$n, \")\"),\n             xlab = \"Estimate\", ylab = \"Density\",\n             xlim = xlim)\n        first &lt;- FALSE\n      } else {\n        hist(vals, breaks = 50, probability = TRUE,\n             col = adjustcolor(cols[i], 0.2),\n             border = adjustcolor(cols[i], 0.5),\n             add = TRUE)\n      }\n    }\n\n    abline(v = d$mu, lty = 2, lwd = 2.5, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(d$ests, paste0(\"True \\u03bc = \", d$mu)),\n           col = c(cols[seq_along(d$ests)], \"#2c3e50\"),\n           lwd = c(rep(8, length(d$ests)), 2.5),\n           lty = c(rep(1, length(d$ests)), 2))\n  })\n\n  output$mse_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 8, 3, 1))\n\n    mses &lt;- sapply(d$results, function(r) r$mse)\n    biases2 &lt;- sapply(d$results, function(r) r$bias^2)\n    vars &lt;- sapply(d$results, function(r) r$variance)\n\n    cols &lt;- c(\"#3498db\", \"#e74c3c\", \"#27ae60\", \"#9b59b6\")\n    n_est &lt;- length(d$ests)\n\n    # Stacked bar: bias^2 + variance = MSE\n    mat &lt;- rbind(biases2, vars)\n\n    bp &lt;- barplot(mat, names.arg = d$ests, col = c(\"#e74c3c80\", \"#3498db80\"),\n                  horiz = TRUE, las = 1,\n                  main = \"MSE decomposition\",\n                  xlab = \"MSE = Bias\\u00b2 + Variance\",\n                  border = NA, cex.names = 0.75)\n\n    legend(\"bottomright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Bias\\u00b2\", \"Variance\"),\n           fill = c(\"#e74c3c80\", \"#3498db80\"), border = NA)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    lines &lt;- paste0(sapply(d$ests, function(est) {\n      r &lt;- d$results[[est]]\n      paste0(\n        \"&lt;b&gt;\", est, \":&lt;/b&gt;&lt;br&gt;\",\n        \"&nbsp; Bias: \", round(r$bias, 4), \"&lt;br&gt;\",\n        \"&nbsp; Var: \", round(r$variance, 4), \"&lt;br&gt;\",\n        \"&nbsp; MSE: \", round(r$mse, 4), \"&lt;br&gt;\"\n      )\n    }), collapse = \"&lt;hr style='margin:6px 0'&gt;\")\n\n    tags$div(class = \"stats-box\", HTML(lines))\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Inference",
      "Monte Carlo Experiments"
    ]
  },
  {
    "objectID": "monte-carlo.html#simulation-2-consistency-estimates-tighten-with-n",
    "href": "monte-carlo.html#simulation-2-consistency-estimates-tighten-with-n",
    "title": "Monte Carlo Experiments: How We Understand Estimators",
    "section": "Simulation 2: Consistency — estimates tighten with n",
    "text": "Simulation 2: Consistency — estimates tighten with n\nA consistent estimator converges to the true parameter as \\(n \\to \\infty\\). Watch how the sampling distribution of each estimator gets narrower and more centered as you increase the sample size.\n#| standalone: true\n#| viewerHeight: 520\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"dgp2\", \"DGP:\",\n                  choices = c(\"Normal(5, 2)\",\n                              \"Exponential(rate=0.5)\",\n                              \"Contaminated normal\")),\n\n      selectInput(\"est2\", \"Estimator:\",\n                  choices = c(\"Mean\", \"Median\", \"10% trimmed mean\")),\n\n      sliderInput(\"sims2\", \"Simulations per n:\",\n                  min = 500, max = 3000, value = 1000, step = 250),\n\n      actionButton(\"go2\", \"Run\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"consistency_plot\", height = \"420px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  draw_dgp &lt;- function(n, dgp) {\n    switch(dgp,\n      \"Normal(5, 2)\"          = rnorm(n, mean = 5, sd = 2),\n      \"Exponential(rate=0.5)\" = rexp(n, rate = 0.5),\n      \"Contaminated normal\"   = {\n        k &lt;- rbinom(n, 1, 0.1)\n        (1 - k) * rnorm(n, 5, 1) + k * rnorm(n, 5, 10)\n      }\n    )\n  }\n\n  true_mu &lt;- function(dgp) {\n    switch(dgp,\n      \"Normal(5, 2)\"          = 5,\n      \"Exponential(rate=0.5)\" = 2,\n      \"Contaminated normal\"   = 5\n    )\n  }\n\n  compute_est &lt;- function(x, est) {\n    switch(est,\n      \"Mean\"             = mean(x),\n      \"Median\"           = median(x),\n      \"10% trimmed mean\" = mean(x, trim = 0.1)\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go2\n    dgp  &lt;- input$dgp2\n    est  &lt;- input$est2\n    sims &lt;- input$sims2\n    mu   &lt;- true_mu(dgp)\n\n    ns &lt;- c(5, 10, 25, 50, 100, 200)\n    all_results &lt;- list()\n    for (n in ns) {\n      all_results[[as.character(n)]] &lt;- replicate(\n        sims, compute_est(draw_dgp(n, dgp), est)\n      )\n    }\n\n    list(all_results = all_results, ns = ns, mu = mu,\n         dgp = dgp, est = est, sims = sims)\n  })\n\n  output$consistency_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    colors &lt;- colorRampPalette(c(\"#e74c3c\", \"#3498db\"))(length(d$ns))\n\n    all_vals &lt;- unlist(d$all_results)\n    xlim &lt;- quantile(all_vals, c(0.01, 0.99))\n\n    plot(NULL, xlim = xlim,\n         ylim = c(0.5, length(d$ns) * 1.5 + 0.5),\n         yaxt = \"n\", ylab = \"\",\n         xlab = paste0(\"Estimate (\", d$est, \")\"),\n         main = paste0(\"Consistency: \", d$est,\n                       \" under \", d$dgp))\n    positions &lt;- seq_along(d$ns) * 1.5\n    axis(2, at = positions, labels = paste0(\"n=\", d$ns),\n         las = 1, cex.axis = 0.9)\n\n    for (i in seq_along(d$ns)) {\n      vals &lt;- d$all_results[[i]]\n      dens &lt;- density(vals)\n      dens$y &lt;- dens$y / max(dens$y) * 0.65\n      polygon(dens$x, dens$y + positions[i],\n              col = adjustcolor(colors[i], 0.4),\n              border = colors[i], lwd = 1.5)\n    }\n\n    abline(v = d$mu, lty = 2, lwd = 2.5, col = \"#2c3e50\")\n    text(d$mu, max(positions) + 0.8,\n         paste0(\"True \\u03bc = \", d$mu), cex = 0.9)\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- dat()\n    lines &lt;- sapply(seq_along(d$ns), function(i) {\n      vals &lt;- d$all_results[[i]]\n      paste0(\"n=\", d$ns[i], \": SD = \", round(sd(vals), 4))\n    })\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Spread shrinks with n:&lt;/b&gt;&lt;br&gt;\",\n        paste(lines, collapse = \"&lt;br&gt;\")\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nNormal DGP, Mean vs Median: the mean has smaller variance (it’s the efficient estimator for normal data). The median is less efficient but still unbiased.\nContaminated normal, Mean vs Median: now the median wins. The outliers from the contamination inflate the variance of the mean, but barely affect the median. This is why robust estimators exist.\n10% trimmed mean: a compromise — it trims the most extreme 10% of observations. It handles outliers better than the mean, with less variance than the median. Often the best of both worlds.\nMidrange: the average of the min and max. Under normal data it’s surprisingly efficient for small \\(n\\), but terrible for heavy-tailed or skewed data. The MSE plot shows exactly why.\nConsistency (Sim 2): all three estimators converge to \\(\\mu\\) as \\(n\\) grows. The ridgeline plot gets tighter and more centered with each step.\n\n\n\nThe bottom line\n\nMonte Carlo is the universal verification tool in statistics. If you claim an estimator has some property (unbiased, consistent, 95% coverage), you can check it by simulation.\nBias = does the estimator aim at the right target? Variance = how spread out are the estimates? MSE = bias² + variance = overall accuracy.\nSometimes a biased estimator beats an unbiased one if it has much lower variance. This is the bias-variance tradeoff in miniature.\n\n\n\n\nDid you know?\n\nThe Monte Carlo method was invented by Stanislaw Ulam and John von Neumann during the Manhattan Project in the late 1940s. Ulam was recovering from brain surgery and playing solitaire when he realized that it was easier to estimate the probability of winning by playing many random games than by computing it analytically. He and von Neumann applied this idea to nuclear physics simulations.\nThe name “Monte Carlo” was suggested by Nicholas Metropolis, a colleague, as a reference to the Monte Carlo Casino in Monaco — a nod to the role of randomness. The method was classified as part of the hydrogen bomb program until the 1950s.\nToday, Monte Carlo methods are everywhere: pricing financial derivatives, training AI models, simulating protein folding, predicting elections, and rendering 3D graphics in movies. The basic idea — replace hard math with random simulation — is one of the most powerful tricks in computational science.",
    "crumbs": [
      "Inference",
      "Monte Carlo Experiments"
    ]
  },
  {
    "objectID": "heteroskedasticity.html",
    "href": "heteroskedasticity.html",
    "title": "Homoskedasticity & Heteroskedasticity",
    "section": "",
    "text": "Homoskedasticity (homo = same, skedasis = spread): the variance of the errors is constant across all values of \\(X\\).\nHeteroskedasticity (hetero = different): the variance changes with \\(X\\).\n\nThink of income vs. age. Young people’s incomes are clustered (entry-level jobs). But at age 50, some people earn $40k and others earn $500k. The spread of income increases with age — that’s heteroskedasticity.\n\n\nOLS is still unbiased under heteroskedasticity — the coefficient estimates are fine. But the standard errors are wrong. And wrong standard errors mean wrong confidence intervals, wrong t-statistics, wrong p-values. You might declare something significant when it isn’t (or miss something real).\nThe fix: use robust standard errors (HC, or “sandwich” standard errors) that don’t assume constant variance.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"type\", \"Error structure:\",\n                  choices = c(\"Homoskedastic\",\n                              \"Fan-shaped (variance grows with X)\",\n                              \"U-shaped variance\",\n                              \"Group heteroskedasticity\")),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 50, max = 500, value = 200, step = 50),\n\n      sliderInput(\"b1\", HTML(\"True &beta;&lt;sub&gt;1&lt;/sub&gt;:\"),\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(4, plotOutput(\"scatter\", height = \"380px\")),\n        column(4, plotOutput(\"resid_plot\", height = \"380px\")),\n        column(4, plotOutput(\"ci_plot\", height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    b1   &lt;- input$b1\n    type &lt;- input$type\n\n    x &lt;- runif(n, 0.5, 10)\n\n    if (type == \"Homoskedastic\") {\n      eps &lt;- rnorm(n, sd = 2)\n    } else if (type == \"Fan-shaped (variance grows with X)\") {\n      eps &lt;- rnorm(n, sd = 0.3 * x)\n    } else if (type == \"U-shaped variance\") {\n      eps &lt;- rnorm(n, sd = 0.5 + 0.8 * abs(x - 5))\n    } else {\n      eps &lt;- rnorm(n, sd = ifelse(x &gt; 5, 4, 0.8))\n    }\n\n    y &lt;- 2 + b1 * x + eps\n    fit &lt;- lm(y ~ x)\n\n    # OLS SE\n    ols_se &lt;- summary(fit)$coefficients[2, 2]\n\n    # HC robust SE (manual HC1)\n    r &lt;- resid(fit)\n    X &lt;- cbind(1, x)\n    bread &lt;- solve(t(X) %*% X)\n    meat &lt;- t(X) %*% diag(r^2 * n / (n - 2)) %*% X\n    robust_vcov &lt;- bread %*% meat %*% bread\n    robust_se &lt;- sqrt(robust_vcov[2, 2])\n\n    # Simulation: repeat 500 times, see how often CI covers\n    covers_ols &lt;- 0\n    covers_robust &lt;- 0\n    reps &lt;- 500\n    for (i in seq_len(reps)) {\n      xx &lt;- runif(n, 0.5, 10)\n      if (type == \"Homoskedastic\") {\n        ee &lt;- rnorm(n, sd = 2)\n      } else if (type == \"Fan-shaped (variance grows with X)\") {\n        ee &lt;- rnorm(n, sd = 0.3 * xx)\n      } else if (type == \"U-shaped variance\") {\n        ee &lt;- rnorm(n, sd = 0.5 + 0.8 * abs(xx - 5))\n      } else {\n        ee &lt;- rnorm(n, sd = ifelse(xx &gt; 5, 4, 0.8))\n      }\n      yy &lt;- 2 + b1 * xx + ee\n      ff &lt;- lm(yy ~ xx)\n      b_hat &lt;- coef(ff)[2]\n      ols_s &lt;- summary(ff)$coefficients[2, 2]\n\n      rr &lt;- resid(ff)\n      XX &lt;- cbind(1, xx)\n      br &lt;- solve(t(XX) %*% XX)\n      mt &lt;- t(XX) %*% diag(rr^2 * n / (n - 2)) %*% XX\n      rob_s &lt;- sqrt((br %*% mt %*% br)[2, 2])\n\n      if (b_hat - 1.96 * ols_s &lt;= b1 && b1 &lt;= b_hat + 1.96 * ols_s)\n        covers_ols &lt;- covers_ols + 1\n      if (b_hat - 1.96 * rob_s &lt;= b1 && b1 &lt;= b_hat + 1.96 * rob_s)\n        covers_robust &lt;- covers_robust + 1\n    }\n\n    list(x = x, y = y, fit = fit, type = type, b1 = b1,\n         ols_se = ols_se, robust_se = robust_se,\n         cover_ols = covers_ols / reps,\n         cover_robust = covers_robust / reps)\n  })\n\n  output$scatter &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$x, d$y, pch = 16, col = \"#3498db60\", cex = 0.7,\n         xlab = \"X\", ylab = \"Y\", main = \"Data + OLS fit\")\n    abline(d$fit, col = \"#e74c3c\", lwd = 2.5)\n  })\n\n  output$resid_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    r  &lt;- resid(d$fit)\n    fv &lt;- fitted(d$fit)\n    plot(fv, r, pch = 16, col = \"#9b59b660\", cex = 0.7,\n         xlab = \"Fitted values\", ylab = \"Residuals\",\n         main = \"Residuals vs Fitted\")\n    abline(h = 0, lty = 2, col = \"gray40\", lwd = 1.5)\n\n    # Envelope\n    lo_abs &lt;- loess(abs(r) ~ fv)\n    ox &lt;- order(fv)\n    env &lt;- predict(lo_abs)[ox]\n    lines(fv[ox], env, col = \"#e67e22\", lwd = 2, lty = 1)\n    lines(fv[ox], -env, col = \"#e67e22\", lwd = 2, lty = 1)\n  })\n\n  output$ci_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 6, 3, 1))\n\n    b_hat &lt;- coef(d$fit)[2]\n\n    ci_ols &lt;- c(b_hat - 1.96 * d$ols_se, b_hat + 1.96 * d$ols_se)\n    ci_rob &lt;- c(b_hat - 1.96 * d$robust_se, b_hat + 1.96 * d$robust_se)\n\n    xlim &lt;- range(c(ci_ols, ci_rob, d$b1)) + c(-0.3, 0.3)\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, 2.5),\n         yaxt = \"n\", ylab = \"\", xlab = expression(beta[1]),\n         main = \"95% Confidence Intervals\")\n    axis(2, at = 1:2, labels = c(\"OLS SE\", \"Robust SE\"), las = 1, cex.axis = 0.85)\n\n    segments(ci_ols[1], 1, ci_ols[2], 1, lwd = 4, col = \"#e74c3c\")\n    points(b_hat, 1, pch = 19, cex = 1.5, col = \"#e74c3c\")\n\n    segments(ci_rob[1], 2, ci_rob[2], 2, lwd = 4, col = \"#27ae60\")\n    points(b_hat, 2, pch = 19, cex = 1.5, col = \"#27ae60\")\n\n    abline(v = d$b1, lty = 2, lwd = 2, col = \"#2c3e50\")\n    text(d$b1, 2.4, expression(\"True \" * beta[1]), cex = 0.9)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;OLS SE:&lt;/b&gt; \", round(d$ols_se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Robust SE:&lt;/b&gt; \", round(d$robust_se, 4), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;OLS CI coverage:&lt;/b&gt; &lt;span class='\",\n        ifelse(abs(d$cover_ols - 0.95) &gt; 0.03, \"bad\", \"good\"), \"'&gt;\",\n        round(d$cover_ols * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Robust CI coverage:&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$cover_robust * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;small&gt;Target: 95%. Over 500 simulations.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nHomoskedastic: both SEs are similar, both CIs have ~95% coverage. Constant variance = OLS SEs are fine.\nFan-shaped: the residual plot shows a clear funnel. OLS SEs are wrong — coverage drops below 95%. Robust SEs fix it.\nGroup heteroskedasticity: one group (X &gt; 5) is much noisier. Look at the residual plot — the right side has wider scatter. OLS pretends the variance is the same everywhere.\nCompare the right panel: under heteroskedasticity, the OLS CI (red) is often the wrong width. The robust CI (green) adjusts.\n\n\n\n\nIn applied work, always use robust standard errors (or clustered SEs). They are valid whether or not heteroskedasticity is present. If the errors happen to be homoskedastic, robust SEs give you the same answer anyway — there’s no downside.\n\n\n\nSoftware\nHow to get robust SEs\n\n\n\n\nR\nlmtest::coeftest(fit, vcov = sandwich::vcovHC)\n\n\nStata\nreg y x, robust\n\n\nPython\nsm.OLS(y, X).fit(cov_type='HC3')\n\n\n\n\n\n\n\n\nNobody can agree on how to spell it. “Heteroskedasticity” (with a k) is the original Greek-derived spelling (skedasis = scattering). “Heteroscedasticity” (with a c) is the Latinized version. Both are correct. Econometricians tend to use k; other fields use c. The important thing is that your standard errors are right, not your spelling.\nHalbert White published his famous robust standard error estimator (HC0) in 1980. It was so influential that “White standard errors” became the default in applied economics. The paper has over 30,000 citations.\nThe HC in HC0, HC1, HC2, HC3 stands for “heteroskedasticity-consistent.” HC3 is generally preferred for small samples because it gives each observation a slightly larger weight, correcting for leverage points.\nFun debugging tip: if your robust SEs are much larger than your OLS SEs, you likely have heteroskedasticity. If they’re much smaller, something is probably wrong with your data.",
    "crumbs": [
      "Regression & Diagnostics",
      "Homo- & Heteroskedasticity"
    ]
  },
  {
    "objectID": "heteroskedasticity.html#what-do-these-words-mean",
    "href": "heteroskedasticity.html#what-do-these-words-mean",
    "title": "Homoskedasticity & Heteroskedasticity",
    "section": "",
    "text": "Homoskedasticity (homo = same, skedasis = spread): the variance of the errors is constant across all values of \\(X\\).\nHeteroskedasticity (hetero = different): the variance changes with \\(X\\).\n\nThink of income vs. age. Young people’s incomes are clustered (entry-level jobs). But at age 50, some people earn $40k and others earn $500k. The spread of income increases with age — that’s heteroskedasticity.\n\n\nOLS is still unbiased under heteroskedasticity — the coefficient estimates are fine. But the standard errors are wrong. And wrong standard errors mean wrong confidence intervals, wrong t-statistics, wrong p-values. You might declare something significant when it isn’t (or miss something real).\nThe fix: use robust standard errors (HC, or “sandwich” standard errors) that don’t assume constant variance.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"type\", \"Error structure:\",\n                  choices = c(\"Homoskedastic\",\n                              \"Fan-shaped (variance grows with X)\",\n                              \"U-shaped variance\",\n                              \"Group heteroskedasticity\")),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 50, max = 500, value = 200, step = 50),\n\n      sliderInput(\"b1\", HTML(\"True &beta;&lt;sub&gt;1&lt;/sub&gt;:\"),\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(4, plotOutput(\"scatter\", height = \"380px\")),\n        column(4, plotOutput(\"resid_plot\", height = \"380px\")),\n        column(4, plotOutput(\"ci_plot\", height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    b1   &lt;- input$b1\n    type &lt;- input$type\n\n    x &lt;- runif(n, 0.5, 10)\n\n    if (type == \"Homoskedastic\") {\n      eps &lt;- rnorm(n, sd = 2)\n    } else if (type == \"Fan-shaped (variance grows with X)\") {\n      eps &lt;- rnorm(n, sd = 0.3 * x)\n    } else if (type == \"U-shaped variance\") {\n      eps &lt;- rnorm(n, sd = 0.5 + 0.8 * abs(x - 5))\n    } else {\n      eps &lt;- rnorm(n, sd = ifelse(x &gt; 5, 4, 0.8))\n    }\n\n    y &lt;- 2 + b1 * x + eps\n    fit &lt;- lm(y ~ x)\n\n    # OLS SE\n    ols_se &lt;- summary(fit)$coefficients[2, 2]\n\n    # HC robust SE (manual HC1)\n    r &lt;- resid(fit)\n    X &lt;- cbind(1, x)\n    bread &lt;- solve(t(X) %*% X)\n    meat &lt;- t(X) %*% diag(r^2 * n / (n - 2)) %*% X\n    robust_vcov &lt;- bread %*% meat %*% bread\n    robust_se &lt;- sqrt(robust_vcov[2, 2])\n\n    # Simulation: repeat 500 times, see how often CI covers\n    covers_ols &lt;- 0\n    covers_robust &lt;- 0\n    reps &lt;- 500\n    for (i in seq_len(reps)) {\n      xx &lt;- runif(n, 0.5, 10)\n      if (type == \"Homoskedastic\") {\n        ee &lt;- rnorm(n, sd = 2)\n      } else if (type == \"Fan-shaped (variance grows with X)\") {\n        ee &lt;- rnorm(n, sd = 0.3 * xx)\n      } else if (type == \"U-shaped variance\") {\n        ee &lt;- rnorm(n, sd = 0.5 + 0.8 * abs(xx - 5))\n      } else {\n        ee &lt;- rnorm(n, sd = ifelse(xx &gt; 5, 4, 0.8))\n      }\n      yy &lt;- 2 + b1 * xx + ee\n      ff &lt;- lm(yy ~ xx)\n      b_hat &lt;- coef(ff)[2]\n      ols_s &lt;- summary(ff)$coefficients[2, 2]\n\n      rr &lt;- resid(ff)\n      XX &lt;- cbind(1, xx)\n      br &lt;- solve(t(XX) %*% XX)\n      mt &lt;- t(XX) %*% diag(rr^2 * n / (n - 2)) %*% XX\n      rob_s &lt;- sqrt((br %*% mt %*% br)[2, 2])\n\n      if (b_hat - 1.96 * ols_s &lt;= b1 && b1 &lt;= b_hat + 1.96 * ols_s)\n        covers_ols &lt;- covers_ols + 1\n      if (b_hat - 1.96 * rob_s &lt;= b1 && b1 &lt;= b_hat + 1.96 * rob_s)\n        covers_robust &lt;- covers_robust + 1\n    }\n\n    list(x = x, y = y, fit = fit, type = type, b1 = b1,\n         ols_se = ols_se, robust_se = robust_se,\n         cover_ols = covers_ols / reps,\n         cover_robust = covers_robust / reps)\n  })\n\n  output$scatter &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$x, d$y, pch = 16, col = \"#3498db60\", cex = 0.7,\n         xlab = \"X\", ylab = \"Y\", main = \"Data + OLS fit\")\n    abline(d$fit, col = \"#e74c3c\", lwd = 2.5)\n  })\n\n  output$resid_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    r  &lt;- resid(d$fit)\n    fv &lt;- fitted(d$fit)\n    plot(fv, r, pch = 16, col = \"#9b59b660\", cex = 0.7,\n         xlab = \"Fitted values\", ylab = \"Residuals\",\n         main = \"Residuals vs Fitted\")\n    abline(h = 0, lty = 2, col = \"gray40\", lwd = 1.5)\n\n    # Envelope\n    lo_abs &lt;- loess(abs(r) ~ fv)\n    ox &lt;- order(fv)\n    env &lt;- predict(lo_abs)[ox]\n    lines(fv[ox], env, col = \"#e67e22\", lwd = 2, lty = 1)\n    lines(fv[ox], -env, col = \"#e67e22\", lwd = 2, lty = 1)\n  })\n\n  output$ci_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 6, 3, 1))\n\n    b_hat &lt;- coef(d$fit)[2]\n\n    ci_ols &lt;- c(b_hat - 1.96 * d$ols_se, b_hat + 1.96 * d$ols_se)\n    ci_rob &lt;- c(b_hat - 1.96 * d$robust_se, b_hat + 1.96 * d$robust_se)\n\n    xlim &lt;- range(c(ci_ols, ci_rob, d$b1)) + c(-0.3, 0.3)\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, 2.5),\n         yaxt = \"n\", ylab = \"\", xlab = expression(beta[1]),\n         main = \"95% Confidence Intervals\")\n    axis(2, at = 1:2, labels = c(\"OLS SE\", \"Robust SE\"), las = 1, cex.axis = 0.85)\n\n    segments(ci_ols[1], 1, ci_ols[2], 1, lwd = 4, col = \"#e74c3c\")\n    points(b_hat, 1, pch = 19, cex = 1.5, col = \"#e74c3c\")\n\n    segments(ci_rob[1], 2, ci_rob[2], 2, lwd = 4, col = \"#27ae60\")\n    points(b_hat, 2, pch = 19, cex = 1.5, col = \"#27ae60\")\n\n    abline(v = d$b1, lty = 2, lwd = 2, col = \"#2c3e50\")\n    text(d$b1, 2.4, expression(\"True \" * beta[1]), cex = 0.9)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;OLS SE:&lt;/b&gt; \", round(d$ols_se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Robust SE:&lt;/b&gt; \", round(d$robust_se, 4), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;OLS CI coverage:&lt;/b&gt; &lt;span class='\",\n        ifelse(abs(d$cover_ols - 0.95) &gt; 0.03, \"bad\", \"good\"), \"'&gt;\",\n        round(d$cover_ols * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Robust CI coverage:&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$cover_robust * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;small&gt;Target: 95%. Over 500 simulations.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nHomoskedastic: both SEs are similar, both CIs have ~95% coverage. Constant variance = OLS SEs are fine.\nFan-shaped: the residual plot shows a clear funnel. OLS SEs are wrong — coverage drops below 95%. Robust SEs fix it.\nGroup heteroskedasticity: one group (X &gt; 5) is much noisier. Look at the residual plot — the right side has wider scatter. OLS pretends the variance is the same everywhere.\nCompare the right panel: under heteroskedasticity, the OLS CI (red) is often the wrong width. The robust CI (green) adjusts.\n\n\n\n\nIn applied work, always use robust standard errors (or clustered SEs). They are valid whether or not heteroskedasticity is present. If the errors happen to be homoskedastic, robust SEs give you the same answer anyway — there’s no downside.\n\n\n\nSoftware\nHow to get robust SEs\n\n\n\n\nR\nlmtest::coeftest(fit, vcov = sandwich::vcovHC)\n\n\nStata\nreg y x, robust\n\n\nPython\nsm.OLS(y, X).fit(cov_type='HC3')\n\n\n\n\n\n\n\n\nNobody can agree on how to spell it. “Heteroskedasticity” (with a k) is the original Greek-derived spelling (skedasis = scattering). “Heteroscedasticity” (with a c) is the Latinized version. Both are correct. Econometricians tend to use k; other fields use c. The important thing is that your standard errors are right, not your spelling.\nHalbert White published his famous robust standard error estimator (HC0) in 1980. It was so influential that “White standard errors” became the default in applied economics. The paper has over 30,000 citations.\nThe HC in HC0, HC1, HC2, HC3 stands for “heteroskedasticity-consistent.” HC3 is generally preferred for small samples because it gives each observation a slightly larger weight, correcting for leverage points.\nFun debugging tip: if your robust SEs are much larger than your OLS SEs, you likely have heteroskedasticity. If they’re much smaller, something is probably wrong with your data.",
    "crumbs": [
      "Regression & Diagnostics",
      "Homo- & Heteroskedasticity"
    ]
  },
  {
    "objectID": "variance-sd-se.html",
    "href": "variance-sd-se.html",
    "title": "Variance, SD & Standard Error",
    "section": "",
    "text": "Imagine you’re measuring the heights of students in a class. Some are tall, some are short — there’s spread. Statistics gives us precise language for that spread:\n\n\n\n\n\n\n\n\nConcept\nWhat it measures\nFormula\n\n\n\n\nVariance\nAverage squared distance from the mean\n\\(\\sigma^2 = \\frac{1}{N}\\sum(x_i - \\mu)^2\\)\n\n\nStandard Deviation (SD)\nAverage distance from the mean (in original units)\n\\(\\sigma = \\sqrt{\\sigma^2}\\)\n\n\nStandard Error (SE)\nHow much the sample mean bounces around\n\\(SE = \\frac{\\sigma}{\\sqrt{n}}\\)\n\n\n\nThe key insight: SD measures spread in your data. SE measures spread in your estimates.",
    "crumbs": [
      "Probability & Uncertainty",
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#the-big-picture",
    "href": "variance-sd-se.html#the-big-picture",
    "title": "Variance, SD & Standard Error",
    "section": "",
    "text": "Imagine you’re measuring the heights of students in a class. Some are tall, some are short — there’s spread. Statistics gives us precise language for that spread:\n\n\n\n\n\n\n\n\nConcept\nWhat it measures\nFormula\n\n\n\n\nVariance\nAverage squared distance from the mean\n\\(\\sigma^2 = \\frac{1}{N}\\sum(x_i - \\mu)^2\\)\n\n\nStandard Deviation (SD)\nAverage distance from the mean (in original units)\n\\(\\sigma = \\sqrt{\\sigma^2}\\)\n\n\nStandard Error (SE)\nHow much the sample mean bounces around\n\\(SE = \\frac{\\sigma}{\\sqrt{n}}\\)\n\n\n\nThe key insight: SD measures spread in your data. SE measures spread in your estimates.",
    "crumbs": [
      "Probability & Uncertainty",
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#variance-measuring-spread",
    "href": "variance-sd-se.html#variance-measuring-spread",
    "title": "Variance, SD & Standard Error",
    "section": "Variance: Measuring Spread",
    "text": "Variance: Measuring Spread\nVariance answers: “On average, how far are values from the center?”\nWe square the deviations because:\n\nPositive and negative deviations would cancel out otherwise\nIt penalizes large deviations more than small ones\nIt has nice mathematical properties (additive for independent variables)\n\n\nPopulation Variance vs Sample Variance\nHere’s where it gets tricky. There are two formulas:\nPopulation variance (when you have the entire population): \\[\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\mu)^2\\]\nSample variance (when you have a sample from a larger population): \\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\\]\n\n\nWhy n - 1? (Bessel’s Correction)\nWhen you compute deviations from the sample mean \\(\\bar{x}\\) instead of the true mean \\(\\mu\\), you’re using the data twice — once to compute \\(\\bar{x}\\), then again to compute deviations from it. The sample mean is always closer to the data points than the true mean would be, so dividing by \\(n\\) systematically underestimates the true variance.\nThink of it this way: if you have \\(n = 1\\) data point, the sample mean equals that point, so the deviation is 0. But that doesn’t mean there’s no variability in the population! Dividing by \\(n - 1 = 0\\) makes the formula undefined, which correctly tells you: you can’t estimate spread from a single observation.\nThe \\(n - 1\\) is called the degrees of freedom — you “used up” one degree of freedom estimating the mean.\nTry the simulation below to see this in action:\n#| standalone: true\n#| viewerHeight: 750\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Why n - 1?\"),\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n      sliderInput(\"pop_sd\", \"Population SD (\\u03c3):\", min = 2, max = 25, value = 10, step = 1),\n      sliderInput(\"samp_n\", \"Sample size (n):\", min = 2, max = 100, value = 10),\n      sliderInput(\"n_reps\", \"Number of samples:\", min = 100, max = 2000, value = 500, step = 100),\n      hr(),\n      h4(\"Results\"),\n      uiOutput(\"results_box\"),\n      hr(),\n      p(strong(\"Bias check:\"), \"If the average of many sample estimates equals the true value, the estimator is unbiased.\"),\n      p(\"Notice: dividing by n underestimates. Dividing by n-1 gets it right on average.\")\n    ),\n    mainPanel(\n      width = 9,\n      plotOutput(\"dist_plot\", height = \"600px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  sim &lt;- reactive({\n    mu    &lt;- 50\n    sigma &lt;- input$pop_sd\n    n     &lt;- input$samp_n\n    n_reps &lt;- input$n_reps\n\n    var_n  &lt;- numeric(n_reps)\n    var_n1 &lt;- numeric(n_reps)\n\n    for (i in 1:n_reps) {\n      samp &lt;- rnorm(n, mu, sigma)\n      xbar &lt;- mean(samp)\n      devs &lt;- (samp - xbar)^2\n      var_n[i]  &lt;- sum(devs) / n\n      var_n1[i] &lt;- sum(devs) / (n - 1)\n    }\n\n    list(var_n = var_n, var_n1 = var_n1, true_var = sigma^2, n = n)\n  })\n\n  output$dist_plot &lt;- renderPlot({\n    v &lt;- sim()\n\n    par(mfrow = c(2, 1), mar = c(4.5, 4.5, 3, 1))\n\n    xlims &lt;- range(c(v$var_n, v$var_n1))\n    hist(v$var_n, breaks = 40, col = adjustcolor(\"#3498db\", 0.6), border = \"white\",\n         main = paste0(\"Divide by n (biased)  \\u2014  n = \", v$n),\n         xlab = \"Estimated Variance\", xlim = xlims, cex.main = 1.3)\n    abline(v = v$true_var, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n    abline(v = mean(v$var_n), col = \"#2c3e50\", lwd = 2)\n    legend(\"topright\", legend = c(paste0(\"True = \", v$true_var),\n           paste0(\"Avg = \", round(mean(v$var_n), 2))),\n           col = c(\"#e74c3c\", \"#2c3e50\"), lwd = 2, lty = c(2, 1), bty = \"n\")\n\n    hist(v$var_n1, breaks = 40, col = adjustcolor(\"#2ecc71\", 0.6), border = \"white\",\n         main = \"Divide by n\\u22121 (unbiased)\",\n         xlab = \"Estimated Variance\", xlim = xlims, cex.main = 1.3)\n    abline(v = v$true_var, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n    abline(v = mean(v$var_n1), col = \"#2c3e50\", lwd = 2)\n    legend(\"topright\", legend = c(paste0(\"True = \", v$true_var),\n           paste0(\"Avg = \", round(mean(v$var_n1), 2))),\n           col = c(\"#e74c3c\", \"#2c3e50\"), lwd = 2, lty = c(2, 1), bty = \"n\")\n  })\n\n  output$results_box &lt;- renderUI({\n    v &lt;- sim()\n    tags$div(\n      style = \"background:#f0f4f8; border-radius:6px; padding:12px; font-size:14px; line-height:1.9;\",\n      HTML(paste0(\n        \"&lt;b&gt;True variance:&lt;/b&gt; \", v$true_var, \"&lt;br&gt;\",\n        \"&lt;b&gt;Avg (/ n):&lt;/b&gt; \", round(mean(v$var_n), 2), \"&lt;br&gt;\",\n        \"&lt;b&gt;Avg (/ n\\u22121):&lt;/b&gt; \", round(mean(v$var_n1), 2)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Probability & Uncertainty",
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#standard-deviation-back-to-original-units",
    "href": "variance-sd-se.html#standard-deviation-back-to-original-units",
    "title": "Variance, SD & Standard Error",
    "section": "Standard Deviation: Back to Original Units",
    "text": "Standard Deviation: Back to Original Units\nVariance is in squared units. If heights are in cm, variance is in cm². That’s hard to interpret.\nStandard deviation = \\(\\sqrt{\\text{variance}}\\) — it puts spread back in the original units.\n\\[\\sigma = \\sqrt{\\sigma^2} \\qquad \\text{(population)} \\qquad\\qquad s = \\sqrt{s^2} \\qquad \\text{(sample)}\\]\nRules of thumb (for roughly normal data):\n\n~68% of data falls within ±1 SD of the mean\n~95% of data falls within ±2 SD of the mean\n~99.7% falls within ±3 SD",
    "crumbs": [
      "Probability & Uncertainty",
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#standard-error-the-sd-of-your-estimate",
    "href": "variance-sd-se.html#standard-error-the-sd-of-your-estimate",
    "title": "Variance, SD & Standard Error",
    "section": "Standard Error: The SD of Your Estimate",
    "text": "Standard Error: The SD of Your Estimate\nHere’s where people get confused. The standard error is not about your data — it’s about your estimate.\nIf you took many samples of size \\(n\\) and computed the mean each time, those means would form a distribution (the sampling distribution). The standard deviation of that distribution is the standard error:\n\\[SE(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}}\\]\nIn practice, we don’t know \\(\\sigma\\), so we plug in \\(s\\):\n\\[\\widehat{SE}(\\bar{x}) = \\frac{s}{\\sqrt{n}}\\]\n\nWhy does SE = SD / √n? The Intuition\nImagine you want to know the average commute time in your city. You ask one person — they say 45 minutes. But that’s just one person. Maybe they live far away. Your estimate is noisy — it could easily be off by 20 minutes (the SD of commute times).\nNow you ask 4 people and average: 45, 25, 60, 30 → mean = 40 minutes. Some are high, some are low — the errors partially cancel out. Your estimate is less noisy. But not 4× less noisy — only 2× less noisy (because \\(\\sqrt{4} = 2\\)).\nAsk 100 people: the cancellation is even stronger. Your average is now 10× more precise than a single observation (\\(\\sqrt{100} = 10\\)).\nWhy square root and not just n? Because the errors don’t perfectly cancel — they’re random, so sometimes more are high than low. The cancellation is partial. Mathematically, independent errors cancel at the rate of \\(\\sqrt{n}\\), not \\(n\\). If errors perfectly cancelled, the SE would be \\(\\sigma / n\\) and you’d barely need any data. If they didn’t cancel at all, the SE would stay at \\(\\sigma\\) forever and more data would be useless. The \\(\\sqrt{n}\\) is the sweet spot of reality.\n\n\nThe Math Behind It\nEach observation \\(x_i\\) is an independent draw with variance \\(\\sigma^2\\). What does that actually mean?\nThe population has a distribution with some spread — that spread is \\(\\sigma^2\\). When you sample one person, you don’t know what you’ll get. You might get someone with a long commute or a short one. Before you observe them, that measurement is uncertain — it could land anywhere in the population distribution. That uncertainty is \\(\\sigma^2\\). Every observation carries the same amount of uncertainty because every observation comes from the same population.\n“Independent” means knowing what the first person said tells you nothing about what the second person will say. Each person is a fresh roll of the dice. So you’re averaging \\(n\\) equally-noisy, independent measurements — and the question is: how much noise survives the averaging?\nThe sample mean is:\n\\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\]\nStep 1: The variance of a sum of independent random variables equals the sum of their variances. Each \\(x_i\\) contributes \\(\\sigma^2\\):\n\\[\\text{Var}\\left(\\sum_{i=1}^n x_i\\right) = \\sigma^2 + \\sigma^2 + \\cdots + \\sigma^2 = n\\sigma^2\\]\nThe noise grows with \\(n\\) — more observations means more total variability in the sum. But we don’t want the sum, we want the average.\nStep 2: The mean divides the sum by \\(n\\). When you multiply a random variable by a constant \\(c\\), its variance gets multiplied by \\(c^2\\) (because variance is in squared units):\n\\[\\text{Var}(cX) = c^2 \\cdot \\text{Var}(X)\\]\nSo dividing by \\(n\\) means multiplying by \\(1/n\\), which divides the variance by \\(n^2\\):\n\\[\\text{Var}(\\bar{x}) = \\frac{1}{n^2} \\cdot n\\sigma^2 = \\frac{\\sigma^2}{n}\\]\nStep 3: Take the square root to get back to original units:\n\\[SE = \\sqrt{\\text{Var}(\\bar{x})} = \\sqrt{\\frac{\\sigma^2}{n}} = \\frac{\\sigma}{\\sqrt{n}}\\]\nThe sum’s noise grows as \\(n\\), but dividing by \\(n\\) shrinks it by \\(n^2\\). Net effect: noise shrinks by \\(n^2 / n = n\\), and the square root gives us \\(\\sqrt{n}\\). Averaging \\(n\\) independent measurements reduces noise by \\(\\sqrt{n}\\).\n\n\nThe Key Relationship\n\\[\\boxed{SE = \\frac{SD}{\\sqrt{n}}}\\]\nThis single formula connects everything:\n\nSD tells you how spread out individual data points are\nn is your sample size\nSE tells you how precisely you’ve estimated the mean\nMore data → smaller SE → more precise estimate\n\n#| standalone: true\n#| viewerHeight: 800\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"SD vs SE: What's the Difference?\"),\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n      sliderInput(\"true_sd\", \"Population SD:\", min = 5, max = 40, value = 15, step = 1),\n      sliderInput(\"n_obs\", \"Sample size (n):\", min = 5, max = 200, value = 25),\n      sliderInput(\"n_samples\", \"Samples to draw:\", min = 50, max = 1000, value = 300, step = 50),\n      hr(),\n      h4(\"Theory\"),\n      uiOutput(\"theory_box\"),\n      hr(),\n      h4(\"Simulation\"),\n      uiOutput(\"sim_box\")\n    ),\n    mainPanel(\n      width = 9,\n      plotOutput(\"main_plot\", height = \"650px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  sim &lt;- reactive({\n    mu    &lt;- 100\n    sigma &lt;- input$true_sd\n    n     &lt;- input$n_obs\n    n_samp &lt;- input$n_samples\n\n    one_sample &lt;- rnorm(n, mu, sigma)\n    means &lt;- replicate(n_samp, mean(rnorm(n, mu, sigma)))\n\n    list(one_sample = one_sample, means = means, mu = mu, sigma = sigma, n = n)\n  })\n\n  output$theory_box &lt;- renderUI({\n    d &lt;- sim()\n    se_theory &lt;- round(d$sigma / sqrt(d$n), 2)\n    tagList(\n      p(paste0(\"SD = \", d$sigma)),\n      p(paste0(\"SE = SD/\\u221an = \", d$sigma, \"/\\u221a\", d$n, \" = \", se_theory))\n    )\n  })\n\n  output$sim_box &lt;- renderUI({\n    d &lt;- sim()\n    tagList(\n      p(paste0(\"SD of one sample: \", round(sd(d$one_sample), 2))),\n      p(paste0(\"SD of sample means: \", round(sd(d$means), 2))),\n      p(paste0(\"Ratio: \", round(sd(d$one_sample) / sd(d$means), 1), \"x narrower\"))\n    )\n  })\n\n  output$main_plot &lt;- renderPlot({\n    d &lt;- sim()\n\n    sample_sd &lt;- sd(d$one_sample)\n    se_actual &lt;- sd(d$means)\n    xbar &lt;- mean(d$one_sample)\n\n    xlims &lt;- c(d$mu - 3.5 * d$sigma, d$mu + 3.5 * d$sigma)\n\n    par(mfrow = c(2, 1), mar = c(4.5, 4.5, 3.5, 1))\n\n    hist(d$one_sample, breaks = 30, col = adjustcolor(\"#3498db\", 0.6), border = \"white\",\n         main = paste0(\"One Sample (n = \", d$n, \") \\u2014 spread = SD\"),\n         xlab = \"Value\", xlim = xlims, freq = FALSE, cex.main = 1.3)\n    abline(v = xbar, col = \"#e74c3c\", lwd = 2.5)\n    segments(xbar - sample_sd, 0, xbar + sample_sd, 0, col = \"#e67e22\", lwd = 4)\n    mtext(paste0(\"Mean = \", round(xbar, 1), \"  |  SD = \", round(sample_sd, 1)),\n          side = 3, line = 0, cex = 1.1, font = 2)\n\n    hist(d$means, breaks = 40, col = adjustcolor(\"#2ecc71\", 0.6), border = \"white\",\n         main = \"Sampling Distribution of the Mean \\u2014 spread = SE\",\n         xlab = \"Sample Mean\", xlim = xlims, freq = FALSE, cex.main = 1.3)\n    xseq &lt;- seq(xlims[1], xlims[2], length.out = 300)\n    lines(xseq, dnorm(xseq, d$mu, d$sigma / sqrt(d$n)), col = \"#8e44ad\", lwd = 2.5)\n    abline(v = d$mu, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n    segments(d$mu - se_actual, 0, d$mu + se_actual, 0, col = \"#e67e22\", lwd = 4)\n    mtext(paste0(\"Mean of means = \", round(mean(d$means), 1), \"  |  SE = \", round(se_actual, 2)),\n          side = 3, line = 0, cex = 1.1, font = 2)\n    legend(\"topright\", \"Theoretical N(\\u03bc, SD/\\u221an)\", col = \"#8e44ad\", lwd = 2.5, bty = \"n\")\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Probability & Uncertainty",
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#how-they-all-connect",
    "href": "variance-sd-se.html#how-they-all-connect",
    "title": "Variance, SD & Standard Error",
    "section": "How They All Connect",
    "text": "How They All Connect\nLet’s see all three in one place. Watch what happens as you increase \\(n\\):\n\nSD stays the same — it’s a property of the population, not the sample size\nSE shrinks — more data means a more precise estimate\nSample variance (s²) bounces around σ² — but is unbiased on average\n\n#| standalone: true\n#| viewerHeight: 700\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"SD vs SE as n Grows\"),\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n      sliderInput(\"sigma\", \"Population SD:\", min = 2, max = 30, value = 10),\n      sliderInput(\"n_size\", \"Sample size (n):\", min = 5, max = 500, value = 10),\n      p(style = \"margin-top:12px;\",\n        strong(\"What to notice:\"),\n        br(),\n        \"Blue histogram (data) stays equally wide — SD doesn't shrink with n\",\n        br(), br(),\n        \"Green histogram (means) gets narrower — SE shrinks with sqrt(n)\",\n        br(), br(),\n        \"At n = 100, SE is 10x smaller than SD\"\n      )\n    ),\n    mainPanel(\n      width = 9,\n      plotOutput(\"combo_plot\", height = \"280px\"),\n      plotOutput(\"se_curve\", height = \"300px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  output$combo_plot &lt;- renderPlot({\n    mu &lt;- 50\n    sigma &lt;- input$sigma\n    n &lt;- input$n_size\n\n    one_samp &lt;- rnorm(n, mu, sigma)\n    means &lt;- replicate(500, mean(rnorm(n, mu, sigma)))\n\n    xlims &lt;- c(mu - 3.5 * sigma, mu + 3.5 * sigma)\n\n    par(mfrow = c(1, 2), mar = c(4, 4, 3, 1))\n\n    hist(one_samp, breaks = 30, col = adjustcolor(\"#3498db\", 0.6), border = \"white\",\n         main = paste0(\"One Sample (SD = \", round(sd(one_samp), 1), \")\"),\n         xlab = \"Value\", xlim = xlims, freq = FALSE, cex.main = 1.2)\n    abline(v = mu, col = \"#e74c3c\", lwd = 2, lty = 2)\n\n    hist(means, breaks = 40, col = adjustcolor(\"#2ecc71\", 0.6), border = \"white\",\n         main = paste0(\"Sample Means (SE = \", round(sd(means), 2), \")\"),\n         xlab = \"Value\", xlim = xlims, freq = FALSE, cex.main = 1.2)\n    abline(v = mu, col = \"#e74c3c\", lwd = 2, lty = 2)\n  })\n\n  output$se_curve &lt;- renderPlot({\n    sigma &lt;- input$sigma\n    n_curr &lt;- input$n_size\n\n    ns &lt;- 2:500\n    ses &lt;- sigma / sqrt(ns)\n    se_now &lt;- sigma / sqrt(n_curr)\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(ns, ses, type = \"l\", lwd = 2.5, col = \"#2c3e50\",\n         xlab = \"Sample Size (n)\", ylab = \"Standard Error\",\n         main = \"SE = SD / sqrt(n) — diminishing returns\",\n         ylim = c(0, max(ses) * 1.1), cex.main = 1.3)\n\n    abline(h = sigma, col = \"#3498db\", lwd = 2, lty = 2)\n    points(n_curr, se_now, pch = 19, cex = 2, col = \"#e74c3c\")\n    segments(n_curr, 0, n_curr, se_now, lty = 2, col = \"#e74c3c\")\n    segments(0, se_now, n_curr, se_now, lty = 2, col = \"#e74c3c\")\n\n    text(400, sigma + 0.8, paste0(\"SD = \", sigma), col = \"#3498db\", cex = 1.2, font = 2)\n    text(n_curr + 25, se_now + 0.8, paste0(\"SE = \", round(se_now, 2)),\n         col = \"#e74c3c\", cex = 1.1, font = 2, adj = 0)\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Probability & Uncertainty",
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#se-and-mde-why-se-determines-your-experiments-power",
    "href": "variance-sd-se.html#se-and-mde-why-se-determines-your-experiments-power",
    "title": "Variance, SD & Standard Error",
    "section": "SE and MDE: Why SE Determines Your Experiment’s Power",
    "text": "SE and MDE: Why SE Determines Your Experiment’s Power\nIf you run an experiment, the Minimum Detectable Effect (MDE) — the smallest effect you can reliably catch — is directly driven by the SE:\n\\[MDE = (z_{\\alpha/2} + z_{\\beta}) \\times SE\\]\nFor a two-sample experiment with equal groups (\\(\\sigma = 1\\) for simplicity):\n\\[MDE = (z_{\\alpha/2} + z_{\\beta}) \\times \\sqrt{\\frac{2}{n}}\\]\nThat \\(\\sqrt{2/n}\\) term is the SE of the difference in means. So MDE is just a scaled-up SE. The critical values (\\(z_{\\alpha/2} + z_{\\beta} \\approx 2.8\\) for 5% significance and 80% power) are just multipliers.\nThis means:\n\nShrink SE → shrink MDE → detect smaller effects. The only levers are: increase \\(n\\), or reduce \\(\\sigma\\) (better measurement, stratification, controls).\nPower analysis is really an SE calculation. You’re asking: “How small can I make my SE with this sample size?”\nIf your SE is too large, no statistical test can save you. The effect will be buried in noise.\n\nSee the Power, Alpha, Beta & MDE page for interactive simulations.",
    "crumbs": [
      "Probability & Uncertainty",
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#quick-reference",
    "href": "variance-sd-se.html#quick-reference",
    "title": "Variance, SD & Standard Error",
    "section": "Quick Reference",
    "text": "Quick Reference\n\n\n\n\n\n\n\n\n\n\nVariance (σ² or s²)\nStandard Deviation (σ or s)\nStandard Error (SE)\n\n\n\n\nMeasures\nSpread of data (squared)\nSpread of data (original units)\nPrecision of an estimate\n\n\nPopulation\n\\(\\sigma^2 = \\frac{1}{N}\\sum(x_i - \\mu)^2\\)\n\\(\\sigma = \\sqrt{\\sigma^2}\\)\n\\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\n\nSample\n\\(s^2 = \\frac{1}{n-1}\\sum(x_i - \\bar{x})^2\\)\n\\(s = \\sqrt{s^2}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\nChanges with n?\nConverges to σ²\nConverges to σ\nShrinks as \\(1/\\sqrt{n}\\)\n\n\nUsed for\nMath convenience\nDescribing data\nConfidence intervals, hypothesis tests\n\n\n\n\nCommon Mistakes\n\nReporting SD when you mean SE (or vice versa). SD describes how variable the data is. SE describes how precisely you’ve estimated the mean. A study with low SD but high SE has consistent data but too few observations.\nThinking SE measures data quality. SE can be tiny even with messy data — you just need a big enough \\(n\\). It says nothing about whether your data is clean or your measurements are accurate.\nForgetting that SE applies to any estimator, not just the mean. The slope in a regression has an SE. A proportion has an SE. Any time you estimate something from data, there’s uncertainty in that estimate — and the SE quantifies it.",
    "crumbs": [
      "Probability & Uncertainty",
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#did-you-know",
    "href": "variance-sd-se.html#did-you-know",
    "title": "Variance, SD & Standard Error",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe n − 1 Story\nThe correction for dividing by \\(n - 1\\) instead of \\(n\\) is called Bessel’s correction, named after Friedrich Bessel (1784–1846), a German astronomer who was obsessed with measurement precision. He needed to compute the orbits of stars and realized that using \\(n\\) in the denominator systematically underestimated how uncertain his measurements really were. His fix — dividing by \\(n - 1\\) — is now used billions of times a day in software from Excel to R to Python. Bessel never saw a computer, but his correction is baked into every call to var() and sd() in R and numpy.std(ddof=1) in Python.\n\n\nWhy “Standard” Error?\nThe word “standard” in “standard deviation” and “standard error” simply means “typical.” Karl Pearson coined “standard deviation” in 1893 because he wanted a word for the typical amount by which values deviate from their average. “Standard error” then naturally means the typical amount by which your estimate deviates from the truth. Nothing fancy — just “how far off is typical?”",
    "crumbs": [
      "Probability & Uncertainty",
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "foundations.html",
    "href": "foundations.html",
    "title": "Statistical Foundations",
    "section": "",
    "text": "A distribution is a model of variability. Imagine measuring the commute time of every person in a city. Some people take 10 minutes, most take around 30, a few are stuck for over an hour. If you made a histogram of all those commute times, the shape you’d see is the distribution — it tells you which values are common, which are rare, and how spread out things are.\nWhy do we care? Because variation is everywhere. Two patients given the same drug respond differently. Two students who study the same hours get different exam scores. Two identical ads shown to similar users get different click rates. None of that is a mistake — it’s the nature of data. A distribution is simply a mathematical way of describing how much things vary and in what pattern.\nOnce you have a distribution, you can answer useful questions: What’s the most likely outcome? How often do we see extreme values? Where does the middle 50% of the data sit?\nKey objects: histogram (shape), CDF (cumulative probabilities), quantiles (where does 50% of the data fall?).\nExplore different distributions below — notice how each has a distinct shape, center, and spread.\n#| standalone: true\n#| viewerHeight: 450\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n      selectInput(\"dist\", \"Distribution:\",\n                  choices = c(\"Uniform(0,1)\", \"Normal(0,1)\",\n                              \"Exponential(1)\", \"Chi-squared(3)\",\n                              \"Bimodal\")),\n      sliderInput(\"n\", \"Sample size:\", min = 50, max = 5000, value = 1000, step = 50),\n      actionButton(\"draw\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n      uiOutput(\"stats\")\n    ),\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"hist_plot\", height = \"350px\")),\n        column(6, plotOutput(\"cdf_plot\",  height = \"350px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  samp &lt;- reactive({\n    input$draw\n    n &lt;- input$n\n    switch(input$dist,\n      \"Uniform(0,1)\"   = runif(n),\n      \"Normal(0,1)\"    = rnorm(n),\n      \"Exponential(1)\" = rexp(n),\n      \"Chi-squared(3)\" = rchisq(n, df = 3),\n      \"Bimodal\"        = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, -2, 0.6) + (1 - k) * rnorm(n, 2, 0.6)\n      }\n    )\n  })\n\n  output$hist_plot &lt;- renderPlot({\n    x &lt;- samp()\n    par(mar = c(4.5, 4, 3, 1))\n    hist(x, breaks = 50, probability = TRUE,\n         col = \"#d5e8d4\", border = \"#82b366\",\n         main = paste(\"Histogram:\", input$dist),\n         xlab = \"x\", ylab = \"Density\")\n  })\n\n  output$cdf_plot &lt;- renderPlot({\n    x &lt;- samp()\n    par(mar = c(4.5, 4, 3, 1))\n    plot(ecdf(x), col = \"#3498db\", lwd = 2,\n         main = paste(\"CDF:\", input$dist),\n         xlab = \"x\", ylab = \"F(x)\")\n  })\n\n  output$stats &lt;- renderUI({\n    x &lt;- samp()\n    q &lt;- round(quantile(x, c(0.25, 0.5, 0.75)), 3)\n    tags$div(style = \"background:#f0f4f8; border-radius:6px; padding:12px; margin-top:12px; font-size:14px; line-height:1.8;\",\n      HTML(paste0(\n        \"&lt;b&gt;Mean:&lt;/b&gt; \", round(mean(x), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;SD:&lt;/b&gt; \", round(sd(x), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Q25:&lt;/b&gt; \", q[1], \"&lt;br&gt;\",\n        \"&lt;b&gt;Median:&lt;/b&gt; \", q[2], \"&lt;br&gt;\",\n        \"&lt;b&gt;Q75:&lt;/b&gt; \", q[3]\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Probability & Uncertainty",
      "Foundations"
    ]
  },
  {
    "objectID": "foundations.html#data-distributions",
    "href": "foundations.html#data-distributions",
    "title": "Statistical Foundations",
    "section": "",
    "text": "A distribution is a model of variability. Imagine measuring the commute time of every person in a city. Some people take 10 minutes, most take around 30, a few are stuck for over an hour. If you made a histogram of all those commute times, the shape you’d see is the distribution — it tells you which values are common, which are rare, and how spread out things are.\nWhy do we care? Because variation is everywhere. Two patients given the same drug respond differently. Two students who study the same hours get different exam scores. Two identical ads shown to similar users get different click rates. None of that is a mistake — it’s the nature of data. A distribution is simply a mathematical way of describing how much things vary and in what pattern.\nOnce you have a distribution, you can answer useful questions: What’s the most likely outcome? How often do we see extreme values? Where does the middle 50% of the data sit?\nKey objects: histogram (shape), CDF (cumulative probabilities), quantiles (where does 50% of the data fall?).\nExplore different distributions below — notice how each has a distinct shape, center, and spread.\n#| standalone: true\n#| viewerHeight: 450\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n      selectInput(\"dist\", \"Distribution:\",\n                  choices = c(\"Uniform(0,1)\", \"Normal(0,1)\",\n                              \"Exponential(1)\", \"Chi-squared(3)\",\n                              \"Bimodal\")),\n      sliderInput(\"n\", \"Sample size:\", min = 50, max = 5000, value = 1000, step = 50),\n      actionButton(\"draw\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n      uiOutput(\"stats\")\n    ),\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"hist_plot\", height = \"350px\")),\n        column(6, plotOutput(\"cdf_plot\",  height = \"350px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  samp &lt;- reactive({\n    input$draw\n    n &lt;- input$n\n    switch(input$dist,\n      \"Uniform(0,1)\"   = runif(n),\n      \"Normal(0,1)\"    = rnorm(n),\n      \"Exponential(1)\" = rexp(n),\n      \"Chi-squared(3)\" = rchisq(n, df = 3),\n      \"Bimodal\"        = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, -2, 0.6) + (1 - k) * rnorm(n, 2, 0.6)\n      }\n    )\n  })\n\n  output$hist_plot &lt;- renderPlot({\n    x &lt;- samp()\n    par(mar = c(4.5, 4, 3, 1))\n    hist(x, breaks = 50, probability = TRUE,\n         col = \"#d5e8d4\", border = \"#82b366\",\n         main = paste(\"Histogram:\", input$dist),\n         xlab = \"x\", ylab = \"Density\")\n  })\n\n  output$cdf_plot &lt;- renderPlot({\n    x &lt;- samp()\n    par(mar = c(4.5, 4, 3, 1))\n    plot(ecdf(x), col = \"#3498db\", lwd = 2,\n         main = paste(\"CDF:\", input$dist),\n         xlab = \"x\", ylab = \"F(x)\")\n  })\n\n  output$stats &lt;- renderUI({\n    x &lt;- samp()\n    q &lt;- round(quantile(x, c(0.25, 0.5, 0.75)), 3)\n    tags$div(style = \"background:#f0f4f8; border-radius:6px; padding:12px; margin-top:12px; font-size:14px; line-height:1.8;\",\n      HTML(paste0(\n        \"&lt;b&gt;Mean:&lt;/b&gt; \", round(mean(x), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;SD:&lt;/b&gt; \", round(sd(x), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Q25:&lt;/b&gt; \", q[1], \"&lt;br&gt;\",\n        \"&lt;b&gt;Median:&lt;/b&gt; \", q[2], \"&lt;br&gt;\",\n        \"&lt;b&gt;Q75:&lt;/b&gt; \", q[3]\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Probability & Uncertainty",
      "Foundations"
    ]
  },
  {
    "objectID": "foundations.html#sampling-uncertainty",
    "href": "foundations.html#sampling-uncertainty",
    "title": "Statistical Foundations",
    "section": "2. Sampling & Uncertainty",
    "text": "2. Sampling & Uncertainty\nSampling is why statistics exists. A sample mean is not a fixed truth — it is one draw from a distribution of possible sample means. Repeat the sampling and you get a different answer every time.\nThe key insight: larger samples produce less variable estimates. The spread of the sampling distribution shrinks at rate \\(1/\\sqrt{n}\\).\nTry it: press “New draw” a few times at \\(n = 10\\), then slide up to \\(n = 200\\) and watch how the sample means cluster tighter around the true mean.\n#| standalone: true\n#| viewerHeight: 420\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n      sliderInput(\"n\", \"Sample size (n):\", min = 5, max = 200, value = 10, step = 5),\n      sliderInput(\"reps\", \"Repeated samples:\", min = 5, max = 50, value = 20, step = 5),\n      actionButton(\"go\", \"Draw samples\", class = \"btn-primary\", width = \"100%\")\n    ),\n    mainPanel(\n      width = 9,\n      plotOutput(\"dot_plot\", height = \"350px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  output$dot_plot &lt;- renderPlot({\n    input$go\n    n    &lt;- input$n\n    reps &lt;- input$reps\n\n    means &lt;- replicate(reps, mean(rnorm(n, mean = 5, sd = 2)))\n\n    par(mar = c(4.5, 2, 3, 1))\n    stripchart(means, method = \"stack\", pch = 19, cex = 1.5,\n               col = \"#3498db\", offset = 0.5,\n               xlim = c(3, 7),\n               main = paste0(reps, \" sample means (n = \", n, \")\"),\n               xlab = \"Sample mean\")\n    abline(v = 5, lty = 2, lwd = 2, col = \"#e74c3c\")\n    legend(\"topright\", legend = \"True mean = 5\",\n           col = \"#e74c3c\", lty = 2, lwd = 2, bty = \"n\")\n  })\n}\n\nshinyApp(ui, server)\nPractice questions:\n\nWhat happens to the spread of sample means as \\(n\\) increases?\nDoes the population distribution change when you change \\(n\\)?\nCould a single sample mean be far from the truth? Is that more likely with small or large \\(n\\)?",
    "crumbs": [
      "Probability & Uncertainty",
      "Foundations"
    ]
  },
  {
    "objectID": "foundations.html#confidence-intervals",
    "href": "foundations.html#confidence-intervals",
    "title": "Statistical Foundations",
    "section": "3. Confidence Intervals",
    "text": "3. Confidence Intervals\nA 95% confidence interval does not mean “there’s a 95% probability the true value is inside.” The true value is fixed — it’s either in there or not.\nThe correct interpretation: if you repeated the experiment many times and built a CI each time, 95% of those intervals would contain the true value.\nThe simulation below shows exactly this. Each horizontal line is one CI from a fresh sample. Most cover the true mean (blue), but about 5% miss (red).\n#| standalone: true\n#| viewerHeight: 500\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n      sliderInput(\"n\", \"Sample size (n):\", min = 10, max = 200, value = 30, step = 10),\n      sliderInput(\"k\", \"Number of CIs:\", min = 20, max = 100, value = 50, step = 10),\n      sliderInput(\"conf\", \"Confidence level:\", min = 0.80, max = 0.99, value = 0.95, step = 0.01),\n      actionButton(\"go\", \"New experiment\", class = \"btn-primary\", width = \"100%\"),\n      uiOutput(\"coverage\")\n    ),\n    mainPanel(\n      width = 9,\n      plotOutput(\"ci_plot\", height = \"420px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  res &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    k    &lt;- input$k\n    conf &lt;- input$conf\n    mu   &lt;- 0\n\n    z &lt;- qnorm(1 - (1 - conf) / 2)\n\n    ci &lt;- t(replicate(k, {\n      x   &lt;- rnorm(n, mean = mu, sd = 1)\n      xbar &lt;- mean(x)\n      se   &lt;- sd(x) / sqrt(n)\n      c(xbar, xbar - z * se, xbar + z * se)\n    }))\n\n    covers &lt;- ci[, 2] &lt;= mu & ci[, 3] &gt;= mu\n\n    list(ci = ci, covers = covers, mu = mu, k = k, conf = conf)\n  })\n\n  output$ci_plot &lt;- renderPlot({\n    r &lt;- res()\n    par(mar = c(4.5, 4, 3, 1))\n\n    plot(NULL, xlim = range(r$ci[, 2:3]), ylim = c(1, r$k),\n         xlab = \"Value\", ylab = \"Sample #\",\n         main = paste0(round(r$conf * 100), \"% Confidence Intervals\"))\n\n    for (i in seq_len(r$k)) {\n      clr &lt;- if (r$covers[i]) \"#3498db\" else \"#e74c3c\"\n      lw  &lt;- if (r$covers[i]) 1.5 else 2.5\n      segments(r$ci[i, 2], i, r$ci[i, 3], i, col = clr, lwd = lw)\n      points(r$ci[i, 1], i, pch = 16, cex = 0.5, col = clr)\n    }\n\n    abline(v = r$mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n  })\n\n  output$coverage &lt;- renderUI({\n    r &lt;- res()\n    pct &lt;- round(100 * mean(r$covers), 1)\n    miss &lt;- sum(!r$covers)\n    tags$div(style = \"background:#f0f4f8; border-radius:6px; padding:12px; margin-top:12px; font-size:14px; line-height:1.8;\",\n      HTML(paste0(\n        \"&lt;b&gt;Coverage:&lt;/b&gt; \", pct, \"%&lt;br&gt;\",\n        \"&lt;b&gt;Missed:&lt;/b&gt; \", miss, \" / \", r$k, \"&lt;br&gt;\",\n        \"&lt;small&gt;Target: \", round(r$conf * 100), \"%&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\nKey takeaways:\n\nA confidence interval quantifies uncertainty, not probability about the parameter\nWider intervals = more uncertainty (small \\(n\\), high confidence level)\nThe coverage rate converges to the nominal level over many experiments\n\n\n\nDid you know?\n\nFlorence Nightingale wasn’t just a nurse — she was a pioneering statistician. She invented the polar area diagram (a variant of the pie chart) to convince the British government that soldiers were dying from preventable disease, not combat wounds. Her charts changed military policy and saved thousands of lives.\nThe word “statistics” comes from the German Statistik, meaning “science of the state” — it originally referred to collecting data about populations for government use.\nJohn Graunt (1620–1674) is considered the father of demography. He analyzed London’s death records and discovered that more boys are born than girls, that urban death rates exceed rural ones, and that plague deaths follow seasonal patterns — all from just counting.",
    "crumbs": [
      "Probability & Uncertainty",
      "Foundations"
    ]
  },
  {
    "objectID": "lln.html",
    "href": "lln.html",
    "title": "LLN vs CLT: Why Averages Stabilize Before They Become Normal",
    "section": "",
    "text": "The Law of Large Numbers (LLN) and the Central Limit Theorem (CLT) are the two pillars of statistics, but they say different things:\n\n\n\n\n\n\n\n\n\nLLN\nCLT\n\n\n\n\nWhat it says\nThe sample mean converges to \\(\\mu\\)\nThe distribution of sample means is approximately normal\n\n\nAbout\nOne running average\nMany sample averages\n\n\nPromise\nYour estimate gets close to the truth\nYou know the shape of the uncertainty\n\n\nRequires\nFinite mean\nFinite variance\n\n\n\nLLN says the mean settles down. CLT says where it settles follows a normal distribution.\nThink of it this way: LLN tells you that if you flip a fair coin enough times, the fraction of heads approaches 0.5. CLT tells you that if you repeat the entire experiment many times, the distribution of those fractions is bell-shaped.",
    "crumbs": [
      "The Central Limit Theorem",
      "LLN vs CLT"
    ]
  },
  {
    "objectID": "lln.html#two-theorems-two-different-promises",
    "href": "lln.html#two-theorems-two-different-promises",
    "title": "LLN vs CLT: Why Averages Stabilize Before They Become Normal",
    "section": "",
    "text": "The Law of Large Numbers (LLN) and the Central Limit Theorem (CLT) are the two pillars of statistics, but they say different things:\n\n\n\n\n\n\n\n\n\nLLN\nCLT\n\n\n\n\nWhat it says\nThe sample mean converges to \\(\\mu\\)\nThe distribution of sample means is approximately normal\n\n\nAbout\nOne running average\nMany sample averages\n\n\nPromise\nYour estimate gets close to the truth\nYou know the shape of the uncertainty\n\n\nRequires\nFinite mean\nFinite variance\n\n\n\nLLN says the mean settles down. CLT says where it settles follows a normal distribution.\nThink of it this way: LLN tells you that if you flip a fair coin enough times, the fraction of heads approaches 0.5. CLT tells you that if you repeat the entire experiment many times, the distribution of those fractions is bell-shaped.",
    "crumbs": [
      "The Central Limit Theorem",
      "LLN vs CLT"
    ]
  },
  {
    "objectID": "lln.html#simulation-1-the-running-average",
    "href": "lln.html#simulation-1-the-running-average",
    "title": "LLN vs CLT: Why Averages Stabilize Before They Become Normal",
    "section": "Simulation 1: The running average",
    "text": "Simulation 1: The running average\nWatch one running average converge to the true mean as you add observations one at a time. This is LLN in action — the cumulative mean stabilizes. Try different distributions and notice that convergence happens regardless of the population shape.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"pop\", \"Population:\",\n                  choices = c(\"Uniform(0, 1)\",\n                              \"Exponential(1)\",\n                              \"Bimodal\",\n                              \"Bernoulli(0.3)\",\n                              \"Heavy-tailed (t, df=2)\")),\n\n      sliderInput(\"n\", \"Number of observations:\",\n                  min = 50, max = 5000, value = 500, step = 50),\n\n      sliderInput(\"paths\", \"Number of paths:\",\n                  min = 1, max = 10, value = 3, step = 1),\n\n      actionButton(\"go\", \"Draw new paths\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"running_avg\", height = \"500px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  draw_pop &lt;- function(n, pop) {\n    switch(pop,\n      \"Uniform(0, 1)\"          = runif(n),\n      \"Exponential(1)\"         = rexp(n, rate = 1),\n      \"Bimodal\"                = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, -2, 0.6) + (1 - k) * rnorm(n, 2, 0.6)\n      },\n      \"Bernoulli(0.3)\"         = rbinom(n, 1, 0.3),\n      \"Heavy-tailed (t, df=2)\" = rt(n, df = 2)\n    )\n  }\n\n  pop_mu &lt;- function(pop) {\n    switch(pop,\n      \"Uniform(0, 1)\"          = 0.5,\n      \"Exponential(1)\"         = 1,\n      \"Bimodal\"                = 0,\n      \"Bernoulli(0.3)\"         = 0.3,\n      \"Heavy-tailed (t, df=2)\" = 0\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n\n    paths &lt;- input$paths\n    pop   &lt;- input$pop\n    mu    &lt;- pop_mu(pop)\n\n    all_paths &lt;- lapply(seq_len(paths), function(i) {\n      x &lt;- draw_pop(n, pop)\n      cumsum(x) / seq_along(x)\n    })\n\n    list(all_paths = all_paths, n = n, mu = mu, pop = pop, paths = paths)\n  })\n\n  output$running_avg &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ylim &lt;- range(unlist(d$all_paths))\n    ylim &lt;- ylim + c(-1, 1) * 0.1 * diff(ylim)\n\n    cols &lt;- c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"#9b59b6\", \"#e67e22\",\n              \"#1abc9c\", \"#34495e\", \"#f39c12\", \"#2ecc71\", \"#c0392b\")\n\n    plot(NULL, xlim = c(1, d$n), ylim = ylim,\n         xlab = \"Number of observations\",\n         ylab = \"Cumulative mean\",\n         main = paste0(\"LLN: Running average converges to \\u03bc = \", d$mu))\n\n    abline(h = d$mu, lty = 2, lwd = 2.5, col = \"#2c3e50\")\n\n    for (i in seq_along(d$all_paths)) {\n      lines(seq_along(d$all_paths[[i]]), d$all_paths[[i]],\n            col = adjustcolor(cols[i], 0.7), lwd = 1.5)\n    }\n\n    legend(\"topright\", bty = \"n\", cex = 0.9,\n           legend = c(paste0(\"True mean (\\u03bc = \", d$mu, \")\"),\n                      paste0(d$paths, \" sample path(s)\")),\n           col = c(\"#2c3e50\", cols[1]),\n           lwd = c(2.5, 1.5), lty = c(2, 1))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    final_means &lt;- sapply(d$all_paths, function(p) p[length(p)])\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True mean:&lt;/b&gt; \", d$mu, \"&lt;br&gt;\",\n        \"&lt;b&gt;Final running avg(s):&lt;/b&gt;&lt;br&gt;\",\n        paste(round(final_means, 4), collapse = \", \"), \"&lt;br&gt;\",\n        \"&lt;b&gt;Max deviation:&lt;/b&gt; \",\n        round(max(abs(final_means - d$mu)), 4)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "The Central Limit Theorem",
      "LLN vs CLT"
    ]
  },
  {
    "objectID": "lln.html#simulation-2-lln-vs-clt-side-by-side",
    "href": "lln.html#simulation-2-lln-vs-clt-side-by-side",
    "title": "LLN vs CLT: Why Averages Stabilize Before They Become Normal",
    "section": "Simulation 2: LLN vs CLT side by side",
    "text": "Simulation 2: LLN vs CLT side by side\nNow see both theorems at once. The left panel shows LLN — one running average converging to \\(\\mu\\). The right panel shows CLT — the histogram of many sample means forming a normal distribution. Same data, different questions.\n#| standalone: true\n#| viewerHeight: 580\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"pop2\", \"Population:\",\n                  choices = c(\"Uniform(0, 1)\",\n                              \"Exponential(1)\",\n                              \"Bimodal\",\n                              \"Bernoulli(0.3)\")),\n\n      sliderInput(\"n2\", \"Sample size per experiment:\",\n                  min = 5, max = 200, value = 30, step = 5),\n\n      sliderInput(\"reps2\", \"Number of experiments:\",\n                  min = 200, max = 3000, value = 1000, step = 100),\n\n      actionButton(\"go2\", \"Run experiments\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"lln_plot\", height = \"430px\")),\n        column(6, plotOutput(\"clt_plot\", height = \"430px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  draw_pop &lt;- function(n, pop) {\n    switch(pop,\n      \"Uniform(0, 1)\"  = runif(n),\n      \"Exponential(1)\" = rexp(n, rate = 1),\n      \"Bimodal\"        = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, -2, 0.6) + (1 - k) * rnorm(n, 2, 0.6)\n      },\n      \"Bernoulli(0.3)\" = rbinom(n, 1, 0.3)\n    )\n  }\n\n  pop_mu &lt;- function(pop) {\n    switch(pop,\n      \"Uniform(0, 1)\"  = 0.5,\n      \"Exponential(1)\" = 1,\n      \"Bimodal\"        = 0,\n      \"Bernoulli(0.3)\" = 0.3\n    )\n  }\n\n  pop_sigma &lt;- function(pop) {\n    switch(pop,\n      \"Uniform(0, 1)\"  = sqrt(1 / 12),\n      \"Exponential(1)\" = 1,\n      \"Bimodal\"        = sqrt(0.6^2 + 4),\n      \"Bernoulli(0.3)\" = sqrt(0.3 * 0.7)\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go2\n    n    &lt;- input$n2\n    reps &lt;- input$reps2\n    pop  &lt;- input$pop2\n    mu   &lt;- pop_mu(pop)\n    sig  &lt;- pop_sigma(pop)\n\n    # One long draw for LLN\n    long_draw &lt;- draw_pop(n * 5, pop)\n    running   &lt;- cumsum(long_draw) / seq_along(long_draw)\n\n    # Many experiments for CLT\n    means &lt;- replicate(reps, mean(draw_pop(n, pop)))\n\n    list(running = running, means = means, mu = mu, sig = sig,\n         n = n, reps = reps, pop = pop)\n  })\n\n  output$lln_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(seq_along(d$running), d$running, type = \"l\",\n         col = \"#3498db\", lwd = 1.5,\n         xlab = \"Number of observations\",\n         ylab = \"Cumulative mean\",\n         main = \"LLN: One running average\")\n    abline(h = d$mu, lty = 2, lwd = 2.5, col = \"#e74c3c\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Running average\",\n                      paste0(\"True \\u03bc = \", d$mu)),\n           col = c(\"#3498db\", \"#e74c3c\"),\n           lwd = c(1.5, 2.5), lty = c(1, 2))\n  })\n\n  output$clt_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$means, breaks = 40, probability = TRUE,\n         col = \"#3498db30\", border = \"#3498db80\",\n         main = paste0(\"CLT: \", d$reps, \" sample means (n=\", d$n, \")\"),\n         xlab = \"Sample mean\", ylab = \"Density\")\n\n    x_seq &lt;- seq(min(d$means), max(d$means), length.out = 300)\n    se &lt;- d$sig / sqrt(d$n)\n    lines(x_seq, dnorm(x_seq, mean = d$mu, sd = se),\n          col = \"#e74c3c\", lwd = 2.5)\n\n    abline(v = d$mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Sampling distribution\",\n                      paste0(\"N(\\u03bc, \\u03c3/\\u221an) overlay\"),\n                      paste0(\"True \\u03bc = \", d$mu)),\n           col = c(\"#3498db80\", \"#e74c3c\", \"#2c3e50\"),\n           lwd = c(8, 2.5, 2), lty = c(1, 1, 2))\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- dat()\n    se &lt;- d$sig / sqrt(d$n)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;LLN (left):&lt;/b&gt;&lt;br&gt;\",\n        \"Final avg: \", round(d$running[length(d$running)], 4), \"&lt;br&gt;\",\n        \"True \\u03bc: \", d$mu, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;CLT (right):&lt;/b&gt;&lt;br&gt;\",\n        \"Mean of means: \", round(mean(d$means), 4), \"&lt;br&gt;\",\n        \"SD of means: \", round(sd(d$means), 4), \"&lt;br&gt;\",\n        \"Theoretical SE: \", round(se, 4)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nExponential, n = 5: the LLN panel shows the running average still wobbling, while the CLT histogram is visibly skewed. Neither theorem has fully “kicked in” yet — but they will at different rates.\nExponential, n = 100: the running average has settled (LLN), and the histogram is now bell-shaped (CLT). Both theorems have done their job.\nBimodal: the running average converges to 0 (the midpoint), even though no individual observation is near 0. LLN doesn’t care about the shape.\nMultiple paths (Sim 1): all paths converge to the same \\(\\mu\\), but they take different routes. The early wobble is sampling variability; the eventual convergence is LLN.\n\n\n\nThe bottom line\n\nLLN = your estimate gets closer to the truth as \\(n\\) grows. It’s about accuracy of a single estimate.\nCLT = the shape of the sampling distribution is approximately normal. It’s about the distribution of the estimator across repeated experiments.\nYou need LLN before CLT matters. If the mean hasn’t stabilized, knowing its distribution is normal doesn’t help much.\n\n\n\n\nDid you know?\n\nJacob Bernoulli proved the first version of the Law of Large Numbers in his book Ars Conjectandi, published posthumously in 1713. He called it his “golden theorem” and spent over 20 years working on the proof. The result seems obvious in hindsight — of course averages converge — but rigorously proving why required entirely new mathematical machinery.\nThere are actually two versions: the Weak LLN (convergence in probability, proved by Bernoulli and later Chebyshev) and the Strong LLN (almost sure convergence, proved by Kolmogorov in 1933). The strong version says that convergence happens with probability 1, not just “usually.”\nThe heavy-tailed \\(t\\)-distribution with \\(\\text{df} = 1\\) (the Cauchy distribution) has no finite mean, so LLN doesn’t apply. The running average never settles down, no matter how much data you collect. Try it in the simulation above — it keeps jumping around forever.",
    "crumbs": [
      "The Central Limit Theorem",
      "LLN vs CLT"
    ]
  },
  {
    "objectID": "fwl.html",
    "href": "fwl.html",
    "title": "Frisch-Waugh-Lovell Theorem",
    "section": "",
    "text": "The Frisch-Waugh-Lovell (FWL) theorem says: the coefficient on \\(X_1\\) in \\(Y = \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon\\) is identical to the slope from regressing the residualized \\(Y\\) on the residualized \\(X_1\\) — after partialling out \\(X_2\\) from both.\nDrag the sliders to see it hold for any DGP.\n#| standalone: true\n#| viewerHeight: 700\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .eq-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-bottom: 14px; font-size: 14px; line-height: 1.9;\n    }\n    .eq-box b { color: #2c3e50; }\n    .match  { color: #27ae60; font-weight: bold; }\n    .coef   { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 50, max = 500, value = 200, step = 50),\n\n      sliderInput(\"b1\", HTML(\"True &beta;&lt;sub&gt;1&lt;/sub&gt; (effect of X&lt;sub&gt;1&lt;/sub&gt;):\"),\n                  min = -3, max = 3, value = 1.5, step = 0.1),\n\n      sliderInput(\"b2\", HTML(\"True &beta;&lt;sub&gt;2&lt;/sub&gt; (effect of X&lt;sub&gt;2&lt;/sub&gt;):\"),\n                  min = -3, max = 3, value = -1, step = 0.1),\n\n      sliderInput(\"rho\", HTML(\"Corr(X&lt;sub&gt;1&lt;/sub&gt;, X&lt;sub&gt;2&lt;/sub&gt;):\"),\n                  min = -0.9, max = 0.9, value = 0.6, step = 0.1),\n\n      sliderInput(\"sigma\", HTML(\"Error SD (&sigma;):\"),\n                  min = 0.5, max = 5, value = 1, step = 0.5),\n\n      actionButton(\"resim\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_box\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(4, plotOutput(\"plot_full\",    height = \"350px\")),\n        column(4, plotOutput(\"plot_partial\", height = \"350px\")),\n        column(4, plotOutput(\"plot_fwl\",     height = \"350px\"))\n      ),\n      fluidRow(\n        column(12, uiOutput(\"step_text\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$resim\n    n     &lt;- input$n\n    b1    &lt;- input$b1\n    b2    &lt;- input$b2\n    rho   &lt;- input$rho\n    sigma &lt;- input$sigma\n\n    # Generate correlated X1, X2\n    z1 &lt;- rnorm(n)\n    z2 &lt;- rnorm(n)\n    x1 &lt;- z1\n    x2 &lt;- rho * z1 + sqrt(1 - rho^2) * z2\n\n    eps &lt;- rnorm(n, sd = sigma)\n    y   &lt;- b1 * x1 + b2 * x2 + eps\n\n    # Full OLS\n    full_fit &lt;- lm(y ~ x1 + x2)\n\n    # FWL steps\n    ey &lt;- resid(lm(y  ~ x2))   # residualise Y on X2\n    ex &lt;- resid(lm(x1 ~ x2))   # residualise X1 on X2\n    fwl_fit &lt;- lm(ey ~ ex)\n\n    list(x1 = x1, x2 = x2, y = y,\n         ey = ey, ex = ex,\n         full_fit = full_fit, fwl_fit = fwl_fit,\n         b1 = b1, b2 = b2)\n  })\n\n  # --- Plot 1: Y vs X1 (naive scatter) ---\n  output$plot_full &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4, 3, 1))\n    plot(d$x1, d$y, pch = 16, col = \"#3498db80\", cex = 0.8,\n         xlab = expression(X[1]), ylab = \"Y\",\n         main = expression(\"Y vs \" * X[1] * \" (raw)\"))\n    abline(lm(d$y ~ d$x1), col = \"#e74c3c\", lwd = 2.5)\n    naive_b &lt;- round(coef(lm(d$y ~ d$x1))[2], 4)\n    legend(\"topleft\", bty = \"n\", cex = 0.9,\n           legend = paste(\"Naive slope =\", naive_b))\n  })\n\n  # --- Plot 2: residualised X1 (partial out X2) ---\n  output$plot_partial &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4, 3, 1))\n    plot(d$x1, d$ex, pch = 16, col = \"#9b59b680\", cex = 0.8,\n         xlab = expression(X[1]),\n         ylab = expression(e[X[1]]),\n         main = expression(\"Residualise \" * X[1] * \" on \" * X[2]))\n    abline(h = 0, lty = 2, col = \"gray50\")\n    abline(lm(d$ex ~ d$x1), col = \"#8e44ad\", lwd = 2)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = expression(\"Variation in \" * X[1] * \" independent of \" * X[2]))\n  })\n\n  # --- Plot 3: FWL regression ---\n  output$plot_fwl &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4, 3, 1))\n    plot(d$ex, d$ey, pch = 16, col = \"#2ecc7180\", cex = 0.8,\n         xlab = expression(e[X[1]]),\n         ylab = expression(e[Y]),\n         main = \"FWL: Residual Y vs Residual X1\")\n    abline(d$fwl_fit, col = \"#e74c3c\", lwd = 2.5)\n    fwl_b &lt;- round(coef(d$fwl_fit)[2], 4)\n    legend(\"topleft\", bty = \"n\", cex = 0.9,\n           legend = paste(\"FWL slope =\", fwl_b))\n  })\n\n  # --- Results comparison ---\n  output$results_box &lt;- renderUI({\n    d &lt;- dat()\n    full_b1 &lt;- round(coef(d$full_fit)[\"x1\"], 4)\n    fwl_b1  &lt;- round(coef(d$fwl_fit)[2], 4)\n    naive_b &lt;- round(coef(lm(d$y ~ d$x1))[2], 4)\n\n    tags$div(class = \"eq-box\", style = \"margin-top: 16px;\",\n      HTML(paste0(\n        \"&lt;b&gt;True &beta;&lt;sub&gt;1&lt;/sub&gt;:&lt;/b&gt; \", d$b1, \"&lt;br&gt;\",\n        \"&lt;b&gt;Full OLS &beta;&lt;sub&gt;1&lt;/sub&gt;:&lt;/b&gt; &lt;span class='coef'&gt;\", full_b1, \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;FWL &beta;&lt;sub&gt;1&lt;/sub&gt;:&lt;/b&gt; &lt;span class='coef'&gt;\", fwl_b1, \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;span class='match'&gt;&#10003; They match!&lt;/span&gt;&lt;br&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Naive slope:&lt;/b&gt; \", naive_b, \"&lt;br&gt;\",\n        \"&lt;small&gt;(biased by omitting X&lt;sub&gt;2&lt;/sub&gt;)&lt;/small&gt;\"\n      ))\n    )\n  })\n\n  # --- Step explanation ---\n  output$step_text &lt;- renderUI({\n    tags$div(class = \"eq-box\", style = \"margin-top: 8px;\",\n      HTML(paste0(\n        \"&lt;b&gt;Steps:&lt;/b&gt; \",\n        \"(1) Regress Y on X&lt;sub&gt;2&lt;/sub&gt; &rarr; residuals &lt;i&gt;e&lt;sub&gt;Y&lt;/sub&gt;&lt;/i&gt; &nbsp;|&nbsp; \",\n        \"(2) Regress X&lt;sub&gt;1&lt;/sub&gt; on X&lt;sub&gt;2&lt;/sub&gt; &rarr; residuals &lt;i&gt;e&lt;sub&gt;X₁&lt;/sub&gt;&lt;/i&gt; &nbsp;|&nbsp; \",\n        \"(3) Regress &lt;i&gt;e&lt;sub&gt;Y&lt;/sub&gt;&lt;/i&gt; on &lt;i&gt;e&lt;sub&gt;X₁&lt;/sub&gt;&lt;/i&gt; &rarr; \",\n        \"slope = &beta;&lt;sub&gt;1&lt;/sub&gt; from full regression\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nDid you know?\n\nRagnar Frisch and Jan Tinbergen won the very first Nobel Prize in Economics in 1969. Frisch coined the terms “econometrics,” “microeconomics,” and “macroeconomics.” The FWL theorem appeared in Frisch & Waugh (1933).\nMichael Lovell extended the result in 1963, showing it applies to any partitioned regression — not just the two-variable case. That’s why it’s FWL, not just FW.\nFWL is the theoretical foundation behind “partialling out” and “controlling for” variables. Every time you add a control to a regression, you’re implicitly doing the residualization that FWL describes.\nIn machine learning, the same idea appears as “residualization” in double/debiased ML (Chernozhukov et al., 2018) — one of the most important recent developments in causal ML.",
    "crumbs": [
      "Regression & Diagnostics",
      "Frisch-Waugh-Lovell"
    ]
  }
]