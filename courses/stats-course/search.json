[
  {
    "objectID": "fwl.html",
    "href": "fwl.html",
    "title": "Frisch-Waugh-Lovell Theorem",
    "section": "",
    "text": "The Frisch-Waugh-Lovell (FWL) theorem says: the coefficient on \\(X_1\\) in \\(Y = \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon\\) is identical to the slope from regressing the residualized \\(Y\\) on the residualized \\(X_1\\) — after partialling out \\(X_2\\) from both.\nDrag the sliders to see it hold for any DGP.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .eq-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-bottom: 14px; font-size: 14px; line-height: 1.9;\n    }\n    .eq-box b { color: #2c3e50; }\n    .match  { color: #27ae60; font-weight: bold; }\n    .coef   { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 50, max = 500, value = 200, step = 50),\n\n      sliderInput(\"b1\", HTML(\"True &beta;&lt;sub&gt;1&lt;/sub&gt; (effect of X&lt;sub&gt;1&lt;/sub&gt;):\"),\n                  min = -3, max = 3, value = 1.5, step = 0.1),\n\n      sliderInput(\"b2\", HTML(\"True &beta;&lt;sub&gt;2&lt;/sub&gt; (effect of X&lt;sub&gt;2&lt;/sub&gt;):\"),\n                  min = -3, max = 3, value = -1, step = 0.1),\n\n      sliderInput(\"rho\", HTML(\"Corr(X&lt;sub&gt;1&lt;/sub&gt;, X&lt;sub&gt;2&lt;/sub&gt;):\"),\n                  min = -0.9, max = 0.9, value = 0.6, step = 0.1),\n\n      sliderInput(\"sigma\", HTML(\"Error SD (&sigma;):\"),\n                  min = 0.5, max = 5, value = 1, step = 0.5),\n\n      actionButton(\"resim\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_box\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(4, plotOutput(\"plot_full\",    height = \"350px\")),\n        column(4, plotOutput(\"plot_partial\", height = \"350px\")),\n        column(4, plotOutput(\"plot_fwl\",     height = \"350px\"))\n      ),\n      fluidRow(\n        column(12, uiOutput(\"step_text\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$resim\n    n     &lt;- input$n\n    b1    &lt;- input$b1\n    b2    &lt;- input$b2\n    rho   &lt;- input$rho\n    sigma &lt;- input$sigma\n\n    # Generate correlated X1, X2\n    z1 &lt;- rnorm(n)\n    z2 &lt;- rnorm(n)\n    x1 &lt;- z1\n    x2 &lt;- rho * z1 + sqrt(1 - rho^2) * z2\n\n    eps &lt;- rnorm(n, sd = sigma)\n    y   &lt;- b1 * x1 + b2 * x2 + eps\n\n    # Full OLS\n    full_fit &lt;- lm(y ~ x1 + x2)\n\n    # FWL steps\n    ey &lt;- resid(lm(y  ~ x2))   # residualise Y on X2\n    ex &lt;- resid(lm(x1 ~ x2))   # residualise X1 on X2\n    fwl_fit &lt;- lm(ey ~ ex)\n\n    list(x1 = x1, x2 = x2, y = y,\n         ey = ey, ex = ex,\n         full_fit = full_fit, fwl_fit = fwl_fit,\n         b1 = b1, b2 = b2)\n  })\n\n  # --- Plot 1: Y vs X1 (naive scatter) ---\n  output$plot_full &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4, 3, 1))\n    plot(d$x1, d$y, pch = 16, col = \"#3498db80\", cex = 0.8,\n         xlab = expression(X[1]), ylab = \"Y\",\n         main = expression(\"Y vs \" * X[1] * \" (raw)\"))\n    abline(lm(d$y ~ d$x1), col = \"#e74c3c\", lwd = 2.5)\n    naive_b &lt;- round(coef(lm(d$y ~ d$x1))[2], 4)\n    legend(\"topleft\", bty = \"n\", cex = 0.9,\n           legend = paste(\"Naive slope =\", naive_b))\n  })\n\n  # --- Plot 2: residualised X1 (partial out X2) ---\n  output$plot_partial &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4, 3, 1))\n    plot(d$x1, d$ex, pch = 16, col = \"#9b59b680\", cex = 0.8,\n         xlab = expression(X[1]),\n         ylab = expression(e[X[1]]),\n         main = expression(\"Residualise \" * X[1] * \" on \" * X[2]))\n    abline(h = 0, lty = 2, col = \"gray50\")\n    abline(lm(d$ex ~ d$x1), col = \"#8e44ad\", lwd = 2)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = expression(\"Variation in \" * X[1] * \" independent of \" * X[2]))\n  })\n\n  # --- Plot 3: FWL regression ---\n  output$plot_fwl &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4, 3, 1))\n    plot(d$ex, d$ey, pch = 16, col = \"#2ecc7180\", cex = 0.8,\n         xlab = expression(e[X[1]]),\n         ylab = expression(e[Y]),\n         main = \"FWL: Residual Y vs Residual X1\")\n    abline(d$fwl_fit, col = \"#e74c3c\", lwd = 2.5)\n    fwl_b &lt;- round(coef(d$fwl_fit)[2], 4)\n    legend(\"topleft\", bty = \"n\", cex = 0.9,\n           legend = paste(\"FWL slope =\", fwl_b))\n  })\n\n  # --- Results comparison ---\n  output$results_box &lt;- renderUI({\n    d &lt;- dat()\n    full_b1 &lt;- round(coef(d$full_fit)[\"x1\"], 4)\n    fwl_b1  &lt;- round(coef(d$fwl_fit)[2], 4)\n    naive_b &lt;- round(coef(lm(d$y ~ d$x1))[2], 4)\n\n    tags$div(class = \"eq-box\", style = \"margin-top: 16px;\",\n      HTML(paste0(\n        \"&lt;b&gt;True &beta;&lt;sub&gt;1&lt;/sub&gt;:&lt;/b&gt; \", d$b1, \"&lt;br&gt;\",\n        \"&lt;b&gt;Full OLS &beta;&lt;sub&gt;1&lt;/sub&gt;:&lt;/b&gt; &lt;span class='coef'&gt;\", full_b1, \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;FWL &beta;&lt;sub&gt;1&lt;/sub&gt;:&lt;/b&gt; &lt;span class='coef'&gt;\", fwl_b1, \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;span class='match'&gt;&#10003; They match!&lt;/span&gt;&lt;br&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Naive slope:&lt;/b&gt; \", naive_b, \"&lt;br&gt;\",\n        \"&lt;small&gt;(biased by omitting X&lt;sub&gt;2&lt;/sub&gt;)&lt;/small&gt;\"\n      ))\n    )\n  })\n\n  # --- Step explanation ---\n  output$step_text &lt;- renderUI({\n    tags$div(class = \"eq-box\", style = \"margin-top: 8px;\",\n      HTML(paste0(\n        \"&lt;b&gt;Steps:&lt;/b&gt; \",\n        \"(1) Regress Y on X&lt;sub&gt;2&lt;/sub&gt; &rarr; residuals &lt;i&gt;e&lt;sub&gt;Y&lt;/sub&gt;&lt;/i&gt; &nbsp;|&nbsp; \",\n        \"(2) Regress X&lt;sub&gt;1&lt;/sub&gt; on X&lt;sub&gt;2&lt;/sub&gt; &rarr; residuals &lt;i&gt;e&lt;sub&gt;X₁&lt;/sub&gt;&lt;/i&gt; &nbsp;|&nbsp; \",\n        \"(3) Regress &lt;i&gt;e&lt;sub&gt;Y&lt;/sub&gt;&lt;/i&gt; on &lt;i&gt;e&lt;sub&gt;X₁&lt;/sub&gt;&lt;/i&gt; &rarr; \",\n        \"slope = &beta;&lt;sub&gt;1&lt;/sub&gt; from full regression\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nDid you know?\n\nRagnar Frisch and Jan Tinbergen won the very first Nobel Prize in Economics in 1969. Frisch coined the terms “econometrics,” “microeconomics,” and “macroeconomics.” The FWL theorem appeared in Frisch & Waugh (1933).\nMichael Lovell extended the result in 1963, showing it applies to any partitioned regression — not just the two-variable case. That’s why it’s FWL, not just FW.\nFWL is the theoretical foundation behind “partialling out” and “controlling for” variables. Every time you add a control to a regression, you’re implicitly doing the residualization that FWL describes.\nIn machine learning, the same idea appears as “residualization” in double/debiased ML (Chernozhukov et al., 2018) — one of the most important recent developments in causal ML.",
    "crumbs": [
      "Frisch-Waugh-Lovell"
    ]
  },
  {
    "objectID": "foundations.html",
    "href": "foundations.html",
    "title": "Statistical Foundations",
    "section": "",
    "text": "A distribution is a model of variability. Imagine measuring the commute time of every person in a city. Some people take 10 minutes, most take around 30, a few are stuck for over an hour. If you made a histogram of all those commute times, the shape you’d see is the distribution — it tells you which values are common, which are rare, and how spread out things are.\nWhy do we care? Because variation is everywhere. Two patients given the same drug respond differently. Two students who study the same hours get different exam scores. Two identical ads shown to similar users get different click rates. None of that is a mistake — it’s the nature of data. A distribution is simply a mathematical way of describing how much things vary and in what pattern.\nOnce you have a distribution, you can answer useful questions: What’s the most likely outcome? How often do we see extreme values? Where does the middle 50% of the data sit?\nKey objects: histogram (shape), CDF (cumulative probabilities), quantiles (where does 50% of the data fall?).\nExplore different distributions below — notice how each has a distinct shape, center, and spread.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 450\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n      selectInput(\"dist\", \"Distribution:\",\n                  choices = c(\"Uniform(0,1)\", \"Normal(0,1)\",\n                              \"Exponential(1)\", \"Chi-squared(3)\",\n                              \"Bimodal\")),\n      sliderInput(\"n\", \"Sample size:\", min = 50, max = 5000, value = 1000, step = 50),\n      actionButton(\"draw\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n      uiOutput(\"stats\")\n    ),\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"hist_plot\", height = \"350px\")),\n        column(6, plotOutput(\"cdf_plot\",  height = \"350px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  samp &lt;- reactive({\n    input$draw\n    n &lt;- input$n\n    switch(input$dist,\n      \"Uniform(0,1)\"   = runif(n),\n      \"Normal(0,1)\"    = rnorm(n),\n      \"Exponential(1)\" = rexp(n),\n      \"Chi-squared(3)\" = rchisq(n, df = 3),\n      \"Bimodal\"        = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, -2, 0.6) + (1 - k) * rnorm(n, 2, 0.6)\n      }\n    )\n  })\n\n  output$hist_plot &lt;- renderPlot({\n    x &lt;- samp()\n    par(mar = c(4.5, 4, 3, 1))\n    hist(x, breaks = 50, probability = TRUE,\n         col = \"#d5e8d4\", border = \"#82b366\",\n         main = paste(\"Histogram:\", input$dist),\n         xlab = \"x\", ylab = \"Density\")\n  })\n\n  output$cdf_plot &lt;- renderPlot({\n    x &lt;- samp()\n    par(mar = c(4.5, 4, 3, 1))\n    plot(ecdf(x), col = \"#3498db\", lwd = 2,\n         main = paste(\"CDF:\", input$dist),\n         xlab = \"x\", ylab = \"F(x)\")\n  })\n\n  output$stats &lt;- renderUI({\n    x &lt;- samp()\n    q &lt;- round(quantile(x, c(0.25, 0.5, 0.75)), 3)\n    tags$div(style = \"background:#f0f4f8; border-radius:6px; padding:12px; margin-top:12px; font-size:14px; line-height:1.8;\",\n      HTML(paste0(\n        \"&lt;b&gt;Mean:&lt;/b&gt; \", round(mean(x), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;SD:&lt;/b&gt; \", round(sd(x), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Q25:&lt;/b&gt; \", q[1], \"&lt;br&gt;\",\n        \"&lt;b&gt;Median:&lt;/b&gt; \", q[2], \"&lt;br&gt;\",\n        \"&lt;b&gt;Q75:&lt;/b&gt; \", q[3]\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Foundations"
    ]
  },
  {
    "objectID": "foundations.html#data-distributions",
    "href": "foundations.html#data-distributions",
    "title": "Statistical Foundations",
    "section": "",
    "text": "A distribution is a model of variability. Imagine measuring the commute time of every person in a city. Some people take 10 minutes, most take around 30, a few are stuck for over an hour. If you made a histogram of all those commute times, the shape you’d see is the distribution — it tells you which values are common, which are rare, and how spread out things are.\nWhy do we care? Because variation is everywhere. Two patients given the same drug respond differently. Two students who study the same hours get different exam scores. Two identical ads shown to similar users get different click rates. None of that is a mistake — it’s the nature of data. A distribution is simply a mathematical way of describing how much things vary and in what pattern.\nOnce you have a distribution, you can answer useful questions: What’s the most likely outcome? How often do we see extreme values? Where does the middle 50% of the data sit?\nKey objects: histogram (shape), CDF (cumulative probabilities), quantiles (where does 50% of the data fall?).\nExplore different distributions below — notice how each has a distinct shape, center, and spread.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 450\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n      selectInput(\"dist\", \"Distribution:\",\n                  choices = c(\"Uniform(0,1)\", \"Normal(0,1)\",\n                              \"Exponential(1)\", \"Chi-squared(3)\",\n                              \"Bimodal\")),\n      sliderInput(\"n\", \"Sample size:\", min = 50, max = 5000, value = 1000, step = 50),\n      actionButton(\"draw\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n      uiOutput(\"stats\")\n    ),\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"hist_plot\", height = \"350px\")),\n        column(6, plotOutput(\"cdf_plot\",  height = \"350px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  samp &lt;- reactive({\n    input$draw\n    n &lt;- input$n\n    switch(input$dist,\n      \"Uniform(0,1)\"   = runif(n),\n      \"Normal(0,1)\"    = rnorm(n),\n      \"Exponential(1)\" = rexp(n),\n      \"Chi-squared(3)\" = rchisq(n, df = 3),\n      \"Bimodal\"        = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, -2, 0.6) + (1 - k) * rnorm(n, 2, 0.6)\n      }\n    )\n  })\n\n  output$hist_plot &lt;- renderPlot({\n    x &lt;- samp()\n    par(mar = c(4.5, 4, 3, 1))\n    hist(x, breaks = 50, probability = TRUE,\n         col = \"#d5e8d4\", border = \"#82b366\",\n         main = paste(\"Histogram:\", input$dist),\n         xlab = \"x\", ylab = \"Density\")\n  })\n\n  output$cdf_plot &lt;- renderPlot({\n    x &lt;- samp()\n    par(mar = c(4.5, 4, 3, 1))\n    plot(ecdf(x), col = \"#3498db\", lwd = 2,\n         main = paste(\"CDF:\", input$dist),\n         xlab = \"x\", ylab = \"F(x)\")\n  })\n\n  output$stats &lt;- renderUI({\n    x &lt;- samp()\n    q &lt;- round(quantile(x, c(0.25, 0.5, 0.75)), 3)\n    tags$div(style = \"background:#f0f4f8; border-radius:6px; padding:12px; margin-top:12px; font-size:14px; line-height:1.8;\",\n      HTML(paste0(\n        \"&lt;b&gt;Mean:&lt;/b&gt; \", round(mean(x), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;SD:&lt;/b&gt; \", round(sd(x), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Q25:&lt;/b&gt; \", q[1], \"&lt;br&gt;\",\n        \"&lt;b&gt;Median:&lt;/b&gt; \", q[2], \"&lt;br&gt;\",\n        \"&lt;b&gt;Q75:&lt;/b&gt; \", q[3]\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Foundations"
    ]
  },
  {
    "objectID": "foundations.html#sampling-uncertainty",
    "href": "foundations.html#sampling-uncertainty",
    "title": "Statistical Foundations",
    "section": "2. Sampling & Uncertainty",
    "text": "2. Sampling & Uncertainty\nSampling is why statistics exists. A sample mean is not a fixed truth — it is one draw from a distribution of possible sample means. Repeat the sampling and you get a different answer every time.\nThe key insight: larger samples produce less variable estimates. The spread of the sampling distribution shrinks at rate \\(1/\\sqrt{n}\\).\nTry it: press “New draw” a few times at \\(n = 10\\), then slide up to \\(n = 200\\) and watch how the sample means cluster tighter around the true mean.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 420\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n      sliderInput(\"n\", \"Sample size (n):\", min = 5, max = 200, value = 10, step = 5),\n      sliderInput(\"reps\", \"Repeated samples:\", min = 5, max = 50, value = 20, step = 5),\n      actionButton(\"go\", \"Draw samples\", class = \"btn-primary\", width = \"100%\")\n    ),\n    mainPanel(\n      width = 9,\n      plotOutput(\"dot_plot\", height = \"350px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  output$dot_plot &lt;- renderPlot({\n    input$go\n    n    &lt;- input$n\n    reps &lt;- input$reps\n\n    means &lt;- replicate(reps, mean(rnorm(n, mean = 5, sd = 2)))\n\n    par(mar = c(4.5, 2, 3, 1))\n    stripchart(means, method = \"stack\", pch = 19, cex = 1.5,\n               col = \"#3498db\", offset = 0.5,\n               xlim = c(3, 7),\n               main = paste0(reps, \" sample means (n = \", n, \")\"),\n               xlab = \"Sample mean\")\n    abline(v = 5, lty = 2, lwd = 2, col = \"#e74c3c\")\n    legend(\"topright\", legend = \"True mean = 5\",\n           col = \"#e74c3c\", lty = 2, lwd = 2, bty = \"n\")\n  })\n}\n\nshinyApp(ui, server)\nPractice questions:\n\nWhat happens to the spread of sample means as \\(n\\) increases?\nDoes the population distribution change when you change \\(n\\)?\nCould a single sample mean be far from the truth? Is that more likely with small or large \\(n\\)?",
    "crumbs": [
      "Foundations"
    ]
  },
  {
    "objectID": "foundations.html#confidence-intervals",
    "href": "foundations.html#confidence-intervals",
    "title": "Statistical Foundations",
    "section": "3. Confidence Intervals",
    "text": "3. Confidence Intervals\nA 95% confidence interval does not mean “there’s a 95% probability the true value is inside.” The true value is fixed — it’s either in there or not.\nThe correct interpretation: if you repeated the experiment many times and built a CI each time, 95% of those intervals would contain the true value.\nThe simulation below shows exactly this. Each horizontal line is one CI from a fresh sample. Most cover the true mean (blue), but about 5% miss (red).\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 500\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n      sliderInput(\"n\", \"Sample size (n):\", min = 10, max = 200, value = 30, step = 10),\n      sliderInput(\"k\", \"Number of CIs:\", min = 20, max = 100, value = 50, step = 10),\n      sliderInput(\"conf\", \"Confidence level:\", min = 0.80, max = 0.99, value = 0.95, step = 0.01),\n      actionButton(\"go\", \"New experiment\", class = \"btn-primary\", width = \"100%\"),\n      uiOutput(\"coverage\")\n    ),\n    mainPanel(\n      width = 9,\n      plotOutput(\"ci_plot\", height = \"420px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  res &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    k    &lt;- input$k\n    conf &lt;- input$conf\n    mu   &lt;- 0\n\n    z &lt;- qnorm(1 - (1 - conf) / 2)\n\n    ci &lt;- t(replicate(k, {\n      x   &lt;- rnorm(n, mean = mu, sd = 1)\n      xbar &lt;- mean(x)\n      se   &lt;- sd(x) / sqrt(n)\n      c(xbar, xbar - z * se, xbar + z * se)\n    }))\n\n    covers &lt;- ci[, 2] &lt;= mu & ci[, 3] &gt;= mu\n\n    list(ci = ci, covers = covers, mu = mu, k = k, conf = conf)\n  })\n\n  output$ci_plot &lt;- renderPlot({\n    r &lt;- res()\n    par(mar = c(4.5, 4, 3, 1))\n\n    plot(NULL, xlim = range(r$ci[, 2:3]), ylim = c(1, r$k),\n         xlab = \"Value\", ylab = \"Sample #\",\n         main = paste0(round(r$conf * 100), \"% Confidence Intervals\"))\n\n    for (i in seq_len(r$k)) {\n      clr &lt;- if (r$covers[i]) \"#3498db\" else \"#e74c3c\"\n      lw  &lt;- if (r$covers[i]) 1.5 else 2.5\n      segments(r$ci[i, 2], i, r$ci[i, 3], i, col = clr, lwd = lw)\n      points(r$ci[i, 1], i, pch = 16, cex = 0.5, col = clr)\n    }\n\n    abline(v = r$mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n  })\n\n  output$coverage &lt;- renderUI({\n    r &lt;- res()\n    pct &lt;- round(100 * mean(r$covers), 1)\n    miss &lt;- sum(!r$covers)\n    tags$div(style = \"background:#f0f4f8; border-radius:6px; padding:12px; margin-top:12px; font-size:14px; line-height:1.8;\",\n      HTML(paste0(\n        \"&lt;b&gt;Coverage:&lt;/b&gt; \", pct, \"%&lt;br&gt;\",\n        \"&lt;b&gt;Missed:&lt;/b&gt; \", miss, \" / \", r$k, \"&lt;br&gt;\",\n        \"&lt;small&gt;Target: \", round(r$conf * 100), \"%&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\nKey takeaways:\n\nA confidence interval quantifies uncertainty, not probability about the parameter\nWider intervals = more uncertainty (small \\(n\\), high confidence level)\nThe coverage rate converges to the nominal level over many experiments\n\n\n\nDid you know?\n\nFlorence Nightingale wasn’t just a nurse — she was a pioneering statistician. She invented the polar area diagram (a variant of the pie chart) to convince the British government that soldiers were dying from preventable disease, not combat wounds. Her charts changed military policy and saved thousands of lives.\nThe word “statistics” comes from the German Statistik, meaning “science of the state” — it originally referred to collecting data about populations for government use.\nJohn Graunt (1620–1674) is considered the father of demography. He analyzed London’s death records and discovered that more boys are born than girls, that urban death rates exceed rural ones, and that plague deaths follow seasonal patterns — all from just counting.",
    "crumbs": [
      "Foundations"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "variance-sd-se.html",
    "href": "variance-sd-se.html",
    "title": "Variance, SD & Standard Error",
    "section": "",
    "text": "Imagine you’re measuring the heights of students in a class. Some are tall, some are short — there’s spread. Statistics gives us precise language for that spread:\n\n\n\n\n\n\n\n\nConcept\nWhat it measures\nFormula\n\n\n\n\nVariance\nAverage squared distance from the mean\n\\(\\sigma^2 = \\frac{1}{N}\\sum(x_i - \\mu)^2\\)\n\n\nStandard Deviation (SD)\nAverage distance from the mean (in original units)\n\\(\\sigma = \\sqrt{\\sigma^2}\\)\n\n\nStandard Error (SE)\nHow much the sample mean bounces around\n\\(SE = \\frac{\\sigma}{\\sqrt{n}}\\)\n\n\n\nThe key insight: SD measures spread in your data. SE measures spread in your estimates.",
    "crumbs": [
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#the-big-picture",
    "href": "variance-sd-se.html#the-big-picture",
    "title": "Variance, SD & Standard Error",
    "section": "",
    "text": "Imagine you’re measuring the heights of students in a class. Some are tall, some are short — there’s spread. Statistics gives us precise language for that spread:\n\n\n\n\n\n\n\n\nConcept\nWhat it measures\nFormula\n\n\n\n\nVariance\nAverage squared distance from the mean\n\\(\\sigma^2 = \\frac{1}{N}\\sum(x_i - \\mu)^2\\)\n\n\nStandard Deviation (SD)\nAverage distance from the mean (in original units)\n\\(\\sigma = \\sqrt{\\sigma^2}\\)\n\n\nStandard Error (SE)\nHow much the sample mean bounces around\n\\(SE = \\frac{\\sigma}{\\sqrt{n}}\\)\n\n\n\nThe key insight: SD measures spread in your data. SE measures spread in your estimates.",
    "crumbs": [
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#variance-measuring-spread",
    "href": "variance-sd-se.html#variance-measuring-spread",
    "title": "Variance, SD & Standard Error",
    "section": "Variance: Measuring Spread",
    "text": "Variance: Measuring Spread\nVariance answers: “On average, how far are values from the center?”\nWe square the deviations because:\n\nPositive and negative deviations would cancel out otherwise\nIt penalizes large deviations more than small ones\nIt has nice mathematical properties (additive for independent variables)\n\n\nPopulation Variance vs Sample Variance\nHere’s where it gets tricky. There are two formulas:\nPopulation variance (when you have the entire population): \\[\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\mu)^2\\]\nSample variance (when you have a sample from a larger population): \\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\\]\n\n\nWhy n - 1? (Bessel’s Correction)\nWhen you compute deviations from the sample mean \\(\\bar{x}\\) instead of the true mean \\(\\mu\\), you’re using the data twice — once to compute \\(\\bar{x}\\), then again to compute deviations from it. The sample mean is always closer to the data points than the true mean would be, so dividing by \\(n\\) systematically underestimates the true variance.\nThink of it this way: if you have \\(n = 1\\) data point, the sample mean equals that point, so the deviation is 0. But that doesn’t mean there’s no variability in the population! Dividing by \\(n - 1 = 0\\) makes the formula undefined, which correctly tells you: you can’t estimate spread from a single observation.\nThe \\(n - 1\\) is called the degrees of freedom — you “used up” one degree of freedom estimating the mean.\nTry the simulation below to see this in action:\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 750\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Why n - 1?\"),\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n      sliderInput(\"pop_mean\", \"Population mean (mu):\", 50, 30, 70, step = 1),\n      sliderInput(\"pop_sd\", \"Population SD (sigma):\", 10, 2, 25, step = 1),\n      sliderInput(\"samp_n\", \"Sample size (n):\", 10, 2, 100),\n      sliderInput(\"n_reps\", \"Number of samples:\", 500, 100, 2000, step = 100),\n      actionButton(\"go\", \"Draw new samples\", class = \"btn-primary btn-block\"),\n      hr(),\n      h4(\"Results\"),\n      tableOutput(\"results_table\"),\n      hr(),\n      p(strong(\"Bias check:\"), \"If the average of many sample estimates equals the true value, the estimator is unbiased.\"),\n      p(\"Notice: dividing by n underestimates. Dividing by n-1 gets it right on average.\")\n    ),\n    mainPanel(\n      width = 9,\n      plotOutput(\"dist_plot\", height = \"600px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  vals &lt;- reactiveVal(NULL)\n\n  observeEvent(input$go, {\n    mu &lt;- input$pop_mean\n    sigma &lt;- input$pop_sd\n    n &lt;- input$samp_n\n    n_reps &lt;- input$n_reps\n\n    var_n &lt;- numeric(n_reps)\n    var_n1 &lt;- numeric(n_reps)\n\n    for (i in 1:n_reps) {\n      samp &lt;- rnorm(n, mu, sigma)\n      xbar &lt;- mean(samp)\n      devs &lt;- (samp - xbar)^2\n      var_n[i] &lt;- sum(devs) / n\n      var_n1[i] &lt;- sum(devs) / (n - 1)\n    }\n\n    vals(list(var_n = var_n, var_n1 = var_n1, true_var = sigma^2, n = n))\n  }, ignoreNULL = FALSE)\n\n  output$dist_plot &lt;- renderPlot({\n    v &lt;- vals()\n    if (is.null(v)) return()\n\n    par(mfrow = c(2, 1), mar = c(4.5, 4.5, 3, 1))\n\n    # Top: divide by n (biased)\n    xlims &lt;- range(c(v$var_n, v$var_n1))\n    hist(v$var_n, breaks = 40, col = adjustcolor(\"#3498db\", 0.6), border = \"white\",\n         main = paste0(\"Divide by n (biased)  —  n = \", v$n),\n         xlab = \"Estimated Variance\", xlim = xlims, cex.main = 1.3)\n    abline(v = v$true_var, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n    abline(v = mean(v$var_n), col = \"#2c3e50\", lwd = 2)\n    legend(\"topright\", legend = c(paste0(\"True = \", v$true_var),\n           paste0(\"Avg = \", round(mean(v$var_n), 2))),\n           col = c(\"#e74c3c\", \"#2c3e50\"), lwd = 2, lty = c(2, 1), bty = \"n\")\n\n    # Bottom: divide by n-1 (unbiased)\n    hist(v$var_n1, breaks = 40, col = adjustcolor(\"#2ecc71\", 0.6), border = \"white\",\n         main = \"Divide by n-1 (unbiased)\",\n         xlab = \"Estimated Variance\", xlim = xlims, cex.main = 1.3)\n    abline(v = v$true_var, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n    abline(v = mean(v$var_n1), col = \"#2c3e50\", lwd = 2)\n    legend(\"topright\", legend = c(paste0(\"True = \", v$true_var),\n           paste0(\"Avg = \", round(mean(v$var_n1), 2))),\n           col = c(\"#e74c3c\", \"#2c3e50\"), lwd = 2, lty = c(2, 1), bty = \"n\")\n  })\n\n  output$results_table &lt;- renderTable({\n    v &lt;- vals()\n    if (is.null(v)) return()\n    data.frame(\n      Estimator = c(\"True var\", \"Avg (/ n)\", \"Avg (/ n-1)\"),\n      Value = round(c(v$true_var, mean(v$var_n), mean(v$var_n1)), 2)\n    )\n  }, striped = TRUE)\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#standard-deviation-back-to-original-units",
    "href": "variance-sd-se.html#standard-deviation-back-to-original-units",
    "title": "Variance, SD & Standard Error",
    "section": "Standard Deviation: Back to Original Units",
    "text": "Standard Deviation: Back to Original Units\nVariance is in squared units. If heights are in cm, variance is in cm². That’s hard to interpret.\nStandard deviation = \\(\\sqrt{\\text{variance}}\\) — it puts spread back in the original units.\n\\[\\sigma = \\sqrt{\\sigma^2} \\qquad \\text{(population)} \\qquad\\qquad s = \\sqrt{s^2} \\qquad \\text{(sample)}\\]\nRules of thumb (for roughly normal data):\n\n~68% of data falls within ±1 SD of the mean\n~95% of data falls within ±2 SD of the mean\n~99.7% falls within ±3 SD",
    "crumbs": [
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#standard-error-the-sd-of-your-estimate",
    "href": "variance-sd-se.html#standard-error-the-sd-of-your-estimate",
    "title": "Variance, SD & Standard Error",
    "section": "Standard Error: The SD of Your Estimate",
    "text": "Standard Error: The SD of Your Estimate\nHere’s where people get confused. The standard error is not about your data — it’s about your estimate.\nIf you took many samples of size \\(n\\) and computed the mean each time, those means would form a distribution (the sampling distribution). The standard deviation of that distribution is the standard error:\n\\[SE(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}}\\]\nIn practice, we don’t know \\(\\sigma\\), so we plug in \\(s\\):\n\\[\\widehat{SE}(\\bar{x}) = \\frac{s}{\\sqrt{n}}\\]\n\nWhy does SE = SD / √n? The Intuition\nImagine you want to know the average commute time in your city. You ask one person — they say 45 minutes. But that’s just one person. Maybe they live far away. Your estimate is noisy — it could easily be off by 20 minutes (the SD of commute times).\nNow you ask 4 people and average: 45, 25, 60, 30 → mean = 40 minutes. Some are high, some are low — the errors partially cancel out. Your estimate is less noisy. But not 4× less noisy — only 2× less noisy (because \\(\\sqrt{4} = 2\\)).\nAsk 100 people: the cancellation is even stronger. Your average is now 10× more precise than a single observation (\\(\\sqrt{100} = 10\\)).\nWhy square root and not just n? Because the errors don’t perfectly cancel — they’re random, so sometimes more are high than low. The cancellation is partial. Mathematically, independent errors cancel at the rate of \\(\\sqrt{n}\\), not \\(n\\). If errors perfectly cancelled, the SE would be \\(\\sigma / n\\) and you’d barely need any data. If they didn’t cancel at all, the SE would stay at \\(\\sigma\\) forever and more data would be useless. The \\(\\sqrt{n}\\) is the sweet spot of reality.\n\n\nThe Math Behind It\nEach observation \\(x_i\\) is an independent draw with variance \\(\\sigma^2\\). The sample mean is:\n\\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\]\nStep 1: The variance of a sum of independent random variables equals the sum of their variances. Each \\(x_i\\) contributes \\(\\sigma^2\\):\n\\[\\text{Var}\\left(\\sum_{i=1}^n x_i\\right) = \\sigma^2 + \\sigma^2 + \\cdots + \\sigma^2 = n\\sigma^2\\]\nThe noise grows with \\(n\\) — more observations means more total variability in the sum. But we don’t want the sum, we want the average.\nStep 2: The mean divides the sum by \\(n\\). When you multiply a random variable by a constant \\(c\\), its variance gets multiplied by \\(c^2\\) (because variance is in squared units):\n\\[\\text{Var}(cX) = c^2 \\cdot \\text{Var}(X)\\]\nSo dividing by \\(n\\) means multiplying by \\(1/n\\), which divides the variance by \\(n^2\\):\n\\[\\text{Var}(\\bar{x}) = \\frac{1}{n^2} \\cdot n\\sigma^2 = \\frac{\\sigma^2}{n}\\]\nStep 3: Take the square root to get back to original units:\n\\[SE = \\sqrt{\\text{Var}(\\bar{x})} = \\sqrt{\\frac{\\sigma^2}{n}} = \\frac{\\sigma}{\\sqrt{n}}\\]\nThe sum’s noise grows as \\(n\\), but dividing by \\(n\\) shrinks it by \\(n^2\\). Net effect: noise shrinks by \\(n^2 / n = n\\), and the square root gives us \\(\\sqrt{n}\\). Averaging \\(n\\) independent measurements reduces noise by \\(\\sqrt{n}\\).\n\n\nThe Key Relationship\n\\[\\boxed{SE = \\frac{SD}{\\sqrt{n}}}\\]\nThis single formula connects everything:\n\nSD tells you how spread out individual data points are\nn is your sample size\nSE tells you how precisely you’ve estimated the mean\nMore data → smaller SE → more precise estimate\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 800\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"SD vs SE: What's the Difference?\"),\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n      sliderInput(\"true_sd\", \"Population SD:\", 15, 5, 40, step = 1),\n      sliderInput(\"n_obs\", \"Sample size (n):\", 25, 5, 200),\n      sliderInput(\"n_samples\", \"Samples to draw:\", 300, 50, 1000, step = 50),\n      actionButton(\"redraw\", \"Redraw\", class = \"btn-primary btn-block\"),\n      hr(),\n      h4(\"Theory\"),\n      uiOutput(\"theory_box\"),\n      hr(),\n      h4(\"Simulation\"),\n      uiOutput(\"sim_box\")\n    ),\n    mainPanel(\n      width = 9,\n      plotOutput(\"main_plot\", height = \"650px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  data &lt;- reactiveVal(NULL)\n\n  observeEvent(input$redraw, {\n    mu &lt;- 100\n    sigma &lt;- input$true_sd\n    n &lt;- input$n_obs\n    n_samp &lt;- input$n_samples\n\n    one_sample &lt;- rnorm(n, mu, sigma)\n    means &lt;- replicate(n_samp, mean(rnorm(n, mu, sigma)))\n\n    data(list(one_sample = one_sample, means = means, mu = mu, sigma = sigma, n = n))\n  }, ignoreNULL = FALSE)\n\n  output$theory_box &lt;- renderUI({\n    d &lt;- data()\n    if (is.null(d)) return()\n    se_theory &lt;- round(d$sigma / sqrt(d$n), 2)\n    tagList(\n      p(paste0(\"SD = \", d$sigma)),\n      p(paste0(\"SE = SD/sqrt(n) = \", d$sigma, \"/sqrt(\", d$n, \") = \", se_theory))\n    )\n  })\n\n  output$sim_box &lt;- renderUI({\n    d &lt;- data()\n    if (is.null(d)) return()\n    tagList(\n      p(paste0(\"SD of one sample: \", round(sd(d$one_sample), 2))),\n      p(paste0(\"SD of sample means: \", round(sd(d$means), 2))),\n      p(paste0(\"Ratio: \", round(sd(d$one_sample) / sd(d$means), 1), \"x narrower\"))\n    )\n  })\n\n  output$main_plot &lt;- renderPlot({\n    d &lt;- data()\n    if (is.null(d)) return()\n\n    sample_sd &lt;- sd(d$one_sample)\n    se_actual &lt;- sd(d$means)\n    xbar &lt;- mean(d$one_sample)\n\n    # Use same x-axis for both so you can see the difference in spread\n    xlims &lt;- c(d$mu - 3.5 * d$sigma, d$mu + 3.5 * d$sigma)\n\n    par(mfrow = c(2, 1), mar = c(4.5, 4.5, 3.5, 1))\n\n    # Top: one sample — spread = SD\n    hist(d$one_sample, breaks = 30, col = adjustcolor(\"#3498db\", 0.6), border = \"white\",\n         main = paste0(\"One Sample (n = \", d$n, \") — spread = SD\"),\n         xlab = \"Value\", xlim = xlims, freq = FALSE, cex.main = 1.3)\n    abline(v = xbar, col = \"#e74c3c\", lwd = 2.5)\n    segments(xbar - sample_sd, 0, xbar + sample_sd, 0, col = \"#e67e22\", lwd = 4)\n    mtext(paste0(\"Mean = \", round(xbar, 1), \"  |  SD = \", round(sample_sd, 1)),\n          side = 3, line = 0, cex = 1.1, font = 2)\n\n    # Bottom: sampling distribution of means — spread = SE\n    hist(d$means, breaks = 40, col = adjustcolor(\"#2ecc71\", 0.6), border = \"white\",\n         main = \"Sampling Distribution of the Mean — spread = SE\",\n         xlab = \"Sample Mean\", xlim = xlims, freq = FALSE, cex.main = 1.3)\n    # Theoretical normal curve\n    xseq &lt;- seq(xlims[1], xlims[2], length.out = 300)\n    lines(xseq, dnorm(xseq, d$mu, d$sigma / sqrt(d$n)), col = \"#8e44ad\", lwd = 2.5)\n    abline(v = d$mu, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n    segments(d$mu - se_actual, 0, d$mu + se_actual, 0, col = \"#e67e22\", lwd = 4)\n    mtext(paste0(\"Mean of means = \", round(mean(d$means), 1), \"  |  SE = \", round(se_actual, 2)),\n          side = 3, line = 0, cex = 1.1, font = 2)\n    legend(\"topright\", \"Theoretical N(mu, SD/sqrt(n))\", col = \"#8e44ad\", lwd = 2.5, bty = \"n\")\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#how-they-all-connect",
    "href": "variance-sd-se.html#how-they-all-connect",
    "title": "Variance, SD & Standard Error",
    "section": "How They All Connect",
    "text": "How They All Connect\nLet’s see all three in one place. Watch what happens as you increase \\(n\\):\n\nSD stays the same — it’s a property of the population, not the sample size\nSE shrinks — more data means a more precise estimate\nSample variance (s²) bounces around σ² — but is unbiased on average\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"SD vs SE as n Grows\"),\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n      sliderInput(\"sigma\", \"Population SD:\", 10, 2, 30),\n      sliderInput(\"n_size\", \"Sample size (n):\", 10, 5, 500),\n      p(style = \"margin-top:12px;\",\n        strong(\"What to notice:\"),\n        br(),\n        \"Blue histogram (data) stays equally wide — SD doesn't shrink with n\",\n        br(), br(),\n        \"Green histogram (means) gets narrower — SE shrinks with sqrt(n)\",\n        br(), br(),\n        \"At n = 100, SE is 10x smaller than SD\"\n      )\n    ),\n    mainPanel(\n      width = 9,\n      plotOutput(\"combo_plot\", height = \"280px\"),\n      plotOutput(\"se_curve\", height = \"300px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  output$combo_plot &lt;- renderPlot({\n    mu &lt;- 50\n    sigma &lt;- input$sigma\n    n &lt;- input$n_size\n\n    one_samp &lt;- rnorm(n, mu, sigma)\n    means &lt;- replicate(500, mean(rnorm(n, mu, sigma)))\n\n    xlims &lt;- c(mu - 3.5 * sigma, mu + 3.5 * sigma)\n\n    par(mfrow = c(1, 2), mar = c(4, 4, 3, 1))\n\n    hist(one_samp, breaks = 30, col = adjustcolor(\"#3498db\", 0.6), border = \"white\",\n         main = paste0(\"One Sample (SD = \", round(sd(one_samp), 1), \")\"),\n         xlab = \"Value\", xlim = xlims, freq = FALSE, cex.main = 1.2)\n    abline(v = mu, col = \"#e74c3c\", lwd = 2, lty = 2)\n\n    hist(means, breaks = 40, col = adjustcolor(\"#2ecc71\", 0.6), border = \"white\",\n         main = paste0(\"Sample Means (SE = \", round(sd(means), 2), \")\"),\n         xlab = \"Value\", xlim = xlims, freq = FALSE, cex.main = 1.2)\n    abline(v = mu, col = \"#e74c3c\", lwd = 2, lty = 2)\n  })\n\n  output$se_curve &lt;- renderPlot({\n    sigma &lt;- input$sigma\n    n_curr &lt;- input$n_size\n\n    ns &lt;- 2:500\n    ses &lt;- sigma / sqrt(ns)\n    se_now &lt;- sigma / sqrt(n_curr)\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(ns, ses, type = \"l\", lwd = 2.5, col = \"#2c3e50\",\n         xlab = \"Sample Size (n)\", ylab = \"Standard Error\",\n         main = \"SE = SD / sqrt(n) — diminishing returns\",\n         ylim = c(0, max(ses) * 1.1), cex.main = 1.3)\n\n    abline(h = sigma, col = \"#3498db\", lwd = 2, lty = 2)\n    points(n_curr, se_now, pch = 19, cex = 2, col = \"#e74c3c\")\n    segments(n_curr, 0, n_curr, se_now, lty = 2, col = \"#e74c3c\")\n    segments(0, se_now, n_curr, se_now, lty = 2, col = \"#e74c3c\")\n\n    text(400, sigma + 0.8, paste0(\"SD = \", sigma), col = \"#3498db\", cex = 1.2, font = 2)\n    text(n_curr + 25, se_now + 0.8, paste0(\"SE = \", round(se_now, 2)),\n         col = \"#e74c3c\", cex = 1.1, font = 2, adj = 0)\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#se-and-mde-why-se-determines-your-experiments-power",
    "href": "variance-sd-se.html#se-and-mde-why-se-determines-your-experiments-power",
    "title": "Variance, SD & Standard Error",
    "section": "SE and MDE: Why SE Determines Your Experiment’s Power",
    "text": "SE and MDE: Why SE Determines Your Experiment’s Power\nIf you run an experiment, the Minimum Detectable Effect (MDE) — the smallest effect you can reliably catch — is directly driven by the SE:\n\\[MDE = (z_{\\alpha/2} + z_{\\beta}) \\times SE\\]\nFor a two-sample experiment with equal groups (\\(\\sigma = 1\\) for simplicity):\n\\[MDE = (z_{\\alpha/2} + z_{\\beta}) \\times \\sqrt{\\frac{2}{n}}\\]\nThat \\(\\sqrt{2/n}\\) term is the SE of the difference in means. So MDE is just a scaled-up SE. The critical values (\\(z_{\\alpha/2} + z_{\\beta} \\approx 2.8\\) for 5% significance and 80% power) are just multipliers.\nThis means:\n\nShrink SE → shrink MDE → detect smaller effects. The only levers are: increase \\(n\\), or reduce \\(\\sigma\\) (better measurement, stratification, controls).\nPower analysis is really an SE calculation. You’re asking: “How small can I make my SE with this sample size?”\nIf your SE is too large, no statistical test can save you. The effect will be buried in noise.\n\nSee the Power, Alpha, Beta & MDE page for interactive simulations.",
    "crumbs": [
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#quick-reference",
    "href": "variance-sd-se.html#quick-reference",
    "title": "Variance, SD & Standard Error",
    "section": "Quick Reference",
    "text": "Quick Reference\n\n\n\n\n\n\n\n\n\n\nVariance (σ² or s²)\nStandard Deviation (σ or s)\nStandard Error (SE)\n\n\n\n\nMeasures\nSpread of data (squared)\nSpread of data (original units)\nPrecision of an estimate\n\n\nPopulation\n\\(\\sigma^2 = \\frac{1}{N}\\sum(x_i - \\mu)^2\\)\n\\(\\sigma = \\sqrt{\\sigma^2}\\)\n\\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\n\nSample\n\\(s^2 = \\frac{1}{n-1}\\sum(x_i - \\bar{x})^2\\)\n\\(s = \\sqrt{s^2}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\nChanges with n?\nConverges to σ²\nConverges to σ\nShrinks as \\(1/\\sqrt{n}\\)\n\n\nUsed for\nMath convenience\nDescribing data\nConfidence intervals, hypothesis tests\n\n\n\n\nCommon Mistakes\n\nReporting SD when you mean SE (or vice versa). SD describes how variable the data is. SE describes how precisely you’ve estimated the mean. A study with low SD but high SE has consistent data but too few observations.\nThinking SE measures data quality. SE can be tiny even with messy data — you just need a big enough \\(n\\). It says nothing about whether your data is clean or your measurements are accurate.\nForgetting that SE applies to any estimator, not just the mean. The slope in a regression has an SE. A proportion has an SE. Any time you estimate something from data, there’s uncertainty in that estimate — and the SE quantifies it.",
    "crumbs": [
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#did-you-know",
    "href": "variance-sd-se.html#did-you-know",
    "title": "Variance, SD & Standard Error",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe n − 1 Story\nThe correction for dividing by \\(n - 1\\) instead of \\(n\\) is called Bessel’s correction, named after Friedrich Bessel (1784–1846), a German astronomer who was obsessed with measurement precision. He needed to compute the orbits of stars and realized that using \\(n\\) in the denominator systematically underestimated how uncertain his measurements really were. His fix — dividing by \\(n - 1\\) — is now used billions of times a day in software from Excel to R to Python. Bessel never saw a computer, but his correction is baked into every call to var() and sd() in R and numpy.std(ddof=1) in Python.\n\n\nWhy “Standard” Error?\nThe word “standard” in “standard deviation” and “standard error” simply means “typical.” Karl Pearson coined “standard deviation” in 1893 because he wanted a word for the typical amount by which values deviate from their average. “Standard error” then naturally means the typical amount by which your estimate deviates from the truth. Nothing fancy — just “how far off is typical?”",
    "crumbs": [
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "heteroskedasticity.html",
    "href": "heteroskedasticity.html",
    "title": "Homoskedasticity & Heteroskedasticity",
    "section": "",
    "text": "Homoskedasticity (homo = same, skedasis = spread): the variance of the errors is constant across all values of \\(X\\).\nHeteroskedasticity (hetero = different): the variance changes with \\(X\\).\n\nThink of income vs. age. Young people’s incomes are clustered (entry-level jobs). But at age 50, some people earn $40k and others earn $500k. The spread of income increases with age — that’s heteroskedasticity.\n\n\nOLS is still unbiased under heteroskedasticity — the coefficient estimates are fine. But the standard errors are wrong. And wrong standard errors mean wrong confidence intervals, wrong t-statistics, wrong p-values. You might declare something significant when it isn’t (or miss something real).\nThe fix: use robust standard errors (HC, or “sandwich” standard errors) that don’t assume constant variance.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"type\", \"Error structure:\",\n                  choices = c(\"Homoskedastic\",\n                              \"Fan-shaped (variance grows with X)\",\n                              \"U-shaped variance\",\n                              \"Group heteroskedasticity\")),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 50, max = 500, value = 200, step = 50),\n\n      sliderInput(\"b1\", HTML(\"True &beta;&lt;sub&gt;1&lt;/sub&gt;:\"),\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(4, plotOutput(\"scatter\", height = \"380px\")),\n        column(4, plotOutput(\"resid_plot\", height = \"380px\")),\n        column(4, plotOutput(\"ci_plot\", height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    b1   &lt;- input$b1\n    type &lt;- input$type\n\n    x &lt;- runif(n, 0.5, 10)\n\n    if (type == \"Homoskedastic\") {\n      eps &lt;- rnorm(n, sd = 2)\n    } else if (type == \"Fan-shaped (variance grows with X)\") {\n      eps &lt;- rnorm(n, sd = 0.3 * x)\n    } else if (type == \"U-shaped variance\") {\n      eps &lt;- rnorm(n, sd = 0.5 + 0.8 * abs(x - 5))\n    } else {\n      eps &lt;- rnorm(n, sd = ifelse(x &gt; 5, 4, 0.8))\n    }\n\n    y &lt;- 2 + b1 * x + eps\n    fit &lt;- lm(y ~ x)\n\n    # OLS SE\n    ols_se &lt;- summary(fit)$coefficients[2, 2]\n\n    # HC robust SE (manual HC1)\n    r &lt;- resid(fit)\n    X &lt;- cbind(1, x)\n    bread &lt;- solve(t(X) %*% X)\n    meat &lt;- t(X) %*% diag(r^2 * n / (n - 2)) %*% X\n    robust_vcov &lt;- bread %*% meat %*% bread\n    robust_se &lt;- sqrt(robust_vcov[2, 2])\n\n    # Simulation: repeat 500 times, see how often CI covers\n    covers_ols &lt;- 0\n    covers_robust &lt;- 0\n    reps &lt;- 500\n    for (i in seq_len(reps)) {\n      xx &lt;- runif(n, 0.5, 10)\n      if (type == \"Homoskedastic\") {\n        ee &lt;- rnorm(n, sd = 2)\n      } else if (type == \"Fan-shaped (variance grows with X)\") {\n        ee &lt;- rnorm(n, sd = 0.3 * xx)\n      } else if (type == \"U-shaped variance\") {\n        ee &lt;- rnorm(n, sd = 0.5 + 0.8 * abs(xx - 5))\n      } else {\n        ee &lt;- rnorm(n, sd = ifelse(xx &gt; 5, 4, 0.8))\n      }\n      yy &lt;- 2 + b1 * xx + ee\n      ff &lt;- lm(yy ~ xx)\n      b_hat &lt;- coef(ff)[2]\n      ols_s &lt;- summary(ff)$coefficients[2, 2]\n\n      rr &lt;- resid(ff)\n      XX &lt;- cbind(1, xx)\n      br &lt;- solve(t(XX) %*% XX)\n      mt &lt;- t(XX) %*% diag(rr^2 * n / (n - 2)) %*% XX\n      rob_s &lt;- sqrt((br %*% mt %*% br)[2, 2])\n\n      if (b_hat - 1.96 * ols_s &lt;= b1 && b1 &lt;= b_hat + 1.96 * ols_s)\n        covers_ols &lt;- covers_ols + 1\n      if (b_hat - 1.96 * rob_s &lt;= b1 && b1 &lt;= b_hat + 1.96 * rob_s)\n        covers_robust &lt;- covers_robust + 1\n    }\n\n    list(x = x, y = y, fit = fit, type = type, b1 = b1,\n         ols_se = ols_se, robust_se = robust_se,\n         cover_ols = covers_ols / reps,\n         cover_robust = covers_robust / reps)\n  })\n\n  output$scatter &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$x, d$y, pch = 16, col = \"#3498db60\", cex = 0.7,\n         xlab = \"X\", ylab = \"Y\", main = \"Data + OLS fit\")\n    abline(d$fit, col = \"#e74c3c\", lwd = 2.5)\n  })\n\n  output$resid_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    r  &lt;- resid(d$fit)\n    fv &lt;- fitted(d$fit)\n    plot(fv, r, pch = 16, col = \"#9b59b660\", cex = 0.7,\n         xlab = \"Fitted values\", ylab = \"Residuals\",\n         main = \"Residuals vs Fitted\")\n    abline(h = 0, lty = 2, col = \"gray40\", lwd = 1.5)\n\n    # Envelope\n    lo_abs &lt;- loess(abs(r) ~ fv)\n    ox &lt;- order(fv)\n    env &lt;- predict(lo_abs)[ox]\n    lines(fv[ox], env, col = \"#e67e22\", lwd = 2, lty = 1)\n    lines(fv[ox], -env, col = \"#e67e22\", lwd = 2, lty = 1)\n  })\n\n  output$ci_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 6, 3, 1))\n\n    b_hat &lt;- coef(d$fit)[2]\n\n    ci_ols &lt;- c(b_hat - 1.96 * d$ols_se, b_hat + 1.96 * d$ols_se)\n    ci_rob &lt;- c(b_hat - 1.96 * d$robust_se, b_hat + 1.96 * d$robust_se)\n\n    xlim &lt;- range(c(ci_ols, ci_rob, d$b1)) + c(-0.3, 0.3)\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, 2.5),\n         yaxt = \"n\", ylab = \"\", xlab = expression(beta[1]),\n         main = \"95% Confidence Intervals\")\n    axis(2, at = 1:2, labels = c(\"OLS SE\", \"Robust SE\"), las = 1, cex.axis = 0.85)\n\n    segments(ci_ols[1], 1, ci_ols[2], 1, lwd = 4, col = \"#e74c3c\")\n    points(b_hat, 1, pch = 19, cex = 1.5, col = \"#e74c3c\")\n\n    segments(ci_rob[1], 2, ci_rob[2], 2, lwd = 4, col = \"#27ae60\")\n    points(b_hat, 2, pch = 19, cex = 1.5, col = \"#27ae60\")\n\n    abline(v = d$b1, lty = 2, lwd = 2, col = \"#2c3e50\")\n    text(d$b1, 2.4, expression(\"True \" * beta[1]), cex = 0.9)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;OLS SE:&lt;/b&gt; \", round(d$ols_se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Robust SE:&lt;/b&gt; \", round(d$robust_se, 4), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;OLS CI coverage:&lt;/b&gt; &lt;span class='\",\n        ifelse(abs(d$cover_ols - 0.95) &gt; 0.03, \"bad\", \"good\"), \"'&gt;\",\n        round(d$cover_ols * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Robust CI coverage:&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$cover_robust * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;small&gt;Target: 95%. Over 500 simulations.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nHomoskedastic: both SEs are similar, both CIs have ~95% coverage. Constant variance = OLS SEs are fine.\nFan-shaped: the residual plot shows a clear funnel. OLS SEs are wrong — coverage drops below 95%. Robust SEs fix it.\nGroup heteroskedasticity: one group (X &gt; 5) is much noisier. Look at the residual plot — the right side has wider scatter. OLS pretends the variance is the same everywhere.\nCompare the right panel: under heteroskedasticity, the OLS CI (red) is often the wrong width. The robust CI (green) adjusts.\n\n\n\n\nIn applied work, always use robust standard errors (or clustered SEs). They are valid whether or not heteroskedasticity is present. If the errors happen to be homoskedastic, robust SEs give you the same answer anyway — there’s no downside.\n\n\n\nSoftware\nHow to get robust SEs\n\n\n\n\nR\nlmtest::coeftest(fit, vcov = sandwich::vcovHC)\n\n\nStata\nreg y x, robust\n\n\nPython\nsm.OLS(y, X).fit(cov_type='HC3')\n\n\n\n\n\n\n\n\nNobody can agree on how to spell it. “Heteroskedasticity” (with a k) is the original Greek-derived spelling (skedasis = scattering). “Heteroscedasticity” (with a c) is the Latinized version. Both are correct. Econometricians tend to use k; other fields use c. The important thing is that your standard errors are right, not your spelling.\nHalbert White published his famous robust standard error estimator (HC0) in 1980. It was so influential that “White standard errors” became the default in applied economics. The paper has over 30,000 citations.\nThe HC in HC0, HC1, HC2, HC3 stands for “heteroskedasticity-consistent.” HC3 is generally preferred for small samples because it gives each observation a slightly larger weight, correcting for leverage points.\nFun debugging tip: if your robust SEs are much larger than your OLS SEs, you likely have heteroskedasticity. If they’re much smaller, something is probably wrong with your data.",
    "crumbs": [
      "Homo- & Heteroskedasticity"
    ]
  },
  {
    "objectID": "heteroskedasticity.html#what-do-these-words-mean",
    "href": "heteroskedasticity.html#what-do-these-words-mean",
    "title": "Homoskedasticity & Heteroskedasticity",
    "section": "",
    "text": "Homoskedasticity (homo = same, skedasis = spread): the variance of the errors is constant across all values of \\(X\\).\nHeteroskedasticity (hetero = different): the variance changes with \\(X\\).\n\nThink of income vs. age. Young people’s incomes are clustered (entry-level jobs). But at age 50, some people earn $40k and others earn $500k. The spread of income increases with age — that’s heteroskedasticity.\n\n\nOLS is still unbiased under heteroskedasticity — the coefficient estimates are fine. But the standard errors are wrong. And wrong standard errors mean wrong confidence intervals, wrong t-statistics, wrong p-values. You might declare something significant when it isn’t (or miss something real).\nThe fix: use robust standard errors (HC, or “sandwich” standard errors) that don’t assume constant variance.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"type\", \"Error structure:\",\n                  choices = c(\"Homoskedastic\",\n                              \"Fan-shaped (variance grows with X)\",\n                              \"U-shaped variance\",\n                              \"Group heteroskedasticity\")),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 50, max = 500, value = 200, step = 50),\n\n      sliderInput(\"b1\", HTML(\"True &beta;&lt;sub&gt;1&lt;/sub&gt;:\"),\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(4, plotOutput(\"scatter\", height = \"380px\")),\n        column(4, plotOutput(\"resid_plot\", height = \"380px\")),\n        column(4, plotOutput(\"ci_plot\", height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    b1   &lt;- input$b1\n    type &lt;- input$type\n\n    x &lt;- runif(n, 0.5, 10)\n\n    if (type == \"Homoskedastic\") {\n      eps &lt;- rnorm(n, sd = 2)\n    } else if (type == \"Fan-shaped (variance grows with X)\") {\n      eps &lt;- rnorm(n, sd = 0.3 * x)\n    } else if (type == \"U-shaped variance\") {\n      eps &lt;- rnorm(n, sd = 0.5 + 0.8 * abs(x - 5))\n    } else {\n      eps &lt;- rnorm(n, sd = ifelse(x &gt; 5, 4, 0.8))\n    }\n\n    y &lt;- 2 + b1 * x + eps\n    fit &lt;- lm(y ~ x)\n\n    # OLS SE\n    ols_se &lt;- summary(fit)$coefficients[2, 2]\n\n    # HC robust SE (manual HC1)\n    r &lt;- resid(fit)\n    X &lt;- cbind(1, x)\n    bread &lt;- solve(t(X) %*% X)\n    meat &lt;- t(X) %*% diag(r^2 * n / (n - 2)) %*% X\n    robust_vcov &lt;- bread %*% meat %*% bread\n    robust_se &lt;- sqrt(robust_vcov[2, 2])\n\n    # Simulation: repeat 500 times, see how often CI covers\n    covers_ols &lt;- 0\n    covers_robust &lt;- 0\n    reps &lt;- 500\n    for (i in seq_len(reps)) {\n      xx &lt;- runif(n, 0.5, 10)\n      if (type == \"Homoskedastic\") {\n        ee &lt;- rnorm(n, sd = 2)\n      } else if (type == \"Fan-shaped (variance grows with X)\") {\n        ee &lt;- rnorm(n, sd = 0.3 * xx)\n      } else if (type == \"U-shaped variance\") {\n        ee &lt;- rnorm(n, sd = 0.5 + 0.8 * abs(xx - 5))\n      } else {\n        ee &lt;- rnorm(n, sd = ifelse(xx &gt; 5, 4, 0.8))\n      }\n      yy &lt;- 2 + b1 * xx + ee\n      ff &lt;- lm(yy ~ xx)\n      b_hat &lt;- coef(ff)[2]\n      ols_s &lt;- summary(ff)$coefficients[2, 2]\n\n      rr &lt;- resid(ff)\n      XX &lt;- cbind(1, xx)\n      br &lt;- solve(t(XX) %*% XX)\n      mt &lt;- t(XX) %*% diag(rr^2 * n / (n - 2)) %*% XX\n      rob_s &lt;- sqrt((br %*% mt %*% br)[2, 2])\n\n      if (b_hat - 1.96 * ols_s &lt;= b1 && b1 &lt;= b_hat + 1.96 * ols_s)\n        covers_ols &lt;- covers_ols + 1\n      if (b_hat - 1.96 * rob_s &lt;= b1 && b1 &lt;= b_hat + 1.96 * rob_s)\n        covers_robust &lt;- covers_robust + 1\n    }\n\n    list(x = x, y = y, fit = fit, type = type, b1 = b1,\n         ols_se = ols_se, robust_se = robust_se,\n         cover_ols = covers_ols / reps,\n         cover_robust = covers_robust / reps)\n  })\n\n  output$scatter &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$x, d$y, pch = 16, col = \"#3498db60\", cex = 0.7,\n         xlab = \"X\", ylab = \"Y\", main = \"Data + OLS fit\")\n    abline(d$fit, col = \"#e74c3c\", lwd = 2.5)\n  })\n\n  output$resid_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    r  &lt;- resid(d$fit)\n    fv &lt;- fitted(d$fit)\n    plot(fv, r, pch = 16, col = \"#9b59b660\", cex = 0.7,\n         xlab = \"Fitted values\", ylab = \"Residuals\",\n         main = \"Residuals vs Fitted\")\n    abline(h = 0, lty = 2, col = \"gray40\", lwd = 1.5)\n\n    # Envelope\n    lo_abs &lt;- loess(abs(r) ~ fv)\n    ox &lt;- order(fv)\n    env &lt;- predict(lo_abs)[ox]\n    lines(fv[ox], env, col = \"#e67e22\", lwd = 2, lty = 1)\n    lines(fv[ox], -env, col = \"#e67e22\", lwd = 2, lty = 1)\n  })\n\n  output$ci_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 6, 3, 1))\n\n    b_hat &lt;- coef(d$fit)[2]\n\n    ci_ols &lt;- c(b_hat - 1.96 * d$ols_se, b_hat + 1.96 * d$ols_se)\n    ci_rob &lt;- c(b_hat - 1.96 * d$robust_se, b_hat + 1.96 * d$robust_se)\n\n    xlim &lt;- range(c(ci_ols, ci_rob, d$b1)) + c(-0.3, 0.3)\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, 2.5),\n         yaxt = \"n\", ylab = \"\", xlab = expression(beta[1]),\n         main = \"95% Confidence Intervals\")\n    axis(2, at = 1:2, labels = c(\"OLS SE\", \"Robust SE\"), las = 1, cex.axis = 0.85)\n\n    segments(ci_ols[1], 1, ci_ols[2], 1, lwd = 4, col = \"#e74c3c\")\n    points(b_hat, 1, pch = 19, cex = 1.5, col = \"#e74c3c\")\n\n    segments(ci_rob[1], 2, ci_rob[2], 2, lwd = 4, col = \"#27ae60\")\n    points(b_hat, 2, pch = 19, cex = 1.5, col = \"#27ae60\")\n\n    abline(v = d$b1, lty = 2, lwd = 2, col = \"#2c3e50\")\n    text(d$b1, 2.4, expression(\"True \" * beta[1]), cex = 0.9)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;OLS SE:&lt;/b&gt; \", round(d$ols_se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Robust SE:&lt;/b&gt; \", round(d$robust_se, 4), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;OLS CI coverage:&lt;/b&gt; &lt;span class='\",\n        ifelse(abs(d$cover_ols - 0.95) &gt; 0.03, \"bad\", \"good\"), \"'&gt;\",\n        round(d$cover_ols * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Robust CI coverage:&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$cover_robust * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;small&gt;Target: 95%. Over 500 simulations.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nHomoskedastic: both SEs are similar, both CIs have ~95% coverage. Constant variance = OLS SEs are fine.\nFan-shaped: the residual plot shows a clear funnel. OLS SEs are wrong — coverage drops below 95%. Robust SEs fix it.\nGroup heteroskedasticity: one group (X &gt; 5) is much noisier. Look at the residual plot — the right side has wider scatter. OLS pretends the variance is the same everywhere.\nCompare the right panel: under heteroskedasticity, the OLS CI (red) is often the wrong width. The robust CI (green) adjusts.\n\n\n\n\nIn applied work, always use robust standard errors (or clustered SEs). They are valid whether or not heteroskedasticity is present. If the errors happen to be homoskedastic, robust SEs give you the same answer anyway — there’s no downside.\n\n\n\nSoftware\nHow to get robust SEs\n\n\n\n\nR\nlmtest::coeftest(fit, vcov = sandwich::vcovHC)\n\n\nStata\nreg y x, robust\n\n\nPython\nsm.OLS(y, X).fit(cov_type='HC3')\n\n\n\n\n\n\n\n\nNobody can agree on how to spell it. “Heteroskedasticity” (with a k) is the original Greek-derived spelling (skedasis = scattering). “Heteroscedasticity” (with a c) is the Latinized version. Both are correct. Econometricians tend to use k; other fields use c. The important thing is that your standard errors are right, not your spelling.\nHalbert White published his famous robust standard error estimator (HC0) in 1980. It was so influential that “White standard errors” became the default in applied economics. The paper has over 30,000 citations.\nThe HC in HC0, HC1, HC2, HC3 stands for “heteroskedasticity-consistent.” HC3 is generally preferred for small samples because it gives each observation a slightly larger weight, correcting for leverage points.\nFun debugging tip: if your robust SEs are much larger than your OLS SEs, you likely have heteroskedasticity. If they’re much smaller, something is probably wrong with your data.",
    "crumbs": [
      "Homo- & Heteroskedasticity"
    ]
  },
  {
    "objectID": "bootstrap.html",
    "href": "bootstrap.html",
    "title": "The Bootstrap",
    "section": "",
    "text": "You want a confidence interval for some statistic (mean, median, regression coefficient), but you don’t know the sampling distribution. Maybe the formula is complicated. Maybe there is no formula.\nThe bootstrap says: pretend your sample is the population. Then simulate the sampling process by resampling with replacement from your data, over and over. Each resample gives you a new estimate. The distribution of those estimates approximates the true sampling distribution.\n\n\n\nYou have a sample of \\(n\\) observations.\nDraw \\(n\\) observations with replacement from your sample (some points get picked twice, some not at all).\nCompute your statistic on this resample.\nRepeat B times (typically 1,000–10,000).\nThe spread of those B estimates gives you a standard error and a confidence interval.\n\nThat’s it. No formulas, no distributional assumptions. Just resampling.\n\n\n\nNo — and this is a crucial distinction. There are two different distributions at play:\n\nThe distribution of the data = what individual observations look like (their histogram). This tells you about spread, skew, outliers in your raw data.\nThe sampling distribution = what your statistic (mean, median, slope) would look like if you could repeat the entire experiment many times. This is what you need for standard errors and confidence intervals.\n\nYou can always plot your data. But you cannot see the sampling distribution — you only ran the experiment once. You have one mean, not a distribution of means.\nThe bootstrap bridges that gap. It simulates the “what if I repeated this experiment?” question using only the data you have. The left panel below shows your data distribution. The right panel shows the bootstrap sampling distribution. Notice: they look completely different. Your data might be skewed and spread out, but the sampling distribution of the mean is narrow and roughly normal (thanks to the CLT). The bootstrap discovers this for you without any formulas.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 900\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"stat\", \"Statistic to bootstrap:\",\n                  choices = c(\"Mean\", \"Median\", \"SD\",\n                              \"Correlation\", \"Regression slope\")),\n\n      selectInput(\"pop\", \"Population shape:\",\n                  choices = c(\"Normal\", \"Skewed (exponential)\",\n                              \"Heavy-tailed\", \"Bimodal\")),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 10, max = 200, value = 30, step = 10),\n\n      sliderInput(\"B\", \"Bootstrap resamples (B):\",\n                  min = 100, max = 5000, value = 1000, step = 100),\n\n      actionButton(\"go\", \"New sample + bootstrap\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"sample_plot\", height = \"350px\")),\n        column(6, plotOutput(\"boot_plot\", height = \"350px\"))\n      ),\n      fluidRow(\n        column(12, plotOutput(\"compare_plot\", height = \"350px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  draw_pop &lt;- function(n, pop) {\n    switch(pop,\n      \"Normal\" = rnorm(n, mean = 5, sd = 2),\n      \"Skewed (exponential)\" = rexp(n, rate = 0.5),\n      \"Heavy-tailed\" = rt(n, df = 3) * 2 + 5,\n      \"Bimodal\" = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, 2, 0.8) + (1 - k) * rnorm(n, 7, 0.8)\n      }\n    )\n  }\n\n  compute_stat &lt;- function(x, y = NULL, stat) {\n    switch(stat,\n      \"Mean\" = mean(x),\n      \"Median\" = median(x),\n      \"SD\" = sd(x),\n      \"Correlation\" = cor(x, y),\n      \"Regression slope\" = coef(lm(y ~ x))[2]\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    B    &lt;- input$B\n    stat &lt;- input$stat\n    pop  &lt;- input$pop\n\n    x &lt;- draw_pop(n, pop)\n    y &lt;- NULL\n    if (stat %in% c(\"Correlation\", \"Regression slope\")) {\n      y &lt;- 1 + 0.8 * x + rnorm(n, sd = 2)\n    }\n\n    obs_stat &lt;- compute_stat(x, y, stat)\n\n    # Bootstrap\n    boot_stats &lt;- replicate(B, {\n      idx &lt;- sample(n, n, replace = TRUE)\n      x_b &lt;- x[idx]\n      y_b &lt;- if (!is.null(y)) y[idx] else NULL\n      compute_stat(x_b, y_b, stat)\n    })\n\n    boot_se &lt;- sd(boot_stats)\n    boot_ci &lt;- quantile(boot_stats, c(0.025, 0.975))\n\n    list(x = x, y = y, obs_stat = obs_stat,\n         boot_stats = boot_stats, boot_se = boot_se,\n         boot_ci = boot_ci, stat = stat, n = n, B = B)\n  })\n\n  output$sample_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    if (d$stat %in% c(\"Correlation\", \"Regression slope\")) {\n      plot(d$x, d$y, pch = 16, col = \"#3498db80\", cex = 0.8,\n           xlab = \"X\", ylab = \"Y\", main = \"Your sample\")\n      if (d$stat == \"Regression slope\") {\n        abline(lm(d$y ~ d$x), col = \"#e74c3c\", lwd = 2)\n      }\n    } else {\n      hist(d$x, breaks = 20, col = \"#d5e8d4\", border = \"#82b366\",\n           main = \"Your sample\", xlab = \"X\", ylab = \"Frequency\")\n      abline(v = d$obs_stat, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n      legend(\"topright\", bty = \"n\", cex = 0.85,\n             legend = paste0(d$stat, \" = \", round(d$obs_stat, 3)),\n             col = \"#e74c3c\", lty = 2, lwd = 2)\n    }\n  })\n\n  output$boot_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$boot_stats, breaks = 40, probability = TRUE,\n         col = \"#dae8fc\", border = \"#6c8ebf\",\n         main = paste0(\"Bootstrap distribution (B = \", d$B, \")\"),\n         xlab = paste0(\"Bootstrap \", tolower(d$stat)),\n         ylab = \"Density\")\n\n    abline(v = d$obs_stat, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n    abline(v = d$boot_ci, col = \"#27ae60\", lwd = 2, lty = 3)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(paste0(\"Observed: \", round(d$obs_stat, 3)),\n                      paste0(\"95% CI: [\", round(d$boot_ci[1], 3),\n                             \", \", round(d$boot_ci[2], 3), \"]\")),\n           col = c(\"#e74c3c\", \"#27ae60\"),\n           lty = c(2, 3), lwd = 2)\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    if (d$stat %in% c(\"Correlation\", \"Regression slope\")) {\n      # For bivariate stats, just show bootstrap dist with CI\n      hist(d$boot_stats, breaks = 40, probability = TRUE,\n           col = \"#dae8fc\", border = \"#6c8ebf\",\n           main = \"Bootstrap sampling dist.\",\n           xlab = paste0(\"Bootstrap \", tolower(d$stat)),\n           ylab = \"Density\")\n      abline(v = d$boot_ci, col = \"#27ae60\", lwd = 2, lty = 3)\n      abline(v = d$obs_stat, col = \"#e74c3c\", lwd = 2, lty = 2)\n    } else {\n      # Overlay: data distribution (wide) vs bootstrap distribution (narrow)\n      # Rescale data density to fit on same axes\n      dens_data &lt;- density(d$x)\n      dens_boot &lt;- density(d$boot_stats)\n\n      # Normalize both to peak at 1 for visual comparison\n      dens_data$y &lt;- dens_data$y / max(dens_data$y)\n      dens_boot$y &lt;- dens_boot$y / max(dens_boot$y)\n\n      xlim &lt;- range(c(dens_data$x, dens_boot$x))\n\n      plot(dens_data, col = \"#e74c3c\", lwd = 2.5, xlim = xlim,\n           ylim = c(0, 1.25), main = \"Data vs Sampling Distribution\",\n           xlab = \"Value\", ylab = \"Scaled density\")\n      polygon(dens_data$x, dens_data$y,\n              col = adjustcolor(\"#e74c3c\", 0.15), border = NA)\n\n      lines(dens_boot, col = \"#3498db\", lwd = 2.5)\n      polygon(dens_boot$x, dens_boot$y,\n              col = adjustcolor(\"#3498db\", 0.15), border = NA)\n\n      legend(\"topright\", bty = \"n\", cex = 0.8,\n             legend = c(\"Your data (wide, raw obs.)\",\n                        paste0(\"Sampling dist. of \", tolower(d$stat),\n                               \" (narrow)\")),\n             col = c(\"#e74c3c\", \"#3498db\"), lwd = 2.5)\n    }\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Statistic:&lt;/b&gt; \", d$stat, \"&lt;br&gt;\",\n        \"&lt;b&gt;Observed:&lt;/b&gt; \", round(d$obs_stat, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bootstrap SE:&lt;/b&gt; \", round(d$boot_se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;95% CI:&lt;/b&gt; [\", round(d$boot_ci[1], 4),\n        \", \", round(d$boot_ci[2], 4), \"]&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;CI uses percentile method:&lt;br&gt;\",\n        \"2.5th and 97.5th percentiles of&lt;br&gt;\",\n        \"bootstrap distribution.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nMean with Normal population: the bootstrap distribution looks normal, similar to what CLT gives you analytically.\nMedian with skewed population: no easy formula for the SE of a median. But the bootstrap handles it effortlessly.\nSD with heavy-tailed data: the bootstrap distribution is skewed — the percentile CI is asymmetric, which is exactly right.\nRegression slope: bootstrap the slope to get a CI without assuming homoskedasticity.\nSmall n (10) vs large n (200): with small samples the bootstrap distribution is choppy (limited unique resamples). With more data it smooths out.\n\n\n\n\n\n\n\n\n\n\n\nUse bootstrap when…\nDon’t bother when…\n\n\n\n\nNo formula for the SE exists\nStandard formulas are available and valid\n\n\nThe statistic is complicated (ratios, quantiles)\nYou’re computing a simple mean\n\n\nYou suspect non-normality\nn is large and CLT applies\n\n\nYou want a quick, assumption-free CI\nYou need exact small-sample inference\n\n\n\nThe bootstrap is not magic — it can fail with very small samples or non-smooth statistics. But for most practical situations, it’s the easiest path to a valid confidence interval.\n\n\n\n\n\nBradley Efron invented the bootstrap in 1979 at Stanford. The name comes from the expression “pulling yourself up by your bootstraps” — attributed to the tall tales of Baron Munchausen, who claimed to have pulled himself out of a swamp by his own hair (later versions say bootstraps). The idea that a sample can estimate its own sampling distribution seemed equally absurd at first.\nWhen Efron first presented the bootstrap, many statisticians were skeptical. One famously asked: “You’re just sampling from your sample — how can that tell you anything new?” The answer: it tells you about the variability of your estimate, not about new data points.\nThe bootstrap is now one of the most cited statistical methods ever. Efron’s 1979 paper has over 20,000 citations. He received the International Prize in Statistics (the statistics equivalent of the Nobel) in 2019 for this work.\nBefore the bootstrap, getting standard errors for complex statistics (ratios, quantiles, eigenvalues) required either painful analytical derivations or the delta method — a Taylor expansion trick that often required heroic assumptions. The bootstrap made all of that unnecessary.",
    "crumbs": [
      "The Bootstrap"
    ]
  },
  {
    "objectID": "bootstrap.html#the-idea",
    "href": "bootstrap.html#the-idea",
    "title": "The Bootstrap",
    "section": "",
    "text": "You want a confidence interval for some statistic (mean, median, regression coefficient), but you don’t know the sampling distribution. Maybe the formula is complicated. Maybe there is no formula.\nThe bootstrap says: pretend your sample is the population. Then simulate the sampling process by resampling with replacement from your data, over and over. Each resample gives you a new estimate. The distribution of those estimates approximates the true sampling distribution.\n\n\n\nYou have a sample of \\(n\\) observations.\nDraw \\(n\\) observations with replacement from your sample (some points get picked twice, some not at all).\nCompute your statistic on this resample.\nRepeat B times (typically 1,000–10,000).\nThe spread of those B estimates gives you a standard error and a confidence interval.\n\nThat’s it. No formulas, no distributional assumptions. Just resampling.\n\n\n\nNo — and this is a crucial distinction. There are two different distributions at play:\n\nThe distribution of the data = what individual observations look like (their histogram). This tells you about spread, skew, outliers in your raw data.\nThe sampling distribution = what your statistic (mean, median, slope) would look like if you could repeat the entire experiment many times. This is what you need for standard errors and confidence intervals.\n\nYou can always plot your data. But you cannot see the sampling distribution — you only ran the experiment once. You have one mean, not a distribution of means.\nThe bootstrap bridges that gap. It simulates the “what if I repeated this experiment?” question using only the data you have. The left panel below shows your data distribution. The right panel shows the bootstrap sampling distribution. Notice: they look completely different. Your data might be skewed and spread out, but the sampling distribution of the mean is narrow and roughly normal (thanks to the CLT). The bootstrap discovers this for you without any formulas.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 900\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"stat\", \"Statistic to bootstrap:\",\n                  choices = c(\"Mean\", \"Median\", \"SD\",\n                              \"Correlation\", \"Regression slope\")),\n\n      selectInput(\"pop\", \"Population shape:\",\n                  choices = c(\"Normal\", \"Skewed (exponential)\",\n                              \"Heavy-tailed\", \"Bimodal\")),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 10, max = 200, value = 30, step = 10),\n\n      sliderInput(\"B\", \"Bootstrap resamples (B):\",\n                  min = 100, max = 5000, value = 1000, step = 100),\n\n      actionButton(\"go\", \"New sample + bootstrap\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"sample_plot\", height = \"350px\")),\n        column(6, plotOutput(\"boot_plot\", height = \"350px\"))\n      ),\n      fluidRow(\n        column(12, plotOutput(\"compare_plot\", height = \"350px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  draw_pop &lt;- function(n, pop) {\n    switch(pop,\n      \"Normal\" = rnorm(n, mean = 5, sd = 2),\n      \"Skewed (exponential)\" = rexp(n, rate = 0.5),\n      \"Heavy-tailed\" = rt(n, df = 3) * 2 + 5,\n      \"Bimodal\" = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, 2, 0.8) + (1 - k) * rnorm(n, 7, 0.8)\n      }\n    )\n  }\n\n  compute_stat &lt;- function(x, y = NULL, stat) {\n    switch(stat,\n      \"Mean\" = mean(x),\n      \"Median\" = median(x),\n      \"SD\" = sd(x),\n      \"Correlation\" = cor(x, y),\n      \"Regression slope\" = coef(lm(y ~ x))[2]\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    B    &lt;- input$B\n    stat &lt;- input$stat\n    pop  &lt;- input$pop\n\n    x &lt;- draw_pop(n, pop)\n    y &lt;- NULL\n    if (stat %in% c(\"Correlation\", \"Regression slope\")) {\n      y &lt;- 1 + 0.8 * x + rnorm(n, sd = 2)\n    }\n\n    obs_stat &lt;- compute_stat(x, y, stat)\n\n    # Bootstrap\n    boot_stats &lt;- replicate(B, {\n      idx &lt;- sample(n, n, replace = TRUE)\n      x_b &lt;- x[idx]\n      y_b &lt;- if (!is.null(y)) y[idx] else NULL\n      compute_stat(x_b, y_b, stat)\n    })\n\n    boot_se &lt;- sd(boot_stats)\n    boot_ci &lt;- quantile(boot_stats, c(0.025, 0.975))\n\n    list(x = x, y = y, obs_stat = obs_stat,\n         boot_stats = boot_stats, boot_se = boot_se,\n         boot_ci = boot_ci, stat = stat, n = n, B = B)\n  })\n\n  output$sample_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    if (d$stat %in% c(\"Correlation\", \"Regression slope\")) {\n      plot(d$x, d$y, pch = 16, col = \"#3498db80\", cex = 0.8,\n           xlab = \"X\", ylab = \"Y\", main = \"Your sample\")\n      if (d$stat == \"Regression slope\") {\n        abline(lm(d$y ~ d$x), col = \"#e74c3c\", lwd = 2)\n      }\n    } else {\n      hist(d$x, breaks = 20, col = \"#d5e8d4\", border = \"#82b366\",\n           main = \"Your sample\", xlab = \"X\", ylab = \"Frequency\")\n      abline(v = d$obs_stat, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n      legend(\"topright\", bty = \"n\", cex = 0.85,\n             legend = paste0(d$stat, \" = \", round(d$obs_stat, 3)),\n             col = \"#e74c3c\", lty = 2, lwd = 2)\n    }\n  })\n\n  output$boot_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$boot_stats, breaks = 40, probability = TRUE,\n         col = \"#dae8fc\", border = \"#6c8ebf\",\n         main = paste0(\"Bootstrap distribution (B = \", d$B, \")\"),\n         xlab = paste0(\"Bootstrap \", tolower(d$stat)),\n         ylab = \"Density\")\n\n    abline(v = d$obs_stat, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n    abline(v = d$boot_ci, col = \"#27ae60\", lwd = 2, lty = 3)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(paste0(\"Observed: \", round(d$obs_stat, 3)),\n                      paste0(\"95% CI: [\", round(d$boot_ci[1], 3),\n                             \", \", round(d$boot_ci[2], 3), \"]\")),\n           col = c(\"#e74c3c\", \"#27ae60\"),\n           lty = c(2, 3), lwd = 2)\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    if (d$stat %in% c(\"Correlation\", \"Regression slope\")) {\n      # For bivariate stats, just show bootstrap dist with CI\n      hist(d$boot_stats, breaks = 40, probability = TRUE,\n           col = \"#dae8fc\", border = \"#6c8ebf\",\n           main = \"Bootstrap sampling dist.\",\n           xlab = paste0(\"Bootstrap \", tolower(d$stat)),\n           ylab = \"Density\")\n      abline(v = d$boot_ci, col = \"#27ae60\", lwd = 2, lty = 3)\n      abline(v = d$obs_stat, col = \"#e74c3c\", lwd = 2, lty = 2)\n    } else {\n      # Overlay: data distribution (wide) vs bootstrap distribution (narrow)\n      # Rescale data density to fit on same axes\n      dens_data &lt;- density(d$x)\n      dens_boot &lt;- density(d$boot_stats)\n\n      # Normalize both to peak at 1 for visual comparison\n      dens_data$y &lt;- dens_data$y / max(dens_data$y)\n      dens_boot$y &lt;- dens_boot$y / max(dens_boot$y)\n\n      xlim &lt;- range(c(dens_data$x, dens_boot$x))\n\n      plot(dens_data, col = \"#e74c3c\", lwd = 2.5, xlim = xlim,\n           ylim = c(0, 1.25), main = \"Data vs Sampling Distribution\",\n           xlab = \"Value\", ylab = \"Scaled density\")\n      polygon(dens_data$x, dens_data$y,\n              col = adjustcolor(\"#e74c3c\", 0.15), border = NA)\n\n      lines(dens_boot, col = \"#3498db\", lwd = 2.5)\n      polygon(dens_boot$x, dens_boot$y,\n              col = adjustcolor(\"#3498db\", 0.15), border = NA)\n\n      legend(\"topright\", bty = \"n\", cex = 0.8,\n             legend = c(\"Your data (wide, raw obs.)\",\n                        paste0(\"Sampling dist. of \", tolower(d$stat),\n                               \" (narrow)\")),\n             col = c(\"#e74c3c\", \"#3498db\"), lwd = 2.5)\n    }\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Statistic:&lt;/b&gt; \", d$stat, \"&lt;br&gt;\",\n        \"&lt;b&gt;Observed:&lt;/b&gt; \", round(d$obs_stat, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bootstrap SE:&lt;/b&gt; \", round(d$boot_se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;95% CI:&lt;/b&gt; [\", round(d$boot_ci[1], 4),\n        \", \", round(d$boot_ci[2], 4), \"]&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;CI uses percentile method:&lt;br&gt;\",\n        \"2.5th and 97.5th percentiles of&lt;br&gt;\",\n        \"bootstrap distribution.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nMean with Normal population: the bootstrap distribution looks normal, similar to what CLT gives you analytically.\nMedian with skewed population: no easy formula for the SE of a median. But the bootstrap handles it effortlessly.\nSD with heavy-tailed data: the bootstrap distribution is skewed — the percentile CI is asymmetric, which is exactly right.\nRegression slope: bootstrap the slope to get a CI without assuming homoskedasticity.\nSmall n (10) vs large n (200): with small samples the bootstrap distribution is choppy (limited unique resamples). With more data it smooths out.\n\n\n\n\n\n\n\n\n\n\n\nUse bootstrap when…\nDon’t bother when…\n\n\n\n\nNo formula for the SE exists\nStandard formulas are available and valid\n\n\nThe statistic is complicated (ratios, quantiles)\nYou’re computing a simple mean\n\n\nYou suspect non-normality\nn is large and CLT applies\n\n\nYou want a quick, assumption-free CI\nYou need exact small-sample inference\n\n\n\nThe bootstrap is not magic — it can fail with very small samples or non-smooth statistics. But for most practical situations, it’s the easiest path to a valid confidence interval.\n\n\n\n\n\nBradley Efron invented the bootstrap in 1979 at Stanford. The name comes from the expression “pulling yourself up by your bootstraps” — attributed to the tall tales of Baron Munchausen, who claimed to have pulled himself out of a swamp by his own hair (later versions say bootstraps). The idea that a sample can estimate its own sampling distribution seemed equally absurd at first.\nWhen Efron first presented the bootstrap, many statisticians were skeptical. One famously asked: “You’re just sampling from your sample — how can that tell you anything new?” The answer: it tells you about the variability of your estimate, not about new data points.\nThe bootstrap is now one of the most cited statistical methods ever. Efron’s 1979 paper has over 20,000 citations. He received the International Prize in Statistics (the statistics equivalent of the Nobel) in 2019 for this work.\nBefore the bootstrap, getting standard errors for complex statistics (ratios, quantiles, eigenvalues) required either painful analytical derivations or the delta method — a Taylor expansion trick that often required heroic assumptions. The bootstrap made all of that unnecessary.",
    "crumbs": [
      "The Bootstrap"
    ]
  },
  {
    "objectID": "power.html",
    "href": "power.html",
    "title": "Power, Alpha, Beta & MDE",
    "section": "",
    "text": "You run an experiment to test whether some treatment works. There are only four things that can happen:\n\n\n\n\n\n\n\n\n\nTreatment does nothing (H₀ true)\nTreatment works (H₁ true)\n\n\n\n\nYou say “no effect”\nCorrect\nType II error (miss it) — probability = \\(\\beta\\)\n\n\nYou say “it works!”\nType I error (false alarm) — probability = \\(\\alpha\\)\nCorrect — probability = Power = \\(1 - \\beta\\)\n\n\n\nThat’s it. Everything on this page is about these four cells.",
    "crumbs": [
      "Power, Alpha, Beta & MDE"
    ]
  },
  {
    "objectID": "power.html#the-big-picture",
    "href": "power.html#the-big-picture",
    "title": "Power, Alpha, Beta & MDE",
    "section": "",
    "text": "You run an experiment to test whether some treatment works. There are only four things that can happen:\n\n\n\n\n\n\n\n\n\nTreatment does nothing (H₀ true)\nTreatment works (H₁ true)\n\n\n\n\nYou say “no effect”\nCorrect\nType II error (miss it) — probability = \\(\\beta\\)\n\n\nYou say “it works!”\nType I error (false alarm) — probability = \\(\\alpha\\)\nCorrect — probability = Power = \\(1 - \\beta\\)\n\n\n\nThat’s it. Everything on this page is about these four cells.",
    "crumbs": [
      "Power, Alpha, Beta & MDE"
    ]
  },
  {
    "objectID": "power.html#what-are-alpha-and-beta",
    "href": "power.html#what-are-alpha-and-beta",
    "title": "Power, Alpha, Beta & MDE",
    "section": "What are \\(\\alpha\\) and \\(\\beta\\)?",
    "text": "What are \\(\\alpha\\) and \\(\\beta\\)?\n\\(\\alpha\\) (alpha) is how often you cry wolf. You set this before the experiment — typically 0.05. It’s the false positive rate: the chance you declare “it works!” when the treatment actually does nothing.\n\\(\\beta\\) (beta) is how often you miss a real effect. If the treatment genuinely works, \\(\\beta\\) is the probability you shrug and say “no effect.” You want this to be small.\nPower = \\(1 - \\beta\\) is the flip side: the probability you correctly detect a real effect. Convention is to aim for 0.80 (80%).\n\nThe two-distribution picture\nThe key insight is that there are two worlds — one where the treatment does nothing (null), and one where it has an effect (alternative). Each world gives you a different sampling distribution for your test statistic:\n\nUnder the null, the distribution is centered at 0 (no effect).\nUnder the alternative, the distribution is shifted by the true effect size.\n\nYou pick a critical value (the cutoff). If your test statistic lands past it, you reject H₀. The simulation below shows both distributions. Drag the sliders and watch how \\(\\alpha\\), \\(\\beta\\), and power change.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 580\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"effect\", \"True effect size (d):\",\n                  min = 0, max = 2, value = 0.5, step = 0.05),\n\n      sliderInput(\"n\", \"Sample size per group (n):\",\n                  min = 10, max = 500, value = 50, step = 10),\n\n      sliderInput(\"alpha\", HTML(\"&alpha; (significance level):\"),\n                  min = 0.01, max = 0.10, value = 0.05, step = 0.01),\n\n      uiOutput(\"results_box\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"dist_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  vals &lt;- reactive({\n    d     &lt;- input$effect\n    n     &lt;- input$n\n    alpha &lt;- input$alpha\n\n    se    &lt;- sqrt(2 / n)          # SE of difference in means (sigma=1 each group)\n    shift &lt;- d / se               # noncentrality (in SE units)\n    crit  &lt;- qnorm(1 - alpha)     # one-sided critical value\n\n    power &lt;- 1 - pnorm(crit - shift)\n    beta  &lt;- 1 - power\n\n    list(se = se, shift = shift, crit = crit,\n         power = power, beta = beta, alpha = alpha, d = d, n = n)\n  })\n\n  output$dist_plot &lt;- renderPlot({\n    v &lt;- vals()\n\n    xmin &lt;- min(-4, v$shift - 4)\n    xmax &lt;- max(4, v$shift + 4)\n    x &lt;- seq(xmin, xmax, length.out = 500)\n\n    y_null &lt;- dnorm(x)\n    y_alt  &lt;- dnorm(x, mean = v$shift)\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(x, y_null, type = \"l\", lwd = 2.5, col = \"#2c3e50\",\n         xlab = \"Test statistic (z)\", ylab = \"Density\",\n         main = \"Null vs Alternative Distribution\",\n         ylim = c(0, max(y_null, y_alt) * 1.15),\n         xlim = c(xmin, xmax))\n    lines(x, y_alt, lwd = 2.5, col = \"#3498db\")\n\n    # Critical value line\n    abline(v = v$crit, lty = 2, lwd = 2, col = \"#7f8c8d\")\n\n    # Shade alpha region (right tail of null beyond crit)\n    x_alpha &lt;- seq(v$crit, xmax, length.out = 200)\n    polygon(c(v$crit, x_alpha, xmax),\n            c(0, dnorm(x_alpha), 0),\n            col = adjustcolor(\"#e74c3c\", 0.35), border = NA)\n\n    # Shade beta region (left part of alternative, below crit)\n    x_beta &lt;- seq(xmin, v$crit, length.out = 200)\n    polygon(c(xmin, x_beta, v$crit),\n            c(0, dnorm(x_beta, mean = v$shift), 0),\n            col = adjustcolor(\"#f39c12\", 0.35), border = NA)\n\n    # Shade power region (right part of alternative, beyond crit)\n    x_pow &lt;- seq(v$crit, xmax, length.out = 200)\n    polygon(c(v$crit, x_pow, xmax),\n            c(0, dnorm(x_pow, mean = v$shift), 0),\n            col = adjustcolor(\"#2ecc71\", 0.35), border = NA)\n\n    # Labels\n    legend(\"topleft\", bty = \"n\", cex = 0.9,\n           legend = c(\n             expression(\"Null distribution (H\"[0]*\": no effect)\"),\n             expression(\"Alternative distribution (H\"[1]*\": effect exists)\"),\n             \"Critical value\",\n             expression(alpha * \" (false positive)\"),\n             expression(beta * \" (miss / Type II)\"),\n             \"Power (correct detection)\"\n           ),\n           col = c(\"#2c3e50\", \"#3498db\", \"#7f8c8d\",\n                   adjustcolor(\"#e74c3c\", 0.6),\n                   adjustcolor(\"#f39c12\", 0.6),\n                   adjustcolor(\"#2ecc71\", 0.6)),\n           lwd = c(2.5, 2.5, 2, 8, 8, 8),\n           lty = c(1, 1, 2, 1, 1, 1))\n  })\n\n  output$results_box &lt;- renderUI({\n    v &lt;- vals()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;&alpha;:&lt;/b&gt; \", v$alpha, \"&lt;br&gt;\",\n        \"&lt;b&gt;&beta;:&lt;/b&gt; \", round(v$beta, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Power:&lt;/b&gt; \", round(v$power, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Effect (d):&lt;/b&gt; \", v$d, \"&lt;br&gt;\",\n        \"&lt;b&gt;n per group:&lt;/b&gt; \", v$n, \"&lt;br&gt;\",\n        \"&lt;b&gt;Critical z:&lt;/b&gt; \", round(v$crit, 2)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nSet effect = 0 and watch: there is no alternative distribution to detect. Any rejection is a false positive.\nSet effect = 0.5 with n = 20: power is low. Now slide n up — power climbs. This is why sample size matters.\nSet n = 200 and shrink the effect toward 0: even large samples struggle to detect tiny effects.\nLower \\(\\alpha\\) from 0.05 to 0.01: the critical value moves right, \\(\\alpha\\) shrinks, but \\(\\beta\\) grows. There’s always a tradeoff between false positives and false negatives.",
    "crumbs": [
      "Power, Alpha, Beta & MDE"
    ]
  },
  {
    "objectID": "power.html#minimum-detectable-effect-mde",
    "href": "power.html#minimum-detectable-effect-mde",
    "title": "Power, Alpha, Beta & MDE",
    "section": "Minimum Detectable Effect (MDE)",
    "text": "Minimum Detectable Effect (MDE)\nWhen planning an experiment, you often ask: “Given my sample size, what’s the smallest effect I can reliably detect?” That’s the MDE.\nIt depends on three things: sample size (\\(n\\)), significance level (\\(\\alpha\\)), and desired power (\\(1 - \\beta\\)). The formula for a two-sample test with equal groups is:\n\\[\n\\text{MDE} = (z_{1-\\alpha} + z_{1-\\beta}) \\times \\sqrt{\\frac{2}{n}}\n\\]\nNotice: that \\(\\sqrt{2/n}\\) is just the standard error of the difference in means. So MDE is really just a scaled-up SE:\n\\[MDE = (z_{1-\\alpha} + z_{1-\\beta}) \\times SE\\]\nThe critical values (~2.8 for 5% significance and 80% power) are fixed multipliers. The only thing you control is the SE — by increasing \\(n\\) or reducing \\(\\sigma\\) (through better measurement, stratification, or controls). Power analysis is really just an SE calculation in disguise. See Variance, SD & Standard Error for more on this connection.\nLarger \\(n\\) shrinks the SE, which shrinks the MDE. Higher power demands a larger MDE (or more \\(n\\)).\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 480\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .mde-box {\n      background: #eaf2f8; border-radius: 6px; padding: 16px;\n      margin-top: 14px; font-size: 15px; line-height: 2;\n      text-align: center;\n    }\n    .mde-box .big { font-size: 28px; color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n2\", \"Sample size per group (n):\",\n                  min = 10, max = 1000, value = 100, step = 10),\n\n      sliderInput(\"alpha2\", HTML(\"&alpha;:\"),\n                  min = 0.01, max = 0.10, value = 0.05, step = 0.01),\n\n      sliderInput(\"power2\", \"Desired power:\",\n                  min = 0.50, max = 0.95, value = 0.80, step = 0.05),\n\n      uiOutput(\"mde_box\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"mde_curve\", height = \"400px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  output$mde_curve &lt;- renderPlot({\n    alpha &lt;- input$alpha2\n    power &lt;- input$power2\n    n_now &lt;- input$n2\n\n    ns &lt;- seq(10, 1000, by = 5)\n    mdes &lt;- (qnorm(1 - alpha) + qnorm(power)) * sqrt(2 / ns)\n\n    mde_now &lt;- (qnorm(1 - alpha) + qnorm(power)) * sqrt(2 / n_now)\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(ns, mdes, type = \"l\", lwd = 2.5, col = \"#3498db\",\n         xlab = \"Sample size per group (n)\",\n         ylab = \"MDE (standardized effect size)\",\n         main = paste0(\"MDE curve (\\u03b1 = \", alpha, \", power = \", power, \")\"),\n         ylim = c(0, max(mdes)))\n\n    # Highlight current n\n    points(n_now, mde_now, pch = 19, cex = 2, col = \"#e74c3c\")\n    segments(n_now, 0, n_now, mde_now, lty = 2, col = \"#e74c3c\")\n    segments(0, mde_now, n_now, mde_now, lty = 2, col = \"#e74c3c\")\n\n    text(n_now + 30, mde_now + 0.02,\n         paste0(\"MDE = \", round(mde_now, 3)),\n         col = \"#e74c3c\", cex = 0.95, adj = 0)\n  })\n\n  output$mde_box &lt;- renderUI({\n    alpha &lt;- input$alpha2\n    power &lt;- input$power2\n    n_now &lt;- input$n2\n    mde &lt;- (qnorm(1 - alpha) + qnorm(power)) * sqrt(2 / n_now)\n\n    tags$div(class = \"mde-box\",\n      HTML(paste0(\n        \"With &lt;b&gt;n = \", n_now, \"&lt;/b&gt; per group,&lt;br&gt;\",\n        \"you can detect effects as small as:&lt;br&gt;\",\n        \"&lt;span class='big'&gt;d = \", round(mde, 3), \"&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThe intuition\n\nMDE is your experiment’s resolution. A microscope can’t see atoms; your experiment can’t see effects smaller than the MDE.\nMore data (larger \\(n\\)) = sharper microscope = smaller MDE.\nIf you need to detect a 1% lift in click-through rate but your MDE is 3%, your experiment is pointless — you’ll almost certainly miss it even if the effect is real.\nIn practice: figure out the smallest effect that would matter for your decision, then compute the \\(n\\) needed to detect it.\n\n\n\n\nDid you know?\n\nJacob Cohen, the psychologist who popularized power analysis, found in 1962 that the median power of studies in behavioral science journals was only 0.48 — meaning most studies had less than a coin-flip chance of detecting the effects they were looking for. He spent the rest of his career trying to fix this. His book Statistical Power Analysis (1969) remains a classic.\nCohen’s famous effect size conventions (small = 0.2, medium = 0.5, large = 0.8) were meant as rough guides, not rigid rules. He later regretted that people treated them as gospel: “My intent was that d = 0.5 represents a medium effect… it does not mean that 0.5 is a medium effect in your field.”\nThe replication crisis in psychology and medicine is largely a power problem. Underpowered studies that happen to find significant results are published; the many more that find nothing are filed away. This is publication bias, and it’s a direct consequence of running experiments without power calculations.",
    "crumbs": [
      "Power, Alpha, Beta & MDE"
    ]
  },
  {
    "objectID": "residuals.html",
    "href": "residuals.html",
    "title": "Residuals & Controls",
    "section": "",
    "text": "A residual is what’s left over after your model has done its best:\n\\[e_i = Y_i - \\hat{Y}_i\\]\nIt’s the vertical distance between the actual data point and the regression line. If your model is good, residuals should look like random noise — no patterns, no structure.\nIf the residuals do have structure, your model is missing something. This is the single most important diagnostic in regression.",
    "crumbs": [
      "Residuals & Controls"
    ]
  },
  {
    "objectID": "residuals.html#what-is-a-residual",
    "href": "residuals.html#what-is-a-residual",
    "title": "Residuals & Controls",
    "section": "",
    "text": "A residual is what’s left over after your model has done its best:\n\\[e_i = Y_i - \\hat{Y}_i\\]\nIt’s the vertical distance between the actual data point and the regression line. If your model is good, residuals should look like random noise — no patterns, no structure.\nIf the residuals do have structure, your model is missing something. This is the single most important diagnostic in regression.",
    "crumbs": [
      "Residuals & Controls"
    ]
  },
  {
    "objectID": "residuals.html#residuals-and-the-cef",
    "href": "residuals.html#residuals-and-the-cef",
    "title": "Residuals & Controls",
    "section": "Residuals and the CEF",
    "text": "Residuals and the CEF\nRecall from the CEF page: OLS is the best linear approximation to \\(E[Y \\mid X]\\). When the CEF is nonlinear, the residuals absorb the nonlinearity. That’s why a curved pattern in the residual plot signals model misspecification — the residuals are doing the work the model should be doing.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .info-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.8;\n    }\n    .info-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"dgp\", \"True relationship:\",\n                  choices = c(\"Linear (correct spec)\",\n                              \"Quadratic (misspecified)\",\n                              \"Heteroskedastic\",\n                              \"Outliers\")),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 50, max = 500, value = 200, step = 50),\n\n      sliderInput(\"sigma\", \"Noise (SD):\",\n                  min = 0.5, max = 4, value = 1.5, step = 0.5),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"info\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(4, plotOutput(\"scatter\", height = \"380px\")),\n        column(4, plotOutput(\"resid_fitted\", height = \"380px\")),\n        column(4, plotOutput(\"resid_hist\", height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n\n    sigma &lt;- input$sigma\n    dgp   &lt;- input$dgp\n\n    x &lt;- runif(n, -3, 5)\n\n    if (dgp == \"Linear (correct spec)\") {\n      y &lt;- 2 + 1.5 * x + rnorm(n, sd = sigma)\n    } else if (dgp == \"Quadratic (misspecified)\") {\n      y &lt;- 1 + 0.5 * x - 0.3 * x^2 + rnorm(n, sd = sigma)\n    } else if (dgp == \"Heteroskedastic\") {\n      y &lt;- 2 + 1.5 * x + rnorm(n, sd = sigma * (0.3 + 0.4 * abs(x)))\n    } else {\n      y &lt;- 2 + 1.5 * x + rnorm(n, sd = sigma)\n      # Add outliers\n      outlier_idx &lt;- sample(n, 5)\n      y[outlier_idx] &lt;- y[outlier_idx] + sample(c(-1, 1), 5, replace = TRUE) * 8\n    }\n\n    fit &lt;- lm(y ~ x)\n    list(x = x, y = y, fit = fit, dgp = dgp)\n  })\n\n  output$scatter &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$x, d$y, pch = 16, col = \"#3498db80\", cex = 0.7,\n         xlab = \"X\", ylab = \"Y\", main = \"Data + OLS fit\")\n    abline(d$fit, col = \"#e74c3c\", lwd = 2.5)\n  })\n\n  output$resid_fitted &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    r  &lt;- resid(d$fit)\n    fv &lt;- fitted(d$fit)\n\n    plot(fv, r, pch = 16, col = \"#9b59b680\", cex = 0.7,\n         xlab = \"Fitted values\", ylab = \"Residuals\",\n         main = \"Residuals vs Fitted\")\n    abline(h = 0, lty = 2, col = \"gray40\", lwd = 1.5)\n\n    lo &lt;- loess(r ~ fv)\n    ox &lt;- order(fv)\n    lines(fv[ox], predict(lo)[ox], col = \"#e74c3c\", lwd = 2)\n  })\n\n  output$resid_hist &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    r &lt;- resid(d$fit)\n    hist(r, breaks = 30, probability = TRUE,\n         col = \"#dae8fc\", border = \"#6c8ebf\",\n         main = \"Residual Distribution\",\n         xlab = \"Residuals\", ylab = \"Density\")\n    x_seq &lt;- seq(min(r), max(r), length.out = 200)\n    lines(x_seq, dnorm(x_seq, mean = 0, sd = sd(r)),\n          col = \"#e74c3c\", lwd = 2)\n  })\n\n  output$info &lt;- renderUI({\n    d &lt;- dat()\n    r &lt;- resid(d$fit)\n\n    msg &lt;- switch(d$dgp,\n      \"Linear (correct spec)\" =\n        \"Residuals look like random noise. No patterns in the residual plot. The model is correctly specified.\",\n      \"Quadratic (misspecified)\" =\n        \"U-shaped pattern in residuals! The LOESS curve bends, revealing the quadratic structure OLS is missing.\",\n      \"Heteroskedastic\" =\n        \"Fan shape: residuals spread out as fitted values increase. The variance isn't constant (heteroskedasticity).\",\n      \"Outliers\" =\n        \"A few points have huge residuals. Check the histogram for heavy tails. These points have outsized influence on the fit.\"\n    )\n\n    tags$div(class = \"info-box\", HTML(paste0(\"&lt;b&gt;Diagnosis:&lt;/b&gt;&lt;br&gt;\", msg)))\n  })\n}\n\nshinyApp(ui, server)\n\nReading the three panels\n\nLeft — Scatter + fit: does the line follow the data?\nMiddle — Residuals vs fitted: the key diagnostic. If you see a pattern (curve, fan, clusters), your model is missing something.\nRight — Residual histogram: should look roughly normal and centered at 0. Heavy tails or skew signal problems.\n\nSwitch between the four DGPs above and learn to recognize each pattern — you’ll see these in every applied paper you read.",
    "crumbs": [
      "Residuals & Controls"
    ]
  },
  {
    "objectID": "residuals.html#controls-and-residuals",
    "href": "residuals.html#controls-and-residuals",
    "title": "Residuals & Controls",
    "section": "Controls and residuals",
    "text": "Controls and residuals\nWhen you “control for” a variable in a regression, what you’re really doing is removing its influence via residuals. This is the Frisch-Waugh-Lovell theorem in action (see the FWL page).\nAdding \\(X_2\\) as a control means:\n\nRegress \\(Y\\) on \\(X_2\\) → residuals \\(\\tilde{Y}\\) (variation in \\(Y\\) not explained by \\(X_2\\))\nRegress \\(X_1\\) on \\(X_2\\) → residuals \\(\\tilde{X}_1\\) (variation in \\(X_1\\) not explained by \\(X_2\\))\nRegress \\(\\tilde{Y}\\) on \\(\\tilde{X}_1\\) → the coefficient is \\(\\hat{\\beta}_1\\)\n\n“Controlling for \\(X_2\\)” = looking at the relationship between \\(Y\\) and \\(X_1\\) after removing what \\(X_2\\) explains about each.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 580\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; }\n    .bad  { color: #e74c3c; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\", min = 100, max = 500, value = 300, step = 50),\n\n      sliderInput(\"b1\", HTML(\"True &beta;&lt;sub&gt;1&lt;/sub&gt;:\"),\n                  min = -2, max = 3, value = 0.5, step = 0.25),\n\n      sliderInput(\"b2\", HTML(\"True &beta;&lt;sub&gt;2&lt;/sub&gt; (confounder effect):\"),\n                  min = -3, max = 3, value = 2, step = 0.25),\n\n      sliderInput(\"rho\", HTML(\"Corr(X&lt;sub&gt;1&lt;/sub&gt;, X&lt;sub&gt;2&lt;/sub&gt;):\"),\n                  min = -0.9, max = 0.9, value = 0.7, step = 0.1),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(4, plotOutput(\"raw_plot\",  height = \"380px\")),\n        column(4, plotOutput(\"ctrl_y\",    height = \"380px\")),\n        column(4, plotOutput(\"ctrl_both\", height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    b1  &lt;- input$b1\n    b2  &lt;- input$b2\n    rho &lt;- input$rho\n\n    z1 &lt;- rnorm(n)\n    z2 &lt;- rnorm(n)\n    x1 &lt;- z1\n    x2 &lt;- rho * z1 + sqrt(1 - rho^2) * z2\n    y  &lt;- b1 * x1 + b2 * x2 + rnorm(n)\n\n    # Without control\n    naive_fit &lt;- lm(y ~ x1)\n    naive_b1 &lt;- coef(naive_fit)[2]\n\n    # With control\n    full_fit &lt;- lm(y ~ x1 + x2)\n    full_b1 &lt;- coef(full_fit)[2]\n\n    # FWL residuals\n    ey &lt;- resid(lm(y ~ x2))\n    ex &lt;- resid(lm(x1 ~ x2))\n\n    list(x1 = x1, x2 = x2, y = y, ey = ey, ex = ex,\n         naive_b1 = naive_b1, full_b1 = full_b1,\n         b1 = b1, b2 = b2, rho = rho)\n  })\n\n  output$raw_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$x1, d$y, pch = 16, col = \"#3498db60\", cex = 0.6,\n         xlab = expression(X[1]), ylab = \"Y\",\n         main = \"No control (omit X2)\")\n    abline(lm(d$y ~ d$x1), col = \"#e74c3c\", lwd = 2.5)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = paste0(\"Slope = \", round(d$naive_b1, 3)))\n  })\n\n  output$ctrl_y &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$x1, d$ey, pch = 16, col = \"#9b59b660\", cex = 0.6,\n         xlab = expression(X[1]),\n         ylab = expression(\"Residualized Y  (\" * tilde(Y) * \")\"),\n         main = expression(\"Remove \" * X[2] * \" from Y\"))\n    abline(h = 0, lty = 2, col = \"gray60\")\n  })\n\n  output$ctrl_both &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$ex, d$ey, pch = 16, col = \"#27ae6080\", cex = 0.6,\n         xlab = expression(\"Residualized \" * X[1] * \"  (\" * tilde(X)[1] * \")\"),\n         ylab = expression(\"Residualized Y  (\" * tilde(Y) * \")\"),\n         main = \"After controlling for X2\")\n    abline(lm(d$ey ~ d$ex), col = \"#e74c3c\", lwd = 2.5)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = paste0(\"Slope = \", round(d$full_b1, 3)))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$naive_b1 - d$b1\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True &beta;&lt;sub&gt;1&lt;/sub&gt;:&lt;/b&gt; \", d$b1, \"&lt;br&gt;\",\n        \"&lt;b&gt;Without control:&lt;/b&gt; &lt;span class='bad'&gt;\",\n        round(d$naive_b1, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(bias, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;With control:&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$full_b1, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;Omitted variable bias = &beta;&lt;sub&gt;2&lt;/sub&gt; &times; \",\n        \"corr(X&lt;sub&gt;1&lt;/sub&gt;,X&lt;sub&gt;2&lt;/sub&gt;) / ... &lt;br&gt;\",\n        \"Higher confounding (&beta;&lt;sub&gt;2&lt;/sub&gt;) + higher correlation \",\n        \"= more bias when you don't control.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nTrue \\(\\beta_1\\) = 0.5, Corr = 0.7, \\(\\beta_2\\) = 2: the naive slope (left panel) is way off because \\(X_2\\) confounds the relationship. The controlled slope (right panel) recovers the truth.\nSet Corr = 0: no confounding. Both estimates are the same — controls don’t help when there’s nothing to control for.\nSet \\(\\beta_2\\) = 0: same thing. Even if \\(X_1\\) and \\(X_2\\) are correlated, \\(X_2\\) doesn’t affect \\(Y\\), so omitting it doesn’t bias \\(\\beta_1\\).\nMiddle panel: shows what \\(Y\\) looks like after removing \\(X_2\\)’s contribution. The right panel adds the second step — removing \\(X_2\\) from \\(X_1\\) too — which isolates the independent variation in \\(X_1\\).\n\n\n\n\nDid you know?\n\nCarl Friedrich Gauss invented the method of least squares at age 18 in 1795 to predict the orbit of the asteroid Ceres. When Ceres reappeared exactly where Gauss predicted, he became an overnight celebrity. The entire method is built on minimizing the sum of squared residuals.\nAdrien-Marie Legendre independently published the method in 1805, before Gauss. But Gauss claimed he’d been using it since 1795. The priority dispute was never fully resolved — but we call it “Gaussian” anyway.\nThe idea of “controlling for” a variable sounds scientific, but it’s been criticized. As Edward Leamer wrote in his famous 1983 paper “Let’s Take the Con Out of Econometrics”: adding controls is not the same as running an experiment. The choice of what to control for is often arbitrary and can introduce more bias than it removes (see: bad controls, collider bias).",
    "crumbs": [
      "Residuals & Controls"
    ]
  },
  {
    "objectID": "cef.html",
    "href": "cef.html",
    "title": "Regression & the CEF",
    "section": "",
    "text": "What is the CEF?\nThe conditional expectation function (CEF) answers a simple question: “What do you expect Y to be, given that you know X?”\nSay you’re looking at income (\\(Y\\)) vs. years of education (\\(X\\)). The CEF answers: among everyone with exactly 12 years of education, what’s their average income? What about 16 years? 20 years? If you plot those averages, you get a curve — that’s the CEF. It could be a straight line, or it could bend, flatten out, jump — whatever the data actually does.\n\n\nHow does regression fit in?\nOLS draws a straight line through that. Two cases:\n\nCEF is already a straight line — OLS gets it exactly right. Each extra unit of \\(X\\) adds the same bump to expected \\(Y\\). The line is the CEF.\nCEF is curved — maybe the first few years of education matter a lot, but returns flatten after a PhD. OLS can’t bend, so it draws the best straight line it can through that curve. It’s a useful summary, but it misses the shape.\n\nRegression doesn’t assume the world is linear. It gives you the best linear approximation to whatever the true relationship is. The simulator below lets you see exactly where that approximation works and where it breaks down.\nIn the plots: the red dots show the conditional mean of \\(Y\\) in each bin of \\(X\\) (the empirical CEF), the green curve is the true CEF, and the blue line is OLS. Switch DGPs to see when they agree and when they diverge.\n\n\nReading the residual plot\nThe right panel shows residuals vs. fitted values with a LOESS smoother (red curve). LOESS fits a tiny weighted regression at each point using only nearby observations, producing a flexible curve that follows local patterns. If OLS is correctly specified, the LOESS line should be flat at zero. If it curves, that’s visual evidence of nonlinearity that OLS is missing.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\ndgp_choices &lt;- c(\n  \"Linear\",\n  \"Quadratic\",\n  \"Log (diminishing returns)\",\n  \"Step function\",\n  \"Sine wave\"\n)\n\n# True CEF for each DGP\ncef_fun &lt;- function(x, dgp) {\n  switch(dgp,\n    \"Linear\"                   = 2 + 1.5 * x,\n    \"Quadratic\"                = 1 + 0.8 * x - 0.15 * x^2,\n    \"Log (diminishing returns)\" = 3 * log(x + 1),\n    \"Step function\"            = ifelse(x &lt; 0, -1, 2),\n    \"Sine wave\"                = 2 * sin(x),\n    1.5 * x\n  )\n}\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .info-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.8;\n    }\n    .info-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"dgp\", \"True DGP:\",\n                  choices = dgp_choices),\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 50, max = 1000, value = 300, step = 50),\n\n      sliderInput(\"sigma\", HTML(\"Error SD (&sigma;):\"),\n                  min = 0.5, max = 4, value = 1.5, step = 0.5),\n\n      sliderInput(\"nbins\", \"Bins for CEF:\",\n                  min = 5, max = 30, value = 15, step = 1),\n\n      actionButton(\"redraw\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"info_box\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"scatter_plot\", height = \"420px\")),\n        column(6, plotOutput(\"resid_plot\",   height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$redraw\n    n     &lt;- input$n\n    sigma &lt;- input$sigma\n    dgp   &lt;- input$dgp\n\n    # X range depends on DGP\n    if (dgp == \"Log (diminishing returns)\") {\n      x &lt;- runif(n, 0, 6)\n    } else if (dgp == \"Step function\") {\n      x &lt;- runif(n, -3, 3)\n    } else if (dgp == \"Sine wave\") {\n      x &lt;- runif(n, -pi, 2 * pi)\n    } else {\n      x &lt;- runif(n, -3, 5)\n    }\n\n    mu &lt;- cef_fun(x, dgp)\n    y  &lt;- mu + rnorm(n, sd = sigma)\n\n    ols &lt;- lm(y ~ x)\n\n    # Bin X and compute conditional means\n    nbins &lt;- input$nbins\n    breaks &lt;- seq(min(x), max(x), length.out = nbins + 1)\n    bin    &lt;- cut(x, breaks, include.lowest = TRUE)\n    bin_mid &lt;- tapply(x, bin, mean)\n    bin_cef &lt;- tapply(y, bin, mean)\n    bin_n   &lt;- tapply(y, bin, length)\n\n    list(x = x, y = y, mu = mu, ols = ols, dgp = dgp,\n         bin_mid = bin_mid, bin_cef = bin_cef, bin_n = bin_n)\n  })\n\n  # --- Main scatter plot ---\n  output$scatter_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$y, pch = 16, col = \"#bdc3c780\", cex = 0.6,\n         xlab = \"X\", ylab = \"Y\",\n         main = paste(\"DGP:\", d$dgp))\n\n    # True CEF curve\n    xo &lt;- sort(d$x)\n    lines(xo, cef_fun(xo, d$dgp), col = \"#2ecc71\", lwd = 2.5)\n\n    # OLS line\n    abline(d$ols, col = \"#3498db\", lwd = 2.5)\n\n    # Binned conditional means (empirical CEF)\n    keep &lt;- !is.na(d$bin_cef) & !is.na(d$bin_mid)\n    points(d$bin_mid[keep], d$bin_cef[keep],\n           pch = 19, col = \"#e74c3c\", cex = 1.6)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(expression(\"True \" * E * \"[Y|X]\"),\n                      \"OLS regression\",\n                      expression(\"Binned \" * bar(Y) * \" (empirical CEF)\")),\n           col = c(\"#2ecc71\", \"#3498db\", \"#e74c3c\"),\n           lwd = c(2.5, 2.5, NA),\n           pch = c(NA, NA, 19),\n           pt.cex = c(NA, NA, 1.4))\n  })\n\n  # --- Residual plot ---\n  output$resid_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    r &lt;- resid(d$ols)\n    fv &lt;- fitted(d$ols)\n\n    plot(fv, r, pch = 16, col = \"#9b59b680\", cex = 0.6,\n         xlab = \"Fitted values\", ylab = \"Residuals\",\n         main = \"Residuals vs Fitted\")\n    abline(h = 0, lty = 2, col = \"gray40\", lwd = 1.5)\n\n    # Loess smoother to reveal nonlinearity\n    lo &lt;- loess(r ~ fv)\n    ox &lt;- order(fv)\n    lines(fv[ox], predict(lo)[ox], col = \"#e74c3c\", lwd = 2)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Loess smoother\", \"Zero line\"),\n           col = c(\"#e74c3c\", \"gray40\"),\n           lwd = c(2, 1.5),\n           lty = c(1, 2))\n  })\n\n  # --- Info box ---\n  output$info_box &lt;- renderUI({\n    d &lt;- dat()\n    b0 &lt;- round(coef(d$ols)[1], 3)\n    b1 &lt;- round(coef(d$ols)[2], 3)\n    r2 &lt;- round(summary(d$ols)$r.squared, 3)\n\n    linear &lt;- d$dgp == \"Linear\"\n\n    tags$div(class = \"info-box\",\n      HTML(paste0(\n        \"&lt;b&gt;OLS:&lt;/b&gt; Y = \", b0, \" + \", b1, \"X&lt;br&gt;\",\n        \"&lt;b&gt;R&sup2;:&lt;/b&gt; \", r2, \"&lt;br&gt;&lt;br&gt;\",\n        if (linear) {\n          \"&lt;span style='color:#27ae60;'&gt;&lt;b&gt;&#10003; CEF is linear &mdash; OLS recovers it exactly.&lt;/b&gt;&lt;/span&gt;\"\n        } else {\n          \"&lt;span style='color:#e67e22;'&gt;&lt;b&gt;CEF is nonlinear &mdash; OLS is the best linear approximation.&lt;/b&gt;&lt;br&gt;Check the residual plot for the pattern.&lt;/span&gt;\"\n        }\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nDid you know?\n\nThe word “regression” comes from Francis Galton’s 1886 study of heights. He noticed that tall parents tended to have children who were tall — but not as tall. Short parents had children who were short — but not as short. Children’s heights “regressed toward the mean.” The statistical technique kept the name, even though modern regression has nothing to do with reverting to averages.\nGalton was also Charles Darwin’s half-cousin. He applied statistical thinking to heredity, fingerprints, and even the optimal way to brew tea.\nThe conditional expectation function is sometimes called the “regression function” — which makes sense once you know Galton’s story. OLS literally estimates the function that tells you the expected value of \\(Y\\) given \\(X\\).",
    "crumbs": [
      "Regression & the CEF"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundations of Statistical Inference",
    "section": "",
    "text": "Welcome 👋\nThis course builds intuition for:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#start-here",
    "href": "index.html#start-here",
    "title": "Foundations of Statistical Inference",
    "section": "Start Here",
    "text": "Start Here\n\nFoundations — Distributions, Sampling & Confidence Intervals\nVariance, SD & Standard Error — Population vs Sample, SD vs SE & Why n − 1\nThe Sampling Distribution — Data vs Sampling Distribution & When to Assume Normality\nRegression & the CEF — OLS and the Conditional Expectation Function\nCentral Limit Theorem — Interactive CLT Simulator\nPower, Alpha, Beta & MDE — Hypothesis Testing & Experiment Design\nResiduals & Controls — Diagnostics, Partialling Out & Omitted Variable Bias\nHomo- & Heteroskedasticity — Constant vs Non-Constant Variance & Robust SEs\nFrisch-Waugh-Lovell — Partialling Out & OVB\nThe Bootstrap — Resampling-Based Inference",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "simulations.html",
    "href": "simulations.html",
    "title": "CLT Simulator",
    "section": "",
    "text": "Drag the sample size slider to watch the Central Limit Theorem in action. The left plot shows the true population; the right shows the sampling distribution of the mean — notice how it becomes normal as n grows, regardless of the population shape.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\n\n# ---------------------------------------------------------------------------\n# Helper: draw a single random sample from the chosen distribution\n# ---------------------------------------------------------------------------\ndraw_sample &lt;- function(n, dist) {\n  switch(dist,\n    \"Uniform(0, 1)\"      = runif(n),\n    \"Exponential(1)\"     = rexp(n, rate = 1),\n    \"Right-skewed\"       = rchisq(n, df = 3),\n    \"Bimodal\"            = {\n      k &lt;- rbinom(n, 1, 0.5)\n      k * rnorm(n, mean = -2, sd = 0.6) + (1 - k) * rnorm(n, mean = 2, sd = 0.6)\n    },\n    \"Bernoulli(0.3)\"     = rbinom(n, size = 1, prob = 0.3),\n    runif(n)\n  )\n}\n\n# Theoretical mean & sd of each population distribution\npop_params &lt;- list(\n  \"Uniform(0, 1)\"  = list(mu = 0.5, sigma = sqrt(1 / 12)),\n  \"Exponential(1)\" = list(mu = 1,   sigma = 1),\n  \"Right-skewed\"   = list(mu = 3,   sigma = sqrt(6)),\n  \"Bimodal\"        = list(mu = 0,   sigma = sqrt(0.6^2 + 4)),\n  \"Bernoulli(0.3)\" = list(mu = 0.3, sigma = sqrt(0.3 * 0.7))\n)\n\n# ---------------------------------------------------------------------------\n# UI\n# ---------------------------------------------------------------------------\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #eaf2f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.8;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"dist\", \"Population distribution:\",\n                  choices = names(pop_params)),\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 1, max = 200, value = 5, step = 1),\n\n      sliderInput(\"reps\", \"Number of samples:\",\n                  min = 100, max = 3000, value = 1000, step = 100),\n\n      actionButton(\"resample\", \"Draw new samples\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"stats_box\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"parent_plot\", height = \"380px\")),\n        column(6, plotOutput(\"sampling_plot\", height = \"380px\"))\n      )\n    )\n  )\n)\n\n# ---------------------------------------------------------------------------\n# Server\n# ---------------------------------------------------------------------------\nserver &lt;- function(input, output, session) {\n\n  sim &lt;- reactive({\n    input$resample\n    n    &lt;- input$n\n    reps &lt;- input$reps\n    dist &lt;- input$dist\n\n    means &lt;- replicate(reps, mean(draw_sample(n, dist)))\n\n    params &lt;- pop_params[[dist]]\n    theo_mu &lt;- params$mu\n    theo_se &lt;- params$sigma / sqrt(n)\n\n    list(means = means, dist = dist, n = n, reps = reps,\n         theo_mu = theo_mu, theo_se = theo_se)\n  })\n\n  output$parent_plot &lt;- renderPlot({\n    dist &lt;- input$dist\n    big  &lt;- draw_sample(10000, dist)\n\n    par(mar = c(4.5, 4, 3, 1))\n    hist(big, breaks = 60, probability = TRUE,\n         col = \"#d5e8d4\", border = \"#82b366\",\n         main = paste(\"True Population:\", dist),\n         xlab = \"x\", ylab = \"Density\")\n  })\n\n  output$sampling_plot &lt;- renderPlot({\n    s &lt;- sim()\n\n    par(mar = c(4.5, 4, 3, 1))\n    hist(s$means, breaks = 40, probability = TRUE,\n         col = \"#dae8fc\", border = \"#6c8ebf\",\n         main = paste0(\"Sampling Distribution of the Mean (n = \", s$n, \")\"),\n         xlab = \"Sample mean\", ylab = \"Density\")\n\n    x_seq &lt;- seq(min(s$means), max(s$means), length.out = 300)\n    lines(x_seq, dnorm(x_seq, mean = s$theo_mu, sd = s$theo_se),\n          col = \"#e74c3c\", lwd = 2.5)\n\n    abline(v = s$theo_mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\",\n           legend = c(\"Normal approximation\", \"Theoretical mean\"),\n           col    = c(\"#e74c3c\", \"#2c3e50\"),\n           lwd    = c(2.5, 2),\n           lty    = c(1, 2),\n           bty    = \"n\", cex = 0.9)\n  })\n\n  output$stats_box &lt;- renderUI({\n    s &lt;- sim()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Theoretical mean:&lt;/b&gt; \",   round(s$theo_mu, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Observed mean:&lt;/b&gt; \",      round(mean(s$means), 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Theoretical SE:&lt;/b&gt; \",     round(s$theo_se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Observed SD:&lt;/b&gt; \",        round(sd(s$means), 4)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nDid you know?\n\nThe CLT was first glimpsed by Abraham de Moivre in 1733, who showed that the binomial distribution approaches a bell curve. Laplace generalized it in 1812. But the rigorous proof for arbitrary distributions came from Aleksandr Lyapunov in 1901 — over 150 years after de Moivre’s insight.\nThe normal distribution is sometimes called the “Gaussian” distribution after Carl Friedrich Gauss, but Gauss wasn’t the first to describe it — de Moivre was. Gauss just got better publicity.\nThe CLT explains why so many things in nature look bell-shaped: human heights, blood pressure, measurement errors, IQ scores. Whenever an outcome is the sum of many small independent factors, the CLT kicks in.",
    "crumbs": [
      "Central Limit Theorem"
    ]
  },
  {
    "objectID": "sampling-distribution.html",
    "href": "sampling-distribution.html",
    "title": "The Sampling Distribution",
    "section": "",
    "text": "You run an experiment and compute a statistic — say the sample mean. You get \\(\\bar{x} = 4.7\\). But if you ran the experiment again with new data, you’d get \\(\\bar{x} = 5.1\\). And again: \\(\\bar{x} = 4.3\\).\nThe sampling distribution is the distribution of all possible values your statistic could take across every possible sample you could have drawn. It tells you: how much does my estimate bounce around due to the randomness of sampling?\nYou never observe it directly — you only ran the experiment once. But it’s the foundation of everything: standard errors, confidence intervals, p-values, and hypothesis tests all depend on it.\n\n\nThis is the most common confusion in statistics:\n\n\n\n\n\n\n\n\n\nDistribution of the data\nSampling distribution\n\n\n\n\nWhat is it?\nHistogram of individual observations\nHistogram of the statistic across repeated samples\n\n\nWhat does it show?\nHow spread out individual values are\nHow much the estimate varies\n\n\nShape\nCould be anything (skewed, bimodal, etc.)\nOften approximately normal (CLT)\n\n\nWidth\nDepends on \\(\\sigma\\) (population SD)\nDepends on \\(\\sigma / \\sqrt{n}\\) (standard error)\n\n\nYou can see it?\nYes — plot your data\nNo — you only have one sample\n\n\n\nYour data might be wildly skewed. But the sampling distribution of the mean can still be normal, because averaging smooths things out. That’s the CLT.\nThe simulation below makes this concrete. The left panel shows your data (one sample). The right panel shows what happens when you repeat the experiment 1,000 times — the sampling distribution.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 1050\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"pop\", \"Population shape:\",\n                  choices = c(\"Normal\",\n                              \"Uniform\",\n                              \"Skewed (exponential)\",\n                              \"Heavily skewed (chi-sq 2)\",\n                              \"Bimodal\",\n                              \"Heavy-tailed (t, df=3)\")),\n\n      selectInput(\"stat\", \"Statistic:\",\n                  choices = c(\"Mean\", \"Median\", \"SD\",\n                              \"Max\", \"IQR\")),\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 2, max = 200, value = 5, step = 1),\n\n      sliderInput(\"reps\", \"Repeated experiments:\",\n                  min = 100, max = 5000, value = 1000, step = 100),\n\n      actionButton(\"go\", \"Run experiments\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"data_plot\", height = \"350px\")),\n        column(6, plotOutput(\"samp_dist_plot\", height = \"350px\"))\n      ),\n      fluidRow(\n        column(12, plotOutput(\"convergence_plot\", height = \"450px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  draw_pop &lt;- function(n, pop) {\n    switch(pop,\n      \"Normal\" = rnorm(n, mean = 5, sd = 2),\n      \"Uniform\" = runif(n, 0, 10),\n      \"Skewed (exponential)\" = rexp(n, rate = 0.5),\n      \"Heavily skewed (chi-sq 2)\" = rchisq(n, df = 2),\n      \"Bimodal\" = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, 2, 0.7) + (1 - k) * rnorm(n, 8, 0.7)\n      },\n      \"Heavy-tailed (t, df=3)\" = rt(n, df = 3) * 2 + 5\n    )\n  }\n\n  compute_stat &lt;- function(x, stat) {\n    switch(stat,\n      \"Mean\"   = mean(x),\n      \"Median\" = median(x),\n      \"SD\"     = sd(x),\n      \"Max\"    = max(x),\n      \"IQR\"    = IQR(x)\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    reps &lt;- input$reps\n    pop  &lt;- input$pop\n    stat &lt;- input$stat\n\n    # One sample (for display)\n    one_sample &lt;- draw_pop(n, pop)\n    one_stat   &lt;- compute_stat(one_sample, stat)\n\n    # Big population draw (to show true shape)\n    big_pop &lt;- draw_pop(10000, pop)\n\n    # Repeated experiments\n    samp_stats &lt;- replicate(reps, compute_stat(draw_pop(n, pop), stat))\n\n    # Check normality (Shapiro on subset)\n    shap_sub &lt;- samp_stats[seq_len(min(5000, length(samp_stats)))]\n    shap_p &lt;- tryCatch(shapiro.test(shap_sub)$p.value, error = function(e) NA)\n\n    list(one_sample = one_sample, one_stat = one_stat,\n         big_pop = big_pop, samp_stats = samp_stats,\n         n = n, reps = reps, stat = stat, pop = pop,\n         shap_p = shap_p)\n  })\n\n  # --- Left: data distribution ---\n  output$data_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$big_pop, breaks = 60, probability = TRUE,\n         col = \"#e74c3c20\", border = \"#e74c3c60\",\n         main = paste(\"Population:\", d$pop),\n         xlab = \"X\", ylab = \"Density\")\n\n    # Overlay one sample as rug\n    rug(d$one_sample, col = \"#2c3e50\", lwd = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Population shape\",\n                      paste0(\"Your sample (n = \", d$n, \")\")),\n           col = c(\"#e74c3c60\", \"#2c3e50\"),\n           lwd = c(8, 2), lty = c(1, 1))\n  })\n\n  # --- Right: sampling distribution ---\n  output$samp_dist_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$samp_stats, breaks = 50, probability = TRUE,\n         col = \"#3498db30\", border = \"#3498db80\",\n         main = paste0(\"Sampling dist. of \", d$stat, \" (n = \", d$n, \")\"),\n         xlab = paste0(\"Sample \", tolower(d$stat)),\n         ylab = \"Density\")\n\n    # Normal overlay\n    x_seq &lt;- seq(min(d$samp_stats), max(d$samp_stats), length.out = 300)\n    lines(x_seq,\n          dnorm(x_seq, mean = mean(d$samp_stats), sd = sd(d$samp_stats)),\n          col = \"#e74c3c\", lwd = 2, lty = 2)\n\n    # Mark your one estimate\n    abline(v = d$one_stat, col = \"#2c3e50\", lwd = 2.5, lty = 1)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Sampling distribution\",\n                      \"Normal approximation\",\n                      paste0(\"Your estimate: \", round(d$one_stat, 3))),\n           col = c(\"#3498db80\", \"#e74c3c\", \"#2c3e50\"),\n           lwd = c(8, 2, 2.5), lty = c(1, 2, 1))\n  })\n\n  # --- Bottom: how normality improves with n ---\n  output$convergence_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ns &lt;- c(2, 5, 10, 30, 50, 100)\n    ns &lt;- ns[ns &lt;= 200]\n\n    colors &lt;- colorRampPalette(c(\"#e74c3c\", \"#3498db\"))(length(ns))\n\n    # First pass to get x range\n    all_stats &lt;- list()\n    for (i in seq_along(ns)) {\n      all_stats[[i]] &lt;- replicate(500, compute_stat(draw_pop(ns[i], d$pop), d$stat))\n    }\n    xlim &lt;- range(unlist(all_stats))\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, length(ns) * 1.5 + 1),\n         yaxt = \"n\", ylab = \"\", xlab = paste0(\"Sample \", tolower(d$stat)),\n         main = paste0(\"Sampling distribution of \", d$stat, \" at different n\"))\n    positions &lt;- seq_along(ns) * 1.5\n    axis(2, at = positions, labels = paste0(\"n=\", ns), las = 1, cex.axis = 0.9)\n\n    for (i in seq_along(ns)) {\n      dens &lt;- density(all_stats[[i]])\n      # Scale density to fit in a strip\n      dens$y &lt;- dens$y / max(dens$y) * 0.7\n      polygon(dens$x, dens$y + positions[i], col = adjustcolor(colors[i], 0.4),\n              border = colors[i], lwd = 1.5)\n    }\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    se &lt;- sd(d$samp_stats)\n    normal_enough &lt;- !is.na(d$shap_p) && d$shap_p &gt; 0.05\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Your estimate:&lt;/b&gt; \", round(d$one_stat, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;SE (from sampling dist):&lt;/b&gt; \", round(se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Sampling dist SD:&lt;/b&gt; \", round(sd(d$samp_stats), 4), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Looks normal?&lt;/b&gt; \",\n        if (normal_enough)\n          \"&lt;span style='color:#27ae60;'&gt;Yes (Shapiro p = \" else\n          \"&lt;span style='color:#e74c3c;'&gt;No (Shapiro p = \",\n        round(d$shap_p, 4), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;small&gt;Shapiro-Wilk tests whether the&lt;br&gt;\",\n        \"sampling distribution is normal.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nThe red dashed line on the right panel is the normal approximation. When it fits the histogram well, you can safely use normal-based inference (z-tests, t-tests, standard CIs).\nRules of thumb:\n\n\n\nPopulation\nStatistic\nn needed for normality\n\n\n\n\nSymmetric (normal, uniform)\nMean\n~10–15\n\n\nModerately skewed (exponential)\nMean\n~30\n\n\nHeavily skewed (chi-sq 2)\nMean\n~50–100\n\n\nAny shape\nMean\n~30 is the textbook answer, but check\n\n\nAny shape\nMedian\nSlower — needs larger n\n\n\nAny shape\nMax\nVery slow — often never normal\n\n\nAny shape\nSD\nModerate — ~30–50\n\n\n\n\n\n\n\nExponential, Mean, n = 5: the sampling distribution is visibly skewed. Normal approximation is poor. Shapiro test says “No.”\nSame but n = 50: now it’s nearly bell-shaped. CLT has kicked in.\nAny population, Max, n = 100: the sampling distribution of the maximum is not normal even with large n. It has its own distribution (extreme value theory). Normal approximation fails.\nBimodal, Mean, n = 2: the sampling distribution itself is bimodal! With only 2 observations, averaging doesn’t smooth enough.\nBottom panel (ridgeline): watch the sampling distribution narrow and become more normal as n increases, all on one plot.\n\n\n\n\nThe sampling distribution is not your data. It’s the distribution of your estimate. Whether it’s normal depends on three things:\n\nThe population shape — the uglier it is, the more data you need\nThe sample size — larger n → CLT → normality\nThe statistic — means converge fast, medians slower, maxima barely at all\n\nWhen in doubt, don’t assume — bootstrap it. The bootstrap page shows you how to build the sampling distribution empirically.\n\n\n\n\n\nWilliam Sealy Gosset, a chemist at the Guinness brewery, invented the t-distribution in 1908 because he was working with tiny samples of barley. Guinness didn’t let employees publish under their own names, so he used the pen name “Student” — hence “Student’s t-test.”\nGosset’s problem: with only 3–4 observations, you can’t assume the sampling distribution is normal. The t-distribution has heavier tails to account for the extra uncertainty when \\(n\\) is small. As \\(n\\) grows, the t-distribution converges to the normal — because with enough data, the sampling distribution is normal (CLT).\nThe concept of a sampling distribution was so confusing to early statisticians that R.A. Fisher reportedly said it was the most difficult idea in all of statistics to teach clearly.",
    "crumbs": [
      "The Sampling Distribution"
    ]
  },
  {
    "objectID": "sampling-distribution.html#what-is-a-sampling-distribution",
    "href": "sampling-distribution.html#what-is-a-sampling-distribution",
    "title": "The Sampling Distribution",
    "section": "",
    "text": "You run an experiment and compute a statistic — say the sample mean. You get \\(\\bar{x} = 4.7\\). But if you ran the experiment again with new data, you’d get \\(\\bar{x} = 5.1\\). And again: \\(\\bar{x} = 4.3\\).\nThe sampling distribution is the distribution of all possible values your statistic could take across every possible sample you could have drawn. It tells you: how much does my estimate bounce around due to the randomness of sampling?\nYou never observe it directly — you only ran the experiment once. But it’s the foundation of everything: standard errors, confidence intervals, p-values, and hypothesis tests all depend on it.\n\n\nThis is the most common confusion in statistics:\n\n\n\n\n\n\n\n\n\nDistribution of the data\nSampling distribution\n\n\n\n\nWhat is it?\nHistogram of individual observations\nHistogram of the statistic across repeated samples\n\n\nWhat does it show?\nHow spread out individual values are\nHow much the estimate varies\n\n\nShape\nCould be anything (skewed, bimodal, etc.)\nOften approximately normal (CLT)\n\n\nWidth\nDepends on \\(\\sigma\\) (population SD)\nDepends on \\(\\sigma / \\sqrt{n}\\) (standard error)\n\n\nYou can see it?\nYes — plot your data\nNo — you only have one sample\n\n\n\nYour data might be wildly skewed. But the sampling distribution of the mean can still be normal, because averaging smooths things out. That’s the CLT.\nThe simulation below makes this concrete. The left panel shows your data (one sample). The right panel shows what happens when you repeat the experiment 1,000 times — the sampling distribution.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 1050\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"pop\", \"Population shape:\",\n                  choices = c(\"Normal\",\n                              \"Uniform\",\n                              \"Skewed (exponential)\",\n                              \"Heavily skewed (chi-sq 2)\",\n                              \"Bimodal\",\n                              \"Heavy-tailed (t, df=3)\")),\n\n      selectInput(\"stat\", \"Statistic:\",\n                  choices = c(\"Mean\", \"Median\", \"SD\",\n                              \"Max\", \"IQR\")),\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 2, max = 200, value = 5, step = 1),\n\n      sliderInput(\"reps\", \"Repeated experiments:\",\n                  min = 100, max = 5000, value = 1000, step = 100),\n\n      actionButton(\"go\", \"Run experiments\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"data_plot\", height = \"350px\")),\n        column(6, plotOutput(\"samp_dist_plot\", height = \"350px\"))\n      ),\n      fluidRow(\n        column(12, plotOutput(\"convergence_plot\", height = \"450px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  draw_pop &lt;- function(n, pop) {\n    switch(pop,\n      \"Normal\" = rnorm(n, mean = 5, sd = 2),\n      \"Uniform\" = runif(n, 0, 10),\n      \"Skewed (exponential)\" = rexp(n, rate = 0.5),\n      \"Heavily skewed (chi-sq 2)\" = rchisq(n, df = 2),\n      \"Bimodal\" = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, 2, 0.7) + (1 - k) * rnorm(n, 8, 0.7)\n      },\n      \"Heavy-tailed (t, df=3)\" = rt(n, df = 3) * 2 + 5\n    )\n  }\n\n  compute_stat &lt;- function(x, stat) {\n    switch(stat,\n      \"Mean\"   = mean(x),\n      \"Median\" = median(x),\n      \"SD\"     = sd(x),\n      \"Max\"    = max(x),\n      \"IQR\"    = IQR(x)\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    reps &lt;- input$reps\n    pop  &lt;- input$pop\n    stat &lt;- input$stat\n\n    # One sample (for display)\n    one_sample &lt;- draw_pop(n, pop)\n    one_stat   &lt;- compute_stat(one_sample, stat)\n\n    # Big population draw (to show true shape)\n    big_pop &lt;- draw_pop(10000, pop)\n\n    # Repeated experiments\n    samp_stats &lt;- replicate(reps, compute_stat(draw_pop(n, pop), stat))\n\n    # Check normality (Shapiro on subset)\n    shap_sub &lt;- samp_stats[seq_len(min(5000, length(samp_stats)))]\n    shap_p &lt;- tryCatch(shapiro.test(shap_sub)$p.value, error = function(e) NA)\n\n    list(one_sample = one_sample, one_stat = one_stat,\n         big_pop = big_pop, samp_stats = samp_stats,\n         n = n, reps = reps, stat = stat, pop = pop,\n         shap_p = shap_p)\n  })\n\n  # --- Left: data distribution ---\n  output$data_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$big_pop, breaks = 60, probability = TRUE,\n         col = \"#e74c3c20\", border = \"#e74c3c60\",\n         main = paste(\"Population:\", d$pop),\n         xlab = \"X\", ylab = \"Density\")\n\n    # Overlay one sample as rug\n    rug(d$one_sample, col = \"#2c3e50\", lwd = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Population shape\",\n                      paste0(\"Your sample (n = \", d$n, \")\")),\n           col = c(\"#e74c3c60\", \"#2c3e50\"),\n           lwd = c(8, 2), lty = c(1, 1))\n  })\n\n  # --- Right: sampling distribution ---\n  output$samp_dist_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$samp_stats, breaks = 50, probability = TRUE,\n         col = \"#3498db30\", border = \"#3498db80\",\n         main = paste0(\"Sampling dist. of \", d$stat, \" (n = \", d$n, \")\"),\n         xlab = paste0(\"Sample \", tolower(d$stat)),\n         ylab = \"Density\")\n\n    # Normal overlay\n    x_seq &lt;- seq(min(d$samp_stats), max(d$samp_stats), length.out = 300)\n    lines(x_seq,\n          dnorm(x_seq, mean = mean(d$samp_stats), sd = sd(d$samp_stats)),\n          col = \"#e74c3c\", lwd = 2, lty = 2)\n\n    # Mark your one estimate\n    abline(v = d$one_stat, col = \"#2c3e50\", lwd = 2.5, lty = 1)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Sampling distribution\",\n                      \"Normal approximation\",\n                      paste0(\"Your estimate: \", round(d$one_stat, 3))),\n           col = c(\"#3498db80\", \"#e74c3c\", \"#2c3e50\"),\n           lwd = c(8, 2, 2.5), lty = c(1, 2, 1))\n  })\n\n  # --- Bottom: how normality improves with n ---\n  output$convergence_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ns &lt;- c(2, 5, 10, 30, 50, 100)\n    ns &lt;- ns[ns &lt;= 200]\n\n    colors &lt;- colorRampPalette(c(\"#e74c3c\", \"#3498db\"))(length(ns))\n\n    # First pass to get x range\n    all_stats &lt;- list()\n    for (i in seq_along(ns)) {\n      all_stats[[i]] &lt;- replicate(500, compute_stat(draw_pop(ns[i], d$pop), d$stat))\n    }\n    xlim &lt;- range(unlist(all_stats))\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, length(ns) * 1.5 + 1),\n         yaxt = \"n\", ylab = \"\", xlab = paste0(\"Sample \", tolower(d$stat)),\n         main = paste0(\"Sampling distribution of \", d$stat, \" at different n\"))\n    positions &lt;- seq_along(ns) * 1.5\n    axis(2, at = positions, labels = paste0(\"n=\", ns), las = 1, cex.axis = 0.9)\n\n    for (i in seq_along(ns)) {\n      dens &lt;- density(all_stats[[i]])\n      # Scale density to fit in a strip\n      dens$y &lt;- dens$y / max(dens$y) * 0.7\n      polygon(dens$x, dens$y + positions[i], col = adjustcolor(colors[i], 0.4),\n              border = colors[i], lwd = 1.5)\n    }\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    se &lt;- sd(d$samp_stats)\n    normal_enough &lt;- !is.na(d$shap_p) && d$shap_p &gt; 0.05\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Your estimate:&lt;/b&gt; \", round(d$one_stat, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;SE (from sampling dist):&lt;/b&gt; \", round(se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Sampling dist SD:&lt;/b&gt; \", round(sd(d$samp_stats), 4), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Looks normal?&lt;/b&gt; \",\n        if (normal_enough)\n          \"&lt;span style='color:#27ae60;'&gt;Yes (Shapiro p = \" else\n          \"&lt;span style='color:#e74c3c;'&gt;No (Shapiro p = \",\n        round(d$shap_p, 4), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;small&gt;Shapiro-Wilk tests whether the&lt;br&gt;\",\n        \"sampling distribution is normal.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nThe red dashed line on the right panel is the normal approximation. When it fits the histogram well, you can safely use normal-based inference (z-tests, t-tests, standard CIs).\nRules of thumb:\n\n\n\nPopulation\nStatistic\nn needed for normality\n\n\n\n\nSymmetric (normal, uniform)\nMean\n~10–15\n\n\nModerately skewed (exponential)\nMean\n~30\n\n\nHeavily skewed (chi-sq 2)\nMean\n~50–100\n\n\nAny shape\nMean\n~30 is the textbook answer, but check\n\n\nAny shape\nMedian\nSlower — needs larger n\n\n\nAny shape\nMax\nVery slow — often never normal\n\n\nAny shape\nSD\nModerate — ~30–50\n\n\n\n\n\n\n\nExponential, Mean, n = 5: the sampling distribution is visibly skewed. Normal approximation is poor. Shapiro test says “No.”\nSame but n = 50: now it’s nearly bell-shaped. CLT has kicked in.\nAny population, Max, n = 100: the sampling distribution of the maximum is not normal even with large n. It has its own distribution (extreme value theory). Normal approximation fails.\nBimodal, Mean, n = 2: the sampling distribution itself is bimodal! With only 2 observations, averaging doesn’t smooth enough.\nBottom panel (ridgeline): watch the sampling distribution narrow and become more normal as n increases, all on one plot.\n\n\n\n\nThe sampling distribution is not your data. It’s the distribution of your estimate. Whether it’s normal depends on three things:\n\nThe population shape — the uglier it is, the more data you need\nThe sample size — larger n → CLT → normality\nThe statistic — means converge fast, medians slower, maxima barely at all\n\nWhen in doubt, don’t assume — bootstrap it. The bootstrap page shows you how to build the sampling distribution empirically.\n\n\n\n\n\nWilliam Sealy Gosset, a chemist at the Guinness brewery, invented the t-distribution in 1908 because he was working with tiny samples of barley. Guinness didn’t let employees publish under their own names, so he used the pen name “Student” — hence “Student’s t-test.”\nGosset’s problem: with only 3–4 observations, you can’t assume the sampling distribution is normal. The t-distribution has heavier tails to account for the extra uncertainty when \\(n\\) is small. As \\(n\\) grows, the t-distribution converges to the normal — because with enough data, the sampling distribution is normal (CLT).\nThe concept of a sampling distribution was so confusing to early statisticians that R.A. Fisher reportedly said it was the most difficult idea in all of statistics to teach clearly.",
    "crumbs": [
      "The Sampling Distribution"
    ]
  }
]