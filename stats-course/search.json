[
  {
    "objectID": "bias-variance.html",
    "href": "bias-variance.html",
    "title": "The Bias-Variance Tradeoff",
    "section": "",
    "text": "You might think: “More flexible models fit the data better, so they must be better.” But there’s a catch. A model that fits the training data perfectly will often perform terribly on new data. This is overfitting.\nThe bias-variance tradeoff explains why:\n\n\n\n\n\n\n\n\n\nSimple model (e.g., linear)\nComplex model (e.g., degree-20 polynomial)\n\n\n\n\nBias\nHigh — can’t capture the true shape\nLow — flexible enough to match any pattern\n\n\nVariance\nLow — stable across samples\nHigh — estimates change wildly with new data\n\n\nTraining error\nHigher\nLower (maybe zero)\n\n\nTest error\nU-shaped — sweet spot exists\nU-shaped — sweet spot exists\n\n\n\nThe total prediction error (MSE) decomposes as:\n\\[\\text{MSE} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible noise}\\]\nYou can’t eliminate all three. Reducing bias increases variance, and vice versa. The best model balances both.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we know the true function \\(f(x)\\) that generated the data — the curve we’re trying to learn. We can compute exact bias and variance because we know the target. In practice, you never know \\(f(x)\\). Finding it is the whole point of modeling. You use test error or cross-validation as a proxy for the tradeoff you can’t see directly.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Bias-Variance Tradeoff"
    ]
  },
  {
    "objectID": "bias-variance.html#why-the-best-model-isnt-the-most-complex-one",
    "href": "bias-variance.html#why-the-best-model-isnt-the-most-complex-one",
    "title": "The Bias-Variance Tradeoff",
    "section": "",
    "text": "You might think: “More flexible models fit the data better, so they must be better.” But there’s a catch. A model that fits the training data perfectly will often perform terribly on new data. This is overfitting.\nThe bias-variance tradeoff explains why:\n\n\n\n\n\n\n\n\n\nSimple model (e.g., linear)\nComplex model (e.g., degree-20 polynomial)\n\n\n\n\nBias\nHigh — can’t capture the true shape\nLow — flexible enough to match any pattern\n\n\nVariance\nLow — stable across samples\nHigh — estimates change wildly with new data\n\n\nTraining error\nHigher\nLower (maybe zero)\n\n\nTest error\nU-shaped — sweet spot exists\nU-shaped — sweet spot exists\n\n\n\nThe total prediction error (MSE) decomposes as:\n\\[\\text{MSE} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible noise}\\]\nYou can’t eliminate all three. Reducing bias increases variance, and vice versa. The best model balances both.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we know the true function \\(f(x)\\) that generated the data — the curve we’re trying to learn. We can compute exact bias and variance because we know the target. In practice, you never know \\(f(x)\\). Finding it is the whole point of modeling. You use test error or cross-validation as a proxy for the tradeoff you can’t see directly.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Bias-Variance Tradeoff"
    ]
  },
  {
    "objectID": "bias-variance.html#simulation-1-polynomial-regression-underfitting-vs-overfitting",
    "href": "bias-variance.html#simulation-1-polynomial-regression-underfitting-vs-overfitting",
    "title": "The Bias-Variance Tradeoff",
    "section": "Simulation 1: Polynomial regression — underfitting vs overfitting",
    "text": "Simulation 1: Polynomial regression — underfitting vs overfitting\nFit polynomials of different degrees to noisy data. Low degrees underfit (too rigid). High degrees overfit (too wiggly). Watch the training error decrease monotonically while the test error has a U-shape.\n#| standalone: true\n#| viewerHeight: 700\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"true_fn\", \"True function:\",\n                  choices = c(\"Sine wave\",\n                              \"Quadratic\",\n                              \"Step function\",\n                              \"Linear\")),\n\n      sliderInput(\"n_train\", \"Training points:\",\n                  min = 20, max = 200, value = 50, step = 10),\n\n      sliderInput(\"noise\", \"Noise level (\\u03c3):\",\n                  min = 0.1, max = 2, value = 0.5, step = 0.1),\n\n      sliderInput(\"degree\", \"Polynomial degree:\",\n                  min = 1, max = 20, value = 3, step = 1),\n\n      actionButton(\"go\", \"New data\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(7, plotOutput(\"fit_plot\", height = \"400px\")),\n        column(5, plotOutput(\"error_curve\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  f_true &lt;- function(x, fn) {\n    switch(fn,\n      \"Sine wave\"     = sin(2 * pi * x),\n      \"Quadratic\"     = 2 * (x - 0.5)^2,\n      \"Step function\" = ifelse(x &gt; 0.5, 1, 0),\n      \"Linear\"        = x\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n_train\n    sigma &lt;- input$noise\n    fn    &lt;- input$true_fn\n\n    # Training data\n    x_train &lt;- sort(runif(n, 0, 1))\n    y_train &lt;- f_true(x_train, fn) + rnorm(n, sd = sigma)\n\n    # Test data (fresh)\n    x_test &lt;- sort(runif(n, 0, 1))\n    y_test &lt;- f_true(x_test, fn) + rnorm(n, sd = sigma)\n\n    # Fit polynomials of degree 1 through 20\n    degrees &lt;- 1:20\n    train_mse &lt;- numeric(20)\n    test_mse  &lt;- numeric(20)\n\n    for (d in degrees) {\n      fit &lt;- lm(y_train ~ poly(x_train, d, raw = TRUE))\n      pred_train &lt;- predict(fit)\n      pred_test  &lt;- predict(fit, newdata = data.frame(x_train = x_test))\n      train_mse[d] &lt;- mean((y_train - pred_train)^2)\n      test_mse[d]  &lt;- mean((y_test - pred_test)^2)\n    }\n\n    # Current degree fit for plotting\n    d_now &lt;- input$degree\n    fit_now &lt;- lm(y_train ~ poly(x_train, d_now, raw = TRUE))\n    x_grid &lt;- seq(0, 1, length.out = 300)\n    pred_grid &lt;- predict(fit_now, newdata = data.frame(x_train = x_grid))\n\n    list(x_train = x_train, y_train = y_train,\n         x_test = x_test, y_test = y_test,\n         x_grid = x_grid, pred_grid = pred_grid,\n         train_mse = train_mse, test_mse = test_mse,\n         fn = fn, d_now = d_now, sigma = sigma)\n  })\n\n  output$fit_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x_train, d$y_train, pch = 16,\n         col = \"#3498db80\", cex = 0.8,\n         xlab = \"x\", ylab = \"y\",\n         main = paste0(\"Degree \", d$d_now, \" polynomial fit\"),\n         ylim = range(c(d$y_train, d$pred_grid)))\n\n    # True function\n    x_fine &lt;- seq(0, 1, length.out = 300)\n    lines(x_fine, f_true(x_fine, d$fn),\n          col = \"#27ae60\", lwd = 2, lty = 2)\n\n    # Fitted curve\n    lines(d$x_grid, d$pred_grid, col = \"#e74c3c\", lwd = 2.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Training data\", \"True function\",\n                      paste0(\"Degree \", d$d_now, \" fit\")),\n           col = c(\"#3498db80\", \"#27ae60\", \"#e74c3c\"),\n           pch = c(16, NA, NA), lwd = c(NA, 2, 2.5),\n           lty = c(NA, 2, 1))\n  })\n\n  output$error_curve &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ylim &lt;- c(0, min(max(d$test_mse) * 1.1, max(d$test_mse[1:15]) * 1.5))\n\n    plot(1:20, d$train_mse, type = \"b\", pch = 19, cex = 0.7,\n         col = \"#3498db\", lwd = 2,\n         xlab = \"Polynomial degree\",\n         ylab = \"Mean Squared Error\",\n         main = \"Train vs Test Error\",\n         ylim = ylim)\n    lines(1:20, d$test_mse, type = \"b\", pch = 17, cex = 0.7,\n          col = \"#e74c3c\", lwd = 2)\n\n    # Mark current degree\n    points(d$d_now, d$train_mse[d$d_now], pch = 19, cex = 2, col = \"#3498db\")\n    points(d$d_now, d$test_mse[d$d_now], pch = 17, cex = 2, col = \"#e74c3c\")\n\n    # Mark optimal\n    best &lt;- which.min(d$test_mse)\n    abline(v = best, lty = 3, col = \"#7f8c8d\")\n\n    abline(h = d$sigma^2, lty = 2, col = \"#95a5a6\")\n    text(15, d$sigma^2 * 1.1, expression(\"Irreducible noise (\" * sigma^2 * \")\"),\n         cex = 0.75, col = \"#95a5a6\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Training error\", \"Test error\",\n                      paste0(\"Best degree: \", best)),\n           col = c(\"#3498db\", \"#e74c3c\", \"#7f8c8d\"),\n           pch = c(19, 17, NA), lwd = c(2, 2, 1),\n           lty = c(1, 1, 3))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    best &lt;- which.min(d$test_mse)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Degree \", d$d_now, \":&lt;/b&gt;&lt;br&gt;\",\n        \"Train MSE: \", round(d$train_mse[d$d_now], 4), \"&lt;br&gt;\",\n        \"Test MSE: \", round(d$test_mse[d$d_now], 4), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Optimal degree:&lt;/b&gt; \", best, \"&lt;br&gt;\",\n        \"Test MSE: \", round(d$test_mse[best], 4), \"&lt;br&gt;\",\n        \"&lt;small&gt;Noise floor: \\u03c3\\u00b2 = \",\n        round(d$sigma^2, 3), \"&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Bias-Variance Tradeoff"
    ]
  },
  {
    "objectID": "bias-variance.html#simulation-2-slide-along-the-bias-variance-curve",
    "href": "bias-variance.html#simulation-2-slide-along-the-bias-variance-curve",
    "title": "The Bias-Variance Tradeoff",
    "section": "Simulation 2: Slide along the bias-variance curve",
    "text": "Simulation 2: Slide along the bias-variance curve\nDrag the complexity slider to move the ball along the U-shaped MSE curve — or press ▶ to animate. On the right, see what 20 fits from different training samples look like at each degree: simple models are stable but wrong (bias); complex models scatter wildly (variance).\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .ball-val { font-weight: bold; color: #e67e22; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"fn2\", \"True function:\",\n                  choices = c(\"Sine wave\",\n                              \"Quadratic\",\n                              \"Step function\",\n                              \"Wiggly sine\")),\n\n      sliderInput(\"n2\", \"Training points:\",\n                  min = 20, max = 100, value = 40, step = 10),\n\n      sliderInput(\"noise2\", \"Noise level:\",\n                  min = 0.2, max = 1.5, value = 0.5, step = 0.1),\n\n      tags$hr(),\n\n      sliderInput(\"degree2\", \"Model complexity (degree):\",\n                  min = 1, max = 12, value = 1, step = 1,\n                  animate = animationOptions(\n                    interval = 700, loop = TRUE)),\n\n      actionButton(\"go2\", \"New data\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"bv_plot\", height = \"480px\")),\n        column(6, plotOutput(\"fit_plot2\", height = \"480px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  f_true &lt;- function(x, fn) {\n    switch(fn,\n      \"Sine wave\"     = sin(2 * pi * x),\n      \"Quadratic\"     = 2 * (x - 0.5)^2,\n      \"Step function\" = ifelse(x &gt; 0.5, 1, 0),\n      \"Wiggly sine\"   = sin(4 * pi * x) + 0.5 * cos(6 * pi * x)\n    )\n  }\n\n  # Pre-compute MC results for ALL degrees at once.\n  # Only recomputes when function / n / noise / button changes —\n  # dragging the degree slider is instant.\n  mc_data &lt;- reactive({\n    input$go2\n    fn    &lt;- input$fn2\n    n     &lt;- input$n2\n    sigma &lt;- input$noise2\n    sims  &lt;- 50\n\n    x_eval &lt;- seq(0.05, 0.95, length.out = 40)\n    f_eval &lt;- f_true(x_eval, fn)\n\n    degrees   &lt;- 1:12\n    bias2_vec &lt;- numeric(12)\n    var_vec   &lt;- numeric(12)\n    pred_list &lt;- vector(\"list\", 12)\n\n    for (di in seq_along(degrees)) {\n      d &lt;- degrees[di]\n      pred_mat &lt;- matrix(NA, nrow = sims, ncol = length(x_eval))\n\n      for (s in seq_len(sims)) {\n        x_train &lt;- sort(runif(n, 0, 1))\n        y_train &lt;- f_true(x_train, fn) + rnorm(n, sd = sigma)\n        fit &lt;- tryCatch(\n          lm(y_train ~ poly(x_train, d, raw = TRUE)),\n          error = function(e) NULL)\n        if (!is.null(fit)) {\n          pred_mat[s, ] &lt;- predict(fit,\n            newdata = data.frame(x_train = x_eval))\n        }\n      }\n\n      good &lt;- complete.cases(pred_mat)\n      if (sum(good) &gt; 2) {\n        pred_mat &lt;- pred_mat[good, , drop = FALSE]\n        avg_pred &lt;- colMeans(pred_mat)\n        bias2_vec[di] &lt;- mean((avg_pred - f_eval)^2)\n        var_vec[di]   &lt;- mean(apply(pred_mat, 2, var))\n      }\n      pred_list[[di]] &lt;- pred_mat\n    }\n\n    list(degrees = degrees, bias2 = bias2_vec,\n         variance = var_vec,\n         mse = bias2_vec + var_vec,\n         sigma = sigma, x_eval = x_eval,\n         f_eval = f_eval, pred_list = pred_list,\n         fn = fn)\n  })\n\n  # LEFT PLOT: U-curve with sliding ball\n  output$bv_plot &lt;- renderPlot({\n    d   &lt;- mc_data()\n    deg &lt;- input$degree2\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ylim &lt;- c(0, max(d$mse) * 1.25)\n    x    &lt;- d$degrees\n\n    plot(x, d$mse, type = \"n\",\n         xlab = \"Polynomial degree\", ylab = \"Error\",\n         ylim = ylim,\n         main = \"Bias\\u00b2 + Variance = MSE\",\n         xaxt = \"n\")\n    axis(1, at = 1:12)\n\n    # Stacked area fills\n    polygon(c(x, rev(x)),\n            c(d$variance, rep(0, length(x))),\n            col = adjustcolor(\"#3498db\", 0.25),\n            border = NA)\n    polygon(c(x, rev(x)),\n            c(d$mse, rev(d$variance)),\n            col = adjustcolor(\"#e74c3c\", 0.25),\n            border = NA)\n\n    # Lines\n    lines(x, d$variance, col = \"#3498db\", lwd = 2)\n    lines(x, d$bias2, col = \"#e74c3c\", lwd = 2, lty = 2)\n    lines(x, d$mse, col = \"#2c3e50\", lwd = 2.5)\n\n    # Noise floor\n    abline(h = d$sigma^2, lty = 2, col = \"#95a5a6\")\n    text(11.5, d$sigma^2 + ylim[2] * 0.03,\n         expression(sigma^2), cex = 0.8, col = \"#95a5a6\")\n\n    # Optimal degree\n    best &lt;- which.min(d$mse)\n    abline(v = best, lty = 3, col = \"#7f8c8d\")\n\n    # Vertical line from ball to x-axis\n    segments(deg, 0, deg, d$mse[deg],\n             lty = 2, col = \"#e67e22\", lwd = 1.5)\n\n    # The sliding ball\n    points(deg, d$mse[deg],\n           pch = 21, bg = \"#f39c12\", col = \"#2c3e50\",\n           cex = 3.5, lwd = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"MSE (total)\",\n                      \"Bias\\u00b2\", \"Variance\",\n                      paste0(\"Optimal: degree \", best)),\n           col = c(\"#2c3e50\", \"#e74c3c\",\n                   \"#3498db\", \"#7f8c8d\"),\n           lwd = c(2.5, 2, 2, 1),\n           lty = c(1, 2, 1, 3))\n  })\n\n  # RIGHT PLOT: What fits look like at current degree\n  output$fit_plot2 &lt;- renderPlot({\n    d   &lt;- mc_data()\n    deg &lt;- input$degree2\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    pred_mat &lt;- d$pred_list[[deg]]\n    avg_pred &lt;- colMeans(pred_mat)\n    n_show   &lt;- min(20, nrow(pred_mat))\n\n    # Cap y-range so wild high-degree fits don't\n    # blow up the axis\n    preds_shown &lt;- as.vector(pred_mat[1:n_show, ])\n    yq &lt;- quantile(preds_shown,\n                   c(0.02, 0.98), na.rm = TRUE)\n    ylim &lt;- c(\n      min(yq[1], min(d$f_eval) - 0.3),\n      max(yq[2], max(d$f_eval) + 0.3))\n\n    plot(d$x_eval, d$f_eval, type = \"n\",\n         xlab = \"x\", ylab = \"f(x)\",\n         ylim = ylim,\n         main = paste0(\"Degree \", deg,\n           \" \\u2014 20 fits from different samples\"))\n\n    # Individual fits (semi-transparent)\n    for (i in 1:n_show) {\n      lines(d$x_eval, pred_mat[i, ],\n            col = adjustcolor(\"#3498db\", 0.2),\n            lwd = 1.2)\n    }\n\n    # Average prediction — gap from truth = bias\n    lines(d$x_eval, avg_pred,\n          col = \"#e74c3c\", lwd = 2.5)\n\n    # True function\n    lines(d$x_eval, d$f_eval,\n          col = \"#27ae60\", lwd = 2.5, lty = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"True f(x)\",\n                      \"Average prediction\",\n                      \"Individual fits\"),\n           col = c(\"#27ae60\", \"#e74c3c\",\n                   adjustcolor(\"#3498db\", 0.5)),\n           lwd = c(2.5, 2.5, 1.2),\n           lty = c(2, 1, 1))\n  })\n\n  # Stats box\n  output$results2 &lt;- renderUI({\n    d   &lt;- mc_data()\n    deg &lt;- input$degree2\n    best &lt;- which.min(d$mse)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Degree \", deg, \":&lt;/b&gt;&lt;br&gt;\",\n        \"Bias\\u00b2: \", round(d$bias2[deg], 4), \"&lt;br&gt;\",\n        \"Variance: \", round(d$variance[deg], 4), \"&lt;br&gt;\",\n        \"&lt;span class='ball-val'&gt;MSE: \",\n        round(d$mse[deg], 4), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Optimal:&lt;/b&gt; degree \", best,\n        \" (MSE \", round(d$mse[best], 4), \")\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nSine wave, degree 1: the linear fit can’t capture the curve — high bias, low variance. Classic underfitting.\nSine wave, degree 20: the polynomial goes wild between data points — low bias, high variance. Classic overfitting.\nSine wave, degree 3–5: the sweet spot where bias and variance are balanced. The test error curve reaches its minimum.\nIncrease noise: the optimal degree shifts down. Noisier data means simpler models are better — there’s less signal to extract.\nQuadratic true function, degree 2: the model matches the DGP perfectly. Bias is essentially zero. Going beyond degree 2 only adds variance.\n\n\n\nThe bottom line\n\nSimple models are biased but stable. They miss patterns in the data but give consistent predictions across samples.\nComplex models are flexible but noisy. They capture the training data perfectly but their predictions change wildly with new data.\nThe best model balances both. The U-shaped test error curve is the signature of the bias-variance tradeoff.\nIn practice, we use cross-validation to find the sweet spot without needing access to the true function.\n\n\n\n\nDid you know?\n\nThe bias-variance decomposition was formalized by Stuart Geman and Donald Geman in their landmark 1984 paper on image processing. But the intuition goes back much further — Charles Stein showed in 1956 that the sample mean is inadmissible (not the best estimator) in dimensions ≥ 3. The James-Stein estimator reduces MSE by shrinking toward the grand mean — trading bias for variance. This result was so counterintuitive that Stein’s colleagues initially didn’t believe it.\nThe bias-variance tradeoff is the theoretical foundation of regularization methods like ridge regression, LASSO, and elastic net. These methods intentionally introduce bias (by shrinking coefficients) to dramatically reduce variance, resulting in better predictions overall.\nCross-validation — the practical tool for navigating the tradeoff — was proposed by Seymour Geisser in 1975 and popularized by Mervyn Stone in the same year. The leave-one-out version was known even earlier.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Bias-Variance Tradeoff"
    ]
  },
  {
    "objectID": "limited-dependent.html",
    "href": "limited-dependent.html",
    "title": "Limited Dependent Variables",
    "section": "",
    "text": "When \\(Y \\in \\{0, 1\\}\\), the linear probability model (LPM) treats the regression \\(Y = X'\\beta + \\varepsilon\\) as usual. It works — the coefficients estimate the change in \\(P(Y=1)\\) per unit change in \\(X\\). But there are two problems:\n\nPredictions outside \\([0,1]\\). A linear function eventually predicts negative probabilities or probabilities above 1.\nHeteroskedastic errors. Since \\(Y\\) is binary, \\(\\text{Var}(\\varepsilon | X) = P(1-P)\\), which depends on \\(X\\). OLS standard errors are wrong (use robust SEs).\n\nNear the center of the data, the LPM is often fine. But when the true probability is strongly nonlinear — near 0 or 1 — the LPM misses the shape.",
    "crumbs": [
      "Estimation",
      "Limited Dependent Variables"
    ]
  },
  {
    "objectID": "limited-dependent.html#the-problem-with-ols-for-binary-outcomes",
    "href": "limited-dependent.html#the-problem-with-ols-for-binary-outcomes",
    "title": "Limited Dependent Variables",
    "section": "",
    "text": "When \\(Y \\in \\{0, 1\\}\\), the linear probability model (LPM) treats the regression \\(Y = X'\\beta + \\varepsilon\\) as usual. It works — the coefficients estimate the change in \\(P(Y=1)\\) per unit change in \\(X\\). But there are two problems:\n\nPredictions outside \\([0,1]\\). A linear function eventually predicts negative probabilities or probabilities above 1.\nHeteroskedastic errors. Since \\(Y\\) is binary, \\(\\text{Var}(\\varepsilon | X) = P(1-P)\\), which depends on \\(X\\). OLS standard errors are wrong (use robust SEs).\n\nNear the center of the data, the LPM is often fine. But when the true probability is strongly nonlinear — near 0 or 1 — the LPM misses the shape.",
    "crumbs": [
      "Estimation",
      "Limited Dependent Variables"
    ]
  },
  {
    "objectID": "limited-dependent.html#logit-and-probit",
    "href": "limited-dependent.html#logit-and-probit",
    "title": "Limited Dependent Variables",
    "section": "Logit and probit",
    "text": "Logit and probit\nBoth models pass \\(X'\\beta\\) through a nonlinear link function to keep predictions in \\([0, 1]\\):\n\\[P(Y = 1 \\mid X) = F(X'\\beta)\\]\n\n\n\nModel\nLink function \\(F\\)\nFormula\n\n\n\n\nLogit\nLogistic CDF\n\\(\\Lambda(z) = \\dfrac{e^z}{1 + e^z}\\)\n\n\nProbit\nNormal CDF\n\\(\\Phi(z)\\)\n\n\n\nBoth are estimated by maximum likelihood. In practice, logit and probit give nearly identical results — the logistic and normal CDFs are almost the same shape when properly scaled.",
    "crumbs": [
      "Estimation",
      "Limited Dependent Variables"
    ]
  },
  {
    "objectID": "limited-dependent.html#simulation",
    "href": "limited-dependent.html#simulation",
    "title": "Limited Dependent Variables",
    "section": "Simulation",
    "text": "Simulation\nBinary data with a nonlinear true probability. The LPM fits a straight line; logit and probit fit S-curves. Drag the coefficient to make the nonlinearity more extreme.\n#| standalone: true\n#| viewerHeight: 700\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .eq-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-bottom: 14px; font-size: 14px; line-height: 1.9;\n    }\n    .eq-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 4,\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 100, max = 1000, value = 300, step = 50),\n\n      sliderInput(\"beta\", HTML(\"Coefficient (&beta;):\"),\n                  min = 0.5, max = 5, value = 2, step = 0.25),\n\n      sliderInput(\"intercept\", HTML(\"Intercept (&alpha;):\"),\n                  min = -3, max = 3, value = 0, step = 0.5),\n\n      actionButton(\"resim\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_box\")\n    ),\n\n    mainPanel(\n      width = 8,\n      plotOutput(\"main_plot\", height = \"500px\"),\n      uiOutput(\"note_box\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$resim\n    n    &lt;- input$n\n    beta &lt;- input$beta\n    a    &lt;- input$intercept\n\n    x &lt;- rnorm(n)\n    # True probability via logistic function\n    p_true &lt;- plogis(a + beta * x)\n    y &lt;- rbinom(n, 1, p_true)\n\n    # Fit models\n    lpm_fit   &lt;- lm(y ~ x)\n    logit_fit &lt;- glm(y ~ x, family = binomial(link = \"logit\"))\n    probit_fit &lt;- glm(y ~ x, family = binomial(link = \"probit\"))\n\n    x_grid &lt;- seq(min(x) - 0.5, max(x) + 0.5, length.out = 300)\n\n    lpm_pred   &lt;- coef(lpm_fit)[1] + coef(lpm_fit)[2] * x_grid\n    logit_pred &lt;- plogis(coef(logit_fit)[1] + coef(logit_fit)[2] * x_grid)\n    probit_pred &lt;- pnorm(coef(probit_fit)[1] + coef(probit_fit)[2] * x_grid)\n    true_pred  &lt;- plogis(a + beta * x_grid)\n\n    list(x = x, y = y, x_grid = x_grid,\n         lpm_pred = lpm_pred, logit_pred = logit_pred,\n         probit_pred = probit_pred, true_pred = true_pred,\n         lpm_fit = lpm_fit, logit_fit = logit_fit,\n         probit_fit = probit_fit,\n         beta = beta, a = a)\n  })\n\n  output$main_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(5, 5, 4, 2))\n\n    # Jitter y slightly for visibility\n    y_jit &lt;- d$y + runif(length(d$y), -0.03, 0.03)\n\n    plot(d$x, y_jit, pch = 16, col = adjustcolor(\"#95a5a6\", 0.3),\n         cex = 0.6, xlab = \"X\", ylab = \"P(Y = 1)\",\n         main = \"LPM vs Logit vs Probit\",\n         ylim = c(-0.15, 1.15))\n\n    abline(h = c(0, 1), lty = 3, col = \"#bdc3c7\")\n\n    # True probability\n    lines(d$x_grid, d$true_pred, col = \"#2c3e50\", lwd = 2, lty = 2)\n\n    # LPM\n    lines(d$x_grid, d$lpm_pred, col = \"#e74c3c\", lwd = 2.5)\n\n    # Logit\n    lines(d$x_grid, d$logit_pred, col = \"#3498db\", lwd = 2.5)\n\n    # Probit\n    lines(d$x_grid, d$probit_pred, col = \"#27ae60\", lwd = 2.5)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.9,\n           legend = c(\"True P(Y=1)\", \"LPM (OLS)\", \"Logit\", \"Probit\"),\n           col = c(\"#2c3e50\", \"#e74c3c\", \"#3498db\", \"#27ae60\"),\n           lwd = c(2, 2.5, 2.5, 2.5),\n           lty = c(2, 1, 1, 1))\n  })\n\n  output$results_box &lt;- renderUI({\n    d &lt;- dat()\n    lpm_b   &lt;- round(coef(d$lpm_fit)[2], 3)\n    logit_b &lt;- round(coef(d$logit_fit)[2], 3)\n    probit_b &lt;- round(coef(d$probit_fit)[2], 3)\n    ame_logit &lt;- round(mean(dlogis(predict(d$logit_fit, type = \"link\"))) *\n                       coef(d$logit_fit)[2], 3)\n\n    tags$div(class = \"eq-box\", style = \"margin-top: 16px;\",\n      HTML(paste0(\n        \"&lt;b&gt;Estimated coefficients:&lt;/b&gt;&lt;br&gt;\",\n        \"LPM: &lt;b&gt;\", lpm_b, \"&lt;/b&gt; (= marginal effect)&lt;br&gt;\",\n        \"Logit: &lt;b&gt;\", logit_b, \"&lt;/b&gt; (log-odds)&lt;br&gt;\",\n        \"Probit: &lt;b&gt;\", probit_b, \"&lt;/b&gt; (latent index)&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Logit AME:&lt;/b&gt; \", ame_logit, \"&lt;br&gt;\",\n        \"&lt;b&gt;Logit &beta;/4:&lt;/b&gt; \", round(logit_b / 4, 3), \"&lt;br&gt;\",\n        \"&lt;small&gt;(&beta;/4 approximates the AME)&lt;/small&gt;\"\n      ))\n    )\n  })\n\n  output$note_box &lt;- renderUI({\n    tags$div(class = \"eq-box\", style = \"margin-top: 8px;\",\n      HTML(paste0(\n        \"&lt;b&gt;Notice:&lt;/b&gt; The logit coefficient is &lt;i&gt;not&lt;/i&gt; the marginal effect. \",\n        \"The LPM slope (red) directly reads as &Delta;P per unit &Delta;X, \",\n        \"but the logit/probit slopes refer to the latent index. \",\n        \"Use the AME or the &beta;/4 rule to translate.\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Estimation",
      "Limited Dependent Variables"
    ]
  },
  {
    "objectID": "limited-dependent.html#marginal-effects",
    "href": "limited-dependent.html#marginal-effects",
    "title": "Limited Dependent Variables",
    "section": "Marginal effects",
    "text": "Marginal effects\nIn the LPM, \\(\\beta\\) is the marginal effect: a one-unit increase in \\(X\\) changes \\(P(Y=1)\\) by \\(\\beta\\), everywhere.\nIn logit/probit, \\(\\beta\\) enters through the nonlinear link, so the marginal effect depends on where you are:\n\\[\\frac{\\partial P(Y=1)}{\\partial X_j} = f(X'\\beta) \\cdot \\beta_j\\]\nwhere \\(f = F'\\) is the density of the link function (logistic density for logit, normal density for probit). Two common summaries:\n\n\n\n\n\n\n\n\nSummary\nDefinition\nUse\n\n\n\n\nAverage Marginal Effect (AME)\n\\(\\frac{1}{n}\\sum_i f(X_i'\\hat{\\beta}) \\cdot \\hat{\\beta}_j\\)\nAverage effect across the sample\n\n\nMarginal Effect at the Mean (MEM)\n\\(f(\\bar{X}'\\hat{\\beta}) \\cdot \\hat{\\beta}_j\\)\nEffect for a “typical” observation\n\n\n\nQuick approximation: for logit, the maximum of the logistic density is \\(1/4\\) (at \\(X'\\beta = 0\\)). So as an upper bound, \\(\\text{AME} \\approx \\hat{\\beta}/4\\). This is rough but often surprisingly close.\nStandard errors for marginal effects require the delta method since they are nonlinear functions of \\(\\hat{\\beta}\\).",
    "crumbs": [
      "Estimation",
      "Limited Dependent Variables"
    ]
  },
  {
    "objectID": "limited-dependent.html#tobit-censored-outcomes",
    "href": "limited-dependent.html#tobit-censored-outcomes",
    "title": "Limited Dependent Variables",
    "section": "Tobit: censored outcomes",
    "text": "Tobit: censored outcomes\nWhen the dependent variable is continuous but censored — observed only above (or below) a threshold — OLS on the observed data is biased. The Tobit model handles this:\n\\[Y_i^* = X_i'\\beta + \\varepsilon_i, \\qquad Y_i = \\max(0, Y_i^*)\\]\nwhere \\(Y^*\\) is the latent (uncensored) variable. This is estimated by MLE, combining a probit-like component (probability of being censored) with a truncated normal component (density for uncensored observations).\nThe same logic extends to Heckman selection models, where the censoring isn’t mechanical but results from a separate decision — see Sample Selection & Heckman.\n\n\nConnections\n\nMaximum Likelihood — Logit and probit are MLE estimators\nTraining as MLE — Cross-entropy loss in neural nets is exactly logistic regression’s log-likelihood\nThe Delta Method — Required for marginal effect standard errors\nSample Selection & Heckman — Extends the Tobit idea to non-mechanical selection\n\n\n\n\nDid you know?\n\nThe logistic function was introduced by Pierre-François Verhulst in 1838 to model population growth, not binary outcomes. It took over a century before statisticians adopted it for regression.\nJoseph Berkson coined the term “logit” in 1944 as a portmanteau of “logistic unit,” by analogy with “probit” (probability unit).\nJames Tobin won the 1981 Nobel Prize in Economics. His 1958 Tobit paper (the name is a blend of “Tobin” and “probit”) introduced the censored regression model. He was reportedly amused by the name, which he didn’t coin himself.",
    "crumbs": [
      "Estimation",
      "Limited Dependent Variables"
    ]
  },
  {
    "objectID": "degeneracy.html",
    "href": "degeneracy.html",
    "title": "When Inference Breaks Down",
    "section": "",
    "text": "You need variation to learn anything. When variation disappears in some dimension, the math collapses.\nEvery formula in this course has a denominator that measures variation somewhere. When that variation goes to zero, the formula divides by zero and inference breaks down:\n\n\n\n\n\n\n\n\nWhat disappears\nWhat breaks\nWhy\n\n\n\n\nVariation in data (all values identical)\nSD = 0, SE = 0, CI has zero width\nNothing to estimate — your “sample” is a constant\n\n\nVariation in sample size (n = 1)\n\\(s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\\) divides by 0\nCan’t measure spread from a single point\n\n\nVariation in X (regressor is constant)\nOLS slope is undefined\nCan’t draw a line through a vertical stack of points\n\n\nVariation between regressors (perfect collinearity)\nIndividual coefficients undefined\nCan’t separate the effect of X1 from X2 if they move together perfectly\n\n\n\nThe simulations below let you push each slider to the edge and watch the math collapse in real time.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we set \\(\\sigma\\), the true slope \\(\\beta\\), and \\(SD(X)\\) — we control the data-generating process and can push parameters to their breaking point. In practice, you estimate all of these from data. The danger is that near-degeneracy (tiny Var(X), high collinearity) can silently produce huge SEs without your software raising an error.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "When Inference Breaks Down"
    ]
  },
  {
    "objectID": "degeneracy.html#the-common-thread",
    "href": "degeneracy.html#the-common-thread",
    "title": "When Inference Breaks Down",
    "section": "",
    "text": "You need variation to learn anything. When variation disappears in some dimension, the math collapses.\nEvery formula in this course has a denominator that measures variation somewhere. When that variation goes to zero, the formula divides by zero and inference breaks down:\n\n\n\n\n\n\n\n\nWhat disappears\nWhat breaks\nWhy\n\n\n\n\nVariation in data (all values identical)\nSD = 0, SE = 0, CI has zero width\nNothing to estimate — your “sample” is a constant\n\n\nVariation in sample size (n = 1)\n\\(s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\\) divides by 0\nCan’t measure spread from a single point\n\n\nVariation in X (regressor is constant)\nOLS slope is undefined\nCan’t draw a line through a vertical stack of points\n\n\nVariation between regressors (perfect collinearity)\nIndividual coefficients undefined\nCan’t separate the effect of X1 from X2 if they move together perfectly\n\n\n\nThe simulations below let you push each slider to the edge and watch the math collapse in real time.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we set \\(\\sigma\\), the true slope \\(\\beta\\), and \\(SD(X)\\) — we control the data-generating process and can push parameters to their breaking point. In practice, you estimate all of these from data. The danger is that near-degeneracy (tiny Var(X), high collinearity) can silently produce huge SEs without your software raising an error.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "When Inference Breaks Down"
    ]
  },
  {
    "objectID": "degeneracy.html#simulation-1-too-little-data",
    "href": "degeneracy.html#simulation-1-too-little-data",
    "title": "When Inference Breaks Down",
    "section": "Simulation 1: Too little data",
    "text": "Simulation 1: Too little data\nThe standard error is \\(SE = s / \\sqrt{n}\\). As \\(n\\) shrinks, the SE explodes — your estimate becomes useless. At \\(n = 1\\), the sample variance \\(s^2\\) is undefined (dividing by \\(n - 1 = 0\\)).\nWatch the confidence interval expand until it swallows everything.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .warning { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 1, max = 50, value = 30, step = 1),\n\n      sliderInput(\"true_mu\", \"True mean:\",\n                  min = 0, max = 100, value = 50, step = 5),\n\n      sliderInput(\"true_sd\", \"Population SD:\",\n                  min = 5, max = 30, value = 15, step = 1),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"ci_plot\", height = \"550px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  sim &lt;- reactive({\n    n   &lt;- input$n\n    mu  &lt;- input$true_mu\n    sig &lt;- input$true_sd\n\n    samp &lt;- rnorm(n, mu, sig)\n    xbar &lt;- mean(samp)\n\n    if (n == 1) {\n      list(samp = samp, xbar = xbar, se = NA, ci_lo = NA, ci_hi = NA,\n           s = NA, n = n, mu = mu, degenerate = TRUE)\n    } else {\n      s  &lt;- sd(samp)\n      se &lt;- s / sqrt(n)\n      tc &lt;- qt(0.975, df = n - 1)\n      ci_lo &lt;- xbar - tc * se\n      ci_hi &lt;- xbar + tc * se\n      list(samp = samp, xbar = xbar, se = se, ci_lo = ci_lo, ci_hi = ci_hi,\n           s = s, n = n, mu = mu, tc = tc, degenerate = FALSE)\n    }\n  })\n\n  output$ci_plot &lt;- renderPlot({\n    d &lt;- sim()\n    par(mar = c(4.5, 4.5, 3.5, 1))\n\n    if (d$degenerate) {\n      plot(1, d$samp[1], pch = 19, cex = 2, col = \"#e74c3c\",\n           xlim = c(0.5, 1.5), ylim = c(d$mu - 60, d$mu + 60),\n           xlab = \"\", ylab = \"Value\", xaxt = \"n\",\n           main = \"n = 1: Variance is UNDEFINED\")\n      abline(h = d$mu, col = \"#27ae60\", lwd = 2, lty = 2)\n      text(1, d$mu + 5, paste0(\"True mean = \", d$mu), col = \"#27ae60\", cex = 1.1)\n      text(1, d$samp[1] - 5, paste0(\"Your one observation = \", round(d$samp[1], 1)),\n           col = \"#e74c3c\", cex = 1.1)\n      text(1, d$mu - 40,\n           \"Can't compute SD, SE, or CI from a single point.\",\n           cex = 1.3, font = 2, col = \"#e74c3c\")\n    } else {\n      ylim &lt;- range(c(d$ci_lo, d$ci_hi, d$samp, d$mu)) + c(-10, 10)\n\n      plot(seq_along(d$samp), d$samp, pch = 16, cex = 0.8,\n           col = adjustcolor(\"#3498db\", 0.6),\n           xlim = c(0, d$n + 1), ylim = ylim,\n           xlab = \"Observation\", ylab = \"Value\",\n           main = paste0(\"n = \", d$n, \" — 95% CI\"))\n\n      abline(h = d$mu, col = \"#27ae60\", lwd = 2, lty = 2)\n      abline(h = d$xbar, col = \"#2c3e50\", lwd = 2)\n\n      # CI band\n      rect(0, d$ci_lo, d$n + 1, d$ci_hi,\n           col = adjustcolor(\"#e74c3c\", 0.1), border = NA)\n      abline(h = d$ci_lo, col = \"#e74c3c\", lwd = 1.5, lty = 3)\n      abline(h = d$ci_hi, col = \"#e74c3c\", lwd = 1.5, lty = 3)\n\n      legend(\"topright\", bty = \"n\", cex = 0.9,\n             legend = c(paste0(\"True mean = \", d$mu),\n                        paste0(\"Sample mean = \", round(d$xbar, 1)),\n                        paste0(\"95% CI: [\", round(d$ci_lo, 1), \", \",\n                               round(d$ci_hi, 1), \"]\"),\n                        paste0(\"CI width = \", round(d$ci_hi - d$ci_lo, 1))),\n             col = c(\"#27ae60\", \"#2c3e50\", \"#e74c3c\", NA),\n             lwd = c(2, 2, 1.5, NA), lty = c(2, 1, 3, NA))\n    }\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- sim()\n\n    if (d$degenerate) {\n      tags$div(class = \"stats-box\",\n        HTML(paste0(\n          \"&lt;span class='warning'&gt;DEGENERATE&lt;/span&gt;&lt;br&gt;\",\n          \"&lt;b&gt;n = 1&lt;/b&gt;&lt;br&gt;\",\n          \"s = undefined (0/0)&lt;br&gt;\",\n          \"SE = undefined&lt;br&gt;\",\n          \"CI = undefined&lt;br&gt;\",\n          \"&lt;hr style='margin:8px 0'&gt;\",\n          \"&lt;small&gt;You used up your one degree\",\n          \" of freedom estimating the mean.\",\n          \" None left to estimate spread.&lt;/small&gt;\"\n        ))\n      )\n    } else {\n      tags$div(class = \"stats-box\",\n        HTML(paste0(\n          \"&lt;b&gt;n = &lt;/b&gt;\", d$n, \"&lt;br&gt;\",\n          \"&lt;b&gt;s = &lt;/b&gt;\", round(d$s, 2), \"&lt;br&gt;\",\n          \"&lt;b&gt;SE = &lt;/b&gt;\", round(d$se, 2), \"&lt;br&gt;\",\n          \"&lt;b&gt;t critical = &lt;/b&gt;\", round(d$tc, 2), \"&lt;br&gt;\",\n          \"&lt;b&gt;CI width = &lt;/b&gt;\", round(d$ci_hi - d$ci_lo, 1), \"&lt;br&gt;\",\n          \"&lt;hr style='margin:8px 0'&gt;\",\n          \"&lt;small&gt;df = \", d$n - 1, \"&lt;/small&gt;\"\n        ))\n      )\n    }\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nn = 30: normal-looking CI, reasonably tight around the mean.\nn = 5: CI balloons — the t critical value jumps from ~2 to ~2.8, and the SE is much larger. Your estimate is barely useful.\nn = 2: CI is enormous. The t critical value for df = 1 is 12.7 — you need the sample mean to be over 12 SEs away from zero to reject. You have almost no power.\nn = 1: the math breaks. You can’t compute variance, SE, or a CI. One observation tells you nothing about uncertainty.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "When Inference Breaks Down"
    ]
  },
  {
    "objectID": "degeneracy.html#simulation-2-no-variation-in-x",
    "href": "degeneracy.html#simulation-2-no-variation-in-x",
    "title": "When Inference Breaks Down",
    "section": "Simulation 2: No variation in X",
    "text": "Simulation 2: No variation in X\nThe OLS slope formula is:\n\\[\\hat{\\beta} = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)}\\]\nIf \\(\\text{Var}(X) = 0\\) (the regressor is constant), you’re dividing by zero. You can’t estimate a slope because there’s no variation in X to “explain” variation in Y.\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .warning { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"sd_x\", \"SD of X (variation in regressor):\",\n                  min = 0, max = 5, value = 3, step = 0.1),\n\n      sliderInput(\"beta_true\", \"True slope:\",\n                  min = 0.5, max = 3, value = 1.5, step = 0.1),\n\n      sliderInput(\"n_obs\", \"Sample size:\",\n                  min = 30, max = 200, value = 100, step = 10),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"ols_plot\", height = \"500px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  base_noise &lt;- reactive({\n    n &lt;- input$n_obs\n    list(z = rnorm(n), u = rnorm(n))\n  })\n\n  sim &lt;- reactive({\n    d    &lt;- base_noise()\n    sd_x &lt;- input$sd_x\n    b    &lt;- input$beta_true\n    n    &lt;- input$n_obs\n\n    x &lt;- 5 + sd_x * d$z\n    y &lt;- 1 + b * x + d$u\n\n    if (sd_x &lt; 0.05) {\n      list(x = x, y = y, beta = b, sd_x = sd_x,\n           degenerate = TRUE)\n    } else {\n      fit &lt;- lm(y ~ x)\n      ci  &lt;- confint(fit)[2, ]\n      se  &lt;- summary(fit)$coefficients[2, 2]\n      list(x = x, y = y, beta = b, sd_x = sd_x,\n           b_hat = coef(fit)[2], se = se,\n           ci_lo = ci[1], ci_hi = ci[2],\n           degenerate = FALSE)\n    }\n  })\n\n  output$ols_plot &lt;- renderPlot({\n    d &lt;- sim()\n    par(mar = c(4.5, 4.5, 3.5, 1))\n\n    plot(d$x, d$y, pch = 16, cex = 0.7,\n         col = adjustcolor(\"#3498db\", 0.5),\n         xlab = \"X\", ylab = \"Y\",\n         main = if (d$degenerate) \"Var(X) = 0: Slope is UNDEFINED\"\n                else paste0(\"SD(X) = \", round(d$sd_x, 1),\n                            \" — Slope SE = \", round(d$se, 2)))\n\n    if (!d$degenerate) {\n      abline(lm(d$y ~ d$x), col = \"#e74c3c\", lwd = 3)\n      abline(a = 1, b = d$beta, col = \"#27ae60\", lwd = 2, lty = 2)\n      legend(\"topleft\", bty = \"n\", cex = 0.9,\n             legend = c(paste0(\"OLS: \", round(d$b_hat, 3)),\n                        paste0(\"True: \", d$beta)),\n             col = c(\"#e74c3c\", \"#27ae60\"), lwd = c(3, 2), lty = c(1, 2))\n    } else {\n      text(mean(d$x), mean(d$y),\n           \"All X values are the same.\\nCan't fit a line through\\na vertical stack of points.\",\n           cex = 1.4, font = 2, col = \"#e74c3c\")\n    }\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- sim()\n\n    if (d$degenerate) {\n      tags$div(class = \"stats-box\",\n        HTML(paste0(\n          \"&lt;span class='warning'&gt;DEGENERATE&lt;/span&gt;&lt;br&gt;\",\n          \"Var(X) = 0&lt;br&gt;\",\n          \"Slope = 0/0 = undefined&lt;br&gt;\",\n          \"SE = undefined&lt;br&gt;\",\n          \"&lt;hr style='margin:8px 0'&gt;\",\n          \"&lt;small&gt;No variation in X means you\",\n          \" can't tell if Y changes with X.\",\n          \" Every value of \\u03b2 fits equally well.&lt;/small&gt;\"\n        ))\n      )\n    } else {\n      tags$div(class = \"stats-box\",\n        HTML(paste0(\n          \"&lt;b&gt;True \\u03b2:&lt;/b&gt; \", d$beta, \"&lt;br&gt;\",\n          \"&lt;b&gt;OLS \\u03b2:&lt;/b&gt; \", round(d$b_hat, 3), \"&lt;br&gt;\",\n          \"&lt;b&gt;SE:&lt;/b&gt; \", round(d$se, 3), \"&lt;br&gt;\",\n          \"&lt;b&gt;95% CI:&lt;/b&gt; [\", round(d$ci_lo, 3),\n          \", \", round(d$ci_hi, 3), \"]&lt;br&gt;\",\n          \"&lt;b&gt;CI width:&lt;/b&gt; \", round(d$ci_hi - d$ci_lo, 3), \"&lt;br&gt;\",\n          \"&lt;hr style='margin:8px 0'&gt;\",\n          \"&lt;small&gt;SE(\\u03b2) = \\u03c3 / [\\u221an \\u00b7 SD(X)]&lt;br&gt;\",\n          \"Less variation in X \\u2192 larger SE&lt;/small&gt;\"\n        ))\n      )\n    }\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nSD(X) = 3: data fans out horizontally, OLS estimates the slope precisely.\nSD(X) = 1: the cloud compresses horizontally. SE roughly triples — harder to pin down the slope.\nSD(X) = 0.3: the data is nearly a vertical column. The SE is huge, the CI is wide, and the estimate bounces wildly.\nSD(X) = 0: complete degeneracy. All X values are identical. You can’t draw a line through a point.\n\nThe formula makes it obvious: \\(SE(\\hat{\\beta}) = \\frac{\\sigma}{\\sqrt{n} \\cdot SD(X)}\\). When \\(SD(X) \\to 0\\), the SE \\(\\to \\infty\\).",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "When Inference Breaks Down"
    ]
  },
  {
    "objectID": "degeneracy.html#simulation-3-multicollinearity",
    "href": "degeneracy.html#simulation-3-multicollinearity",
    "title": "When Inference Breaks Down",
    "section": "Simulation 3: Multicollinearity",
    "text": "Simulation 3: Multicollinearity\nWhen two regressors are highly correlated, OLS can estimate their combined effect but can’t tell which one is doing the work. The individual coefficient SEs explode even though predictions remain fine.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .warning { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"rho\", \"Correlation between X1 and X2:\",\n                  min = 0, max = 0.99, value = 0.3, step = 0.01),\n\n      sliderInput(\"b1\", \"True effect of X1:\",\n                  min = 0.5, max = 3, value = 1.5, step = 0.25),\n\n      sliderInput(\"b2\", \"True effect of X2:\",\n                  min = 0.5, max = 3, value = 1.0, step = 0.25),\n\n      sliderInput(\"n3\", \"Sample size:\",\n                  min = 50, max = 300, value = 100, step = 50),\n\n      uiOutput(\"results3\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"coef_plot\", height = \"500px\")),\n        column(6, plotOutput(\"vif_plot\", height = \"500px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  sim &lt;- reactive({\n    n   &lt;- input$n3\n    rho &lt;- input$rho\n    b1  &lt;- input$b1\n    b2  &lt;- input$b2\n\n    nsims &lt;- 300\n\n    betas1 &lt;- numeric(nsims)\n    betas2 &lt;- numeric(nsims)\n\n    for (i in seq_len(nsims)) {\n      z1 &lt;- rnorm(n)\n      z2 &lt;- rnorm(n)\n      x1 &lt;- z1\n      x2 &lt;- rho * z1 + sqrt(1 - rho^2) * z2\n      y  &lt;- 1 + b1 * x1 + b2 * x2 + rnorm(n)\n      fit &lt;- lm(y ~ x1 + x2)\n      betas1[i] &lt;- coef(fit)[2]\n      betas2[i] &lt;- coef(fit)[3]\n    }\n\n    vif &lt;- 1 / (1 - rho^2)\n\n    list(betas1 = betas1, betas2 = betas2,\n         b1 = b1, b2 = b2, rho = rho, vif = vif)\n  })\n\n  output$coef_plot &lt;- renderPlot({\n    d &lt;- sim()\n    par(mar = c(4.5, 4.5, 3.5, 1))\n\n    xlim &lt;- range(c(d$betas1, d$betas2, d$b1, d$b2)) + c(-0.3, 0.3)\n\n    hist(d$betas1, breaks = 30,\n         col = adjustcolor(\"#3498db\", 0.5), border = \"white\",\n         main = paste0(\"Coefficient estimates (\\u03c1 = \", d$rho, \")\"),\n         xlab = \"Estimated coefficient\", xlim = xlim,\n         freq = FALSE, cex.main = 1.2)\n    hist(d$betas2, breaks = 30,\n         col = adjustcolor(\"#e74c3c\", 0.4), border = \"white\",\n         add = TRUE, freq = FALSE)\n\n    abline(v = d$b1, col = \"#3498db\", lwd = 2.5, lty = 2)\n    abline(v = d$b2, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\n             paste0(\"X1 (true=\", d$b1, \", SD=\", round(sd(d$betas1), 3), \")\"),\n             paste0(\"X2 (true=\", d$b2, \", SD=\", round(sd(d$betas2), 3), \")\")\n           ),\n           fill = c(adjustcolor(\"#3498db\", 0.5),\n                    adjustcolor(\"#e74c3c\", 0.4)),\n           border = \"white\")\n  })\n\n  output$vif_plot &lt;- renderPlot({\n    d &lt;- sim()\n    par(mar = c(4.5, 4.5, 3.5, 1))\n\n    rhos &lt;- seq(0, 0.99, by = 0.01)\n    vifs &lt;- 1 / (1 - rhos^2)\n\n    plot(rhos, vifs, type = \"l\", lwd = 2.5, col = \"#2c3e50\",\n         xlab = \"Correlation between X1 and X2\",\n         ylab = \"Variance Inflation Factor (VIF)\",\n         main = \"VIF: How much SEs inflate\",\n         ylim = c(1, min(50, max(vifs))),\n         cex.main = 1.2)\n\n    points(d$rho, d$vif, pch = 19, cex = 2, col = \"#e74c3c\")\n    segments(d$rho, 1, d$rho, d$vif, lty = 2, col = \"#e74c3c\")\n\n    abline(h = 1, lty = 2, col = \"#95a5a6\")\n    abline(h = 10, lty = 3, col = \"#e67e22\")\n    text(0.5, 11, \"Rule of thumb: VIF &gt; 10 = serious problem\",\n         cex = 0.8, col = \"#e67e22\")\n\n    text(d$rho + 0.03, d$vif + 1.5,\n         paste0(\"VIF = \", round(d$vif, 1)),\n         col = \"#e74c3c\", font = 2, cex = 1.1, adj = 0)\n  })\n\n  output$results3 &lt;- renderUI({\n    d &lt;- sim()\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        if (d$vif &gt; 10) \"&lt;span class='warning'&gt;HIGH COLLINEARITY&lt;/span&gt;&lt;br&gt;\" else \"\",\n        \"&lt;b&gt;Correlation:&lt;/b&gt; \", d$rho, \"&lt;br&gt;\",\n        \"&lt;b&gt;VIF:&lt;/b&gt; \", round(d$vif, 1), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;SD of \\u03b21 estimates:&lt;/b&gt; \", round(sd(d$betas1), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;SD of \\u03b22 estimates:&lt;/b&gt; \", round(sd(d$betas2), 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;VIF = 1/(1 \\u2212 \\u03c1\\u00b2)&lt;br&gt;\",\n        \"SE inflates by \\u221aVIF = \",\n        round(sqrt(d$vif), 2), \"x&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\n\\(\\rho\\) = 0: the two coefficient distributions are tight, centered on their true values. No collinearity, no problem.\n\\(\\rho\\) = 0.7: distributions visibly wider. VIF = 2. SEs inflate by ~40%.\n\\(\\rho\\) = 0.9: distributions are quite wide. VIF = 5.3. Hard to tell X1 and X2 apart.\n\\(\\rho\\) = 0.99: distributions are enormous. VIF = 50. The estimates are unbiased (still centered at the truth!) but so noisy they’re useless. One regression might say X1 has a huge effect and X2 has none; the next says the opposite.\n\nThe key insight: multicollinearity doesn’t cause bias — it causes imprecision. OLS still gets it right on average, but any single estimate can be wildly off.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "When Inference Breaks Down"
    ]
  },
  {
    "objectID": "degeneracy.html#the-unifying-principle",
    "href": "degeneracy.html#the-unifying-principle",
    "title": "When Inference Breaks Down",
    "section": "The unifying principle",
    "text": "The unifying principle\nEvery formula in statistics has variation in the denominator:\n\n\n\n\n\n\n\n\nFormula\nDenominator contains\nGoes to zero when\n\n\n\n\n\\(s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\\)\n\\(n - 1\\)\n\\(n = 1\\)\n\n\n\\(SE = \\frac{s}{\\sqrt{n}}\\)\n\\(\\sqrt{n}\\)\n\\(n = 0\\) (no data)\n\n\n\\(\\hat{\\beta} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}\\)\n\\(\\text{Var}(X)\\)\nAll X values identical\n\n\n\\(SE(\\hat{\\beta}_j) \\propto \\frac{1}{\\sqrt{1 - R_j^2}}\\)\n\\(1 - R_j^2\\)\nPerfect collinearity (\\(R_j^2 = 1\\))\n\n\n\nVariation is the fuel of inference. No variation, no learning.\n\nNo variation in Y (your outcome) → can’t estimate spread or uncertainty\nNo variation in X (your regressor) → can’t estimate slopes\nNo variation between X1 and X2 (regressors move together) → can’t separate their effects\nNo variation in treatment assignment (everyone treated or everyone control) → can’t estimate causal effects\n\n\nPrecision problems vs identification problems\nNot all “bad variation” is the same. The SE formula has a numerator and a denominator, and they fail in completely different ways:\n\\[SE(\\hat{\\beta}) = \\frac{\\overbrace{\\sigma}^{\\text{numerator: noise in Y}}}{\\underbrace{\\sqrt{n} \\cdot SD(X)}_{\\text{denominator: signal in X}}}\\]\nA note on \\(\\sigma\\): in the simulations above, you set \\(\\sigma\\) with a slider — you’re the Oracle, generating data from a known distribution so you can watch what happens when things break. In real life, you never know \\(\\sigma\\). You estimate it from the data using the sample SD, \\(s = \\sqrt{\\sum(x_i - \\bar{x})^2 / (n-1)}\\), and plug that in. This is why real inference uses the t-distribution (which accounts for the extra uncertainty of estimating \\(\\sigma\\)) rather than the normal. As \\(n\\) grows, \\(s\\) converges to \\(\\sigma\\) and the t converges to the normal — but with small samples, the distinction matters.\nPrecision problem (numerator too big). High variance in Y means \\(\\sigma\\) is large, so your SE is large and your CIs are wide. But OLS is still unbiased — you still get the right answer on average. You can fix this by collecting more data: \\(SE \\propto \\sigma / \\sqrt{n}\\), so if the noise in Y doubles, quadrupling your sample size gets you back to the same SE. Why does more data help? It doesn’t reduce the noise in any single observation — \\(\\sigma^2\\) is a property of the population, not your sample size. But when you average over more observations, the random noise cancels out: some observations are too high, some too low, and the average lands closer to the truth. Formally, \\(\\text{Var}(\\bar{X}) = \\sigma^2 / n\\). The variance of the average shrinks even though each observation is equally noisy. That’s the Law of Large Numbers. More data always helps a precision problem.\nIdentification problem (denominator goes to zero). When \\(SD(X) \\to 0\\), or \\(n \\to 1\\), or regressors become perfectly collinear, the denominator vanishes and the SE goes to infinity. This isn’t imprecision — the parameter literally isn’t estimable. No amount of data fixes it. If every person in your study has the same income, you cannot estimate the effect of income, period — not with 100 observations, not with a million. The formula divides by zero and OLS either crashes or gives you a meaningless number. More data never helps an identification problem.\n\n\n\n\n\n\n\n\n\nPrecision problem\nIdentification problem\n\n\n\n\nWhat’s wrong\nNumerator too big (\\(\\sigma\\) large)\nDenominator goes to zero\n\n\nBias?\nNo — estimates are centered on truth\nUndefined — no estimate exists\n\n\nCIs?\nWide but valid\nInfinite or meaningless\n\n\nFix with more data?\nYes (\\(SE \\propto 1/\\sqrt{n}\\))\nNo\n\n\nExample\nNoisy outcome\nConstant regressor, perfect collinearity, n = 1\n\n\n\nUnderstanding where the variation comes from — and when it might disappear — is the difference between statistics that works and statistics that lies.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "When Inference Breaks Down"
    ]
  },
  {
    "objectID": "degeneracy.html#did-you-know",
    "href": "degeneracy.html#did-you-know",
    "title": "When Inference Breaks Down",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe problem of perfect multicollinearity is sometimes called the identification problem — there are infinitely many combinations of \\(\\beta_1\\) and \\(\\beta_2\\) that fit the data equally well. This is the same issue that arises in causal inference when you try to separate selection from treatment without an experiment.\nNear-degeneracy can be worse than full degeneracy in practice. When Var(X) = 0, your software throws an error and you know something is wrong. When Var(X) is tiny but nonzero, OLS happily produces an estimate with a massive SE that you might not notice — especially if you don’t look at standard errors. This is why always report SEs, not just point estimates.\nThe condition number of the \\(X'X\\) matrix is used in numerical linear algebra to detect near-degeneracy. A condition number above \\(10^6\\) means your computer’s floating-point arithmetic is losing significant digits when inverting the matrix — your regression coefficients may be numerically unreliable even if they’re theoretically defined.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "When Inference Breaks Down"
    ]
  },
  {
    "objectID": "prediction-vs-causation.html",
    "href": "prediction-vs-causation.html",
    "title": "Prediction vs Causation in Foundation Models",
    "section": "",
    "text": "Foundation models — large language models, vision transformers, multimodal systems — are extraordinarily good at prediction. Given a prompt, they produce a continuation that is statistically plausible. Given an image, they classify it accurately. But prediction and causal reasoning are fundamentally different tasks, and conflating them is one of the most common errors in applied work that uses these models.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Prediction vs Causation in Foundation Models"
    ]
  },
  {
    "objectID": "prediction-vs-causation.html#py-mid-x-vs-py-mid-dox",
    "href": "prediction-vs-causation.html#py-mid-x-vs-py-mid-dox",
    "title": "Prediction vs Causation in Foundation Models",
    "section": "\\(P(Y \\mid X)\\) vs \\(P(Y \\mid do(X))\\)",
    "text": "\\(P(Y \\mid X)\\) vs \\(P(Y \\mid do(X))\\)\nThe distinction is precise. A predictive model learns the conditional distribution:\n\\[\nP(Y \\mid X)\n\\]\nThis answers: “Given that I observe \\(X = x\\), what is the distribution of \\(Y\\)?” It captures statistical associations — correlations, patterns, regularities in the training data.\nA causal model targets the interventional distribution:\n\\[\nP(Y \\mid do(X = x))\n\\]\nThis answers: “If I set \\(X\\) to \\(x\\) (intervening, not observing), what happens to \\(Y\\)?” This requires knowing the causal structure — which variables cause which — not just their joint distribution.\nThe difference matters whenever the relationship between \\(X\\) and \\(Y\\) is confounded. If \\(X\\) and \\(Y\\) are both driven by an unobserved variable \\(U\\):\n\\[\nP(Y \\mid X = x) \\neq P(Y \\mid do(X = x))\n\\]\nObserving that \\(X\\) takes a particular value tells you something about \\(U\\) (and hence \\(Y\\)) that setting \\(X\\) does not. This is the omitted variable bias from Residuals & Controls, expressed in the language of interventions.\n\n\n\n\n\n\nExample. An LLM trained on medical records might learn that patients who receive a certain drug have worse outcomes. But this is because the drug is prescribed to sicker patients (confounding by indication). The conditional distribution \\(P(\\text{outcome} \\mid \\text{drug})\\) reflects the association; the interventional distribution \\(P(\\text{outcome} \\mid do(\\text{drug}))\\) reflects the causal effect. The model learns the first. Policy requires the second.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Prediction vs Causation in Foundation Models"
    ]
  },
  {
    "objectID": "prediction-vs-causation.html#what-foundation-models-actually-learn",
    "href": "prediction-vs-causation.html#what-foundation-models-actually-learn",
    "title": "Prediction vs Causation in Foundation Models",
    "section": "What foundation models actually learn",
    "text": "What foundation models actually learn\nLarge language models are trained to minimize cross-entropy loss — which, as shown in Training as MLE, is maximum likelihood estimation of the conditional distribution of the next token given the context.\nThis means LLMs are, at their core, estimators of \\(P(\\text{next token} \\mid\n\\text{context})\\). They approximate conditional distributions over text, conditioned on the training corpus. They can reproduce causal statements that appear in their training data (“smoking causes cancer”) because those strings have high conditional probability. But reproducing a causal statement is not the same as performing causal reasoning.\nTo perform genuine causal inference, a model would need to:\n\nDistinguish observational from interventional distributions\nIdentify confounders and adjust for them\nReason counterfactually (“what would have happened if…?”)\n\nThese capabilities are not guaranteed by minimizing prediction loss, regardless of model scale. Whether and to what extent LLMs develop emergent causal reasoning abilities is an active research question — but the default assumption should be that prediction and causation are distinct.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Prediction vs Causation in Foundation Models"
    ]
  },
  {
    "objectID": "prediction-vs-causation.html#identification-vs-training",
    "href": "prediction-vs-causation.html#identification-vs-training",
    "title": "Prediction vs Causation in Foundation Models",
    "section": "Identification vs training",
    "text": "Identification vs training\nThis is the conceptual anchor that connects statistical foundations to modern AI.\nTraining asks: given data and a loss function, find the parameters \\(\\hat{\\theta}\\) that minimize prediction error.\n\\[\n\\hat{\\theta} = \\arg\\min_\\theta \\; \\frac{1}{n}\\sum_{i=1}^n \\mathcal{L}(Y_i, f_\\theta(X_i))\n\\]\nThis is an optimization problem. With enough data and a flexible enough model, you can drive the loss arbitrarily close to the Bayes-optimal predictor.\nIdentification asks: given the data-generating process, can the quantity of interest be recovered from the observable data at all — regardless of sample size, model complexity, or computational power?\n\\[\n\\text{Is } P(Y \\mid do(X)) \\text{ recoverable from } P(Y, X, Z)?\n\\]\nThis is a logical problem. If the causal effect is not identified — because of unobserved confounders, selection bias, or measurement error — no amount of data or model sophistication will recover it. A billion observations and a trillion-parameter model will converge to the wrong answer just as confidently as a simple regression.\n\n\n\n\n\n\n\n\n\nTraining\nIdentification\n\n\n\n\nQuestion\n“What minimizes prediction error?”\n“Can the causal parameter be recovered?”\n\n\nConstraint\nComputational (optimization)\nLogical (causal structure)\n\n\nSolved by\nMore data, bigger models\nResearch design, assumptions\n\n\nFailure mode\nOverfitting, distribution shift\nBias, confounding\n\n\n\n\n\n\n\n\n\nThe Manski principle. Identification comes before estimation. If you cannot identify the parameter from the data structure, no estimator — OLS, MLE, neural network, or otherwise — will give you the right answer. Identification is about what the data can tell you, not how cleverly you analyze it.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Prediction vs Causation in Foundation Models"
    ]
  },
  {
    "objectID": "prediction-vs-causation.html#in-context-learning-and-bayesian-updating",
    "href": "prediction-vs-causation.html#in-context-learning-and-bayesian-updating",
    "title": "Prediction vs Causation in Foundation Models",
    "section": "In-context learning and Bayesian updating",
    "text": "In-context learning and Bayesian updating\nA suggestive connection: when you provide examples in a prompt, the model’s predictions shift toward the pattern in those examples. This resembles Bayesian updating — the pre-trained model is the “prior,” and the in-context examples are the “data” that update it.\nFor simple settings — linear regression, basic classification — this analogy has formal support. Garg et al. (2022) and others have shown that transformers trained on linear regression tasks implement something close to Bayesian posterior computation in their forward pass.\nHowever, this equivalence does not generally hold for deep transformers on complex tasks. In-context learning may involve pattern matching, retrieval from training data, or other mechanisms that are not well-described as Bayesian updating. The analogy is useful as intuition but should not be treated as a general theoretical result.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Prediction vs Causation in Foundation Models"
    ]
  },
  {
    "objectID": "prediction-vs-causation.html#implications-for-applied-work",
    "href": "prediction-vs-causation.html#implications-for-applied-work",
    "title": "Prediction vs Causation in Foundation Models",
    "section": "Implications for applied work",
    "text": "Implications for applied work\nUsing LLMs for prediction: appropriate. If the goal is forecasting, classification, or text generation, the model is optimized for exactly this task. Evaluate on held-out data, check calibration, and be aware of distribution shift.\nUsing LLMs for causal claims: requires extreme caution. If the goal is to estimate a treatment effect, evaluate a policy, or make a causal argument, the model’s predictions reflect associations in the training data, not causal relationships. The standard tools of causal inference — randomized experiments, instrumental variables, difference-in-differences, regression discontinuity — remain necessary.\nUsing LLMs as research tools: promising but limited. LLMs can assist with literature review, code generation, data cleaning, and hypothesis generation. But the identification strategy — the argument for why your estimate is causal — must come from the researcher, not the model.\nProduct experimentation in AI systems — “which prompt works better?”, “does this fine-tuning improve user satisfaction?” — is fundamentally a randomized controlled trial. The statistical tools are the ones already in this course: power analysis, p-values, multiple testing corrections. The fact that the treatment is an AI system doesn’t change the experimental design principles.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Prediction vs Causation in Foundation Models"
    ]
  },
  {
    "objectID": "measurement-error.html",
    "href": "measurement-error.html",
    "title": "Measurement Error & Attenuation Bias",
    "section": "",
    "text": "You want to estimate the effect of \\(X^*\\) on \\(Y\\):\n\\[Y_i = \\alpha + \\beta X_i^* + u_i\\]\nBut you don’t observe \\(X^*\\) perfectly. Instead you observe \\(X\\) with noise:\n\\[X_i = X_i^* + \\eta_i, \\qquad \\eta_i \\sim (0, \\sigma_\\eta^2)\\]\nwhere \\(\\eta\\) is measurement error — independent of \\(X^*\\) and \\(u\\).\nWhen you run OLS on the mismeasured \\(X\\), you don’t get \\(\\beta\\). You get:\n\\[\\hat{\\beta}_{OLS} \\xrightarrow{p} \\beta \\times \\underbrace{\\frac{\\text{Var}(X^*)}{\\text{Var}(X^*) + \\sigma_\\eta^2}}_{\\lambda}\\]\nThat fraction \\(\\lambda\\) is always between 0 and 1. So the estimate is biased toward zero. This is attenuation bias.\n\n\nThink of it this way. The measurement error adds random noise to \\(X\\). From OLS’s perspective, some of the variation in \\(X\\) is real signal (correlated with \\(Y\\)) and some is pure noise (uncorrelated with \\(Y\\)). OLS can’t tell which is which, so it averages over both — diluting the estimated slope.\nMore noise → more dilution → flatter slope.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we know the true \\(X^*\\) and we set \\(\\sigma_\\eta\\) (the measurement error). We can compare the regression on true \\(X^*\\) vs mismeasured \\(X\\) side by side. In practice, you only observe \\(X\\) — you never see \\(X^*\\) and you don’t know how much noise \\(\\eta\\) adds. That’s what makes measurement error so insidious: you can’t tell it’s there just by looking at your data.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Measurement Error"
    ]
  },
  {
    "objectID": "measurement-error.html#the-problem",
    "href": "measurement-error.html#the-problem",
    "title": "Measurement Error & Attenuation Bias",
    "section": "",
    "text": "You want to estimate the effect of \\(X^*\\) on \\(Y\\):\n\\[Y_i = \\alpha + \\beta X_i^* + u_i\\]\nBut you don’t observe \\(X^*\\) perfectly. Instead you observe \\(X\\) with noise:\n\\[X_i = X_i^* + \\eta_i, \\qquad \\eta_i \\sim (0, \\sigma_\\eta^2)\\]\nwhere \\(\\eta\\) is measurement error — independent of \\(X^*\\) and \\(u\\).\nWhen you run OLS on the mismeasured \\(X\\), you don’t get \\(\\beta\\). You get:\n\\[\\hat{\\beta}_{OLS} \\xrightarrow{p} \\beta \\times \\underbrace{\\frac{\\text{Var}(X^*)}{\\text{Var}(X^*) + \\sigma_\\eta^2}}_{\\lambda}\\]\nThat fraction \\(\\lambda\\) is always between 0 and 1. So the estimate is biased toward zero. This is attenuation bias.\n\n\nThink of it this way. The measurement error adds random noise to \\(X\\). From OLS’s perspective, some of the variation in \\(X\\) is real signal (correlated with \\(Y\\)) and some is pure noise (uncorrelated with \\(Y\\)). OLS can’t tell which is which, so it averages over both — diluting the estimated slope.\nMore noise → more dilution → flatter slope.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we know the true \\(X^*\\) and we set \\(\\sigma_\\eta\\) (the measurement error). We can compare the regression on true \\(X^*\\) vs mismeasured \\(X\\) side by side. In practice, you only observe \\(X\\) — you never see \\(X^*\\) and you don’t know how much noise \\(\\eta\\) adds. That’s what makes measurement error so insidious: you can’t tell it’s there just by looking at your data.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Measurement Error"
    ]
  },
  {
    "objectID": "measurement-error.html#simulation-1-watch-the-slope-attenuate",
    "href": "measurement-error.html#simulation-1-watch-the-slope-attenuate",
    "title": "Measurement Error & Attenuation Bias",
    "section": "Simulation 1: Watch the slope attenuate",
    "text": "Simulation 1: Watch the slope attenuate\nIncrease the measurement error and watch the estimated slope shrink toward zero. The true relationship stays the same — only the noise changes.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"beta\", \"True slope (\\u03b2):\",\n                  min = 0.5, max = 3, value = 1.5, step = 0.1),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 50, max = 500, value = 200, step = 50),\n\n      sliderInput(\"sigma_x\", \"SD of true X*:\",\n                  min = 1, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"sigma_u\", \"SD of outcome noise (u):\",\n                  min = 0.5, max = 3, value = 1, step = 0.25),\n\n      sliderInput(\"sigma_eta\", \"Measurement error (\\u03c3\\u03b7):\",\n                  min = 0, max = 5, value = 0, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"scatter\", height = \"550px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  # Store base data in reactiveValues so it only changes on button/param change\n  rv &lt;- reactiveValues(x_star = NULL, y = NULL, eta_base = NULL,\n                       beta = 1.5, sx = 2)\n\n  observe({\n    input$go\n    n  &lt;- input$n\n    b  &lt;- input$beta\n    sx &lt;- input$sigma_x\n    su &lt;- input$sigma_u\n\n    rv$x_star   &lt;- rnorm(n, 0, sx)\n    rv$y        &lt;- 2 + b * rv$x_star + rnorm(n, 0, su)\n    rv$eta_base &lt;- rnorm(n)\n    rv$beta     &lt;- b\n    rv$sx       &lt;- sx\n  })\n\n  # Measurement error slider scales the fixed eta pattern — no data regeneration\n  sim &lt;- reactive({\n    req(rv$x_star)\n    se      &lt;- input$sigma_eta\n    x_star  &lt;- rv$x_star\n    y       &lt;- rv$y\n    x_obs   &lt;- x_star + rv$eta_base * se\n\n    fit_true &lt;- lm(y ~ x_star)\n    fit_obs  &lt;- lm(y ~ x_obs)\n    lambda   &lt;- rv$sx^2 / (rv$sx^2 + se^2)\n\n    list(x_star = x_star, x_obs = x_obs, y = y,\n         b_true = coef(fit_true)[2], b_obs = coef(fit_obs)[2],\n         int_true = coef(fit_true)[1], int_obs = coef(fit_obs)[1],\n         beta = rv$beta, lambda = lambda, sigma_eta = se)\n  })\n\n  output$scatter &lt;- renderPlot({\n    d &lt;- sim()\n    par(mfrow = c(1, 2), mar = c(4.5, 4.5, 3.5, 1))\n\n    # Left: true X*\n    plot(d$x_star, d$y, pch = 16, cex = 0.6,\n         col = adjustcolor(\"#3498db\", 0.5),\n         xlab = \"True X*\", ylab = \"Y\",\n         main = \"Regression on true X*\")\n    abline(a = d$int_true, b = d$b_true, col = \"#27ae60\", lwd = 3)\n    mtext(paste0(\"Slope = \", round(d$b_true, 3)),\n          side = 3, line = 0, cex = 1.1, font = 2, col = \"#27ae60\")\n\n    # Right: observed X with error\n    plot(d$x_obs, d$y, pch = 16, cex = 0.6,\n         col = adjustcolor(\"#e74c3c\", 0.4),\n         xlab = \"Observed X (with error)\", ylab = \"Y\",\n         main = \"Regression on mismeasured X\")\n    abline(a = d$int_obs, b = d$b_obs, col = \"#e74c3c\", lwd = 3)\n    abline(a = d$int_true, b = d$beta, col = \"#27ae60\", lwd = 2, lty = 2)\n    mtext(paste0(\"Slope = \", round(d$b_obs, 3),\n                 \"  (true = \", d$beta, \")\"),\n          side = 3, line = 0, cex = 1.1, font = 2, col = \"#e74c3c\")\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"OLS on mismeasured X\", \"True slope\"),\n           col = c(\"#e74c3c\", \"#27ae60\"), lwd = c(3, 2), lty = c(1, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- sim()\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True \\u03b2:&lt;/b&gt; \", d$beta, \"&lt;br&gt;\",\n        \"&lt;b&gt;OLS on X*:&lt;/b&gt; \", round(d$b_true, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;OLS on X:&lt;/b&gt; \", round(d$b_obs, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Attenuation factor (\\u03bb):&lt;/b&gt;&lt;br&gt;\",\n        \"Var(X*) / [Var(X*) + Var(\\u03b7)]&lt;br&gt;\",\n        \"= \", round(d$lambda, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;\\u03b2 \\u00d7 \\u03bb = &lt;/b&gt;\",\n        round(d$beta * d$lambda, 3)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nStart with \\(\\eta\\) = 0: both panels are identical. No measurement error, no bias.\nSlowly increase \\(\\eta\\): watch the right panel’s slope flatten. The cloud of points spreads horizontally (noise in X), so OLS “sees” a weaker relationship.\n\\(\\eta\\) = 5, SD of X* = 2: the attenuation factor drops to ~0.14. Your estimate is 86% too small.\nIncrease n: the slope doesn’t recover! Attenuation bias is not a small-sample problem — it persists no matter how much data you have. More data just gives you a more precise estimate of the wrong number.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Measurement Error"
    ]
  },
  {
    "objectID": "measurement-error.html#simulation-2-attenuation-is-systematic-not-just-noisy",
    "href": "measurement-error.html#simulation-2-attenuation-is-systematic-not-just-noisy",
    "title": "Measurement Error & Attenuation Bias",
    "section": "Simulation 2: Attenuation is systematic, not just noisy",
    "text": "Simulation 2: Attenuation is systematic, not just noisy\nRun 500 regressions, each with fresh measurement error. The distribution of slope estimates is centered below the true \\(\\beta\\) — it’s not random noise, it’s a systematic downward bias.\n#| standalone: true\n#| viewerHeight: 550\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"beta2\", \"True slope (\\u03b2):\",\n                  min = 0.5, max = 3, value = 1.5, step = 0.1),\n\n      sliderInput(\"n2\", \"Sample size per run:\",\n                  min = 50, max = 300, value = 100, step = 50),\n\n      sliderInput(\"sigma_eta2\", \"Measurement error (\\u03c3\\u03b7):\",\n                  min = 0, max = 5, value = 2, step = 0.25),\n\n      sliderInput(\"n_sims\", \"Number of simulations:\",\n                  min = 100, max = 1000, value = 500, step = 100),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"mc_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  sim &lt;- reactive({\n    n     &lt;- input$n2\n    b     &lt;- input$beta2\n    se    &lt;- input$sigma_eta2\n    nsims &lt;- input$n_sims\n    sx    &lt;- 2\n\n    betas_true &lt;- numeric(nsims)\n    betas_obs  &lt;- numeric(nsims)\n\n    for (i in seq_len(nsims)) {\n      x_star &lt;- rnorm(n, 0, sx)\n      u      &lt;- rnorm(n)\n      y      &lt;- 2 + b * x_star + u\n\n      eta   &lt;- rnorm(n, 0, se)\n      x_obs &lt;- x_star + eta\n\n      betas_true[i] &lt;- coef(lm(y ~ x_star))[2]\n      betas_obs[i]  &lt;- coef(lm(y ~ x_obs))[2]\n    }\n\n    lambda &lt;- sx^2 / (sx^2 + se^2)\n\n    list(betas_true = betas_true, betas_obs = betas_obs,\n         beta = b, lambda = lambda)\n  })\n\n  output$mc_plot &lt;- renderPlot({\n    d &lt;- sim()\n\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    all_b &lt;- c(d$betas_true, d$betas_obs)\n    xlim  &lt;- range(all_b) + c(-0.1, 0.1)\n\n    # No-error distribution\n    hist(d$betas_true, breaks = 40,\n         col = adjustcolor(\"#27ae60\", 0.5), border = \"white\",\n         main = \"Distribution of slope estimates across simulations\",\n         xlab = expression(hat(beta)), xlim = xlim,\n         freq = FALSE, cex.main = 1.3)\n\n    # With-error distribution\n    hist(d$betas_obs, breaks = 40,\n         col = adjustcolor(\"#e74c3c\", 0.45), border = \"white\",\n         add = TRUE, freq = FALSE)\n\n    abline(v = d$beta, col = \"#2c3e50\", lwd = 2.5, lty = 2)\n    abline(v = d$beta * d$lambda, col = \"#e74c3c\", lwd = 2, lty = 3)\n    abline(v = mean(d$betas_obs), col = \"#e74c3c\", lwd = 1.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.9,\n           legend = c(\n             paste0(\"No error (centered at \\u03b2 = \", d$beta, \")\"),\n             paste0(\"With error (centered at \", round(mean(d$betas_obs), 3), \")\"),\n             paste0(\"Theory: \\u03b2\\u03bb = \", round(d$beta * d$lambda, 3))\n           ),\n           fill = c(adjustcolor(\"#27ae60\", 0.5),\n                    adjustcolor(\"#e74c3c\", 0.45), NA),\n           border = c(\"white\", \"white\", NA),\n           col = c(NA, NA, \"#e74c3c\"),\n           lwd = c(NA, NA, 2), lty = c(NA, NA, 3))\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- sim()\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True \\u03b2:&lt;/b&gt; \", d$beta, \"&lt;br&gt;\",\n        \"&lt;b&gt;Avg estimate (no error):&lt;/b&gt; \",\n        round(mean(d$betas_true), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Avg estimate (with error):&lt;/b&gt; \",\n        round(mean(d$betas_obs), 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Attenuation factor:&lt;/b&gt; \", round(d$lambda, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;\\u03b2 \\u00d7 \\u03bb:&lt;/b&gt; \",\n        round(d$beta * d$lambda, 3), \"&lt;br&gt;\",\n        \"&lt;small&gt;Bias: \",\n        round(mean(d$betas_obs) - d$beta, 3), \"&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\n\\(\\eta\\) = 0: both distributions overlap perfectly — no bias at all.\n\\(\\eta\\) = 2: the red distribution shifts left. The average slope is systematically below the truth.\nIncrease n to 300: the distributions get narrower (more precise) but the red one stays centered at the wrong value. Attenuation bias doesn’t go away with more data.\nCompare theory vs simulation: the theoretical \\(\\beta\\lambda\\) should closely match the average of the red distribution.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Measurement Error"
    ]
  },
  {
    "objectID": "measurement-error.html#measurement-error-in-y-vs-x",
    "href": "measurement-error.html#measurement-error-in-y-vs-x",
    "title": "Measurement Error & Attenuation Bias",
    "section": "Measurement error in Y vs X",
    "text": "Measurement error in Y vs X\nA crucial asymmetry:\n\n\n\n\n\n\n\n\n\nError in X\nError in Y\n\n\n\n\nBias?\nYes — toward zero\nNo bias\n\n\nPrecision?\nSlightly worse\nWorse (larger SEs)\n\n\nGoes away with more data?\nNo\nSEs shrink, but that’s just precision\n\n\n\nWhy the asymmetry? When \\(Y\\) is measured with error, the noise goes into the residual — it’s just more \\(u\\). The slope is unbiased; you just estimate it less precisely. When \\(X\\) is measured with error, the noise is in the regressor, which contaminates the covariance between \\(X\\) and \\(Y\\) and biases the slope.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Measurement Error"
    ]
  },
  {
    "objectID": "measurement-error.html#what-can-you-do-about-it",
    "href": "measurement-error.html#what-can-you-do-about-it",
    "title": "Measurement Error & Attenuation Bias",
    "section": "What can you do about it?",
    "text": "What can you do about it?\n\nMeasure better. The best fix is reducing \\(\\sigma_\\eta\\). Use validated instruments, multiple measurements, averages of repeated measures.\nInstrumental variables (IV). Find a variable \\(Z\\) that predicts \\(X^*\\) but isn’t contaminated by the measurement error. Two-stage least squares recovers the true \\(\\beta\\).\nReliability ratio correction. OLS with measurement error shrinks the coefficient by the signal-to-noise ratio (also called the reliability ratio):\n\n\\[\\hat{\\beta}_{OLS} \\xrightarrow{p} \\beta \\cdot \\underbrace{\\frac{\\text{Var}(X^*)}{\\text{Var}(X^*) + \\text{Var}(\\eta)}}_{\\text{SNR}}\\]\nThe SNR is always between 0 and 1. High noise (\\(\\text{Var}(\\eta) \\gg \\text{Var}(X^*)\\)) → SNR near 0 → coefficient crushed toward zero. Low noise → SNR near 1 → barely any attenuation. If you know the SNR, you can correct: \\(\\hat{\\beta}_{corrected} = \\hat{\\beta}_{OLS} / \\text{SNR}\\).\n\nMultiple indicators. If you have two noisy measures of \\(X^*\\) with independent errors, their covariance identifies \\(\\text{Var}(X^*)\\), letting you compute \\(\\lambda\\) directly.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Measurement Error"
    ]
  },
  {
    "objectID": "measurement-error.html#did-you-know",
    "href": "measurement-error.html#did-you-know",
    "title": "Measurement Error & Attenuation Bias",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe attenuation bias formula was derived by Karl Pearson in the early 1900s, making it one of the oldest results in regression theory. Pearson was studying the relationship between fathers’ and sons’ heights and realized that imprecise measurements of height would make the hereditary correlation look weaker than it really was.\nIn economics, Jerry Hausman (2001) showed that measurement error in survey data on income and consumption can attenuate elasticity estimates by 30–50%. Studies using administrative tax records (with near-zero measurement error) consistently find larger effects than survey-based studies — exactly what attenuation bias predicts.\nThe errors-in-variables literature distinguishes between classical measurement error (what we covered: \\(X = X^* + \\eta\\) with \\(\\eta\\) independent of \\(X^*\\)) and non-classical error (where the error depends on the true value). Non-classical error can bias in either direction, not just toward zero. Mean-reverting error in test scores is a common example.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Measurement Error"
    ]
  },
  {
    "objectID": "foundations.html",
    "href": "foundations.html",
    "title": "Statistical Foundations",
    "section": "",
    "text": "A distribution is a model of variability. Imagine measuring the commute time of every person in a city. Some people take 10 minutes, most take around 30, a few are stuck for over an hour. If you made a histogram of all those commute times, the shape you’d see is the distribution — it tells you which values are common, which are rare, and how spread out things are.\nWhy do we care? Because variation is everywhere. Two patients given the same drug respond differently. Two students who study the same hours get different exam scores. Two identical ads shown to similar users get different click rates. None of that is a mistake — it’s the nature of data. A distribution is simply a mathematical way of describing how much things vary and in what pattern.\nOnce you have a distribution, you can answer useful questions: What’s the most likely outcome? How often do we see extreme values? Where does the middle 50% of the data sit?\nKey objects: histogram (shape), CDF (cumulative probabilities), quantiles (where does 50% of the data fall?).\nExplore different distributions below — notice how each has a distinct shape, center, and spread.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we choose the distribution (Normal, Exponential, etc.) and its parameters (mean, SD). We know the true shape because we built it. In practice, you observe data and don’t know which distribution generated it — you estimate the parameters and hope the shape is close enough.\n\n\n\n#| standalone: true\n#| viewerHeight: 450\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n      selectInput(\"dist\", \"Distribution:\",\n                  choices = c(\"Uniform(0,1)\", \"Normal(0,1)\",\n                              \"Exponential(1)\", \"Chi-squared(3)\",\n                              \"Bimodal\")),\n      sliderInput(\"n\", \"Sample size:\", min = 50, max = 5000, value = 1000, step = 50),\n      actionButton(\"draw\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n      uiOutput(\"stats\")\n    ),\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"hist_plot\", height = \"350px\")),\n        column(6, plotOutput(\"cdf_plot\",  height = \"350px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  samp &lt;- reactive({\n    input$draw\n    n &lt;- input$n\n    switch(input$dist,\n      \"Uniform(0,1)\"   = runif(n),\n      \"Normal(0,1)\"    = rnorm(n),\n      \"Exponential(1)\" = rexp(n),\n      \"Chi-squared(3)\" = rchisq(n, df = 3),\n      \"Bimodal\"        = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, -2, 0.6) + (1 - k) * rnorm(n, 2, 0.6)\n      }\n    )\n  })\n\n  output$hist_plot &lt;- renderPlot({\n    x &lt;- samp()\n    par(mar = c(4.5, 4, 3, 1))\n    hist(x, breaks = 50, probability = TRUE,\n         col = \"#d5e8d4\", border = \"#82b366\",\n         main = paste(\"Histogram:\", input$dist),\n         xlab = \"x\", ylab = \"Density\")\n  })\n\n  output$cdf_plot &lt;- renderPlot({\n    x &lt;- samp()\n    par(mar = c(4.5, 4, 3, 1))\n    plot(ecdf(x), col = \"#3498db\", lwd = 2,\n         main = paste(\"CDF:\", input$dist),\n         xlab = \"x\", ylab = \"F(x)\")\n  })\n\n  output$stats &lt;- renderUI({\n    x &lt;- samp()\n    q &lt;- round(quantile(x, c(0.25, 0.5, 0.75)), 3)\n    tags$div(style = \"background:#f0f4f8; border-radius:6px; padding:12px; margin-top:12px; font-size:14px; line-height:1.8;\",\n      HTML(paste0(\n        \"&lt;b&gt;Mean:&lt;/b&gt; \", round(mean(x), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;SD:&lt;/b&gt; \", round(sd(x), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Q25:&lt;/b&gt; \", q[1], \"&lt;br&gt;\",\n        \"&lt;b&gt;Median:&lt;/b&gt; \", q[2], \"&lt;br&gt;\",\n        \"&lt;b&gt;Q75:&lt;/b&gt; \", q[3]\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Probability & Uncertainty",
      "Foundations"
    ]
  },
  {
    "objectID": "foundations.html#data-distributions",
    "href": "foundations.html#data-distributions",
    "title": "Statistical Foundations",
    "section": "",
    "text": "A distribution is a model of variability. Imagine measuring the commute time of every person in a city. Some people take 10 minutes, most take around 30, a few are stuck for over an hour. If you made a histogram of all those commute times, the shape you’d see is the distribution — it tells you which values are common, which are rare, and how spread out things are.\nWhy do we care? Because variation is everywhere. Two patients given the same drug respond differently. Two students who study the same hours get different exam scores. Two identical ads shown to similar users get different click rates. None of that is a mistake — it’s the nature of data. A distribution is simply a mathematical way of describing how much things vary and in what pattern.\nOnce you have a distribution, you can answer useful questions: What’s the most likely outcome? How often do we see extreme values? Where does the middle 50% of the data sit?\nKey objects: histogram (shape), CDF (cumulative probabilities), quantiles (where does 50% of the data fall?).\nExplore different distributions below — notice how each has a distinct shape, center, and spread.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we choose the distribution (Normal, Exponential, etc.) and its parameters (mean, SD). We know the true shape because we built it. In practice, you observe data and don’t know which distribution generated it — you estimate the parameters and hope the shape is close enough.\n\n\n\n#| standalone: true\n#| viewerHeight: 450\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n      selectInput(\"dist\", \"Distribution:\",\n                  choices = c(\"Uniform(0,1)\", \"Normal(0,1)\",\n                              \"Exponential(1)\", \"Chi-squared(3)\",\n                              \"Bimodal\")),\n      sliderInput(\"n\", \"Sample size:\", min = 50, max = 5000, value = 1000, step = 50),\n      actionButton(\"draw\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n      uiOutput(\"stats\")\n    ),\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"hist_plot\", height = \"350px\")),\n        column(6, plotOutput(\"cdf_plot\",  height = \"350px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  samp &lt;- reactive({\n    input$draw\n    n &lt;- input$n\n    switch(input$dist,\n      \"Uniform(0,1)\"   = runif(n),\n      \"Normal(0,1)\"    = rnorm(n),\n      \"Exponential(1)\" = rexp(n),\n      \"Chi-squared(3)\" = rchisq(n, df = 3),\n      \"Bimodal\"        = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, -2, 0.6) + (1 - k) * rnorm(n, 2, 0.6)\n      }\n    )\n  })\n\n  output$hist_plot &lt;- renderPlot({\n    x &lt;- samp()\n    par(mar = c(4.5, 4, 3, 1))\n    hist(x, breaks = 50, probability = TRUE,\n         col = \"#d5e8d4\", border = \"#82b366\",\n         main = paste(\"Histogram:\", input$dist),\n         xlab = \"x\", ylab = \"Density\")\n  })\n\n  output$cdf_plot &lt;- renderPlot({\n    x &lt;- samp()\n    par(mar = c(4.5, 4, 3, 1))\n    plot(ecdf(x), col = \"#3498db\", lwd = 2,\n         main = paste(\"CDF:\", input$dist),\n         xlab = \"x\", ylab = \"F(x)\")\n  })\n\n  output$stats &lt;- renderUI({\n    x &lt;- samp()\n    q &lt;- round(quantile(x, c(0.25, 0.5, 0.75)), 3)\n    tags$div(style = \"background:#f0f4f8; border-radius:6px; padding:12px; margin-top:12px; font-size:14px; line-height:1.8;\",\n      HTML(paste0(\n        \"&lt;b&gt;Mean:&lt;/b&gt; \", round(mean(x), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;SD:&lt;/b&gt; \", round(sd(x), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Q25:&lt;/b&gt; \", q[1], \"&lt;br&gt;\",\n        \"&lt;b&gt;Median:&lt;/b&gt; \", q[2], \"&lt;br&gt;\",\n        \"&lt;b&gt;Q75:&lt;/b&gt; \", q[3]\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Probability & Uncertainty",
      "Foundations"
    ]
  },
  {
    "objectID": "foundations.html#sampling-uncertainty",
    "href": "foundations.html#sampling-uncertainty",
    "title": "Statistical Foundations",
    "section": "2. Sampling & Uncertainty",
    "text": "2. Sampling & Uncertainty\nSampling is why statistics exists. A sample mean is not a fixed truth — it is one draw from a distribution of possible sample means. Repeat the sampling and you get a different answer every time.\nThe key insight: larger samples produce less variable estimates. The spread of the sampling distribution shrinks at rate \\(1/\\sqrt{n}\\).\nTry it: press “New draw” a few times at \\(n = 10\\), then slide up to \\(n = 200\\) and watch how the sample means cluster tighter around the true mean.\n#| standalone: true\n#| viewerHeight: 420\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n      sliderInput(\"n\", \"Sample size (n):\", min = 5, max = 200, value = 10, step = 5),\n      sliderInput(\"reps\", \"Repeated samples:\", min = 5, max = 50, value = 20, step = 5),\n      actionButton(\"go\", \"Draw samples\", class = \"btn-primary\", width = \"100%\")\n    ),\n    mainPanel(\n      width = 9,\n      plotOutput(\"dot_plot\", height = \"350px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  output$dot_plot &lt;- renderPlot({\n    input$go\n    n    &lt;- input$n\n    reps &lt;- input$reps\n\n    means &lt;- replicate(reps, mean(rnorm(n, mean = 5, sd = 2)))\n\n    par(mar = c(4.5, 2, 3, 1))\n    stripchart(means, method = \"stack\", pch = 19, cex = 1.5,\n               col = \"#3498db\", offset = 0.5,\n               xlim = c(3, 7),\n               main = paste0(reps, \" sample means (n = \", n, \")\"),\n               xlab = \"Sample mean\")\n    abline(v = 5, lty = 2, lwd = 2, col = \"#e74c3c\")\n    legend(\"topright\", legend = \"True mean = 5\",\n           col = \"#e74c3c\", lty = 2, lwd = 2, bty = \"n\")\n  })\n}\n\nshinyApp(ui, server)\nPractice questions:\n\nWhat happens to the spread of sample means as \\(n\\) increases?\nDoes the population distribution change when you change \\(n\\)?\nCould a single sample mean be far from the truth? Is that more likely with small or large \\(n\\)?",
    "crumbs": [
      "Probability & Uncertainty",
      "Foundations"
    ]
  },
  {
    "objectID": "foundations.html#confidence-intervals",
    "href": "foundations.html#confidence-intervals",
    "title": "Statistical Foundations",
    "section": "3. Confidence Intervals",
    "text": "3. Confidence Intervals\nA 95% confidence interval does not mean “there’s a 95% probability the true value is inside.” The true value is fixed — it’s either in there or not.\nThe correct interpretation: if you repeated the experiment many times and built a CI each time, 95% of those intervals would contain the true value.\nThe simulation below shows exactly this. Each horizontal line is one CI from a fresh sample. Most cover the true mean (blue), but about 5% miss (red).\n#| standalone: true\n#| viewerHeight: 500\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n      sliderInput(\"n\", \"Sample size (n):\", min = 10, max = 200, value = 30, step = 10),\n      sliderInput(\"k\", \"Number of CIs:\", min = 20, max = 100, value = 50, step = 10),\n      sliderInput(\"conf\", \"Confidence level:\", min = 0.80, max = 0.99, value = 0.95, step = 0.01),\n      actionButton(\"go\", \"New experiment\", class = \"btn-primary\", width = \"100%\"),\n      uiOutput(\"coverage\")\n    ),\n    mainPanel(\n      width = 9,\n      plotOutput(\"ci_plot\", height = \"420px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  res &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    k    &lt;- input$k\n    conf &lt;- input$conf\n    mu   &lt;- 0\n\n    z &lt;- qnorm(1 - (1 - conf) / 2)\n\n    ci &lt;- t(replicate(k, {\n      x   &lt;- rnorm(n, mean = mu, sd = 1)\n      xbar &lt;- mean(x)\n      se   &lt;- sd(x) / sqrt(n)\n      c(xbar, xbar - z * se, xbar + z * se)\n    }))\n\n    covers &lt;- ci[, 2] &lt;= mu & ci[, 3] &gt;= mu\n\n    list(ci = ci, covers = covers, mu = mu, k = k, conf = conf)\n  })\n\n  output$ci_plot &lt;- renderPlot({\n    r &lt;- res()\n    par(mar = c(4.5, 4, 3, 1))\n\n    plot(NULL, xlim = range(r$ci[, 2:3]), ylim = c(1, r$k),\n         xlab = \"Value\", ylab = \"Sample #\",\n         main = paste0(round(r$conf * 100), \"% Confidence Intervals\"))\n\n    for (i in seq_len(r$k)) {\n      clr &lt;- if (r$covers[i]) \"#3498db\" else \"#e74c3c\"\n      lw  &lt;- if (r$covers[i]) 1.5 else 2.5\n      segments(r$ci[i, 2], i, r$ci[i, 3], i, col = clr, lwd = lw)\n      points(r$ci[i, 1], i, pch = 16, cex = 0.5, col = clr)\n    }\n\n    abline(v = r$mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n  })\n\n  output$coverage &lt;- renderUI({\n    r &lt;- res()\n    pct &lt;- round(100 * mean(r$covers), 1)\n    miss &lt;- sum(!r$covers)\n    tags$div(style = \"background:#f0f4f8; border-radius:6px; padding:12px; margin-top:12px; font-size:14px; line-height:1.8;\",\n      HTML(paste0(\n        \"&lt;b&gt;Coverage:&lt;/b&gt; \", pct, \"%&lt;br&gt;\",\n        \"&lt;b&gt;Missed:&lt;/b&gt; \", miss, \" / \", r$k, \"&lt;br&gt;\",\n        \"&lt;small&gt;Target: \", round(r$conf * 100), \"%&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\nKey takeaways:\n\nA confidence interval quantifies uncertainty, not probability about the parameter\nWider intervals = more uncertainty (small \\(n\\), high confidence level)\nThe coverage rate converges to the nominal level over many experiments\n\n\n\nDid you know?\n\nFlorence Nightingale wasn’t just a nurse — she was a pioneering statistician. She invented the polar area diagram (a variant of the pie chart) to convince the British government that soldiers were dying from preventable disease, not combat wounds. Her charts changed military policy and saved thousands of lives.\nThe word “statistics” comes from the German Statistik, meaning “science of the state” — it originally referred to collecting data about populations for government use.\nJohn Graunt (1620–1674) is considered the father of demography. He analyzed London’s death records and discovered that more boys are born than girls, that urban death rates exceed rural ones, and that plague deaths follow seasonal patterns — all from just counting.",
    "crumbs": [
      "Probability & Uncertainty",
      "Foundations"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "bayesian-estimation.html",
    "href": "bayesian-estimation.html",
    "title": "Bayesian Estimation",
    "section": "",
    "text": "Bayesian Updating showed the mechanics: start with a prior, observe data, apply Bayes’ rule, get a posterior distribution. But that page focused on the updating process — coin-flipping simulations that show how posteriors shift with data. This page is about estimation: how do you extract a point estimate from a posterior, and how does Bayesian estimation connect to MLE and regularization?",
    "crumbs": [
      "Estimation",
      "Bayesian Estimation"
    ]
  },
  {
    "objectID": "bayesian-estimation.html#from-updating-to-estimation",
    "href": "bayesian-estimation.html#from-updating-to-estimation",
    "title": "Bayesian Estimation",
    "section": "From updating to estimation",
    "text": "From updating to estimation\nBayes’ rule gives you a full posterior distribution over the parameter:\n\\[\np(\\theta \\mid \\text{data}) \\propto p(\\text{data} \\mid \\theta) \\cdot p(\\theta)\n\\]\nThat’s a whole distribution, not a single number. But often you need a point estimate — a single “best guess” for \\(\\theta\\). Two natural choices:\n\nPosterior mean: \\(\\hat{\\theta}_{\\text{mean}} = E[\\theta \\mid \\text{data}]\\) — the center of mass of the posterior\nMAP (Maximum A Posteriori): \\(\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_\\theta \\; p(\\theta \\mid \\text{data})\\) — the peak of the posterior\n\nThese are different summaries of the same distribution, and they can give different answers (especially when the posterior is skewed). Each has its own optimality property, as we’ll see below.",
    "crumbs": [
      "Estimation",
      "Bayesian Estimation"
    ]
  },
  {
    "objectID": "bayesian-estimation.html#map-estimation",
    "href": "bayesian-estimation.html#map-estimation",
    "title": "Bayesian Estimation",
    "section": "MAP estimation",
    "text": "MAP estimation\nThe MAP estimator maximizes the posterior:\n\\[\n\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_\\theta \\; p(\\theta \\mid \\text{data}) = \\arg\\max_\\theta \\; p(\\text{data} \\mid \\theta) \\cdot p(\\theta)\n\\]\nTaking logs (since \\(\\log\\) is monotone):\n\\[\n\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_\\theta \\; \\Big[\\underbrace{\\log p(\\text{data} \\mid \\theta)}_{\\text{log-likelihood}} + \\underbrace{\\log p(\\theta)}_{\\text{log-prior}}\\Big]\n\\]\nThis decomposition is revealing. MAP is MLE plus a correction from the prior.\nWith a flat (uniform) prior, \\(\\log p(\\theta)\\) is constant, so it drops out of the optimization:\n\\[\n\\hat{\\theta}_{\\text{MAP}} = \\hat{\\theta}_{\\text{MLE}}\n\\]\nA flat prior says “I have no preference” — so the data speak for themselves, and the MAP estimate is pure maximum likelihood.\nWith an informative prior, the \\(\\log p(\\theta)\\) term pulls the estimate toward the prior’s center. The more concentrated the prior (the stronger your belief), the harder it pulls. The more data you have, the more the likelihood dominates and the prior’s influence fades.\n\n\n\n\n\n\nIntuition. MAP balances two forces: the likelihood wants to fit the data; the prior wants to stay near your initial beliefs. With little data, the prior wins. With lots of data, the likelihood wins. This is the same tug-of-war described in Bayesian Updating, but now viewed through the lens of optimization.",
    "crumbs": [
      "Estimation",
      "Bayesian Estimation"
    ]
  },
  {
    "objectID": "bayesian-estimation.html#map-as-regularization",
    "href": "bayesian-estimation.html#map-as-regularization",
    "title": "Bayesian Estimation",
    "section": "MAP as regularization",
    "text": "MAP as regularization\nHere’s where Bayesian estimation connects to something you may have seen in machine learning. Consider a linear model where you want to estimate \\(\\beta\\):\n\nNormal prior → Ridge regression\nIf \\(\\beta \\sim N(0, \\tau^2 I)\\), the log-prior is:\n\\[\n\\log p(\\beta) = -\\frac{1}{2\\tau^2}\\|\\beta\\|_2^2 + \\text{const}\n\\]\nThe MAP objective becomes:\n\\[\n\\hat{\\beta}_{\\text{MAP}} = \\arg\\max_\\beta \\; \\left[-\\frac{1}{2\\sigma^2}\\sum_i(Y_i - X_i'\\beta)^2 - \\frac{1}{2\\tau^2}\\|\\beta\\|_2^2\\right]\n\\]\nwhich is equivalent to:\n\\[\n\\hat{\\beta}_{\\text{MAP}} = \\arg\\min_\\beta \\; \\left[\\sum_i(Y_i - X_i'\\beta)^2 + \\lambda\\|\\beta\\|_2^2\\right]\n\\]\nwhere \\(\\lambda = \\sigma^2/\\tau^2\\). This is ridge regression — OLS with an L2 penalty. The “penalty” is just the log-prior.\n\n\nLaplace prior → Lasso\nIf \\(\\beta_j \\sim \\text{Laplace}(0, b)\\) independently, the log-prior is:\n\\[\n\\log p(\\beta) = -\\frac{1}{b}\\|\\beta\\|_1 + \\text{const}\n\\]\nThe MAP objective becomes:\n\\[\n\\hat{\\beta}_{\\text{MAP}} = \\arg\\min_\\beta \\; \\left[\\sum_i(Y_i - X_i'\\beta)^2 + \\lambda\\|\\beta\\|_1\\right]\n\\]\nThis is the lasso — OLS with an L1 penalty that produces sparse solutions (some coefficients exactly zero).\n\n\nThe correspondence\n\n\n\nPrior on \\(\\beta\\)\nPenalty\nRegularization method\n\n\n\n\nNormal (Gaussian)\n\\(\\lambda\\|\\beta\\|_2^2\\)\nRidge\n\n\nLaplace (double exponential)\n\\(\\lambda\\|\\beta\\|_1\\)\nLasso\n\n\nFlat (uniform)\nNone\nOLS / MLE\n\n\n\nThe penalty parameter \\(\\lambda\\) maps directly to the prior’s precision: a tight prior (small \\(\\tau^2\\), large \\(\\lambda\\)) imposes heavy regularization; a vague prior (large \\(\\tau^2\\), small \\(\\lambda\\)) lets the data dominate.\n\n\n\n\n\n\nThe takeaway. Every regularized regression is implicitly doing Bayesian MAP estimation with some prior. And every Bayesian MAP estimate is implicitly doing regularized regression with some penalty. The two frameworks are saying the same thing in different languages.\n\n\n\n\n\nSimulation: MAP vs MLE with different priors\nPick a prior type, adjust its strength, and watch how MAP departs from MLE. With a flat prior the two coincide; with a tight prior MAP is pulled toward zero — exactly the regularization story above.\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"prior_type\", \"Prior type:\",\n                  choices = c(\"Flat (uniform)\" = \"flat\",\n                              \"Normal (ridge)\" = \"normal\",\n                              \"Laplace (lasso)\" = \"laplace\")),\n\n      sliderInput(\"tau\", HTML(\"Prior strength &tau;:\"),\n                  min = 0.1, max = 10, value = 2, step = 0.1),\n\n      sliderInput(\"n_obs\", \"Sample size n:\",\n                  min = 5, max = 200, value = 20, step = 5),\n\n      sliderInput(\"true_beta\", HTML(\"True &beta;:\"),\n                  min = 0, max = 5, value = 3, step = 0.1),\n\n      actionButton(\"draw\", \"New draw\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"stats\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"obj_plot\", height = \"420px\")),\n        column(6, plotOutput(\"est_plot\", height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  sim &lt;- reactive({\n    input$draw\n    n     &lt;- input$n_obs\n    b     &lt;- input$true_beta\n    tau   &lt;- input$tau\n    ptype &lt;- input$prior_type\n\n    set.seed(NULL)\n    x &lt;- rnorm(n)\n    y &lt;- b * x + rnorm(n)\n\n    # MLE / OLS\n    beta_mle &lt;- sum(x * y) / sum(x^2)\n\n    # Negative log-likelihood (up to constant) as function of beta\n    nll &lt;- function(beta) sum((y - beta * x)^2) / 2\n\n    # Negative log-prior (up to constant)\n    nlp &lt;- function(beta) {\n      switch(ptype,\n        \"flat\"    = rep(0, length(beta)),\n        \"normal\"  = beta^2 / (2 * tau^2),\n        \"laplace\" = abs(beta) / tau\n      )\n    }\n\n    # MAP estimate\n    obj &lt;- function(beta) nll(beta) + nlp(beta)\n    opt &lt;- optimise(obj, interval = c(-10, 10))\n    beta_map &lt;- opt$minimum\n\n    list(beta_mle = beta_mle, beta_map = beta_map,\n         true_beta = b, tau = tau, ptype = ptype,\n         nll = nll, nlp = nlp, obj = obj, n = n)\n  })\n\n  output$obj_plot &lt;- renderPlot({\n    d &lt;- sim()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    bseq &lt;- seq(-2, 8, length.out = 500)\n    y_nll &lt;- sapply(bseq, d$nll)\n    y_nlp &lt;- sapply(bseq, d$nlp)\n    y_obj &lt;- y_nll + y_nlp\n\n    ylim &lt;- c(0, max(y_obj[y_obj &lt; quantile(y_obj, 0.85)]) * 1.3)\n\n    plot(bseq, y_nll, type = \"l\", lwd = 2, col = \"#95a5a6\", lty = 2,\n         xlab = expression(beta), ylab = \"Objective\",\n         main = \"Neg log-lik + neg log-prior\",\n         ylim = ylim)\n    lines(bseq, y_nlp, lwd = 2, col = \"#9b59b6\", lty = 3)\n    lines(bseq, y_obj, lwd = 3, col = \"#e74c3c\")\n\n    abline(v = d$beta_mle, lwd = 2, col = \"#3498db\", lty = 2)\n    abline(v = d$beta_map, lwd = 2, col = \"#e74c3c\", lty = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Neg log-likelihood\",\n                      \"Neg log-prior\",\n                      \"MAP objective (sum)\",\n                      paste0(\"MLE = \", round(d$beta_mle, 3)),\n                      paste0(\"MAP = \", round(d$beta_map, 3))),\n           col = c(\"#95a5a6\", \"#9b59b6\", \"#e74c3c\", \"#3498db\", \"#e74c3c\"),\n           lwd = c(2, 2, 3, 2, 2),\n           lty = c(2, 3, 1, 2, 2))\n  })\n\n  output$est_plot &lt;- renderPlot({\n    d &lt;- sim()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    pts  &lt;- c(d$true_beta, d$beta_mle, d$beta_map)\n    labs &lt;- c(paste0(\"True = \", round(d$true_beta, 2)),\n              paste0(\"MLE  = \", round(d$beta_mle, 3)),\n              paste0(\"MAP  = \", round(d$beta_map, 3)))\n    cols &lt;- c(\"#2c3e50\", \"#3498db\", \"#e74c3c\")\n\n    rng &lt;- range(pts)\n    pad &lt;- max(0.5, diff(rng) * 0.4)\n    xlim &lt;- c(rng[1] - pad, rng[2] + pad)\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, 3.8),\n         xlab = expression(beta), ylab = \"\", yaxt = \"n\",\n         main = \"Coefficient estimates\")\n\n    abline(h = 1:3, col = \"#ecf0f1\", lwd = 0.8)\n\n    for (i in 1:3) {\n      points(pts[i], i, pch = 19, cex = 2.5, col = cols[i])\n      text(pts[i], i + 0.3, labs[i], cex = 1.1, font = 2, col = cols[i])\n    }\n\n    axis(2, at = 1:3, labels = c(\"True\", \"MLE\", \"MAP\"),\n         las = 1, tick = FALSE, cex.axis = 1.05)\n  })\n\n  output$stats &lt;- renderUI({\n    d &lt;- sim()\n    prior_label &lt;- switch(d$ptype,\n      \"flat\"    = \"Flat (uniform)\",\n      \"normal\"  = paste0(\"Normal(0, \", d$tau, \"\\u00b2)\"),\n      \"laplace\" = paste0(\"Laplace(0, \", d$tau, \")\")\n    )\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True &beta;:&lt;/b&gt; \", d$true_beta, \"&lt;br&gt;\",\n        \"&lt;b&gt;MLE:&lt;/b&gt; \", round(d$beta_mle, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;MAP:&lt;/b&gt; \", round(d$beta_map, 4), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Prior:&lt;/b&gt; \", prior_label, \"&lt;br&gt;\",\n        \"&lt;b&gt;&tau;:&lt;/b&gt; \", d$tau, \"&lt;br&gt;\",\n        \"&lt;b&gt;n:&lt;/b&gt; \", d$n\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Estimation",
      "Bayesian Estimation"
    ]
  },
  {
    "objectID": "bayesian-estimation.html#posterior-mean-vs-map",
    "href": "bayesian-estimation.html#posterior-mean-vs-map",
    "title": "Bayesian Estimation",
    "section": "Posterior mean vs MAP",
    "text": "Posterior mean vs MAP\nThe posterior mean and MAP are both valid point estimates, but they optimize different things:\nPosterior mean \\(E[\\theta \\mid \\text{data}]\\) minimizes expected squared error (posterior risk under squared-error loss). If your loss function is quadratic — you care equally about overestimating and underestimating — the posterior mean is optimal.\nMAP \\(\\arg\\max_\\theta \\; p(\\theta \\mid \\text{data})\\) gives the single most probable value. Under a 0-1 loss (you’re either right or wrong, no partial credit), MAP is optimal.\nFor symmetric posteriors (like the normal), the mean and mode coincide, so MAP and posterior mean are the same. For skewed posteriors, they diverge — the mean is pulled toward the tail, while the MAP stays at the peak.\nWith lots of data, both converge to the MLE. As \\(n\\) grows, the likelihood becomes sharply peaked and overwhelms any finite prior. The posterior concentrates around the MLE, and both the posterior mean and MAP converge to it. This is the Bernstein-von Mises theorem — the Bayesian analog of MLE’s asymptotic normality. It’s the same convergence you saw in the simulations in Bayesian Updating, where the posterior narrowed around the true value as data accumulated.",
    "crumbs": [
      "Estimation",
      "Bayesian Estimation"
    ]
  },
  {
    "objectID": "bayesian-estimation.html#credible-intervals-vs-confidence-intervals",
    "href": "bayesian-estimation.html#credible-intervals-vs-confidence-intervals",
    "title": "Bayesian Estimation",
    "section": "Credible intervals vs confidence intervals",
    "text": "Credible intervals vs confidence intervals\nBayesian estimation gives you a full posterior, which makes uncertainty quantification natural. A 95% credible interval is any interval \\([a, b]\\) such that:\n\\[\nP(\\theta \\in [a, b] \\mid \\text{data}) = 0.95\n\\]\nThis is the interpretation people want confidence intervals to have: “there’s a 95% probability the parameter is in this interval.” Confidence intervals don’t actually mean that — they’re about the procedure’s long-run coverage rate, not the probability of a specific interval containing \\(\\theta\\).\nFor more on the mechanics of how posteriors are computed and updated, see Bayesian Updating. For the frequentist perspective on confidence intervals and why they’re often misinterpreted, see p-values & Confidence Intervals.",
    "crumbs": [
      "Estimation",
      "Bayesian Estimation"
    ]
  },
  {
    "objectID": "bayesian-estimation.html#the-estimation-landscape",
    "href": "bayesian-estimation.html#the-estimation-landscape",
    "title": "Bayesian Estimation",
    "section": "The estimation landscape",
    "text": "The estimation landscape\nHere’s how all four estimation methods from this section fit together:\n\n\n\nMethod\nWhat it optimizes\nAssumptions\nEfficiency\n\n\n\n\nMoM\nMatches moments\nMinimal (just moments)\nLower\n\n\nMLE\nMaximizes likelihood\nFull distribution\nHighest (asymptotically)\n\n\nGMM\nWeighted moment distance\nMoment conditions\nBetween MoM and MLE\n\n\nBayesian\nPosterior (prior \\(\\times\\) likelihood)\nPrior + likelihood\nDepends on prior\n\n\n\nA few patterns emerge:\nAll four are consistent. With enough data, they all converge to the true parameter (for Bayesian: the posterior concentrates there). They differ in how fast they get there and what they assume along the way.\nMLE and Bayesian need the full distribution. Both require you to write down a likelihood \\(f(x \\mid \\theta)\\). MoM and GMM only need moment conditions — weaker assumptions, but you pay for that in efficiency.\nBayesian = MLE + prior. With a flat prior, Bayesian MAP is MLE. With an informative prior, the Bayesian estimate is a compromise between the likelihood and prior beliefs. The prior acts as regularization, which can help in small samples but becomes irrelevant in large ones.\nGMM nests both MoM and MLE. MoM is GMM with a specific choice of moments. MLE is GMM with the score equation as the moment condition. GMM is the general framework.\nThe right method depends on what you know and what you’re willing to assume. More assumptions (full distribution, informative prior) buy you more efficiency when they’re right — but more risk when they’re wrong.",
    "crumbs": [
      "Estimation",
      "Bayesian Estimation"
    ]
  },
  {
    "objectID": "variance-sd-se.html",
    "href": "variance-sd-se.html",
    "title": "Variance, SD & Standard Error",
    "section": "",
    "text": "Imagine you’re measuring the heights of students in a class. Some are tall, some are short — there’s spread. Statistics gives us precise language for that spread:\n\n\n\n\n\n\n\n\nConcept\nWhat it measures\nFormula\n\n\n\n\nVariance\nAverage squared distance from the mean\n\\(\\sigma^2 = \\frac{1}{N}\\sum(x_i - \\mu)^2\\)\n\n\nStandard Deviation (SD)\nAverage distance from the mean (in original units)\n\\(\\sigma = \\sqrt{\\sigma^2}\\)\n\n\nStandard Error (SE)\nHow much the sample mean bounces around\n\\(SE = \\frac{\\sigma}{\\sqrt{n}}\\)\n\n\n\nThe key insight: SD measures spread in your data. SE measures spread in your estimates.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we set the population mean \\(\\mu\\) and population SD \\(\\sigma\\) — then draw samples from that known population. We can see whether our sample statistics (\\(\\bar{x}\\), \\(s\\)) land close to the truth. In practice, you only have your sample. You never see \\(\\mu\\) or \\(\\sigma\\) directly — you estimate them with \\(\\bar{x}\\) and \\(s\\).",
    "crumbs": [
      "Probability & Uncertainty",
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#the-big-picture",
    "href": "variance-sd-se.html#the-big-picture",
    "title": "Variance, SD & Standard Error",
    "section": "",
    "text": "Imagine you’re measuring the heights of students in a class. Some are tall, some are short — there’s spread. Statistics gives us precise language for that spread:\n\n\n\n\n\n\n\n\nConcept\nWhat it measures\nFormula\n\n\n\n\nVariance\nAverage squared distance from the mean\n\\(\\sigma^2 = \\frac{1}{N}\\sum(x_i - \\mu)^2\\)\n\n\nStandard Deviation (SD)\nAverage distance from the mean (in original units)\n\\(\\sigma = \\sqrt{\\sigma^2}\\)\n\n\nStandard Error (SE)\nHow much the sample mean bounces around\n\\(SE = \\frac{\\sigma}{\\sqrt{n}}\\)\n\n\n\nThe key insight: SD measures spread in your data. SE measures spread in your estimates.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we set the population mean \\(\\mu\\) and population SD \\(\\sigma\\) — then draw samples from that known population. We can see whether our sample statistics (\\(\\bar{x}\\), \\(s\\)) land close to the truth. In practice, you only have your sample. You never see \\(\\mu\\) or \\(\\sigma\\) directly — you estimate them with \\(\\bar{x}\\) and \\(s\\).",
    "crumbs": [
      "Probability & Uncertainty",
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#variance-measuring-spread",
    "href": "variance-sd-se.html#variance-measuring-spread",
    "title": "Variance, SD & Standard Error",
    "section": "Variance: Measuring Spread",
    "text": "Variance: Measuring Spread\nVariance answers: “On average, how far are values from the center?”\nWe square the deviations because:\n\nPositive and negative deviations would cancel out otherwise\nIt penalizes large deviations more than small ones\nIt has nice mathematical properties (additive for independent variables)\n\n\nPopulation Variance vs Sample Variance\nHere’s where it gets tricky. There are two formulas:\nPopulation variance (when you have the entire population): \\[\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\mu)^2\\]\nSample variance (when you have a sample from a larger population): \\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\\]\n\n\nWhy n - 1? (Bessel’s Correction)\nWhen you compute deviations from the sample mean \\(\\bar{x}\\) instead of the true mean \\(\\mu\\), you’re using the data twice — once to compute \\(\\bar{x}\\), then again to compute deviations from it. The sample mean is always closer to the data points than the true mean would be, so dividing by \\(n\\) systematically underestimates the true variance.\nThink of it this way: if you have \\(n = 1\\) data point, the sample mean equals that point, so the deviation is 0. But that doesn’t mean there’s no variability in the population! Dividing by \\(n - 1 = 0\\) makes the formula undefined, which correctly tells you: you can’t estimate spread from a single observation.\nThe \\(n - 1\\) is called the degrees of freedom — you “used up” one degree of freedom estimating the mean.\nTry the simulation below to see this in action:\n#| standalone: true\n#| viewerHeight: 750\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Why n - 1?\"),\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n      sliderInput(\"pop_sd\", \"Population SD (\\u03c3):\", min = 2, max = 25, value = 10, step = 1),\n      sliderInput(\"samp_n\", \"Sample size (n):\", min = 2, max = 100, value = 10),\n      sliderInput(\"n_reps\", \"Number of samples:\", min = 100, max = 2000, value = 500, step = 100),\n      hr(),\n      h4(\"Results\"),\n      uiOutput(\"results_box\"),\n      hr(),\n      p(strong(\"Bias check:\"), \"If the average of many sample estimates equals the true value, the estimator is unbiased.\"),\n      p(\"Notice: dividing by n underestimates. Dividing by n-1 gets it right on average.\")\n    ),\n    mainPanel(\n      width = 9,\n      plotOutput(\"dist_plot\", height = \"600px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  sim &lt;- reactive({\n    mu    &lt;- 50\n    sigma &lt;- input$pop_sd\n    n     &lt;- input$samp_n\n    n_reps &lt;- input$n_reps\n\n    var_n  &lt;- numeric(n_reps)\n    var_n1 &lt;- numeric(n_reps)\n\n    for (i in 1:n_reps) {\n      samp &lt;- rnorm(n, mu, sigma)\n      xbar &lt;- mean(samp)\n      devs &lt;- (samp - xbar)^2\n      var_n[i]  &lt;- sum(devs) / n\n      var_n1[i] &lt;- sum(devs) / (n - 1)\n    }\n\n    list(var_n = var_n, var_n1 = var_n1, true_var = sigma^2, n = n)\n  })\n\n  output$dist_plot &lt;- renderPlot({\n    v &lt;- sim()\n\n    par(mfrow = c(2, 1), mar = c(4.5, 4.5, 3, 1))\n\n    xlims &lt;- range(c(v$var_n, v$var_n1))\n    hist(v$var_n, breaks = 40, col = adjustcolor(\"#3498db\", 0.6), border = \"white\",\n         main = paste0(\"Divide by n (biased)  \\u2014  n = \", v$n),\n         xlab = \"Estimated Variance\", xlim = xlims, cex.main = 1.3)\n    abline(v = v$true_var, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n    abline(v = mean(v$var_n), col = \"#2c3e50\", lwd = 2)\n    legend(\"topright\", legend = c(paste0(\"True = \", v$true_var),\n           paste0(\"Avg = \", round(mean(v$var_n), 2))),\n           col = c(\"#e74c3c\", \"#2c3e50\"), lwd = 2, lty = c(2, 1), bty = \"n\")\n\n    hist(v$var_n1, breaks = 40, col = adjustcolor(\"#2ecc71\", 0.6), border = \"white\",\n         main = \"Divide by n\\u22121 (unbiased)\",\n         xlab = \"Estimated Variance\", xlim = xlims, cex.main = 1.3)\n    abline(v = v$true_var, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n    abline(v = mean(v$var_n1), col = \"#2c3e50\", lwd = 2)\n    legend(\"topright\", legend = c(paste0(\"True = \", v$true_var),\n           paste0(\"Avg = \", round(mean(v$var_n1), 2))),\n           col = c(\"#e74c3c\", \"#2c3e50\"), lwd = 2, lty = c(2, 1), bty = \"n\")\n  })\n\n  output$results_box &lt;- renderUI({\n    v &lt;- sim()\n    tags$div(\n      style = \"background:#f0f4f8; border-radius:6px; padding:12px; font-size:14px; line-height:1.9;\",\n      HTML(paste0(\n        \"&lt;b&gt;True variance:&lt;/b&gt; \", v$true_var, \"&lt;br&gt;\",\n        \"&lt;b&gt;Avg (/ n):&lt;/b&gt; \", round(mean(v$var_n), 2), \"&lt;br&gt;\",\n        \"&lt;b&gt;Avg (/ n\\u22121):&lt;/b&gt; \", round(mean(v$var_n1), 2)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Probability & Uncertainty",
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#standard-deviation-back-to-original-units",
    "href": "variance-sd-se.html#standard-deviation-back-to-original-units",
    "title": "Variance, SD & Standard Error",
    "section": "Standard Deviation: Back to Original Units",
    "text": "Standard Deviation: Back to Original Units\nVariance is in squared units. If heights are in cm, variance is in cm². That’s hard to interpret.\nStandard deviation = \\(\\sqrt{\\text{variance}}\\) — it puts spread back in the original units.\n\\[\\sigma = \\sqrt{\\sigma^2} \\qquad \\text{(population)} \\qquad\\qquad s = \\sqrt{s^2} \\qquad \\text{(sample)}\\]\nRules of thumb (for roughly normal data):\n\n~68% of data falls within ±1 SD of the mean\n~95% of data falls within ±2 SD of the mean\n~99.7% falls within ±3 SD",
    "crumbs": [
      "Probability & Uncertainty",
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#standard-error-the-sd-of-your-estimate",
    "href": "variance-sd-se.html#standard-error-the-sd-of-your-estimate",
    "title": "Variance, SD & Standard Error",
    "section": "Standard Error: The SD of Your Estimate",
    "text": "Standard Error: The SD of Your Estimate\nHere’s where people get confused. The standard error is not about your data — it’s about your estimate.\nIf you took many samples of size \\(n\\) and computed the mean each time, those means would form a distribution (the sampling distribution). The standard deviation of that distribution is the standard error:\n\\[SE(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}}\\]\nIn practice, we don’t know \\(\\sigma\\), so we plug in \\(s\\):\n\\[\\widehat{SE}(\\bar{x}) = \\frac{s}{\\sqrt{n}}\\]\n\nWhy does SE = SD / √n? The Intuition\nImagine you want to know the average commute time in your city. You ask one person — they say 45 minutes. But that’s just one person. Maybe they live far away. Your estimate is noisy — it could easily be off by 20 minutes (the SD of commute times).\nNow you ask 4 people and average: 45, 25, 60, 30 → mean = 40 minutes. Some are high, some are low — the errors partially cancel out. Your estimate is less noisy. But not 4× less noisy — only 2× less noisy (because \\(\\sqrt{4} = 2\\)).\nAsk 100 people: the cancellation is even stronger. Your average is now 10× more precise than a single observation (\\(\\sqrt{100} = 10\\)).\nWhy square root and not just n? Because the errors don’t perfectly cancel — they’re random, so sometimes more are high than low. The cancellation is partial. Mathematically, independent errors cancel at the rate of \\(\\sqrt{n}\\), not \\(n\\). If errors perfectly cancelled, the SE would be \\(\\sigma / n\\) and you’d barely need any data. If they didn’t cancel at all, the SE would stay at \\(\\sigma\\) forever and more data would be useless. The \\(\\sqrt{n}\\) is the sweet spot of reality.\n\n\nThe Math Behind It\nEach observation \\(x_i\\) is an independent draw with variance \\(\\sigma^2\\). What does that actually mean?\nThe population has a distribution with some spread — that spread is \\(\\sigma^2\\). When you sample one person, you don’t know what you’ll get. You might get someone with a long commute or a short one. Before you observe them, that measurement is uncertain — it could land anywhere in the population distribution. That uncertainty is \\(\\sigma^2\\). Every observation carries the same amount of uncertainty because every observation comes from the same population.\n“Independent” means knowing what the first person said tells you nothing about what the second person will say. Each person is a fresh roll of the dice. So you’re averaging \\(n\\) equally-noisy, independent measurements — and the question is: how much noise survives the averaging?\nThe sample mean is:\n\\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\]\nStep 1: The variance of a sum of independent random variables equals the sum of their variances. Each \\(x_i\\) contributes \\(\\sigma^2\\):\n\\[\\text{Var}\\left(\\sum_{i=1}^n x_i\\right) = \\sigma^2 + \\sigma^2 + \\cdots + \\sigma^2 = n\\sigma^2\\]\nThe noise grows with \\(n\\) — more observations means more total variability in the sum. But we don’t want the sum, we want the average.\nStep 2: The mean divides the sum by \\(n\\). When you multiply a random variable by a constant \\(c\\), its variance gets multiplied by \\(c^2\\) (because variance is in squared units):\n\\[\\text{Var}(cX) = c^2 \\cdot \\text{Var}(X)\\]\nSo dividing by \\(n\\) means multiplying by \\(1/n\\), which divides the variance by \\(n^2\\):\n\\[\\text{Var}(\\bar{x}) = \\frac{1}{n^2} \\cdot n\\sigma^2 = \\frac{\\sigma^2}{n}\\]\nStep 3: Take the square root to get back to original units:\n\\[SE = \\sqrt{\\text{Var}(\\bar{x})} = \\sqrt{\\frac{\\sigma^2}{n}} = \\frac{\\sigma}{\\sqrt{n}}\\]\nThe sum’s noise grows as \\(n\\), but dividing by \\(n\\) shrinks it by \\(n^2\\). Net effect: noise shrinks by \\(n^2 / n = n\\), and the square root gives us \\(\\sqrt{n}\\). Averaging \\(n\\) independent measurements reduces noise by \\(\\sqrt{n}\\).\n\n\nThe Key Relationship\n\\[\\boxed{SE = \\frac{SD}{\\sqrt{n}}}\\]\nThis single formula connects everything:\n\nSD tells you how spread out individual data points are\nn is your sample size\nSE tells you how precisely you’ve estimated the mean\nMore data → smaller SE → more precise estimate\n\n#| standalone: true\n#| viewerHeight: 800\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"SD vs SE: What's the Difference?\"),\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n      sliderInput(\"true_sd\", \"Population SD:\", min = 5, max = 40, value = 15, step = 1),\n      sliderInput(\"n_obs\", \"Sample size (n):\", min = 5, max = 200, value = 25),\n      sliderInput(\"n_samples\", \"Samples to draw:\", min = 50, max = 1000, value = 300, step = 50),\n      hr(),\n      h4(\"Theory\"),\n      uiOutput(\"theory_box\"),\n      hr(),\n      h4(\"Simulation\"),\n      uiOutput(\"sim_box\")\n    ),\n    mainPanel(\n      width = 9,\n      plotOutput(\"main_plot\", height = \"650px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  sim &lt;- reactive({\n    mu    &lt;- 100\n    sigma &lt;- input$true_sd\n    n     &lt;- input$n_obs\n    n_samp &lt;- input$n_samples\n\n    one_sample &lt;- rnorm(n, mu, sigma)\n    means &lt;- replicate(n_samp, mean(rnorm(n, mu, sigma)))\n\n    list(one_sample = one_sample, means = means, mu = mu, sigma = sigma, n = n)\n  })\n\n  output$theory_box &lt;- renderUI({\n    d &lt;- sim()\n    se_theory &lt;- round(d$sigma / sqrt(d$n), 2)\n    tagList(\n      p(paste0(\"SD = \", d$sigma)),\n      p(paste0(\"SE = SD/\\u221an = \", d$sigma, \"/\\u221a\", d$n, \" = \", se_theory))\n    )\n  })\n\n  output$sim_box &lt;- renderUI({\n    d &lt;- sim()\n    tagList(\n      p(paste0(\"SD of one sample: \", round(sd(d$one_sample), 2))),\n      p(paste0(\"SD of sample means: \", round(sd(d$means), 2))),\n      p(paste0(\"Ratio: \", round(sd(d$one_sample) / sd(d$means), 1), \"x narrower\"))\n    )\n  })\n\n  output$main_plot &lt;- renderPlot({\n    d &lt;- sim()\n\n    sample_sd &lt;- sd(d$one_sample)\n    se_actual &lt;- sd(d$means)\n    xbar &lt;- mean(d$one_sample)\n\n    xlims &lt;- c(d$mu - 3.5 * d$sigma, d$mu + 3.5 * d$sigma)\n\n    par(mfrow = c(2, 1), mar = c(4.5, 4.5, 3.5, 1))\n\n    hist(d$one_sample, breaks = 30, col = adjustcolor(\"#3498db\", 0.6), border = \"white\",\n         main = paste0(\"One Sample (n = \", d$n, \") \\u2014 spread = SD\"),\n         xlab = \"Value\", xlim = xlims, freq = FALSE, cex.main = 1.3)\n    abline(v = xbar, col = \"#e74c3c\", lwd = 2.5)\n    segments(xbar - sample_sd, 0, xbar + sample_sd, 0, col = \"#e67e22\", lwd = 4)\n    mtext(paste0(\"Mean = \", round(xbar, 1), \"  |  SD = \", round(sample_sd, 1)),\n          side = 3, line = 0, cex = 1.1, font = 2)\n\n    hist(d$means, breaks = 40, col = adjustcolor(\"#2ecc71\", 0.6), border = \"white\",\n         main = \"Sampling Distribution of the Mean \\u2014 spread = SE\",\n         xlab = \"Sample Mean\", xlim = xlims, freq = FALSE, cex.main = 1.3)\n    xseq &lt;- seq(xlims[1], xlims[2], length.out = 300)\n    lines(xseq, dnorm(xseq, d$mu, d$sigma / sqrt(d$n)), col = \"#8e44ad\", lwd = 2.5)\n    abline(v = d$mu, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n    segments(d$mu - se_actual, 0, d$mu + se_actual, 0, col = \"#e67e22\", lwd = 4)\n    mtext(paste0(\"Mean of means = \", round(mean(d$means), 1), \"  |  SE = \", round(se_actual, 2)),\n          side = 3, line = 0, cex = 1.1, font = 2)\n    legend(\"topright\", \"Theoretical N(\\u03bc, SD/\\u221an)\", col = \"#8e44ad\", lwd = 2.5, bty = \"n\")\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Probability & Uncertainty",
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#how-they-all-connect",
    "href": "variance-sd-se.html#how-they-all-connect",
    "title": "Variance, SD & Standard Error",
    "section": "How They All Connect",
    "text": "How They All Connect\nLet’s see all three in one place. Watch what happens as you increase \\(n\\):\n\nSD stays the same — it’s a property of the population, not the sample size\nSE shrinks — more data means a more precise estimate\nSample variance (s²) bounces around σ² — but is unbiased on average\n\n#| standalone: true\n#| viewerHeight: 700\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"SD vs SE as n Grows\"),\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n      sliderInput(\"sigma\", \"Population SD:\", min = 2, max = 30, value = 10),\n      sliderInput(\"n_size\", \"Sample size (n):\", min = 5, max = 500, value = 10),\n      p(style = \"margin-top:12px;\",\n        strong(\"What to notice:\"),\n        br(),\n        \"Blue histogram (data) stays equally wide — SD doesn't shrink with n\",\n        br(), br(),\n        \"Green histogram (means) gets narrower — SE shrinks with sqrt(n)\",\n        br(), br(),\n        \"At n = 100, SE is 10x smaller than SD\"\n      )\n    ),\n    mainPanel(\n      width = 9,\n      plotOutput(\"combo_plot\", height = \"280px\"),\n      plotOutput(\"se_curve\", height = \"300px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  output$combo_plot &lt;- renderPlot({\n    mu &lt;- 50\n    sigma &lt;- input$sigma\n    n &lt;- input$n_size\n\n    one_samp &lt;- rnorm(n, mu, sigma)\n    means &lt;- replicate(500, mean(rnorm(n, mu, sigma)))\n\n    xlims &lt;- c(mu - 3.5 * sigma, mu + 3.5 * sigma)\n\n    par(mfrow = c(1, 2), mar = c(4, 4, 3, 1))\n\n    hist(one_samp, breaks = 30, col = adjustcolor(\"#3498db\", 0.6), border = \"white\",\n         main = paste0(\"One Sample (SD = \", round(sd(one_samp), 1), \")\"),\n         xlab = \"Value\", xlim = xlims, freq = FALSE, cex.main = 1.2)\n    abline(v = mu, col = \"#e74c3c\", lwd = 2, lty = 2)\n\n    hist(means, breaks = 40, col = adjustcolor(\"#2ecc71\", 0.6), border = \"white\",\n         main = paste0(\"Sample Means (SE = \", round(sd(means), 2), \")\"),\n         xlab = \"Value\", xlim = xlims, freq = FALSE, cex.main = 1.2)\n    abline(v = mu, col = \"#e74c3c\", lwd = 2, lty = 2)\n  })\n\n  output$se_curve &lt;- renderPlot({\n    sigma &lt;- input$sigma\n    n_curr &lt;- input$n_size\n\n    ns &lt;- 2:500\n    ses &lt;- sigma / sqrt(ns)\n    se_now &lt;- sigma / sqrt(n_curr)\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(ns, ses, type = \"l\", lwd = 2.5, col = \"#2c3e50\",\n         xlab = \"Sample Size (n)\", ylab = \"Standard Error\",\n         main = \"SE = SD / sqrt(n) — diminishing returns\",\n         ylim = c(0, max(ses) * 1.1), cex.main = 1.3)\n\n    abline(h = sigma, col = \"#3498db\", lwd = 2, lty = 2)\n    points(n_curr, se_now, pch = 19, cex = 2, col = \"#e74c3c\")\n    segments(n_curr, 0, n_curr, se_now, lty = 2, col = \"#e74c3c\")\n    segments(0, se_now, n_curr, se_now, lty = 2, col = \"#e74c3c\")\n\n    text(400, sigma + 0.8, paste0(\"SD = \", sigma), col = \"#3498db\", cex = 1.2, font = 2)\n    text(n_curr + 25, se_now + 0.8, paste0(\"SE = \", round(se_now, 2)),\n         col = \"#e74c3c\", cex = 1.1, font = 2, adj = 0)\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Probability & Uncertainty",
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#se-and-mde-why-se-determines-your-experiments-power",
    "href": "variance-sd-se.html#se-and-mde-why-se-determines-your-experiments-power",
    "title": "Variance, SD & Standard Error",
    "section": "SE and MDE: Why SE Determines Your Experiment’s Power",
    "text": "SE and MDE: Why SE Determines Your Experiment’s Power\nIf you run an experiment, the Minimum Detectable Effect (MDE) — the smallest effect you can reliably catch — is directly driven by the SE:\n\\[MDE = (z_{\\alpha/2} + z_{\\beta}) \\times SE\\]\nFor a two-sample experiment with equal groups (\\(\\sigma = 1\\) for simplicity):\n\\[MDE = (z_{\\alpha/2} + z_{\\beta}) \\times \\sqrt{\\frac{2}{n}}\\]\nThat \\(\\sqrt{2/n}\\) term is the SE of the difference in means. So MDE is just a scaled-up SE. The critical values (\\(z_{\\alpha/2} + z_{\\beta} \\approx 2.8\\) for 5% significance and 80% power) are just multipliers.\nThis means:\n\nShrink SE → shrink MDE → detect smaller effects. The only levers are: increase \\(n\\), or reduce \\(\\sigma\\) (better measurement, stratification, controls).\nPower analysis is really an SE calculation. You’re asking: “How small can I make my SE with this sample size?”\nIf your SE is too large, no statistical test can save you. The effect will be buried in noise.\n\nSee the Power, Alpha, Beta & MDE page for interactive simulations.",
    "crumbs": [
      "Probability & Uncertainty",
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#quick-reference",
    "href": "variance-sd-se.html#quick-reference",
    "title": "Variance, SD & Standard Error",
    "section": "Quick Reference",
    "text": "Quick Reference\n\n\n\n\n\n\n\n\n\n\nVariance (σ² or s²)\nStandard Deviation (σ or s)\nStandard Error (SE)\n\n\n\n\nMeasures\nSpread of data (squared)\nSpread of data (original units)\nPrecision of an estimate\n\n\nPopulation\n\\(\\sigma^2 = \\frac{1}{N}\\sum(x_i - \\mu)^2\\)\n\\(\\sigma = \\sqrt{\\sigma^2}\\)\n\\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\n\nSample\n\\(s^2 = \\frac{1}{n-1}\\sum(x_i - \\bar{x})^2\\)\n\\(s = \\sqrt{s^2}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\nChanges with n?\nConverges to σ²\nConverges to σ\nShrinks as \\(1/\\sqrt{n}\\)\n\n\nUsed for\nMath convenience\nDescribing data\nConfidence intervals, hypothesis tests\n\n\n\n\nCommon Mistakes\n\nReporting SD when you mean SE (or vice versa). SD describes how variable the data is. SE describes how precisely you’ve estimated the mean. A study with low SD but high SE has consistent data but too few observations.\nThinking SE measures data quality. SE can be tiny even with messy data — you just need a big enough \\(n\\). It says nothing about whether your data is clean or your measurements are accurate.\nForgetting that SE applies to any estimator, not just the mean. The slope in a regression has an SE. A proportion has an SE. Any time you estimate something from data, there’s uncertainty in that estimate — and the SE quantifies it.",
    "crumbs": [
      "Probability & Uncertainty",
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "variance-sd-se.html#did-you-know",
    "href": "variance-sd-se.html#did-you-know",
    "title": "Variance, SD & Standard Error",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe n − 1 Story\nThe correction for dividing by \\(n - 1\\) instead of \\(n\\) is called Bessel’s correction, named after Friedrich Bessel (1784–1846), a German astronomer who was obsessed with measurement precision. He needed to compute the orbits of stars and realized that using \\(n\\) in the denominator systematically underestimated how uncertain his measurements really were. His fix — dividing by \\(n - 1\\) — is now used billions of times a day in software from Excel to R to Python. Bessel never saw a computer, but his correction is baked into every call to var() and sd() in R and numpy.std(ddof=1) in Python.\n\n\nWhy “Standard” Error?\nThe word “standard” in “standard deviation” and “standard error” simply means “typical.” Karl Pearson coined “standard deviation” in 1893 because he wanted a word for the typical amount by which values deviate from their average. “Standard error” then naturally means the typical amount by which your estimate deviates from the truth. Nothing fancy — just “how far off is typical?”",
    "crumbs": [
      "Probability & Uncertainty",
      "Variance, SD & Standard Error"
    ]
  },
  {
    "objectID": "omitted-variable-bias.html",
    "href": "omitted-variable-bias.html",
    "title": "Omitted Variable Bias",
    "section": "",
    "text": "Suppose the true model is:\n\\[Y = \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon\\]\nIf you omit \\(X_2\\) and run the short regression \\(Y = \\tilde{\\beta}_1 X_1 + u\\), the short-regression estimator converges to:\n\\[\\tilde{\\beta}_1 \\xrightarrow{p} \\beta_1 + \\beta_2 \\, \\delta\\]\nwhere \\(\\delta\\) is the coefficient from regressing \\(X_2\\) on \\(X_1\\) (the auxiliary regression). The bias has a clean interpretation:\n\\[\\text{Bias} = \\underbrace{\\beta_2}_{\\text{effect of omitted}} \\times \\underbrace{\\delta}_{\\text{correlation with included}}\\]\nIf the omitted variable doesn’t affect \\(Y\\) (\\(\\beta_2 = 0\\)) or is uncorrelated with \\(X_1\\) (\\(\\delta = 0\\)), there is no bias. Both links in the chain must be present.",
    "crumbs": [
      "Regression",
      "Omitted Variable Bias"
    ]
  },
  {
    "objectID": "omitted-variable-bias.html#the-formula",
    "href": "omitted-variable-bias.html#the-formula",
    "title": "Omitted Variable Bias",
    "section": "",
    "text": "Suppose the true model is:\n\\[Y = \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon\\]\nIf you omit \\(X_2\\) and run the short regression \\(Y = \\tilde{\\beta}_1 X_1 + u\\), the short-regression estimator converges to:\n\\[\\tilde{\\beta}_1 \\xrightarrow{p} \\beta_1 + \\beta_2 \\, \\delta\\]\nwhere \\(\\delta\\) is the coefficient from regressing \\(X_2\\) on \\(X_1\\) (the auxiliary regression). The bias has a clean interpretation:\n\\[\\text{Bias} = \\underbrace{\\beta_2}_{\\text{effect of omitted}} \\times \\underbrace{\\delta}_{\\text{correlation with included}}\\]\nIf the omitted variable doesn’t affect \\(Y\\) (\\(\\beta_2 = 0\\)) or is uncorrelated with \\(X_1\\) (\\(\\delta = 0\\)), there is no bias. Both links in the chain must be present.",
    "crumbs": [
      "Regression",
      "Omitted Variable Bias"
    ]
  },
  {
    "objectID": "omitted-variable-bias.html#sign-of-bias-table",
    "href": "omitted-variable-bias.html#sign-of-bias-table",
    "title": "Omitted Variable Bias",
    "section": "Sign-of-bias table",
    "text": "Sign-of-bias table\nYou can sign the bias without knowing magnitudes — just think about the two ingredients:\n\n\n\n\n\n\n\n\n\n\\(\\delta &gt; 0\\) (positive correlation)\n\\(\\delta &lt; 0\\) (negative correlation)\n\n\n\n\n\\(\\beta_2 &gt; 0\\) (positive effect)\nPositive bias (overestimate)\nNegative bias (underestimate)\n\n\n\\(\\beta_2 &lt; 0\\) (negative effect)\nNegative bias (underestimate)\nPositive bias (overestimate)\n\n\n\nExample 1 — Returns to education, omitting ability. Ability likely has a positive effect on wages (\\(\\beta_2 &gt; 0\\)) and is positively correlated with education (\\(\\delta &gt; 0\\)). Omitting ability biases the return to education upward.\nExample 2 — Class size and test scores, omitting SES. Higher SES likely raises scores (\\(\\beta_2 &gt; 0\\)) and wealthier districts may have smaller classes (\\(\\delta &lt; 0\\)). Omitting SES biases the class-size effect downward (makes class size look more harmful than it is).\n\n\n\n\n\n\nThe Oracle View. In the simulation below, we set the true \\(\\beta_2\\) and \\(\\delta\\) and can verify the OVB formula exactly. In practice, you don’t observe the omitted variable — otherwise you’d include it. The formula tells you what direction the bias goes, which is often enough to sign the problem.",
    "crumbs": [
      "Regression",
      "Omitted Variable Bias"
    ]
  },
  {
    "objectID": "omitted-variable-bias.html#simulation",
    "href": "omitted-variable-bias.html#simulation",
    "title": "Omitted Variable Bias",
    "section": "Simulation",
    "text": "Simulation\nLeft panel: sampling distributions of the short regression (omitting \\(X_2\\)) vs the long regression (including \\(X_2\\)). Right panel: realized bias across simulations vs the formula prediction \\(\\beta_2 \\times \\delta\\).\n#| standalone: true\n#| viewerHeight: 750\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .eq-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-bottom: 14px; font-size: 14px; line-height: 1.9;\n    }\n    .eq-box b { color: #2c3e50; }\n    .match  { color: #27ae60; font-weight: bold; }\n    .coef   { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 4,\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 50, max = 500, value = 200, step = 50),\n\n      sliderInput(\"b1\", HTML(\"True &beta;&lt;sub&gt;1&lt;/sub&gt;:\"),\n                  min = -3, max = 3, value = 1.5, step = 0.1),\n\n      sliderInput(\"b2\", HTML(\"True &beta;&lt;sub&gt;2&lt;/sub&gt; (omitted variable effect):\"),\n                  min = -3, max = 3, value = 1, step = 0.1),\n\n      sliderInput(\"delta\", HTML(\"&delta; = Corr(X&lt;sub&gt;1&lt;/sub&gt;, X&lt;sub&gt;2&lt;/sub&gt;) direction:\"),\n                  min = -0.9, max = 0.9, value = 0.6, step = 0.1),\n\n      sliderInput(\"sigma\", HTML(\"Error SD (&sigma;):\"),\n                  min = 0.5, max = 5, value = 1, step = 0.5),\n\n      actionButton(\"resim\", \"Run simulations\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_box\")\n    ),\n\n    mainPanel(\n      width = 8,\n      fluidRow(\n        column(6, plotOutput(\"plot_dist\", height = \"450px\")),\n        column(6, plotOutput(\"plot_bias\", height = \"450px\"))\n      ),\n      uiOutput(\"formula_box\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  sim_results &lt;- reactive({\n    input$resim\n    n     &lt;- input$n\n    b1    &lt;- input$b1\n    b2    &lt;- input$b2\n    delta &lt;- input$delta\n    sigma &lt;- input$sigma\n    n_sims &lt;- 500\n\n    short_coefs &lt;- numeric(n_sims)\n    long_coefs  &lt;- numeric(n_sims)\n\n    for (i in seq_len(n_sims)) {\n      z1 &lt;- rnorm(n)\n      z2 &lt;- rnorm(n)\n      x1 &lt;- z1\n      x2 &lt;- delta * z1 + sqrt(1 - delta^2) * z2\n      eps &lt;- rnorm(n, sd = sigma)\n      y &lt;- b1 * x1 + b2 * x2 + eps\n\n      short_coefs[i] &lt;- coef(lm(y ~ x1))[2]\n      long_coefs[i]  &lt;- coef(lm(y ~ x1 + x2))[\"x1\"]\n    }\n\n    list(short = short_coefs, long = long_coefs,\n         b1 = b1, b2 = b2, delta = delta,\n         formula_bias = b2 * delta)\n  })\n\n  output$plot_dist &lt;- renderPlot({\n    d &lt;- sim_results()\n    par(mar = c(5, 5, 4, 2))\n\n    rng &lt;- range(c(d$short, d$long))\n    brks &lt;- seq(rng[1] - 0.1, rng[2] + 0.1, length.out = 40)\n\n    hist(d$long, breaks = brks, col = adjustcolor(\"#27ae60\", 0.4),\n         border = \"white\", main = expression(\"Sampling distributions of \" * hat(beta)[1]),\n         xlab = expression(hat(beta)[1]), freq = FALSE,\n         xlim = rng, ylim = c(0, max(\n           hist(d$long, breaks = brks, plot = FALSE)$density,\n           hist(d$short, breaks = brks, plot = FALSE)$density\n         ) * 1.2))\n    hist(d$short, breaks = brks, col = adjustcolor(\"#e74c3c\", 0.4),\n         border = \"white\", add = TRUE, freq = FALSE)\n\n    abline(v = d$b1, lty = 2, lwd = 2, col = \"#2c3e50\")\n    abline(v = d$b1 + d$formula_bias, lty = 2, lwd = 2, col = \"#e74c3c\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Long regression (unbiased)\",\n                      \"Short regression (biased)\",\n                      expression(\"True \" * beta[1]),\n                      expression(beta[1] + beta[2] * delta)),\n           col = c(adjustcolor(\"#27ae60\", 0.6),\n                   adjustcolor(\"#e74c3c\", 0.6),\n                   \"#2c3e50\", \"#e74c3c\"),\n           pch = c(15, 15, NA, NA), lwd = c(NA, NA, 2, 2),\n           lty = c(NA, NA, 2, 2), pt.cex = 2)\n  })\n\n  output$plot_bias &lt;- renderPlot({\n    d &lt;- sim_results()\n    par(mar = c(5, 5, 4, 2))\n\n    realized_bias &lt;- d$short - d$b1\n    hist(realized_bias, breaks = 35,\n         col = adjustcolor(\"#3498db\", 0.5), border = \"white\",\n         main = \"Realized bias vs formula prediction\",\n         xlab = expression(tilde(beta)[1] - beta[1]),\n         freq = FALSE)\n\n    abline(v = d$formula_bias, col = \"#e74c3c\", lwd = 3)\n    abline(v = mean(realized_bias), col = \"#2c3e50\", lwd = 2, lty = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\n             paste0(\"Formula: \", round(d$formula_bias, 3)),\n             paste0(\"Mean realized: \", round(mean(realized_bias), 3))\n           ),\n           col = c(\"#e74c3c\", \"#2c3e50\"),\n           lwd = c(3, 2), lty = c(1, 2))\n  })\n\n  output$results_box &lt;- renderUI({\n    d &lt;- sim_results()\n    tags$div(class = \"eq-box\", style = \"margin-top: 16px;\",\n      HTML(paste0(\n        \"&lt;b&gt;OVB Formula:&lt;/b&gt;&lt;br&gt;\",\n        \"Bias = &beta;&lt;sub&gt;2&lt;/sub&gt; &times; &delta; = \",\n        d$b2, \" &times; \", d$delta, \" = &lt;span class='coef'&gt;\",\n        round(d$formula_bias, 3), \"&lt;/span&gt;&lt;br&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Mean short estimate:&lt;/b&gt; \", round(mean(d$short), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Mean long estimate:&lt;/b&gt; \", round(mean(d$long), 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;True &beta;&lt;sub&gt;1&lt;/sub&gt;:&lt;/b&gt; \", d$b1\n      ))\n    )\n  })\n\n  output$formula_box &lt;- renderUI({\n    tags$div(class = \"eq-box\", style = \"margin-top: 8px;\",\n      HTML(paste0(\n        \"&lt;b&gt;Key:&lt;/b&gt; The short regression (red) is centered at \",\n        \"&beta;&lt;sub&gt;1&lt;/sub&gt; + &beta;&lt;sub&gt;2&lt;/sub&gt;&delta;, not at &beta;&lt;sub&gt;1&lt;/sub&gt;. \",\n        \"The long regression (green) is centered at the truth. \",\n        \"Both concentrate as n grows, but the short regression concentrates \",\n        \"around the &lt;i&gt;wrong&lt;/i&gt; value.\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nSet \\(\\beta_2 = 0\\): no matter what \\(\\delta\\) is, the short regression is unbiased. The omitted variable doesn’t affect \\(Y\\).\nSet \\(\\delta = 0\\): the omitted variable affects \\(Y\\) but is uncorrelated with \\(X_1\\). No bias — omitting a relevant but orthogonal variable is harmless for \\(\\hat{\\beta}_1\\).\nMake both large: the two histograms separate visibly. The bias is \\(\\beta_2 \\times \\delta\\).\nIncrease \\(n\\): both distributions get tighter, but the short regression still converges to the wrong value.\n\n\n\nThe bottom line\n\nOmitting a variable biases the included coefficient if and only if the omitted variable (1) affects \\(Y\\) and (2) correlates with the included \\(X\\).\nThe bias doesn’t vanish with more data — it’s a probability limit, not a finite-sample problem.\nThe sign-of-bias table lets you reason about direction without knowing magnitudes.\n\n\n\n\nConnections\n\nFrisch-Waugh-Lovell — FWL shows mechanically what controlling for \\(X_2\\) does; OVB shows what happens when you don’t.\nFrom Correlation to Causation — OVB is the main reason correlation ≠ causation.\nSelection on Observables — When you can observe the confounders, controlling for them removes OVB.\n\n\n\n\nDid you know?\n\nThe OVB formula is arguably the single most important result in applied econometrics. Joshua Angrist and Jörn-Steffen Pischke call it the “lingua franca” of empirical economics in Mostly Harmless Econometrics.\nOVB is the formal version of “correlation does not imply causation.” Every confounding story is an OVB story: there exists some \\(X_2\\) that affects \\(Y\\) and correlates with \\(X_1\\).\nThe formula generalizes to the multivariate case via the FWL theorem — the bias from omitting a set of variables equals the effect of those variables times their auxiliary regression coefficients on the included variables.",
    "crumbs": [
      "Regression",
      "Omitted Variable Bias"
    ]
  },
  {
    "objectID": "regression-algebra.html",
    "href": "regression-algebra.html",
    "title": "The Algebra Behind OLS",
    "section": "",
    "text": "Everything so far has described OLS in words: the best linear approximation to the CEF, the line that minimizes squared residuals, the thing that partials out controls. This page shows where the numbers actually come from — the matrix algebra that produces coefficient estimates and their standard errors.",
    "crumbs": [
      "Algebra of Regression",
      "The Algebra Behind OLS"
    ]
  },
  {
    "objectID": "regression-algebra.html#from-scalar-to-matrix",
    "href": "regression-algebra.html#from-scalar-to-matrix",
    "title": "The Algebra Behind OLS",
    "section": "From scalar to matrix",
    "text": "From scalar to matrix\nIn simple regression (\\(Y = \\beta_0 + \\beta_1 X + \\varepsilon\\)), you already know the formula:\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)}\n\\]\nThat’s one regressor and one coefficient. But the moment you add a second regressor — say \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon\\) — this scalar formula breaks down. You can’t just compute \\(\\text{Cov}(X_1, Y) / \\text{Var}(X_1)\\) and call it \\(\\hat{\\beta}_1\\), because that ignores the correlation between \\(X_1\\) and \\(X_2\\). You need to adjust for all the regressors simultaneously.\nMatrix notation handles this cleanly. Stack all \\(n\\) observations into:\n\\[\n\\underset{(n \\times 1)}{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, \\qquad\n\\underset{(n \\times k)}{X} = \\begin{bmatrix} 1 & x_{11} & x_{12} & \\cdots & x_{1,k-1} \\\\ 1 & x_{21} & x_{22} & \\cdots & x_{2,k-1} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & \\cdots & x_{n,k-1} \\end{bmatrix}, \\qquad\n\\underset{(k \\times 1)}{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{k-1} \\end{bmatrix}\n\\]\nThe first column of \\(X\\) is all ones — that’s the intercept. Now the model is just \\(y = X\\beta + \\varepsilon\\), regardless of how many regressors you have.",
    "crumbs": [
      "Algebra of Regression",
      "The Algebra Behind OLS"
    ]
  },
  {
    "objectID": "regression-algebra.html#the-ols-solution",
    "href": "regression-algebra.html#the-ols-solution",
    "title": "The Algebra Behind OLS",
    "section": "The OLS solution",
    "text": "The OLS solution\nOLS minimizes \\(\\sum_i e_i^2 = e'e = (y - X\\hat{\\beta})'(y - X\\hat{\\beta})\\). Take the derivative with respect to \\(\\hat{\\beta}\\), set it to zero, and you get the normal equations:\n\\[\nX'X\\hat{\\beta} = X'y\n\\]\nSolve for \\(\\hat{\\beta}\\):\n\\[\n\\boxed{\\hat{\\beta} = (X'X)^{-1}X'y}\n\\]\nThat’s it. Every OLS coefficient you’ve ever seen comes from this formula.\nWhat is \\((X'X)^{-1}\\) doing? The matrix \\(X'X\\) captures how the regressors relate to each other — their variances and covariances. Inverting it adjusts for the fact that regressors are correlated. This is exactly what Frisch-Waugh-Lovell does when it “partials out” controls: the mechanical operation of partialling out is the \\((X'X)^{-1}\\) adjustment.\n\n\n\n\n\n\nIntuition. If the regressors were perfectly uncorrelated (orthogonal), \\(X'X\\) would be diagonal and inverting it would just divide by each regressor’s variance — exactly the scalar formula \\(\\text{Cov}(X,Y)/\\text{Var}(X)\\) applied separately. Correlation between regressors is what forces us into matrix algebra.",
    "crumbs": [
      "Algebra of Regression",
      "The Algebra Behind OLS"
    ]
  },
  {
    "objectID": "regression-algebra.html#the-gauss-markov-assumptions",
    "href": "regression-algebra.html#the-gauss-markov-assumptions",
    "title": "The Algebra Behind OLS",
    "section": "The Gauss-Markov assumptions",
    "text": "The Gauss-Markov assumptions\nThe OLS formula \\(\\hat{\\beta} = (X'X)^{-1}X'y\\) is purely mechanical — it always gives you a number. But whether that number has good properties (unbiased, minimum variance) depends on a set of assumptions known as the Gauss-Markov conditions:\n\nLinearity. The true model is \\(y = X\\beta + \\varepsilon\\) — the relationship between \\(y\\) and \\(X\\) is linear in the parameters.\nStrict exogeneity. \\(E[\\varepsilon \\mid X] = 0\\) — the errors are mean-zero and uncorrelated with the regressors. This rules out omitted variable bias, reverse causality, and measurement error in \\(X\\).\nNo perfect multicollinearity. The columns of \\(X\\) are linearly independent, so \\(X'X\\) is invertible. (Near-collinearity is allowed but inflates standard errors — see below.)\nHomoskedasticity. \\(\\text{Var}(\\varepsilon \\mid X) = \\sigma^2 I\\) — the error variance is constant across observations.\nNo autocorrelation. \\(\\text{Cov}(\\varepsilon_i, \\varepsilon_j \\mid X) = 0\\) for \\(i \\neq j\\) — errors are uncorrelated with each other.\n\nUnder assumptions 1-3, OLS is unbiased: \\(E[\\hat{\\beta} \\mid X] = \\beta\\). Add 4 and 5, and the Gauss-Markov theorem kicks in: OLS is the Best Linear Unbiased Estimator (BLUE) — no other linear, unbiased estimator has smaller variance.\n\n\n\n\n\n\nWhat BLUE does and doesn’t say. Gauss-Markov says OLS is the best among linear unbiased estimators. It says nothing about nonlinear estimators — and it doesn’t require normality. If you add a sixth assumption (\\(\\varepsilon \\sim\nN(0, \\sigma^2 I)\\)), then OLS is also MLE and is the best unbiased estimator, period (not just among linear ones).\n\n\n\nNotice that assumptions 4 and 5 are about the standard errors, not the coefficient estimates. If they fail, \\(\\hat{\\beta}\\) is still unbiased — but the formula for its variance changes. That’s exactly what heteroskedasticity-robust SEs and clustered SEs fix.",
    "crumbs": [
      "Algebra of Regression",
      "The Algebra Behind OLS"
    ]
  },
  {
    "objectID": "regression-algebra.html#the-variance-covariance-matrix",
    "href": "regression-algebra.html#the-variance-covariance-matrix",
    "title": "The Algebra Behind OLS",
    "section": "The variance-covariance matrix",
    "text": "The variance-covariance matrix\nWe have estimates \\(\\hat{\\beta}\\). How precise are they? Under the Gauss-Markov assumptions (homoskedastic errors with variance \\(\\sigma^2\\), uncorrelated with \\(X\\)):\n\\[\n\\boxed{\\text{Var}(\\hat{\\beta}) = \\sigma^2 (X'X)^{-1}}\n\\]\nThis is a \\(k \\times k\\) matrix — one row and column for each coefficient (including the intercept). Here’s what lives inside it:\n\n\n\n\n\n\n\n\nPosition\nContains\nTells you\n\n\n\n\nDiagonal entry \\((j, j)\\)\n\\(\\text{Var}(\\hat{\\beta}_j)\\)\nHow uncertain you are about \\(\\hat{\\beta}_j\\)\n\n\nOff-diagonal entry \\((j, m)\\)\n\\(\\text{Cov}(\\hat{\\beta}_j, \\hat{\\beta}_m)\\)\nHow estimation error in one coefficient relates to another\n\n\n\nStandard errors are the square roots of the diagonal:\n\\[\n\\text{SE}(\\hat{\\beta}_j) = \\sqrt{\\text{Var}(\\hat{\\beta}_j)} = \\sqrt{\\sigma^2 \\left[(X'X)^{-1}\\right]_{jj}}\n\\]\nIn practice, \\(\\sigma^2\\) is unknown and estimated by:\n\\[\n\\hat{\\sigma}^2 = \\frac{e'e}{n - k} = \\frac{\\sum_i \\hat{e}_i^2}{n - k}\n\\]\nwhere \\(e = y - X\\hat{\\beta}\\) are the residuals and \\(n - k\\) corrects for the degrees of freedom used up by estimating \\(k\\) parameters. This is the same \\(n-1\\) logic from Variance, SD & SE, generalized to \\(k\\) parameters.",
    "crumbs": [
      "Algebra of Regression",
      "The Algebra Behind OLS"
    ]
  },
  {
    "objectID": "regression-algebra.html#what-makes-standard-errors-big-or-small",
    "href": "regression-algebra.html#what-makes-standard-errors-big-or-small",
    "title": "The Algebra Behind OLS",
    "section": "What makes standard errors big or small",
    "text": "What makes standard errors big or small\nThe formula \\(\\text{Var}(\\hat{\\beta}) = \\sigma^2(X'X)^{-1}\\) tells you exactly what controls precision. There are four levers:\nMore data (\\(n\\) large). Each additional observation adds a row to \\(X\\), making \\(X'X\\) larger. A larger \\(X'X\\) means a smaller \\((X'X)^{-1}\\), which means smaller standard errors. This is why standard errors shrink at rate \\(1/\\sqrt{n}\\).\nMore spread in \\(X\\). If your regressor has high variance, \\(X'X\\) is large in the relevant entries. More variation in \\(X\\) gives you more “leverage” to estimate the slope — like fitting a line through points that are far apart rather than bunched together.\nLess noise (\\(\\sigma^2\\) small). If the errors are small, the data points hug the regression line closely, and you can estimate the slope precisely. This enters directly as a multiplier on the whole variance matrix.\nCorrelated regressors (multicollinearity). When regressors are highly correlated, \\(X'X\\) is close to singular — its determinant is near zero. Inverting a nearly singular matrix produces huge numbers, so \\((X'X)^{-1}\\) blows up and standard errors explode. The data can’t tell the regressors apart, so it can’t precisely attribute the effect to one vs. the other. This is multicollinearity: the estimates are still unbiased, but they’re noisy.\n\n\n\n\n\n\nRule of thumb. If adding a regressor barely changes \\(\\hat{\\beta}\\) but doubles its standard error, multicollinearity is probably the culprit.",
    "crumbs": [
      "Algebra of Regression",
      "The Algebra Behind OLS"
    ]
  },
  {
    "objectID": "regression-algebra.html#connecting-to-the-rest-of-the-course",
    "href": "regression-algebra.html#connecting-to-the-rest-of-the-course",
    "title": "The Algebra Behind OLS",
    "section": "Connecting to the rest of the course",
    "text": "Connecting to the rest of the course\nThe formula \\(\\text{Var}(\\hat{\\beta}) = \\sigma^2(X'X)^{-1}\\) assumes homoskedastic, independent errors. The rest of the course shows what happens when those assumptions fail — and each fix modifies this formula in a specific way.\nHeteroskedasticity. When \\(\\sigma^2\\) isn’t constant across observations, the formula \\(\\sigma^2(X'X)^{-1}\\) is wrong. Robust (sandwich) standard errors replace it with:\n\\[\n\\text{Var}(\\hat{\\beta}) = (X'X)^{-1}\\left(\\sum_i \\hat{e}_i^2 \\, x_i x_i'\\right)(X'X)^{-1}\n\\]\nThe middle term lets each observation contribute its own squared residual instead of assuming a single \\(\\sigma^2\\) for everyone.\nClustered SEs. When observations within groups are correlated, the sandwich gets an additional layer: residuals are summed within clusters before forming the middle matrix. This accounts for the fact that 100 observations in 10 clusters carry less information than 100 independent observations.\nFWL. The partialling-out operation in Frisch-Waugh-Lovell is exactly what \\((X'X)^{-1}\\) does mechanically. When you residualize \\(Y\\) and \\(X_1\\) on \\(X_2\\), the slope of the residualized regression equals the \\(\\hat{\\beta}_1\\) from the full regression — because both are doing the same linear algebra.\nResiduals. The residual vector \\(e = y - X\\hat{\\beta}\\) can be written as \\(e = My\\), where:\n\\[\nM = I - X(X'X)^{-1}X'\n\\]\n\\(M\\) is the annihilator matrix — it projects \\(y\\) onto the space orthogonal to the columns of \\(X\\). Everything \\(X\\) can explain gets removed; what’s left is the residual. The matrix \\(H = X(X'X)^{-1}X'\\) is the hat matrix (it puts the “hat” on \\(y\\) to get \\(\\hat{y} = Hy\\)), and \\(M = I - H\\) is its complement.",
    "crumbs": [
      "Algebra of Regression",
      "The Algebra Behind OLS"
    ]
  },
  {
    "objectID": "maximum-likelihood.html",
    "href": "maximum-likelihood.html",
    "title": "Maximum Likelihood Estimation",
    "section": "",
    "text": "The workhorse of parametric estimation. Instead of matching moments, MLE asks: what parameter values make the observed data most probable? It’s more demanding than Method of Moments — you need to specify the entire distribution — but in return, you get the most efficient estimator that’s available in large samples.",
    "crumbs": [
      "Estimation",
      "Maximum Likelihood"
    ]
  },
  {
    "objectID": "maximum-likelihood.html#the-idea",
    "href": "maximum-likelihood.html#the-idea",
    "title": "Maximum Likelihood Estimation",
    "section": "The idea",
    "text": "The idea\nYou have data \\(x_1, \\ldots, x_n\\) and a model that says each observation was drawn from a distribution \\(f(x \\mid \\theta)\\), where \\(\\theta\\) is unknown. The likelihood function treats the data as fixed and asks: how probable is this particular dataset, as a function of \\(\\theta\\)?\n\\[\nL(\\theta) = \\prod_{i=1}^{n} f(x_i \\mid \\theta)\n\\]\nThat’s the joint density of the data, viewed as a function of the parameter rather than the data. The maximum likelihood estimator is the value of \\(\\theta\\) that makes the data most probable:\n\\[\n\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_\\theta \\; L(\\theta)\n\\]\nIn practice, products are annoying and numerically unstable, so we work with the log-likelihood:\n\\[\n\\ell(\\theta) = \\log L(\\theta) = \\sum_{i=1}^{n} \\log f(x_i \\mid \\theta)\n\\]\nSince \\(\\log\\) is monotonically increasing, maximizing \\(\\ell(\\theta)\\) and \\(L(\\theta)\\) give the same answer. The first-order condition is the score equation:\n\\[\n\\frac{\\partial \\ell(\\theta)}{\\partial \\theta} \\bigg|_{\\theta = \\hat{\\theta}} = 0\n\\]",
    "crumbs": [
      "Estimation",
      "Maximum Likelihood"
    ]
  },
  {
    "objectID": "maximum-likelihood.html#a-worked-example",
    "href": "maximum-likelihood.html#a-worked-example",
    "title": "Maximum Likelihood Estimation",
    "section": "A worked example",
    "text": "A worked example\nYou flip a coin \\(n\\) times and observe \\(k\\) heads. The probability of this specific sequence (assuming flips are independent) is:\n\\[\nL(p) = p^k (1-p)^{n-k}\n\\]\nThe log-likelihood is:\n\\[\n\\ell(p) = k \\log p + (n - k) \\log(1 - p)\n\\]\nTake the derivative and set it to zero:\n\\[\n\\frac{\\partial \\ell}{\\partial p} = \\frac{k}{p} - \\frac{n - k}{1 - p} = 0\n\\]\nSolving:\n\\[\n\\hat{p}_{\\text{MLE}} = \\frac{k}{n}\n\\]\nThe MLE is the sample proportion — exactly what you’d expect. Notice this is also what MoM gives you (set \\(E[X] = p\\) equal to the sample mean \\(k/n\\)). For this problem, MLE and MoM agree. That won’t always be the case.\n\n\n\n\n\n\nConnection to Bayesian updating. If you put a flat (uniform) prior on \\(p\\), the posterior distribution is proportional to the likelihood. The posterior mode — the peak of the posterior — equals the MLE. With a non-flat prior, the posterior mode gets pulled away from the MLE, as explored in Bayesian Estimation. For the full mechanics of how priors combine with likelihoods, see Bayesian Updating.\n\n\n\n\nSimulation: Likelihood surface for coin flips\nWatch the log-likelihood curve sharpen as you increase the number of flips. With few flips, many values of \\(p\\) look plausible (flat curve). With many flips, the curve becomes sharply peaked around the MLE — the data strongly distinguish the true parameter. This is Fisher information in action.\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_p\", \"True p:\",\n                  min = 0.1, max = 0.9, value = 0.6, step = 0.05),\n\n      sliderInput(\"n_flips\", \"Number of flips n:\",\n                  min = 5, max = 200, value = 20, step = 5),\n\n      actionButton(\"go\", \"Flip coins\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"ll_plot\", height = \"520px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    true_p  &lt;- input$true_p\n    n_flips &lt;- input$n_flips\n\n    # Simulate flips\n    flips &lt;- rbinom(n_flips, size = 1, prob = true_p)\n    k     &lt;- sum(flips)\n    mle   &lt;- k / n_flips\n\n    list(true_p = true_p, n_flips = n_flips, k = k, mle = mle)\n  })\n\n  output$ll_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    p_seq &lt;- seq(0.001, 0.999, length.out = 1000)\n\n    # Log-likelihood: k*log(p) + (n-k)*log(1-p)\n    ll &lt;- d$k * log(p_seq) + (d$n_flips - d$k) * log(1 - p_seq)\n\n    # Normalize so the max is 0 (relative log-likelihood)\n    ll &lt;- ll - max(ll)\n\n    plot(p_seq, ll, type = \"l\", lwd = 2.5, col = \"#3498db\",\n         xlab = \"p\", ylab = \"Log-likelihood (relative)\",\n         main = paste0(\"Log-likelihood: \", d$k, \" heads in \",\n                       d$n_flips, \" flips\"),\n         ylim = c(max(min(ll), -10), 0.5))\n\n    # MLE vertical line\n    abline(v = d$mle, lwd = 2.5, col = \"#e74c3c\")\n\n    # True p dashed line\n    abline(v = d$true_p, lty = 2, lwd = 2.5, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.9,\n           legend = c(paste0(\"MLE = k/n = \", round(d$mle, 4)),\n                      paste0(\"True p = \", d$true_p),\n                      \"Log-likelihood\"),\n           col = c(\"#e74c3c\", \"#2c3e50\", \"#3498db\"),\n           lwd = 2.5, lty = c(1, 2, 1))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n\n    # Observed Fisher information: curvature at MLE\n    # Second derivative of ll: -k/p^2 - (n-k)/(1-p)^2\n    # At the MLE p_hat = k/n:\n    # -n/p_hat - n/(1-p_hat) = -n/(p_hat*(1-p_hat))\n    # Fisher info = negative of that = n/(p_hat*(1-p_hat))\n    if (d$mle &gt; 0 && d$mle &lt; 1) {\n      fisher &lt;- d$n_flips / (d$mle * (1 - d$mle))\n    } else {\n      fisher &lt;- NA\n    }\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;n flips:&lt;/b&gt; \", d$n_flips, \"&lt;br&gt;\",\n        \"&lt;b&gt;k heads:&lt;/b&gt; \", d$k, \"&lt;br&gt;\",\n        \"&lt;b&gt;MLE = k/n:&lt;/b&gt; \", round(d$mle, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;True p:&lt;/b&gt; \", d$true_p, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Observed Fisher info:&lt;/b&gt; \",\n        if (!is.na(fisher)) round(fisher, 2) else \"undefined\",\n        \"&lt;br&gt;\",\n        \"&lt;b&gt;Approx SE:&lt;/b&gt; \",\n        if (!is.na(fisher) && fisher &gt; 0)\n          round(1 / sqrt(fisher), 4) else \"undefined\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Estimation",
      "Maximum Likelihood"
    ]
  },
  {
    "objectID": "maximum-likelihood.html#ols-is-mle-under-normality",
    "href": "maximum-likelihood.html#ols-is-mle-under-normality",
    "title": "Maximum Likelihood Estimation",
    "section": "OLS is MLE under normality",
    "text": "OLS is MLE under normality\nHere’s a fact that ties the course together. Suppose your regression model is:\n\\[\nY_i = X_i'\\beta + \\varepsilon_i, \\qquad \\varepsilon_i \\sim N(0, \\sigma^2)\n\\]\nThe log-likelihood for one observation is:\n\\[\n\\log f(Y_i \\mid X_i, \\beta, \\sigma^2) = -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(Y_i - X_i'\\beta)^2}{2\\sigma^2}\n\\]\nSum across all observations:\n\\[\n\\ell(\\beta, \\sigma^2) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (Y_i - X_i'\\beta)^2\n\\]\nTo maximize over \\(\\beta\\), you only need to worry about the second term. The \\(\\hat{\\beta}\\) that maximizes this log-likelihood is the one that minimizes \\(\\sum_i (Y_i - X_i'\\beta)^2\\) — which is OLS.\n\\[\n\\hat{\\beta}_{\\text{MLE}} = \\hat{\\beta}_{\\text{OLS}} = (X'X)^{-1}X'y\n\\]\nOLS is maximum likelihood, under the assumption that errors are normal. This is why the OLS formula feels so natural — it’s doing exactly what MLE would do in the Gaussian model. But notice: if errors aren’t normal, OLS is still the best linear unbiased estimator (by Gauss-Markov), but it’s no longer MLE.\nFor the full matrix algebra behind this, see The Algebra Behind OLS.\n\n\n\n\n\n\nWhy is OLS so famous? If MoM, MLE, GMM, and Bayesian estimation are all valid approaches, why does OLS dominate introductory courses and applied work?\n\nClosed-form solution — \\((X'X)^{-1}X'y\\) is a formula, not an iterative algorithm. Before computers, this mattered enormously.\nMinimal assumptions — Gauss-Markov says OLS is the best linear unbiased estimator without even needing normality. You don’t need to specify the full distribution.\nIt’s MLE when errors are normal — so in the most common case, OLS is the most efficient estimator. You get MLE for free.\nInterpretability — coefficients are partial derivatives of the conditional mean. Everyone can understand “a one-unit change in \\(X\\) is associated with a \\(\\hat{\\beta}\\) change in \\(Y\\).”\nHistorical momentum — Gauss and Legendre published it around 1800. By the time MLE (Fisher, 1920s) and GMM (Hansen, 1982) arrived, OLS had a 120+ year head start in textbooks.\n\nOLS hit a sweet spot: minimal assumptions, maximum interpretability, and a formula you can compute by hand. The other methods are more general and more powerful — but OLS solved 80% of problems with 20% of the effort.\nSo where is MLE? Everywhere — just under other names. Logistic regression is MLE. Cross-entropy training is MLE. Poisson regression, probit, Cox proportional hazards — all MLE. Every glm() call and every neural network training run is doing maximum likelihood. But MLE is a principle (“maximize the likelihood”), not a formula. The answer depends on the model: normal errors give you OLS, Bernoulli gives you logistic regression, neural networks give you SGD on cross-entropy. OLS is famous because it’s a concrete tool with a napkin formula. MLE is the invisible engine inside specific tools that get their own names.",
    "crumbs": [
      "Estimation",
      "Maximum Likelihood"
    ]
  },
  {
    "objectID": "maximum-likelihood.html#cross-entropy-is-mle-for-classification",
    "href": "maximum-likelihood.html#cross-entropy-is-mle-for-classification",
    "title": "Maximum Likelihood Estimation",
    "section": "Cross-entropy is MLE for classification",
    "text": "Cross-entropy is MLE for classification\nOLS is MLE when the errors are normal. There’s an equally clean equivalence for classification.\nSuppose you have binary outcomes \\(Y_i \\in \\{0, 1\\}\\) and a model that predicts \\(\\hat{p}_i = P(Y_i = 1 \\mid X_i)\\). The likelihood is:\n\\[\nL = \\prod_{i=1}^n \\hat{p}_i^{Y_i}(1 - \\hat{p}_i)^{1-Y_i}\n\\]\nThe log-likelihood is:\n\\[\n\\ell = \\sum_{i=1}^n \\left[Y_i \\log \\hat{p}_i + (1 - Y_i) \\log(1 - \\hat{p}_i)\\right]\n\\]\nThe cross-entropy loss is the negative of this, divided by \\(n\\):\n\\[\n\\text{CE} = -\\frac{1}{n}\\sum_{i=1}^n \\left[Y_i \\log \\hat{p}_i + (1 - Y_i) \\log(1 - \\hat{p}_i)\\right]\n\\]\nMinimizing cross-entropy = maximizing log-likelihood = MLE. When you train a logistic regression or a neural network with cross-entropy loss, you are doing maximum likelihood estimation. The loss function is not an arbitrary design choice — it comes directly from the Bernoulli likelihood.\nFor multi-class problems with \\(K\\) classes, the likelihood is multinomial and the cross-entropy generalizes to:\n\\[\n\\text{CE} = -\\frac{1}{n}\\sum_{i=1}^n \\sum_{k=1}^K Y_{ik} \\log \\hat{p}_{ik}\n\\]\nwhere \\(Y_{ik} = 1\\) if observation \\(i\\) belongs to class \\(k\\). This says: maximize the predicted probability of the correct class.\n\n\n\n\n\n\nThe pattern. OLS minimizes squared residuals = MLE under normality. Cross-entropy minimizes negative log-probability = MLE under Bernoulli/multinomial. In both cases, the “loss function” is just the negative log-likelihood of a specific probabilistic model. This is why Training as MLE argues that neural network training is MLE in disguise — the loss function is the likelihood.",
    "crumbs": [
      "Estimation",
      "Maximum Likelihood"
    ]
  },
  {
    "objectID": "maximum-likelihood.html#properties-of-mle",
    "href": "maximum-likelihood.html#properties-of-mle",
    "title": "Maximum Likelihood Estimation",
    "section": "Properties of MLE",
    "text": "Properties of MLE\nMLE is popular for good reason. Under regularity conditions (the parameter space is open, the model is identifiable, the likelihood is smooth enough):\nConsistent. \\(\\hat{\\theta}_{\\text{MLE}} \\xrightarrow{p} \\theta_0\\) as \\(n \\to \\infty\\). The estimator converges to the true value.\nAsymptotically efficient. No consistent estimator has smaller variance in large samples. MLE achieves the Cramer-Rao lower bound — it extracts the maximum amount of information from the data.\nAsymptotically normal.\n\\[\n\\sqrt{n}(\\hat{\\theta}_{\\text{MLE}} - \\theta_0) \\xrightarrow{d} N\\big(0, \\, \\mathcal{I}(\\theta_0)^{-1}\\big)\n\\]\nwhere \\(\\mathcal{I}(\\theta)\\) is the Fisher information (defined below). This means in large samples, MLE estimates are approximately normal, centered at the truth, with variance determined by Fisher information.\nHere’s how MLE compares to Method of Moments:\n\n\n\nProperty\nMLE\nMoM\n\n\n\n\nConsistency\nYes\nYes\n\n\nAsymptotic normality\nYes\nYes\n\n\nEfficiency\nAchieves Cramer-Rao bound\nGenerally less efficient\n\n\nAssumptions needed\nFull distribution\nJust moments\n\n\nComputation\nMay need numerical optimization\nUsually closed-form",
    "crumbs": [
      "Estimation",
      "Maximum Likelihood"
    ]
  },
  {
    "objectID": "maximum-likelihood.html#fisher-information-and-standard-errors",
    "href": "maximum-likelihood.html#fisher-information-and-standard-errors",
    "title": "Maximum Likelihood Estimation",
    "section": "Fisher information and standard errors",
    "text": "Fisher information and standard errors\nThe Fisher information measures how much information one observation carries about \\(\\theta\\):\n\\[\n\\mathcal{I}(\\theta) = -E\\!\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta^2}\\right]\n\\]\nThis is the expected curvature of the log-likelihood. A sharply curved log-likelihood — one with a pronounced peak — means a small change in \\(\\theta\\) causes a big drop in \\(\\ell\\). That’s high information: the data strongly distinguish the true \\(\\theta\\) from nearby values. A flat log-likelihood means low information: many parameter values look almost equally plausible.\n\n\n\n\n\n\nIntuition. Think of the log-likelihood as a mountain. Fisher information measures how steep the sides are near the peak. A sharp peak means the data pinpoint \\(\\theta\\) precisely. A broad, flat hilltop means you’re uncertain about where exactly the peak is.\n\n\n\nThe standard error of the MLE is:\n\\[\n\\text{SE}(\\hat{\\theta}) \\approx \\frac{1}{\\sqrt{n \\cdot \\mathcal{I}(\\theta)}}\n\\]\nMore data (\\(n\\) large) and more informative data (\\(\\mathcal{I}\\) large) both shrink the standard error.\nConnection to regression. For the normal linear model, the Fisher information matrix gives back the familiar variance-covariance matrix of OLS:\n\\[\n\\text{Var}(\\hat{\\beta}) = \\sigma^2(X'X)^{-1}\n\\]\nThis is derived in The Algebra Behind OLS. The \\((X'X)^{-1}\\) piece is the inverse of the Fisher information matrix for the regression setting. The two frameworks — “OLS algebra” and “MLE theory” — are telling you the same thing.",
    "crumbs": [
      "Estimation",
      "Maximum Likelihood"
    ]
  },
  {
    "objectID": "maximum-likelihood.html#limitations",
    "href": "maximum-likelihood.html#limitations",
    "title": "Maximum Likelihood Estimation",
    "section": "Limitations",
    "text": "Limitations\nRequires specifying the full distribution. MLE needs the entire density \\(f(x \\mid \\theta)\\), not just a few moments. If you specify the wrong distribution, the estimator will still converge — but to the parameter value that makes the assumed distribution closest to the truth in Kullback-Leibler divergence. That’s not necessarily what you want.\nCan be biased in small samples. Asymptotic efficiency is a large-sample result. In small samples, MLE can be biased. A classic example: the MLE of \\(\\sigma^2\\) in the normal distribution is \\(\\frac{1}{n}\\sum_i (X_i - \\bar{X})^2\\), which divides by \\(n\\) rather than \\(n-1\\) and is biased downward.\nSensitive to model misspecification. If the true data-generating process doesn’t belong to your parametric family, MLE converges to the “pseudo-true” value — the member of your family closest to the truth in KL divergence. This can be far from the parameter you intended to estimate.\nNumerical optimization. For many models, there’s no closed-form MLE. You need iterative algorithms (Newton-Raphson, EM, gradient descent), which can get stuck at local optima or fail to converge.\nThese limitations motivate approaches that require less structure: GMM only needs moment conditions (not the full distribution), and Bayesian Estimation lets you incorporate prior information to stabilize estimates.",
    "crumbs": [
      "Estimation",
      "Maximum Likelihood"
    ]
  },
  {
    "objectID": "monte-carlo.html",
    "href": "monte-carlo.html",
    "title": "Monte Carlo Experiments: How We Understand Estimators",
    "section": "",
    "text": "Every claim in this course — unbiasedness, coverage, power — was verified the same way: by simulation. A Monte Carlo experiment is simple:\n\nSpecify a data-generating process (DGP): decide the true parameter, the distribution, and the sample size.\nDraw a sample from the DGP and compute your estimator.\nRepeat many times (1,000–10,000 simulations).\nSummarize: look at the distribution of your estimates. Is the estimator centered on the truth (unbiased)? How spread out is it (variance)? How often does the CI cover the truth?\n\nThis is how statisticians check their own work. Theory says OLS is unbiased? Prove it — simulate 5,000 datasets and see if the average estimate equals the true value. Theory says the CI has 95% coverage? Simulate it.\n\n\n\n\n\n\nThe Oracle View. Monte Carlo is pure Oracle territory. We specify the entire data-generating process — true parameter, distribution, sample size — then generate thousands of datasets to study how estimators behave. In practice, you get one dataset from an unknown DGP. Monte Carlo is how statisticians verify their tools work before you use them on real data.",
    "crumbs": [
      "Inference",
      "Monte Carlo Experiments"
    ]
  },
  {
    "objectID": "monte-carlo.html#what-is-a-monte-carlo-experiment",
    "href": "monte-carlo.html#what-is-a-monte-carlo-experiment",
    "title": "Monte Carlo Experiments: How We Understand Estimators",
    "section": "",
    "text": "Every claim in this course — unbiasedness, coverage, power — was verified the same way: by simulation. A Monte Carlo experiment is simple:\n\nSpecify a data-generating process (DGP): decide the true parameter, the distribution, and the sample size.\nDraw a sample from the DGP and compute your estimator.\nRepeat many times (1,000–10,000 simulations).\nSummarize: look at the distribution of your estimates. Is the estimator centered on the truth (unbiased)? How spread out is it (variance)? How often does the CI cover the truth?\n\nThis is how statisticians check their own work. Theory says OLS is unbiased? Prove it — simulate 5,000 datasets and see if the average estimate equals the true value. Theory says the CI has 95% coverage? Simulate it.\n\n\n\n\n\n\nThe Oracle View. Monte Carlo is pure Oracle territory. We specify the entire data-generating process — true parameter, distribution, sample size — then generate thousands of datasets to study how estimators behave. In practice, you get one dataset from an unknown DGP. Monte Carlo is how statisticians verify their tools work before you use them on real data.",
    "crumbs": [
      "Inference",
      "Monte Carlo Experiments"
    ]
  },
  {
    "objectID": "monte-carlo.html#simulation-1-compare-estimators",
    "href": "monte-carlo.html#simulation-1-compare-estimators",
    "title": "Monte Carlo Experiments: How We Understand Estimators",
    "section": "Simulation 1: Compare estimators",
    "text": "Simulation 1: Compare estimators\nPick an estimator, pick a DGP, run \\(N\\) simulations. Compare bias, variance, and MSE (mean squared error = bias² + variance) across estimators.\n#| standalone: true\n#| viewerHeight: 1050\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 4,\n\n      selectInput(\"dgp\", \"DGP:\",\n                  choices = c(\"Normal(5, 2)\",\n                              \"Exponential(0.5)\",\n                              \"Contaminated normal\",\n                              \"Uniform(0, 10)\")),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 5, max = 200, value = 20, step = 5),\n\n      sliderInput(\"sims\", \"Simulations:\",\n                  min = 500, max = 5000, value = 2000, step = 500),\n\n      checkboxGroupInput(\"estimators\", \"Estimators:\",\n                         choices = c(\"Mean\", \"Median\",\n                                     \"Trimmed mean\",\n                                     \"Midrange\"),\n                         selected = c(\"Mean\", \"Median\",\n                                      \"Trimmed mean\")),\n\n      actionButton(\"go\", \"Run Monte Carlo\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 8,\n      plotOutput(\"dgp_plot\", height = \"250px\"),\n      plotOutput(\"hist_plot\", height = \"400px\"),\n      plotOutput(\"mse_plot\", height = \"300px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  draw_dgp &lt;- function(n, dgp) {\n    switch(dgp,\n      \"Normal(5, 2)\"          = rnorm(n, mean = 5, sd = 2),\n      \"Exponential(0.5)\" = rexp(n, rate = 0.5),\n      \"Contaminated normal\"   = {\n        k &lt;- rbinom(n, 1, 0.1)\n        (1 - k) * rnorm(n, 5, 1) + k * rnorm(n, 5, 10)\n      },\n      \"Uniform(0, 10)\"        = runif(n, 0, 10)\n    )\n  }\n\n  true_mu &lt;- function(dgp) {\n    switch(dgp,\n      \"Normal(5, 2)\"          = 5,\n      \"Exponential(0.5)\" = 2,\n      \"Contaminated normal\"   = 5,\n      \"Uniform(0, 10)\"        = 5\n    )\n  }\n\n  compute_est &lt;- function(x, est) {\n    switch(est,\n      \"Mean\"              = mean(x),\n      \"Median\"            = median(x),\n      \"Trimmed mean\" = mean(x, trim = 0.1),\n      \"Midrange\"          = (min(x) + max(x)) / 2\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    sims &lt;- input$sims\n    dgp  &lt;- input$dgp\n    ests &lt;- input$estimators\n    mu   &lt;- true_mu(dgp)\n\n    if (length(ests) == 0) ests &lt;- \"Mean\"\n\n    results &lt;- list()\n    for (est in ests) {\n      vals &lt;- replicate(sims, compute_est(draw_dgp(n, dgp), est))\n      bias &lt;- mean(vals) - mu\n      variance &lt;- var(vals)\n      mse &lt;- bias^2 + variance\n      results[[est]] &lt;- list(vals = vals, bias = bias,\n                             variance = variance, mse = mse)\n    }\n\n    list(results = results, mu = mu, n = n, sims = sims,\n         dgp = dgp, ests = ests)\n  })\n\n  output$dgp_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4, 5, 3, 2))\n\n    big &lt;- draw_dgp(10000, d$dgp)\n    hist(big, breaks = 60, probability = TRUE,\n         col = \"#e0e0e0\", border = \"#b0b0b0\",\n         main = paste(\"DGP:\", d$dgp),\n         xlab = \"\", ylab = \"Density\")\n    abline(v = d$mu, lty = 2, lwd = 2, col = \"#e74c3c\")\n  })\n\n  output$hist_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(5, 5, 4, 2))\n\n    cols &lt;- c(\"#3498db\", \"#e74c3c\", \"#27ae60\", \"#9b59b6\")\n    all_vals &lt;- unlist(lapply(d$results, function(r) r$vals))\n    xlim &lt;- quantile(all_vals, c(0.005, 0.995))\n\n    first &lt;- TRUE\n    for (i in seq_along(d$ests)) {\n      vals &lt;- d$results[[d$ests[i]]]$vals\n      if (first) {\n        hist(vals, breaks = 50, probability = TRUE,\n             col = adjustcolor(cols[i], 0.3),\n             border = adjustcolor(cols[i], 0.6),\n             main = paste0(\"Sampling distributions (\",\n                          d$sims, \" simulations, n = \", d$n, \")\"),\n             xlab = \"Estimate\", ylab = \"Density\",\n             xlim = xlim)\n        first &lt;- FALSE\n      } else {\n        hist(vals, breaks = 50, probability = TRUE,\n             col = adjustcolor(cols[i], 0.2),\n             border = adjustcolor(cols[i], 0.5),\n             add = TRUE)\n      }\n    }\n\n    abline(v = d$mu, lty = 2, lwd = 2.5, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(d$ests, paste0(\"True \\u03bc = \", d$mu)),\n           col = c(cols[seq_along(d$ests)], \"#2c3e50\"),\n           lwd = c(rep(8, length(d$ests)), 2.5),\n           lty = c(rep(1, length(d$ests)), 2))\n  })\n\n  output$mse_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(5, 10, 4, 2))\n\n    mses &lt;- sapply(d$results, function(r) r$mse)\n    biases2 &lt;- sapply(d$results, function(r) r$bias^2)\n    vars &lt;- sapply(d$results, function(r) r$variance)\n\n    cols &lt;- c(\"#3498db\", \"#e74c3c\", \"#27ae60\", \"#9b59b6\")\n    n_est &lt;- length(d$ests)\n\n    # Stacked bar: bias^2 + variance = MSE\n    mat &lt;- rbind(biases2, vars)\n\n    bp &lt;- barplot(mat, names.arg = d$ests, col = c(\"#e74c3c80\", \"#3498db80\"),\n                  horiz = TRUE, las = 1,\n                  main = \"MSE decomposition\",\n                  xlab = \"MSE = Bias\\u00b2 + Variance\",\n                  border = NA, cex.names = 0.75)\n\n    legend(\"bottomright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Bias\\u00b2\", \"Variance\"),\n           fill = c(\"#e74c3c80\", \"#3498db80\"), border = NA)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    lines &lt;- paste0(sapply(d$ests, function(est) {\n      r &lt;- d$results[[est]]\n      paste0(\n        \"&lt;b&gt;\", est, \":&lt;/b&gt;&lt;br&gt;\",\n        \"&nbsp; Bias: \", round(r$bias, 4), \"&lt;br&gt;\",\n        \"&nbsp; Var: \", round(r$variance, 4), \"&lt;br&gt;\",\n        \"&nbsp; MSE: \", round(r$mse, 4), \"&lt;br&gt;\"\n      )\n    }), collapse = \"&lt;hr style='margin:6px 0'&gt;\")\n\n    tags$div(class = \"stats-box\", HTML(lines))\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Inference",
      "Monte Carlo Experiments"
    ]
  },
  {
    "objectID": "monte-carlo.html#simulation-2-consistency-estimates-tighten-with-n",
    "href": "monte-carlo.html#simulation-2-consistency-estimates-tighten-with-n",
    "title": "Monte Carlo Experiments: How We Understand Estimators",
    "section": "Simulation 2: Consistency — estimates tighten with n",
    "text": "Simulation 2: Consistency — estimates tighten with n\nA consistent estimator converges to the true parameter as \\(n \\to \\infty\\). Watch how the sampling distribution of each estimator gets narrower and more centered as you increase the sample size.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 4,\n\n      selectInput(\"dgp2\", \"DGP:\",\n                  choices = c(\"Normal(5, 2)\",\n                              \"Exponential(0.5)\",\n                              \"Contaminated normal\")),\n\n      selectInput(\"est2\", \"Estimator:\",\n                  choices = c(\"Mean\", \"Median\", \"Trimmed mean\")),\n\n      sliderInput(\"sims2\", \"Simulations per n:\",\n                  min = 500, max = 3000, value = 1000, step = 250),\n\n      actionButton(\"go2\", \"Run\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 8,\n      plotOutput(\"consistency_plot\", height = \"530px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  draw_dgp &lt;- function(n, dgp) {\n    switch(dgp,\n      \"Normal(5, 2)\"          = rnorm(n, mean = 5, sd = 2),\n      \"Exponential(0.5)\" = rexp(n, rate = 0.5),\n      \"Contaminated normal\"   = {\n        k &lt;- rbinom(n, 1, 0.1)\n        (1 - k) * rnorm(n, 5, 1) + k * rnorm(n, 5, 10)\n      }\n    )\n  }\n\n  true_mu &lt;- function(dgp) {\n    switch(dgp,\n      \"Normal(5, 2)\"          = 5,\n      \"Exponential(0.5)\" = 2,\n      \"Contaminated normal\"   = 5\n    )\n  }\n\n  compute_est &lt;- function(x, est) {\n    switch(est,\n      \"Mean\"             = mean(x),\n      \"Median\"           = median(x),\n      \"Trimmed mean\" = mean(x, trim = 0.1)\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go2\n    dgp  &lt;- input$dgp2\n    est  &lt;- input$est2\n    sims &lt;- input$sims2\n    mu   &lt;- true_mu(dgp)\n\n    ns &lt;- c(5, 10, 25, 50, 100, 200)\n    all_results &lt;- list()\n    for (n in ns) {\n      all_results[[as.character(n)]] &lt;- replicate(\n        sims, compute_est(draw_dgp(n, dgp), est)\n      )\n    }\n\n    list(all_results = all_results, ns = ns, mu = mu,\n         dgp = dgp, est = est, sims = sims)\n  })\n\n  output$consistency_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(5, 6, 4, 2))\n\n    colors &lt;- colorRampPalette(c(\"#e74c3c\", \"#3498db\"))(length(d$ns))\n\n    all_vals &lt;- unlist(d$all_results)\n    xlim &lt;- quantile(all_vals, c(0.01, 0.99))\n\n    plot(NULL, xlim = xlim,\n         ylim = c(0.5, length(d$ns) * 1.5 + 0.5),\n         yaxt = \"n\", ylab = \"\",\n         xlab = paste0(\"Estimate (\", d$est, \")\"),\n         main = paste0(\"Consistency: \", d$est,\n                       \" under \", d$dgp))\n    positions &lt;- seq_along(d$ns) * 1.5\n    axis(2, at = positions, labels = paste0(\"n=\", d$ns),\n         las = 1, cex.axis = 0.9)\n\n    for (i in seq_along(d$ns)) {\n      vals &lt;- d$all_results[[i]]\n      dens &lt;- density(vals)\n      dens$y &lt;- dens$y / max(dens$y) * 0.65\n      polygon(dens$x, dens$y + positions[i],\n              col = adjustcolor(colors[i], 0.4),\n              border = colors[i], lwd = 1.5)\n    }\n\n    abline(v = d$mu, lty = 2, lwd = 2.5, col = \"#2c3e50\")\n    text(d$mu, max(positions) + 0.8,\n         paste0(\"True \\u03bc = \", d$mu), cex = 0.9)\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- dat()\n    lines &lt;- sapply(seq_along(d$ns), function(i) {\n      vals &lt;- d$all_results[[i]]\n      paste0(\"n=\", d$ns[i], \": SD = \", round(sd(vals), 4))\n    })\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Spread shrinks with n:&lt;/b&gt;&lt;br&gt;\",\n        paste(lines, collapse = \"&lt;br&gt;\")\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nNormal DGP, Mean vs Median: the mean has smaller variance (it’s the efficient estimator for normal data). The median is less efficient but still unbiased.\nContaminated normal, Mean vs Median: now the median wins. The outliers from the contamination inflate the variance of the mean, but barely affect the median. This is why robust estimators exist.\n10% trimmed mean: a compromise — it trims the most extreme 10% of observations. It handles outliers better than the mean, with less variance than the median. Often the best of both worlds.\nMidrange: the average of the min and max. Under normal data it’s surprisingly efficient for small \\(n\\), but terrible for heavy-tailed or skewed data. The MSE plot shows exactly why.\nConsistency (Sim 2): all three estimators converge to \\(\\mu\\) as \\(n\\) grows. The ridgeline plot gets tighter and more centered with each step.\n\n\n\nThe bottom line\n\nMonte Carlo is the universal verification tool in statistics. If you claim an estimator has some property (unbiased, consistent, 95% coverage), you can check it by simulation.\nBias = does the estimator aim at the right target? Variance = how spread out are the estimates? MSE = bias² + variance = overall accuracy.\nSometimes a biased estimator beats an unbiased one if it has much lower variance. This is the bias-variance tradeoff in miniature.\n\n\n\n\nDid you know?\n\nThe Monte Carlo method was invented by Stanislaw Ulam and John von Neumann during the Manhattan Project in the late 1940s. Ulam was recovering from brain surgery and playing solitaire when he realized that it was easier to estimate the probability of winning by playing many random games than by computing it analytically. He and von Neumann applied this idea to nuclear physics simulations.\nThe name “Monte Carlo” was suggested by Nicholas Metropolis, a colleague, as a reference to the Monte Carlo Casino in Monaco — a nod to the role of randomness. The method was classified as part of the hydrogen bomb program until the 1950s.\nToday, Monte Carlo methods are everywhere: pricing financial derivatives, training AI models, simulating protein folding, predicting elections, and rendering 3D graphics in movies. The basic idea — replace hard math with random simulation — is one of the most powerful tricks in computational science.",
    "crumbs": [
      "Inference",
      "Monte Carlo Experiments"
    ]
  },
  {
    "objectID": "clustered-se.html",
    "href": "clustered-se.html",
    "title": "Robust vs Clustered SEs: When Observations Aren’t Independent",
    "section": "",
    "text": "OLS assumes that observations are independent. But in many real datasets they aren’t:\n\nStudents within the same school share teachers and resources\nPatients within the same hospital get similar care\nPurchases by the same customer are correlated over time\nEmployees within the same firm face the same management\n\nWhen observations within a group (cluster) are correlated, OLS standard errors are too small — often dramatically so. Your t-statistics are inflated, your p-values are too small, and you reject the null far more than 5% of the time.\n\n\n\n\n\n\n\n\n\n\nSE type\nWhat it assumes\nWhen to use\n\n\n\n\nOLS (classical)\nErrors are i.i.d. (independent, constant variance)\nAlmost never in practice\n\n\nRobust (HC)\nErrors can have different variances, but are independent\nCross-sectional data with heteroskedasticity\n\n\nClustered\nErrors can be correlated within clusters\nAny grouped/panel data\n\n\n\nRobust SEs fix heteroskedasticity but not within-cluster correlation. Clustered SEs fix both. If your data has clusters, you need clustered SEs.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we set the cluster structure and the intra-cluster correlation (ICC) — we know exactly how much correlation exists within groups. In practice, you identify clusters from your study design (schools, hospitals, firms) and let the clustered SE estimator handle the rest without knowing the true ICC.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Clustered SEs"
    ]
  },
  {
    "objectID": "clustered-se.html#the-independence-assumption-you-forgot-about",
    "href": "clustered-se.html#the-independence-assumption-you-forgot-about",
    "title": "Robust vs Clustered SEs: When Observations Aren’t Independent",
    "section": "",
    "text": "OLS assumes that observations are independent. But in many real datasets they aren’t:\n\nStudents within the same school share teachers and resources\nPatients within the same hospital get similar care\nPurchases by the same customer are correlated over time\nEmployees within the same firm face the same management\n\nWhen observations within a group (cluster) are correlated, OLS standard errors are too small — often dramatically so. Your t-statistics are inflated, your p-values are too small, and you reject the null far more than 5% of the time.\n\n\n\n\n\n\n\n\n\n\nSE type\nWhat it assumes\nWhen to use\n\n\n\n\nOLS (classical)\nErrors are i.i.d. (independent, constant variance)\nAlmost never in practice\n\n\nRobust (HC)\nErrors can have different variances, but are independent\nCross-sectional data with heteroskedasticity\n\n\nClustered\nErrors can be correlated within clusters\nAny grouped/panel data\n\n\n\nRobust SEs fix heteroskedasticity but not within-cluster correlation. Clustered SEs fix both. If your data has clusters, you need clustered SEs.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we set the cluster structure and the intra-cluster correlation (ICC) — we know exactly how much correlation exists within groups. In practice, you identify clusters from your study design (schools, hospitals, firms) and let the clustered SE estimator handle the rest without knowing the true ICC.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Clustered SEs"
    ]
  },
  {
    "objectID": "clustered-se.html#simulation-1-clustered-data-and-wrong-ses",
    "href": "clustered-se.html#simulation-1-clustered-data-and-wrong-ses",
    "title": "Robust vs Clustered SEs: When Observations Aren’t Independent",
    "section": "Simulation 1: Clustered data and wrong SEs",
    "text": "Simulation 1: Clustered data and wrong SEs\nGenerate data with students nested in schools. Within each school, outcomes are correlated (shared school effect). Compare the three types of standard errors.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_clusters\", \"Number of schools:\",\n                  min = 10, max = 100, value = 30, step = 5),\n\n      sliderInput(\"cluster_size\", \"Students per school:\",\n                  min = 5, max = 50, value = 20, step = 5),\n\n      sliderInput(\"icc\", \"ICC (intra-cluster correlation):\",\n                  min = 0, max = 0.8, value = 0.3, step = 0.05),\n\n      helpText(\"ICC = share of total variance due to the\n               school-level component. Higher = more clustering.\"),\n\n      actionButton(\"go\", \"New draw\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"scatter\", height = \"400px\")),\n        column(6, plotOutput(\"se_compare\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    G  &lt;- input$n_clusters\n    m  &lt;- input$cluster_size\n    icc &lt;- input$icc\n\n    # Variance decomposition: total var = 1\n    sigma_b &lt;- sqrt(icc)        # between-cluster SD\n    sigma_w &lt;- sqrt(1 - icc)    # within-cluster SD\n\n    # Generate clustered data (no true effect: beta = 0)\n    cluster_id &lt;- rep(seq_len(G), each = m)\n    school_effect &lt;- rep(rnorm(G, sd = sigma_b), each = m)\n    x &lt;- rnorm(G * m)\n    y &lt;- 0 * x + school_effect + rnorm(G * m, sd = sigma_w)\n\n    fit &lt;- lm(y ~ x)\n    b_hat &lt;- coef(fit)[2]\n    n &lt;- G * m\n\n    # OLS SE\n    ols_se &lt;- summary(fit)$coefficients[2, 2]\n\n    # Robust SE (HC1)\n    r &lt;- resid(fit)\n    X &lt;- cbind(1, x)\n    bread &lt;- solve(t(X) %*% X)\n    meat_hc &lt;- t(X) %*% diag(r^2 * n / (n - 2)) %*% X\n    robust_se &lt;- sqrt((bread %*% meat_hc %*% bread)[2, 2])\n\n    # Clustered SE (Liang-Zeger)\n    meat_cl &lt;- matrix(0, 2, 2)\n    for (g in seq_len(G)) {\n      idx &lt;- which(cluster_id == g)\n      Xg &lt;- X[idx, , drop = FALSE]\n      rg &lt;- r[idx]\n      meat_cl &lt;- meat_cl + t(Xg) %*% (rg %*% t(rg)) %*% Xg\n    }\n    adj &lt;- G / (G - 1) * (n - 1) / (n - 2)\n    cl_vcov &lt;- adj * bread %*% meat_cl %*% bread\n    clustered_se &lt;- sqrt(cl_vcov[2, 2])\n\n    list(x = x, y = y, cluster_id = cluster_id, fit = fit,\n         b_hat = b_hat, ols_se = ols_se, robust_se = robust_se,\n         clustered_se = clustered_se, G = G, m = m, icc = icc)\n  })\n\n  output$scatter &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Color by cluster (cycle through colors)\n    palette &lt;- rep(c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"#9b59b6\",\n                     \"#e67e22\", \"#1abc9c\", \"#34495e\", \"#f39c12\"),\n                   length.out = d$G)\n    cols &lt;- palette[d$cluster_id]\n\n    plot(d$x, d$y, pch = 16, col = adjustcolor(cols, 0.5), cex = 0.6,\n         xlab = \"X\", ylab = \"Y\",\n         main = paste0(d$G, \" schools, \", d$m, \" students each\"))\n    abline(d$fit, col = \"#2c3e50\", lwd = 2.5)\n    abline(h = 0, lty = 3, col = \"gray60\")\n  })\n\n  output$se_compare &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 8, 3, 1))\n\n    ses &lt;- c(d$ols_se, d$robust_se, d$clustered_se)\n    names_se &lt;- c(\"OLS SE\", \"Robust SE\", \"Clustered SE\")\n    cols &lt;- c(\"#e74c3c\", \"#e67e22\", \"#27ae60\")\n\n    # CIs\n    lo &lt;- d$b_hat - 1.96 * ses\n    hi &lt;- d$b_hat + 1.96 * ses\n\n    xlim &lt;- range(c(lo, hi, 0)) + c(-0.3, 0.3)\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, 3.5),\n         yaxt = \"n\", ylab = \"\", xlab = expression(hat(beta)),\n         main = \"95% CIs with different SEs\")\n    axis(2, at = 1:3, labels = names_se, las = 1, cex.axis = 0.85)\n\n    for (i in 1:3) {\n      segments(lo[i], i, hi[i], i, lwd = 4, col = cols[i])\n      points(d$b_hat, i, pch = 19, cex = 1.5, col = cols[i])\n    }\n\n    abline(v = 0, lty = 2, lwd = 2, col = \"#2c3e50\")\n    text(0, 3.4, expression(\"True \" * beta * \" = 0\"), cex = 0.9)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    ratio &lt;- d$clustered_se / d$ols_se\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;\", expression(\"hat(beta)\"), \":&lt;/b&gt; \",\n        round(d$b_hat, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;OLS SE:&lt;/b&gt; &lt;span class='bad'&gt;\",\n        round(d$ols_se, 4), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Robust SE:&lt;/b&gt; \",\n        round(d$robust_se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Clustered SE:&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$clustered_se, 4), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Cluster/OLS ratio:&lt;/b&gt; \", round(ratio, 2), \"x&lt;br&gt;\",\n        \"&lt;small&gt;With ICC = \", d$icc, \", clustered SE is&lt;br&gt;\",\n        round(ratio, 1), \"x larger than OLS SE.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Clustered SEs"
    ]
  },
  {
    "objectID": "clustered-se.html#simulation-2-rejection-rates",
    "href": "clustered-se.html#simulation-2-rejection-rates",
    "title": "Robust vs Clustered SEs: When Observations Aren’t Independent",
    "section": "Simulation 2: Rejection rates",
    "text": "Simulation 2: Rejection rates\nThe real test: run 500 experiments where the true effect is zero. How often does each type of SE incorrectly reject H₀ at the 5% level? OLS SEs reject far too often. Clustered SEs get it right.\n#| standalone: true\n#| viewerHeight: 520\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"G2\", \"Number of clusters:\",\n                  min = 10, max = 100, value = 30, step = 5),\n\n      sliderInput(\"m2\", \"Obs per cluster:\",\n                  min = 5, max = 50, value = 20, step = 5),\n\n      sliderInput(\"icc2\", \"ICC:\",\n                  min = 0, max = 0.8, value = 0.3, step = 0.05),\n\n      sliderInput(\"sims\", \"Number of simulations:\",\n                  min = 200, max = 1000, value = 500, step = 100),\n\n      actionButton(\"go2\", \"Run simulations\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"rejection_plot\", height = \"420px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go2\n    G    &lt;- input$G2\n    m    &lt;- input$m2\n    icc  &lt;- input$icc2\n    sims &lt;- input$sims\n\n    sigma_b &lt;- sqrt(icc)\n    sigma_w &lt;- sqrt(1 - icc)\n    n &lt;- G * m\n\n    reject_ols &lt;- 0\n    reject_robust &lt;- 0\n    reject_cluster &lt;- 0\n\n    for (s in seq_len(sims)) {\n      cluster_id &lt;- rep(seq_len(G), each = m)\n      school_eff &lt;- rep(rnorm(G, sd = sigma_b), each = m)\n      x &lt;- rnorm(n)\n      y &lt;- 0 * x + school_eff + rnorm(n, sd = sigma_w)\n\n      fit &lt;- lm(y ~ x)\n      b_hat &lt;- coef(fit)[2]\n      r &lt;- resid(fit)\n      X &lt;- cbind(1, x)\n\n      # OLS\n      ols_se &lt;- summary(fit)$coefficients[2, 2]\n\n      # Robust\n      bread &lt;- solve(t(X) %*% X)\n      meat_hc &lt;- t(X) %*% diag(r^2 * n / (n - 2)) %*% X\n      robust_se &lt;- sqrt((bread %*% meat_hc %*% bread)[2, 2])\n\n      # Clustered\n      meat_cl &lt;- matrix(0, 2, 2)\n      for (g in seq_len(G)) {\n        idx &lt;- which(cluster_id == g)\n        Xg &lt;- X[idx, , drop = FALSE]\n        rg &lt;- r[idx]\n        meat_cl &lt;- meat_cl + t(Xg) %*% (rg %*% t(rg)) %*% Xg\n      }\n      adj &lt;- G / (G - 1) * (n - 1) / (n - 2)\n      cl_se &lt;- sqrt((adj * bread %*% meat_cl %*% bread)[2, 2])\n\n      if (abs(b_hat / ols_se) &gt; 1.96)    reject_ols &lt;- reject_ols + 1\n      if (abs(b_hat / robust_se) &gt; 1.96) reject_robust &lt;- reject_robust + 1\n      if (abs(b_hat / cl_se) &gt; 1.96)     reject_cluster &lt;- reject_cluster + 1\n    }\n\n    list(rej_ols = reject_ols / sims,\n         rej_robust = reject_robust / sims,\n         rej_cluster = reject_cluster / sims,\n         sims = sims, G = G, m = m, icc = icc)\n  })\n\n  output$rejection_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 8, 3, 1))\n\n    rates &lt;- c(d$rej_ols, d$rej_robust, d$rej_cluster) * 100\n    names_se &lt;- c(\"OLS SE\", \"Robust SE\", \"Clustered SE\")\n    cols &lt;- c(\"#e74c3c\", \"#e67e22\", \"#27ae60\")\n\n    bp &lt;- barplot(rates, names.arg = names_se, col = cols,\n                  horiz = TRUE, xlim = c(0, max(rates, 10) * 1.3),\n                  xlab = \"Rejection rate (%)\",\n                  main = paste0(\"False rejection rate at 5% level\\n(\",\n                               d$sims, \" simulations, ICC = \",\n                               d$icc, \")\"),\n                  las = 1, border = NA)\n\n    abline(v = 5, lty = 2, lwd = 2.5, col = \"#2c3e50\")\n    text(5, max(bp) + 0.6, \"Target: 5%\", cex = 0.85)\n\n    text(rates + 1, bp, paste0(round(rates, 1), \"%\"),\n         cex = 0.9, adj = 0)\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;OLS rejection rate:&lt;/b&gt; &lt;span class='\",\n        ifelse(d$rej_ols &gt; 0.08, \"bad\", \"good\"), \"'&gt;\",\n        round(d$rej_ols * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Robust rejection rate:&lt;/b&gt; &lt;span class='\",\n        ifelse(d$rej_robust &gt; 0.08, \"bad\", \"good\"), \"'&gt;\",\n        round(d$rej_robust * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Clustered rejection rate:&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$rej_cluster * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;True \\u03b2 = 0 in all simulations.&lt;br&gt;\",\n        \"Correct rejection rate is 5%.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nICC = 0 (no clustering): all three SEs give ~5% rejection. When there’s no within-cluster correlation, OLS SEs are fine.\nICC = 0.3: OLS rejection shoots up to ~15–25%. Robust SEs help a little but not enough. Clustered SEs stay near 5%.\nICC = 0.5: OLS can reject 30–40% of the time — catastrophic.\nIncrease cluster size, hold clusters fixed: the problem gets worse with more observations per cluster. More correlated data doesn’t help — it just gives you a false sense of precision.\nIncrease number of clusters: this does help. The effective sample size for clustered data is closer to the number of clusters than the number of observations.\n\n\n\nCode reference\n\n\n\n\n\n\n\nSoftware\nClustered SEs\n\n\n\n\nR\nlmtest::coeftest(fit, vcov = sandwich::vcovCL, cluster = ~school)\n\n\nStata\nreg y x, cluster(school)\n\n\nPython\nsm.OLS(y, X).fit(cov_type='cluster', cov_kwds={'groups': school})\n\n\n\n\n\nThe practical rule\n\nAlways cluster at the level of treatment assignment. If you randomized schools, cluster by school. If you randomized classrooms, cluster by classroom.\nWhen in doubt, cluster at the highest level that makes sense. Clustering too aggressively (too few clusters) can be a problem, but not clustering when you should is always worse.\nThe minimum number of clusters for reliable clustered SEs is roughly 30–50. With fewer clusters, consider the wild cluster bootstrap.\n\n\n\n\nDid you know?\n\nBrent Moulton (1990) published a now-classic paper showing that regressions using aggregate variables (like state-level policies) with individual-level data produce t-statistics that are wildly inflated if you ignore clustering. He called it the “Moulton problem.” Many published results in economics were likely spurious because of this.\nThe Moulton factor — the ratio of the true variance to the (too-small) OLS variance — is approximately \\(1 + (m-1) \\times ICC\\), where \\(m\\) is the average cluster size. With 50 students per school and ICC = 0.2, the Moulton factor is about 11, meaning your OLS SE is \\(\\sqrt{11} \\approx\n3.3\\) times too small. Your t-statistic is 3x too large.\nCameron, Gelbach, and Miller (2008) showed that even clustered SEs can be unreliable when the number of clusters is small (&lt; 30–50), leading to the development of the wild cluster bootstrap as a more robust alternative.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Clustered SEs"
    ]
  },
  {
    "objectID": "residuals.html",
    "href": "residuals.html",
    "title": "Residuals & Controls",
    "section": "",
    "text": "A residual is what’s left over after your model has done its best:\n\\[e_i = Y_i - \\hat{Y}_i\\]\nIt’s the vertical distance between the actual data point and the regression line. If your model is good, residuals should look like random noise — no patterns, no structure.\nIf the residuals do have structure, your model is missing something. This is the single most important diagnostic in regression.",
    "crumbs": [
      "Regression",
      "Residuals & Controls"
    ]
  },
  {
    "objectID": "residuals.html#what-is-a-residual",
    "href": "residuals.html#what-is-a-residual",
    "title": "Residuals & Controls",
    "section": "",
    "text": "A residual is what’s left over after your model has done its best:\n\\[e_i = Y_i - \\hat{Y}_i\\]\nIt’s the vertical distance between the actual data point and the regression line. If your model is good, residuals should look like random noise — no patterns, no structure.\nIf the residuals do have structure, your model is missing something. This is the single most important diagnostic in regression.",
    "crumbs": [
      "Regression",
      "Residuals & Controls"
    ]
  },
  {
    "objectID": "residuals.html#residuals-and-the-cef",
    "href": "residuals.html#residuals-and-the-cef",
    "title": "Residuals & Controls",
    "section": "Residuals and the CEF",
    "text": "Residuals and the CEF\nRecall from the CEF page: OLS is the best linear approximation to \\(E[Y \\mid X]\\). When the CEF is nonlinear, the residuals absorb the nonlinearity. That’s why a curved pattern in the residual plot signals model misspecification — the residuals are doing the work the model should be doing.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we set the true DGP — including any omitted variables and the true functional form. We can see exactly what the residuals are “absorbing.” In practice, you don’t know what’s missing from your model. Residual diagnostics are your flashlight in the dark.\n\n\n\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .info-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.8;\n    }\n    .info-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"dgp\", \"True relationship:\",\n                  choices = c(\"Linear (correct spec)\",\n                              \"Quadratic (misspecified)\",\n                              \"Heteroskedastic\",\n                              \"Outliers\")),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 50, max = 500, value = 200, step = 50),\n\n      sliderInput(\"sigma\", \"Noise (SD):\",\n                  min = 0.5, max = 4, value = 1.5, step = 0.5),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"info\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(4, plotOutput(\"scatter\", height = \"380px\")),\n        column(4, plotOutput(\"resid_fitted\", height = \"380px\")),\n        column(4, plotOutput(\"resid_hist\", height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n\n    sigma &lt;- input$sigma\n    dgp   &lt;- input$dgp\n\n    x &lt;- runif(n, -3, 5)\n\n    if (dgp == \"Linear (correct spec)\") {\n      y &lt;- 2 + 1.5 * x + rnorm(n, sd = sigma)\n    } else if (dgp == \"Quadratic (misspecified)\") {\n      y &lt;- 1 + 0.5 * x - 0.3 * x^2 + rnorm(n, sd = sigma)\n    } else if (dgp == \"Heteroskedastic\") {\n      y &lt;- 2 + 1.5 * x + rnorm(n, sd = sigma * (0.3 + 0.4 * abs(x)))\n    } else {\n      y &lt;- 2 + 1.5 * x + rnorm(n, sd = sigma)\n      # Add outliers\n      outlier_idx &lt;- sample(n, 5)\n      y[outlier_idx] &lt;- y[outlier_idx] + sample(c(-1, 1), 5, replace = TRUE) * 8\n    }\n\n    fit &lt;- lm(y ~ x)\n    list(x = x, y = y, fit = fit, dgp = dgp)\n  })\n\n  output$scatter &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$x, d$y, pch = 16, col = \"#3498db80\", cex = 0.7,\n         xlab = \"X\", ylab = \"Y\", main = \"Data + OLS fit\")\n    abline(d$fit, col = \"#e74c3c\", lwd = 2.5)\n  })\n\n  output$resid_fitted &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    r  &lt;- resid(d$fit)\n    fv &lt;- fitted(d$fit)\n\n    plot(fv, r, pch = 16, col = \"#9b59b680\", cex = 0.7,\n         xlab = \"Fitted values\", ylab = \"Residuals\",\n         main = \"Residuals vs Fitted\")\n    abline(h = 0, lty = 2, col = \"gray40\", lwd = 1.5)\n\n    lo &lt;- loess(r ~ fv)\n    ox &lt;- order(fv)\n    lines(fv[ox], predict(lo)[ox], col = \"#e74c3c\", lwd = 2)\n  })\n\n  output$resid_hist &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    r &lt;- resid(d$fit)\n    hist(r, breaks = 30, probability = TRUE,\n         col = \"#dae8fc\", border = \"#6c8ebf\",\n         main = \"Residual Distribution\",\n         xlab = \"Residuals\", ylab = \"Density\")\n    x_seq &lt;- seq(min(r), max(r), length.out = 200)\n    lines(x_seq, dnorm(x_seq, mean = 0, sd = sd(r)),\n          col = \"#e74c3c\", lwd = 2)\n  })\n\n  output$info &lt;- renderUI({\n    d &lt;- dat()\n    r &lt;- resid(d$fit)\n\n    msg &lt;- switch(d$dgp,\n      \"Linear (correct spec)\" =\n        \"Residuals look like random noise. No patterns in the residual plot. The model is correctly specified.\",\n      \"Quadratic (misspecified)\" =\n        \"U-shaped pattern in residuals! The LOESS curve bends, revealing the quadratic structure OLS is missing.\",\n      \"Heteroskedastic\" =\n        \"Fan shape: residuals spread out as fitted values increase. The variance isn't constant (heteroskedasticity).\",\n      \"Outliers\" =\n        \"A few points have huge residuals. Check the histogram for heavy tails. These points have outsized influence on the fit.\"\n    )\n\n    tags$div(class = \"info-box\", HTML(paste0(\"&lt;b&gt;Diagnosis:&lt;/b&gt;&lt;br&gt;\", msg)))\n  })\n}\n\nshinyApp(ui, server)\n\nReading the three panels\n\nLeft — Scatter + fit: does the line follow the data?\nMiddle — Residuals vs fitted: the key diagnostic. If you see a pattern (curve, fan, clusters), your model is missing something.\nRight — Residual histogram: should look roughly normal and centered at 0. Heavy tails or skew signal problems.\n\nSwitch between the four DGPs above and learn to recognize each pattern — you’ll see these in every applied paper you read.",
    "crumbs": [
      "Regression",
      "Residuals & Controls"
    ]
  },
  {
    "objectID": "residuals.html#controls-and-residuals",
    "href": "residuals.html#controls-and-residuals",
    "title": "Residuals & Controls",
    "section": "Controls and residuals",
    "text": "Controls and residuals\nWhen you “control for” a variable in a regression, what you’re really doing is removing its influence via residuals. This is the Frisch-Waugh-Lovell theorem in action (see the FWL page).\nAdding \\(X_2\\) as a control means:\n\nRegress \\(Y\\) on \\(X_2\\) → residuals \\(\\tilde{Y}\\) (variation in \\(Y\\) not explained by \\(X_2\\))\nRegress \\(X_1\\) on \\(X_2\\) → residuals \\(\\tilde{X}_1\\) (variation in \\(X_1\\) not explained by \\(X_2\\))\nRegress \\(\\tilde{Y}\\) on \\(\\tilde{X}_1\\) → the coefficient is \\(\\hat{\\beta}_1\\)\n\n“Controlling for \\(X_2\\)” = looking at the relationship between \\(Y\\) and \\(X_1\\) after removing what \\(X_2\\) explains about each.\n#| standalone: true\n#| viewerHeight: 580\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; }\n    .bad  { color: #e74c3c; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size:\", min = 100, max = 500, value = 300, step = 50),\n\n      sliderInput(\"b1\", HTML(\"True &beta;&lt;sub&gt;1&lt;/sub&gt;:\"),\n                  min = -2, max = 3, value = 0.5, step = 0.25),\n\n      sliderInput(\"b2\", HTML(\"True &beta;&lt;sub&gt;2&lt;/sub&gt; (confounder effect):\"),\n                  min = -3, max = 3, value = 2, step = 0.25),\n\n      sliderInput(\"rho\", HTML(\"Corr(X&lt;sub&gt;1&lt;/sub&gt;, X&lt;sub&gt;2&lt;/sub&gt;):\"),\n                  min = -0.9, max = 0.9, value = 0.7, step = 0.1),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(4, plotOutput(\"raw_plot\",  height = \"380px\")),\n        column(4, plotOutput(\"ctrl_y\",    height = \"380px\")),\n        column(4, plotOutput(\"ctrl_both\", height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n   &lt;- input$n\n    b1  &lt;- input$b1\n    b2  &lt;- input$b2\n    rho &lt;- input$rho\n\n    z1 &lt;- rnorm(n)\n    z2 &lt;- rnorm(n)\n    x1 &lt;- z1\n    x2 &lt;- rho * z1 + sqrt(1 - rho^2) * z2\n    y  &lt;- b1 * x1 + b2 * x2 + rnorm(n)\n\n    # Without control\n    naive_fit &lt;- lm(y ~ x1)\n    naive_b1 &lt;- coef(naive_fit)[2]\n\n    # With control\n    full_fit &lt;- lm(y ~ x1 + x2)\n    full_b1 &lt;- coef(full_fit)[2]\n\n    # FWL residuals\n    ey &lt;- resid(lm(y ~ x2))\n    ex &lt;- resid(lm(x1 ~ x2))\n\n    list(x1 = x1, x2 = x2, y = y, ey = ey, ex = ex,\n         naive_b1 = naive_b1, full_b1 = full_b1,\n         b1 = b1, b2 = b2, rho = rho)\n  })\n\n  output$raw_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$x1, d$y, pch = 16, col = \"#3498db60\", cex = 0.6,\n         xlab = expression(X[1]), ylab = \"Y\",\n         main = \"No control (omit X2)\")\n    abline(lm(d$y ~ d$x1), col = \"#e74c3c\", lwd = 2.5)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = paste0(\"Slope = \", round(d$naive_b1, 3)))\n  })\n\n  output$ctrl_y &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$x1, d$ey, pch = 16, col = \"#9b59b660\", cex = 0.6,\n         xlab = expression(X[1]),\n         ylab = expression(\"Residualized Y  (\" * tilde(Y) * \")\"),\n         main = expression(\"Remove \" * X[2] * \" from Y\"))\n    abline(h = 0, lty = 2, col = \"gray60\")\n  })\n\n  output$ctrl_both &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$ex, d$ey, pch = 16, col = \"#27ae6080\", cex = 0.6,\n         xlab = expression(\"Residualized \" * X[1] * \"  (\" * tilde(X)[1] * \")\"),\n         ylab = expression(\"Residualized Y  (\" * tilde(Y) * \")\"),\n         main = \"After controlling for X2\")\n    abline(lm(d$ey ~ d$ex), col = \"#e74c3c\", lwd = 2.5)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = paste0(\"Slope = \", round(d$full_b1, 3)))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    bias &lt;- d$naive_b1 - d$b1\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True &beta;&lt;sub&gt;1&lt;/sub&gt;:&lt;/b&gt; \", d$b1, \"&lt;br&gt;\",\n        \"&lt;b&gt;Without control:&lt;/b&gt; &lt;span class='bad'&gt;\",\n        round(d$naive_b1, 3), \"&lt;/span&gt;\",\n        \" (bias: \", round(bias, 3), \")&lt;br&gt;\",\n        \"&lt;b&gt;With control:&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$full_b1, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;Omitted variable bias = &beta;&lt;sub&gt;2&lt;/sub&gt; &times; \",\n        \"corr(X&lt;sub&gt;1&lt;/sub&gt;,X&lt;sub&gt;2&lt;/sub&gt;) / ... &lt;br&gt;\",\n        \"Higher confounding (&beta;&lt;sub&gt;2&lt;/sub&gt;) + higher correlation \",\n        \"= more bias when you don't control.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nTrue \\(\\beta_1\\) = 0.5, Corr = 0.7, \\(\\beta_2\\) = 2: the naive slope (left panel) is way off because \\(X_2\\) confounds the relationship. The controlled slope (right panel) recovers the truth.\nSet Corr = 0: no confounding. Both estimates are the same — controls don’t help when there’s nothing to control for.\nSet \\(\\beta_2\\) = 0: same thing. Even if \\(X_1\\) and \\(X_2\\) are correlated, \\(X_2\\) doesn’t affect \\(Y\\), so omitting it doesn’t bias \\(\\beta_1\\).\nMiddle panel: shows what \\(Y\\) looks like after removing \\(X_2\\)’s contribution. The right panel adds the second step — removing \\(X_2\\) from \\(X_1\\) too — which isolates the independent variation in \\(X_1\\).\n\n\n\n\nDid you know?\n\nCarl Friedrich Gauss invented the method of least squares at age 18 in 1795 to predict the orbit of the asteroid Ceres. When Ceres reappeared exactly where Gauss predicted, he became an overnight celebrity. The entire method is built on minimizing the sum of squared residuals.\nAdrien-Marie Legendre independently published the method in 1805, before Gauss. But Gauss claimed he’d been using it since 1795. The priority dispute was never fully resolved — but we call it “Gaussian” anyway.\nThe idea of “controlling for” a variable sounds scientific, but it’s been criticized. As Edward Leamer wrote in his famous 1983 paper “Let’s Take the Con Out of Econometrics”: adding controls is not the same as running an experiment. The choice of what to control for is often arbitrary and can introduce more bias than it removes (see: bad controls, collider bias).",
    "crumbs": [
      "Regression",
      "Residuals & Controls"
    ]
  },
  {
    "objectID": "regularization-priors.html",
    "href": "regularization-priors.html",
    "title": "Regularization as Bayesian Inference",
    "section": "",
    "text": "The Bayesian Estimation page showed that MAP with a normal prior gives ridge regression, and MAP with a Laplace prior gives lasso. This page extends that idea to the regularization techniques used in modern machine learning. The central claim is precise: every standard regularizer corresponds to a prior, and every prior corresponds to a regularizer. This is not an analogy. It is an algebraic equivalence.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Regularization as Bayesian Inference"
    ]
  },
  {
    "objectID": "regularization-priors.html#the-equivalence-restated",
    "href": "regularization-priors.html#the-equivalence-restated",
    "title": "Regularization as Bayesian Inference",
    "section": "The equivalence, restated",
    "text": "The equivalence, restated\nRecall from Bayesian Estimation that the MAP objective is:\n\\[\n\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_\\theta \\left[\\log p(\\text{data} \\mid \\theta) + \\log p(\\theta)\\right]\n\\]\nThe first term is the log-likelihood (the loss function). The second term is the log-prior (the regularization penalty). Minimizing a regularized loss is maximizing a penalized log-likelihood, which is computing a MAP estimate under some prior.\n\n\n\n\n\n\n\n\nRegularization technique\nPrior\nPenalty term\n\n\n\n\nL2 (weight decay / ridge)\n\\(\\theta_j \\sim N(0, \\tau^2)\\)\n\\(\\lambda \\|\\theta\\|_2^2\\)\n\n\nL1 (lasso)\n\\(\\theta_j \\sim \\text{Laplace}(0, b)\\)\n\\(\\lambda \\|\\theta\\|_1\\)\n\n\nElastic net\nMixture of normal and Laplace\n\\(\\lambda_1 \\|\\theta\\|_1 + \\lambda_2 \\|\\theta\\|_2^2\\)\n\n\nNo regularization\nFlat (uniform) prior\nNone (pure MLE)\n\n\n\nThe penalty strength \\(\\lambda\\) maps to the prior precision: a large \\(\\lambda\\) is a tight prior that strongly constrains the parameters toward zero. A small \\(\\lambda\\) is a vague prior that lets the data dominate.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Regularization as Bayesian Inference"
    ]
  },
  {
    "objectID": "regularization-priors.html#weight-decay-in-neural-networks",
    "href": "regularization-priors.html#weight-decay-in-neural-networks",
    "title": "Regularization as Bayesian Inference",
    "section": "Weight decay in neural networks",
    "text": "Weight decay in neural networks\nThe most common regularizer in deep learning is weight decay: add \\(\\lambda \\|\\theta\\|_2^2\\) to the loss. In the Bayesian interpretation, this is MAP estimation under a Gaussian prior centered at zero.\nWhat does this prior say? “In the absence of data, I believe the weights should be small.” This is a reasonable default — large weights produce extreme predictions and amplify noise. The prior encodes a preference for smooth, conservative functions.\nThe weight decay coefficient \\(\\lambda\\) controls the bias-variance tradeoff from Bias-Variance: too small and the model overfits (high variance); too large and the model underfits (high bias). The Bayesian interpretation makes this tradeoff precise — \\(\\lambda\\) is the ratio of the noise variance to the prior variance, \\(\\sigma^2 / \\tau^2\\).\nThe simulation below generates regression data with 8 predictors — the first 5 have true nonzero coefficients, the last 3 are pure noise — and shows how ridge (L2) and lasso (L1) shrink the coefficient estimates as \\(\\lambda\\) increases. The key visual: ridge shrinks all coefficients toward zero proportionally, while lasso drives small coefficients to exactly zero.\n\nThings to try\n\nSmall λ (≈ 0): both ridge and lasso estimates are close to OLS — minimal shrinkage. The noise coefficients (6–8) are nonzero due to overfitting.\nIncrease λ: ridge shrinks all coefficients proportionally toward zero. Lasso shrinks small coefficients to exactly zero first (variable selection).\nLarge λ: all coefficients shrink toward zero. Lasso may zero out even the true signal predictors.\nHigh noise (σ): OLS estimates are noisier, and the benefits of regularization become more apparent.\n\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 50, max = 500, value = 100, step = 50),\n\n      sliderInput(\"sigma\", \"Noise level (σ):\",\n                  min = 0.5, max = 5, value = 2, step = 0.5),\n\n      sliderInput(\"log_lambda\", \"log(λ):\",\n                  min = -4, max = 4, value = 0, step = 0.2),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"ridge_plot\", height = \"470px\")),\n        column(6, plotOutput(\"lasso_plot\", height = \"470px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  # Soft-thresholding operator for lasso\n  soft_thresh &lt;- function(x, t) sign(x) * pmax(abs(x) - t, 0)\n\n  # Coordinate descent for lasso\n  lasso_fit &lt;- function(X, y, lambda, beta_init = NULL, max_iter = 200) {\n    n &lt;- nrow(X)\n    p &lt;- ncol(X)\n    beta &lt;- if (!is.null(beta_init)) beta_init else rep(0, p)\n    # Precompute column norms\n    col_norm2 &lt;- colSums(X^2) / n\n\n    for (iter in 1:max_iter) {\n      for (j in 1:p) {\n        r_j &lt;- y - X[, -j, drop = FALSE] %*% beta[-j]\n        beta[j] &lt;- soft_thresh(sum(X[, j] * r_j) / n, lambda) / col_norm2[j]\n      }\n    }\n    beta\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n\n    sigma &lt;- input$sigma\n    p     &lt;- 8\n\n    # True coefficients: 5 nonzero, 3 zero\n    beta_true &lt;- c(3, -2, 1.5, -1, 0.5, 0, 0, 0)\n\n    # Generate data (standardized X)\n    X &lt;- matrix(rnorm(n * p), n, p)\n    Y &lt;- X %*% beta_true + rnorm(n) * sigma\n\n    # Center Y for penalized regression\n    y_c &lt;- Y - mean(Y)\n\n    # Precompute\n    XtX &lt;- t(X) %*% X\n    Xty &lt;- t(X) %*% y_c\n\n    # OLS\n    beta_ols &lt;- as.numeric(solve(XtX) %*% Xty)\n\n    # Compute paths over a grid of lambda values\n    lambda_grid &lt;- exp(seq(-4, 4, length.out = 60))\n\n    ridge_path &lt;- matrix(NA, 60, p)\n    lasso_path &lt;- matrix(NA, 60, p)\n\n    # Ridge path (closed form)\n    for (l in 1:60) {\n      ridge_path[l, ] &lt;- as.numeric(\n        solve(XtX + lambda_grid[l] * n * diag(p)) %*% Xty\n      )\n    }\n\n    # Lasso path (coordinate descent with warm starts)\n    # Process from largest to smallest lambda\n    beta_warm &lt;- rep(0, p)\n    ord &lt;- order(lambda_grid, decreasing = TRUE)\n    for (l in ord) {\n      beta_warm &lt;- lasso_fit(X, y_c, lambda_grid[l], beta_warm, max_iter = 100)\n      lasso_path[l, ] &lt;- beta_warm\n    }\n\n    list(beta_true = beta_true, beta_ols = beta_ols,\n         ridge_path = ridge_path, lasso_path = lasso_path,\n         lambda_grid = lambda_grid, p = p, n = n, sigma = sigma)\n  })\n\n  output$ridge_plot &lt;- renderPlot({\n    d &lt;- dat()\n    lam &lt;- exp(input$log_lambda)\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    # Color palette for 8 coefficients\n    cols &lt;- c(\"#e74c3c\", \"#3498db\", \"#2ecc71\", \"#f39c12\",\n              \"#9b59b6\", \"#95a5a6\", \"#95a5a6\", \"#95a5a6\")\n\n    log_grid &lt;- log(d$lambda_grid)\n\n    ylims &lt;- range(d$ridge_path) * 1.1\n\n    plot(NULL, xlim = range(log_grid), ylim = ylims,\n         main = \"Ridge (L2) Coefficient Path\",\n         xlab = expression(log(lambda)),\n         ylab = \"Coefficient estimate\")\n\n    for (j in 1:d$p) {\n      lty_j &lt;- if (j &lt;= 5) 1 else 2\n      lines(log_grid, d$ridge_path[, j],\n            col = cols[j], lwd = 2, lty = lty_j)\n    }\n\n    abline(v = log(lam), col = \"#2c3e50\", lwd = 2, lty = 3)\n    abline(h = 0, col = \"#7f8c8d\", lty = 2)\n\n    # True values as points on right margin\n    for (j in 1:d$p) {\n      points(max(log_grid) + 0.3, d$beta_true[j],\n             pch = 4, col = cols[j], cex = 1.2, lwd = 2)\n    }\n\n    legend(\"topright\", bty = \"n\", cex = 0.7,\n           legend = c(paste0(\"β\", 1:5, \" (true ≠ 0)\"), \"β6–β8 (true = 0)\"),\n           col = c(cols[1:5], \"#95a5a6\"),\n           lwd = 2, lty = c(rep(1, 5), 2))\n  })\n\n  output$lasso_plot &lt;- renderPlot({\n    d &lt;- dat()\n    lam &lt;- exp(input$log_lambda)\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    cols &lt;- c(\"#e74c3c\", \"#3498db\", \"#2ecc71\", \"#f39c12\",\n              \"#9b59b6\", \"#95a5a6\", \"#95a5a6\", \"#95a5a6\")\n\n    log_grid &lt;- log(d$lambda_grid)\n\n    ylims &lt;- range(d$lasso_path) * 1.1\n\n    plot(NULL, xlim = range(log_grid), ylim = ylims,\n         main = \"Lasso (L1) Coefficient Path\",\n         xlab = expression(log(lambda)),\n         ylab = \"Coefficient estimate\")\n\n    for (j in 1:d$p) {\n      lty_j &lt;- if (j &lt;= 5) 1 else 2\n      lines(log_grid, d$lasso_path[, j],\n            col = cols[j], lwd = 2, lty = lty_j)\n    }\n\n    abline(v = log(lam), col = \"#2c3e50\", lwd = 2, lty = 3)\n    abline(h = 0, col = \"#7f8c8d\", lty = 2)\n\n    for (j in 1:d$p) {\n      points(max(log_grid) + 0.3, d$beta_true[j],\n             pch = 4, col = cols[j], cex = 1.2, lwd = 2)\n    }\n\n    legend(\"topright\", bty = \"n\", cex = 0.7,\n           legend = c(paste0(\"β\", 1:5, \" (true ≠ 0)\"), \"β6–β8 (true = 0)\"),\n           col = c(cols[1:5], \"#95a5a6\"),\n           lwd = 2, lty = c(rep(1, 5), 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    lam &lt;- exp(input$log_lambda)\n\n    # Find closest lambda in grid\n    idx &lt;- which.min(abs(d$lambda_grid - lam))\n\n    ridge_at &lt;- d$ridge_path[idx, ]\n    lasso_at &lt;- d$lasso_path[idx, ]\n\n    n_zero_lasso &lt;- sum(abs(lasso_at) &lt; 1e-6)\n    n_zero_ridge &lt;- sum(abs(ridge_at) &lt; 1e-6)\n\n    # MSE of estimates vs true\n    mse_ols   &lt;- mean((d$beta_ols - d$beta_true)^2)\n    mse_ridge &lt;- mean((ridge_at - d$beta_true)^2)\n    mse_lasso &lt;- mean((lasso_at - d$beta_true)^2)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;λ = \", round(lam, 3), \"&lt;/b&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Coefficients set to zero:&lt;/b&gt;&lt;br&gt;\",\n        \"&nbsp; Ridge: \", n_zero_ridge,\n        \" | Lasso: \", n_zero_lasso, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;MSE vs true β:&lt;/b&gt;&lt;br&gt;\",\n        \"&nbsp; OLS: \", round(mse_ols, 3), \"&lt;br&gt;\",\n        \"&nbsp; Ridge: \", round(mse_ridge, 3), \"&lt;br&gt;\",\n        \"&nbsp; Lasso: \", round(mse_lasso, 3)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Statistical Foundations of AI",
      "Regularization as Bayesian Inference"
    ]
  },
  {
    "objectID": "regularization-priors.html#dropout-as-approximate-bayesian-inference",
    "href": "regularization-priors.html#dropout-as-approximate-bayesian-inference",
    "title": "Regularization as Bayesian Inference",
    "section": "Dropout as approximate Bayesian inference",
    "text": "Dropout as approximate Bayesian inference\nDropout randomly sets a fraction of neural network activations to zero during training. At test time, all units are active but scaled by the dropout probability. This was introduced as a heuristic to prevent overfitting.\nThe Bayesian connection: Gal and Ghahramani (2016) showed that training with dropout is approximately equivalent to variational inference in a Bayesian neural network. Specifically, dropout training minimizes a divergence between an approximate posterior and the true posterior over the weights.\nThis means:\n\nRunning the network multiple times with dropout at test time (Monte Carlo dropout) produces samples from an approximate posterior predictive distribution\nThe spread of these predictions approximates the model’s epistemic uncertainty\nThis connects dropout to the uncertainty quantification discussed in Calibration and Uncertainty\n\n\n\n\n\n\n\nA caveat. The equivalence between dropout and variational inference is approximate and depends on modeling assumptions that may not hold in practice. The quality of the uncertainty estimates from Monte Carlo dropout is debated in the literature. The connection is theoretically grounded but should not be taken as exact.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Regularization as Bayesian Inference"
    ]
  },
  {
    "objectID": "regularization-priors.html#early-stopping-as-implicit-regularization",
    "href": "regularization-priors.html#early-stopping-as-implicit-regularization",
    "title": "Regularization as Bayesian Inference",
    "section": "Early stopping as implicit regularization",
    "text": "Early stopping as implicit regularization\nEarly stopping — halting training before convergence — also has a Bayesian interpretation. In gradient descent with small learning rate, the trajectory of the parameters from initialization traces out a path from the prior (the initial weights) toward the MLE. Stopping early means the final estimate stays closer to the initialization, which functions as an implicit prior.\nFor linear models, early stopping with gradient descent is exactly equivalent to L2 regularization: the number of gradient steps plays the role of \\(1/\\lambda\\). Fewer steps = more regularization = tighter prior. For nonlinear models the equivalence is approximate, but the intuition holds.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Regularization as Bayesian Inference"
    ]
  },
  {
    "objectID": "regularization-priors.html#the-rlhf-penalty-as-a-prior",
    "href": "regularization-priors.html#the-rlhf-penalty-as-a-prior",
    "title": "Regularization as Bayesian Inference",
    "section": "The RLHF penalty as a prior",
    "text": "The RLHF penalty as a prior\nReinforcement Learning from Human Feedback (RLHF) fine-tunes a language model to align with human preferences. The standard objective is:\n\\[\n\\max_\\theta \\; E\\left[R(y \\mid x)\\right] - \\beta \\, D_{KL}\\!\\left(\\pi_\\theta \\| \\pi_{\\text{ref}}\\right)\n\\]\nwhere \\(R\\) is a reward model, \\(\\pi_\\theta\\) is the policy being trained, and \\(\\pi_{\\text{ref}}\\) is the reference (pre-trained) model. The KL divergence term penalizes deviation from the reference model.\nIn the Bayesian frame, the reference model acts as a prior: it encodes what the model “knew” before seeing human preference data. The KL penalty pulls the fine-tuned model back toward this prior, preventing it from drifting too far in pursuit of reward. The coefficient \\(\\beta\\) controls how much the prior matters — analogous to \\(\\lambda\\) in weight decay or \\(1/\\tau^2\\) in the Bayesian MAP framework.\nThis is the same tug-of-war described in Bayesian Updating: data (human preferences) push the model in one direction; the prior (reference model) pulls it back. With more preference data, the reward signal dominates. With less, the model stays close to its pre-trained behavior.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Regularization as Bayesian Inference"
    ]
  },
  {
    "objectID": "regularization-priors.html#what-the-bayesian-lens-buys-you",
    "href": "regularization-priors.html#what-the-bayesian-lens-buys-you",
    "title": "Regularization as Bayesian Inference",
    "section": "What the Bayesian lens buys you",
    "text": "What the Bayesian lens buys you\nViewing regularization as Bayesian inference is not just a mathematical curiosity. It provides:\nPrincipled choice of \\(\\lambda\\). Instead of tuning the regularization strength by cross-validation alone, you can reason about what prior is appropriate for the problem. If you have domain knowledge that parameters should be small, a tight prior (large \\(\\lambda\\)) is justified. If you expect sparse effects, a Laplace prior (L1) is appropriate.\nA path to uncertainty quantification. Pure MLE (unregularized training) gives point predictions with no uncertainty. The Bayesian interpretation opens the door to posterior distributions, credible intervals, and predictive uncertainty — tools explored in Calibration and Uncertainty.\nUnified understanding. Weight decay, dropout, early stopping, and the RLHF penalty all look different mechanically. But they are all doing the same thing: encoding a prior belief that constrains the model. Recognizing this prevents treating them as unrelated “tricks” and enables reasoning about their interactions.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Regularization as Bayesian Inference"
    ]
  },
  {
    "objectID": "power.html",
    "href": "power.html",
    "title": "Power, Alpha, Beta & MDE",
    "section": "",
    "text": "You run an experiment to test whether some treatment works. There are only four things that can happen:\n\n\n\n\n\n\n\n\n\nTreatment does nothing (H₀ true)\nTreatment works (H₁ true)\n\n\n\n\nYou say “no effect”\nCorrect\nType II error (miss it) — probability = \\(\\beta\\)\n\n\nYou say “it works!”\nType I error (false alarm) — probability = \\(\\alpha\\)\nCorrect — probability = Power = \\(1 - \\beta\\)\n\n\n\nThat’s it. Everything on this page is about these four cells.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we set the true effect size and know whether the treatment works. We can label every rejection as correct or false. In practice, you design the experiment without knowing the effect size — you guess it from pilot data or prior studies. You never find out which cell of the table you landed in.",
    "crumbs": [
      "Inference",
      "Power, Alpha, Beta & MDE"
    ]
  },
  {
    "objectID": "power.html#the-big-picture",
    "href": "power.html#the-big-picture",
    "title": "Power, Alpha, Beta & MDE",
    "section": "",
    "text": "You run an experiment to test whether some treatment works. There are only four things that can happen:\n\n\n\n\n\n\n\n\n\nTreatment does nothing (H₀ true)\nTreatment works (H₁ true)\n\n\n\n\nYou say “no effect”\nCorrect\nType II error (miss it) — probability = \\(\\beta\\)\n\n\nYou say “it works!”\nType I error (false alarm) — probability = \\(\\alpha\\)\nCorrect — probability = Power = \\(1 - \\beta\\)\n\n\n\nThat’s it. Everything on this page is about these four cells.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we set the true effect size and know whether the treatment works. We can label every rejection as correct or false. In practice, you design the experiment without knowing the effect size — you guess it from pilot data or prior studies. You never find out which cell of the table you landed in.",
    "crumbs": [
      "Inference",
      "Power, Alpha, Beta & MDE"
    ]
  },
  {
    "objectID": "power.html#what-are-alpha-and-beta",
    "href": "power.html#what-are-alpha-and-beta",
    "title": "Power, Alpha, Beta & MDE",
    "section": "What are \\(\\alpha\\) and \\(\\beta\\)?",
    "text": "What are \\(\\alpha\\) and \\(\\beta\\)?\n\\(\\alpha\\) (alpha) is how often you cry wolf. You set this before the experiment — typically 0.05. It’s the false positive rate: the chance you declare “it works!” when the treatment actually does nothing.\n\\(\\beta\\) (beta) is how often you miss a real effect. If the treatment genuinely works, \\(\\beta\\) is the probability you shrug and say “no effect.” You want this to be small.\nPower = \\(1 - \\beta\\) is the flip side: the probability you correctly detect a real effect. Convention is to aim for 0.80 (80%).\n\nThe two-distribution picture\nThe key insight is that there are two worlds — one where the treatment does nothing (null), and one where it has an effect (alternative). Each world gives you a different sampling distribution for your test statistic:\n\nUnder the null, the distribution is centered at 0 (no effect).\nUnder the alternative, the distribution is shifted by the true effect size.\n\nYou pick a critical value (the cutoff). If your test statistic lands past it, you reject H₀. The simulation below shows both distributions. Drag the sliders and watch how \\(\\alpha\\), \\(\\beta\\), and power change.\n#| standalone: true\n#| viewerHeight: 580\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"effect\", \"True effect size (d):\",\n                  min = 0, max = 2, value = 0.5, step = 0.05),\n\n      sliderInput(\"n\", \"Sample size per group (n):\",\n                  min = 10, max = 500, value = 50, step = 10),\n\n      sliderInput(\"alpha\", HTML(\"&alpha; (significance level):\"),\n                  min = 0.01, max = 0.10, value = 0.05, step = 0.01),\n\n      uiOutput(\"results_box\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"dist_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  vals &lt;- reactive({\n    d     &lt;- input$effect\n    n     &lt;- input$n\n    alpha &lt;- input$alpha\n\n    se    &lt;- sqrt(2 / n)          # SE of difference in means (sigma=1 each group)\n    shift &lt;- d / se               # noncentrality (in SE units)\n    crit  &lt;- qnorm(1 - alpha)     # one-sided critical value\n\n    power &lt;- 1 - pnorm(crit - shift)\n    beta  &lt;- 1 - power\n\n    list(se = se, shift = shift, crit = crit,\n         power = power, beta = beta, alpha = alpha, d = d, n = n)\n  })\n\n  output$dist_plot &lt;- renderPlot({\n    v &lt;- vals()\n\n    xmin &lt;- min(-4, v$shift - 4)\n    xmax &lt;- max(4, v$shift + 4)\n    x &lt;- seq(xmin, xmax, length.out = 500)\n\n    y_null &lt;- dnorm(x)\n    y_alt  &lt;- dnorm(x, mean = v$shift)\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(x, y_null, type = \"l\", lwd = 2.5, col = \"#2c3e50\",\n         xlab = \"Test statistic (z)\", ylab = \"Density\",\n         main = \"Null vs Alternative Distribution\",\n         ylim = c(0, max(y_null, y_alt) * 1.15),\n         xlim = c(xmin, xmax))\n    lines(x, y_alt, lwd = 2.5, col = \"#3498db\")\n\n    # Critical value line\n    abline(v = v$crit, lty = 2, lwd = 2, col = \"#7f8c8d\")\n\n    # Shade alpha region (right tail of null beyond crit)\n    x_alpha &lt;- seq(v$crit, xmax, length.out = 200)\n    polygon(c(v$crit, x_alpha, xmax),\n            c(0, dnorm(x_alpha), 0),\n            col = adjustcolor(\"#e74c3c\", 0.35), border = NA)\n\n    # Shade beta region (left part of alternative, below crit)\n    x_beta &lt;- seq(xmin, v$crit, length.out = 200)\n    polygon(c(xmin, x_beta, v$crit),\n            c(0, dnorm(x_beta, mean = v$shift), 0),\n            col = adjustcolor(\"#f39c12\", 0.35), border = NA)\n\n    # Shade power region (right part of alternative, beyond crit)\n    x_pow &lt;- seq(v$crit, xmax, length.out = 200)\n    polygon(c(v$crit, x_pow, xmax),\n            c(0, dnorm(x_pow, mean = v$shift), 0),\n            col = adjustcolor(\"#2ecc71\", 0.35), border = NA)\n\n    # Labels\n    legend(\"topleft\", bty = \"n\", cex = 0.9,\n           legend = c(\n             expression(\"Null distribution (H\"[0]*\": no effect)\"),\n             expression(\"Alternative distribution (H\"[1]*\": effect exists)\"),\n             \"Critical value\",\n             expression(alpha * \" (false positive)\"),\n             expression(beta * \" (miss / Type II)\"),\n             \"Power (correct detection)\"\n           ),\n           col = c(\"#2c3e50\", \"#3498db\", \"#7f8c8d\",\n                   adjustcolor(\"#e74c3c\", 0.6),\n                   adjustcolor(\"#f39c12\", 0.6),\n                   adjustcolor(\"#2ecc71\", 0.6)),\n           lwd = c(2.5, 2.5, 2, 8, 8, 8),\n           lty = c(1, 1, 2, 1, 1, 1))\n  })\n\n  output$results_box &lt;- renderUI({\n    v &lt;- vals()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;&alpha;:&lt;/b&gt; \", v$alpha, \"&lt;br&gt;\",\n        \"&lt;b&gt;&beta;:&lt;/b&gt; \", round(v$beta, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Power:&lt;/b&gt; \", round(v$power, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Effect (d):&lt;/b&gt; \", v$d, \"&lt;br&gt;\",\n        \"&lt;b&gt;n per group:&lt;/b&gt; \", v$n, \"&lt;br&gt;\",\n        \"&lt;b&gt;Critical z:&lt;/b&gt; \", round(v$crit, 2)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nSet effect = 0 and watch: there is no alternative distribution to detect. Any rejection is a false positive.\nSet effect = 0.5 with n = 20: power is low. Now slide n up — power climbs. This is why sample size matters.\nSet n = 200 and shrink the effect toward 0: even large samples struggle to detect tiny effects.\nLower \\(\\alpha\\) from 0.05 to 0.01: the critical value moves right, \\(\\alpha\\) shrinks, but \\(\\beta\\) grows. There’s always a tradeoff between false positives and false negatives.",
    "crumbs": [
      "Inference",
      "Power, Alpha, Beta & MDE"
    ]
  },
  {
    "objectID": "power.html#minimum-detectable-effect-mde",
    "href": "power.html#minimum-detectable-effect-mde",
    "title": "Power, Alpha, Beta & MDE",
    "section": "Minimum Detectable Effect (MDE)",
    "text": "Minimum Detectable Effect (MDE)\nWhen planning an experiment, you often ask: “Given my sample size, what’s the smallest effect I can reliably detect?” That’s the MDE.\nIt depends on three things: sample size (\\(n\\)), significance level (\\(\\alpha\\)), and desired power (\\(1 - \\beta\\)). The formula for a two-sample test with equal groups is:\n\\[\n\\text{MDE} = (z_{1-\\alpha} + z_{1-\\beta}) \\times \\sqrt{\\frac{2}{n}}\n\\]\nNotice: that \\(\\sqrt{2/n}\\) is just the standard error of the difference in means. So MDE is really just a scaled-up SE:\n\\[MDE = (z_{1-\\alpha} + z_{1-\\beta}) \\times SE\\]\nThe critical values (~2.8 for 5% significance and 80% power) are fixed multipliers. The only thing you control is the SE — by increasing \\(n\\) or reducing \\(\\sigma\\) (through better measurement, stratification, or controls). Power analysis is really just an SE calculation in disguise. See Variance, SD & Standard Error for more on this connection.\nLarger \\(n\\) shrinks the SE, which shrinks the MDE. Higher power demands a larger MDE (or more \\(n\\)).\n#| standalone: true\n#| viewerHeight: 480\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .mde-box {\n      background: #eaf2f8; border-radius: 6px; padding: 16px;\n      margin-top: 14px; font-size: 15px; line-height: 2;\n      text-align: center;\n    }\n    .mde-box .big { font-size: 28px; color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n2\", \"Sample size per group (n):\",\n                  min = 10, max = 1000, value = 100, step = 10),\n\n      sliderInput(\"alpha2\", HTML(\"&alpha;:\"),\n                  min = 0.01, max = 0.10, value = 0.05, step = 0.01),\n\n      sliderInput(\"power2\", \"Desired power:\",\n                  min = 0.50, max = 0.95, value = 0.80, step = 0.05),\n\n      uiOutput(\"mde_box\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"mde_curve\", height = \"400px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  output$mde_curve &lt;- renderPlot({\n    alpha &lt;- input$alpha2\n    power &lt;- input$power2\n    n_now &lt;- input$n2\n\n    ns &lt;- seq(10, 1000, by = 5)\n    mdes &lt;- (qnorm(1 - alpha) + qnorm(power)) * sqrt(2 / ns)\n\n    mde_now &lt;- (qnorm(1 - alpha) + qnorm(power)) * sqrt(2 / n_now)\n\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(ns, mdes, type = \"l\", lwd = 2.5, col = \"#3498db\",\n         xlab = \"Sample size per group (n)\",\n         ylab = \"MDE (standardized effect size)\",\n         main = paste0(\"MDE curve (\\u03b1 = \", alpha, \", power = \", power, \")\"),\n         ylim = c(0, max(mdes)))\n\n    # Highlight current n\n    points(n_now, mde_now, pch = 19, cex = 2, col = \"#e74c3c\")\n    segments(n_now, 0, n_now, mde_now, lty = 2, col = \"#e74c3c\")\n    segments(0, mde_now, n_now, mde_now, lty = 2, col = \"#e74c3c\")\n\n    text(n_now + 30, mde_now + 0.02,\n         paste0(\"MDE = \", round(mde_now, 3)),\n         col = \"#e74c3c\", cex = 0.95, adj = 0)\n  })\n\n  output$mde_box &lt;- renderUI({\n    alpha &lt;- input$alpha2\n    power &lt;- input$power2\n    n_now &lt;- input$n2\n    mde &lt;- (qnorm(1 - alpha) + qnorm(power)) * sqrt(2 / n_now)\n\n    tags$div(class = \"mde-box\",\n      HTML(paste0(\n        \"With &lt;b&gt;n = \", n_now, \"&lt;/b&gt; per group,&lt;br&gt;\",\n        \"you can detect effects as small as:&lt;br&gt;\",\n        \"&lt;span class='big'&gt;d = \", round(mde, 3), \"&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThe intuition\n\nMDE is your experiment’s resolution. A microscope can’t see atoms; your experiment can’t see effects smaller than the MDE.\nMore data (larger \\(n\\)) = sharper microscope = smaller MDE.\nIf you need to detect a 1% lift in click-through rate but your MDE is 3%, your experiment is pointless — you’ll almost certainly miss it even if the effect is real.\nIn practice: figure out the smallest effect that would matter for your decision, then compute the \\(n\\) needed to detect it.\n\n\n\n\nDid you know?\n\nJacob Cohen, the psychologist who popularized power analysis, found in 1962 that the median power of studies in behavioral science journals was only 0.48 — meaning most studies had less than a coin-flip chance of detecting the effects they were looking for. He spent the rest of his career trying to fix this. His book Statistical Power Analysis (1969) remains a classic.\nCohen’s famous effect size conventions (small = 0.2, medium = 0.5, large = 0.8) were meant as rough guides, not rigid rules. He later regretted that people treated them as gospel: “My intent was that d = 0.5 represents a medium effect… it does not mean that 0.5 is a medium effect in your field.”\nThe replication crisis in psychology and medicine is largely a power problem. Underpowered studies that happen to find significant results are published; the many more that find nothing are filed away. This is publication bias, and it’s a direct consequence of running experiments without power calculations.",
    "crumbs": [
      "Inference",
      "Power, Alpha, Beta & MDE"
    ]
  },
  {
    "objectID": "training-as-mle.html",
    "href": "training-as-mle.html",
    "title": "Training Neural Networks as Maximum Likelihood",
    "section": "",
    "text": "The loss functions used to train neural networks are not arbitrary design choices. Most of them are negative log-likelihoods in disguise. If you understand MLE, you already understand what neural network training is doing — and, critically, what it is not doing.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Training as Maximum Likelihood"
    ]
  },
  {
    "objectID": "training-as-mle.html#cross-entropy-is-negative-log-likelihood",
    "href": "training-as-mle.html#cross-entropy-is-negative-log-likelihood",
    "title": "Training Neural Networks as Maximum Likelihood",
    "section": "Cross-entropy is negative log-likelihood",
    "text": "Cross-entropy is negative log-likelihood\nThe standard loss for classification is cross-entropy. For a binary outcome \\(Y_i \\in \\{0, 1\\}\\) and a model that predicts \\(\\hat{p}_i = P(Y_i = 1 \\mid X_i)\\):\n\\[\n\\mathcal{L} = -\\frac{1}{n}\\sum_{i=1}^n \\left[Y_i \\log \\hat{p}_i + (1 - Y_i)\\log(1 - \\hat{p}_i)\\right]\n\\]\nCompare this to the log-likelihood of a Bernoulli model from the MLE page:\n\\[\n\\ell(p) = \\sum_{i=1}^n \\left[Y_i \\log p_i + (1 - Y_i)\\log(1 - p_i)\\right]\n\\]\nThey are the same expression, up to a sign and a scaling constant. Minimizing cross-entropy loss is maximizing the Bernoulli log-likelihood. The neural network’s output layer (sigmoid activation) parameterizes \\(p_i\\) as a flexible function of \\(X_i\\), but the estimation principle is identical to logistic regression — which is itself MLE.\nFor multi-class classification with \\(K\\) categories, softmax cross-entropy is the negative log-likelihood of a multinomial model. For regression with squared-error loss:\n\\[\n\\mathcal{L} = \\frac{1}{n}\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\n\\]\nthis is the negative log-likelihood of a Gaussian model with constant variance — exactly the same equivalence shown on the MLE page between OLS and MLE under normality.\n\n\n\n\n\n\n\n\nLoss function\nEquivalent to\nImplicit distributional assumption\n\n\n\n\nSquared error (MSE)\nGaussian MLE\n\\(Y \\mid X \\sim N(\\hat{Y}, \\sigma^2)\\)\n\n\nBinary cross-entropy\nBernoulli MLE\n\\(Y \\mid X \\sim \\text{Bernoulli}(\\hat{p})\\)\n\n\nCategorical cross-entropy\nMultinomial MLE\n\\(Y \\mid X \\sim \\text{Multinomial}(\\hat{p}_1, \\ldots, \\hat{p}_K)\\)\n\n\n\n\n\n\n\n\n\nWhat this means. When a paper says “we trained a neural network with cross-entropy loss,” it is saying “we found the parameters that maximize the likelihood of the observed labels under a Bernoulli model.” The network architecture determines the function class; the loss function determines the estimation principle. The estimation principle, in most cases, is MLE.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Training as Maximum Likelihood"
    ]
  },
  {
    "objectID": "training-as-mle.html#sgd-as-approximate-mle",
    "href": "training-as-mle.html#sgd-as-approximate-mle",
    "title": "Training Neural Networks as Maximum Likelihood",
    "section": "SGD as approximate MLE",
    "text": "SGD as approximate MLE\nClassical MLE computes the gradient of the full log-likelihood and solves the score equation exactly. Neural networks can’t do this — the models are nonconvex and the datasets are enormous. Instead, they use stochastic gradient descent (SGD): at each step, sample a mini-batch of data, compute the gradient of the loss on that mini-batch, and take a step.\nThis is approximate MLE. The mini-batch gradient is a noisy, unbiased estimate of the full gradient. Over many steps, SGD traces out a path that (under regularity conditions) converges to a local maximum of the likelihood — though not necessarily the global one.\nThe analogy to classical statistics is instructive:\n\n\n\nClassical MLE\nNeural network training\n\n\n\n\nFull-sample gradient, solve exactly\nMini-batch gradient, iterate\n\n\nConvex log-likelihood (often)\nNonconvex loss landscape\n\n\nSingle global optimum (typically)\nMultiple local optima\n\n\nClosed-form or Newton-Raphson\nSGD, Adam, or variants\n\n\nFisher information → standard errors\nNo standard errors by default\n\n\n\nThe last row matters. Classical MLE gives you standard errors through the Fisher information. Neural network training typically does not. The model gives you a point prediction, but no measure of uncertainty — a limitation addressed in Calibration and Uncertainty.\nThe simulation below runs gradient descent on a logistic regression — first with the full sample (full-batch, blue), then with random mini-batches (red). Both converge to the same MLE, but the mini-batch path is noisier — exactly the tradeoff described above.\n\nThings to try\n\nFull batch (batch size = n): the loss curve descends smoothly to the MLE minimum. No noise.\nSmall batch size: the loss curve is noisy — each step uses a different random subset, so the gradient estimate is noisy. But it still converges to the right neighborhood.\nVery small batch + high learning rate: the path oscillates wildly. This is why learning rate schedules (reducing the step size over time) are important in practice.\nLarge n: the mini-batch noise is smaller relative to the signal, so even small batches converge smoothly.\n\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 200, max = 2000, value = 500, step = 200),\n\n      sliderInput(\"batch\", \"Batch size:\",\n                  min = 10, max = 500, value = 50, step = 10),\n\n      sliderInput(\"lr\", \"Learning rate:\",\n                  min = 0.1, max = 5, value = 1, step = 0.1),\n\n      sliderInput(\"n_iter\", \"Iterations:\",\n                  min = 50, max = 500, value = 200, step = 50),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"loss_plot\", height = \"470px\")),\n        column(6, plotOutput(\"beta_plot\", height = \"470px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n      &lt;- input$n\n    bs     &lt;- min(input$batch, n)\n    lr     &lt;- input$lr\n    n_iter &lt;- input$n_iter\n\n    # Generate logistic regression data\n    X      &lt;- rnorm(n)\n    p_true &lt;- 1 / (1 + exp(-(0.5 + 1.5 * X)))\n    Y      &lt;- rbinom(n, 1, p_true)\n\n    # Exact MLE via Newton-Raphson (manual, to avoid glm overhead)\n    beta_mle &lt;- c(0, 0)\n    for (nr in 1:50) {\n      eta   &lt;- beta_mle[1] + beta_mle[2] * X\n      p_hat &lt;- 1 / (1 + exp(-eta))\n      W_nr  &lt;- p_hat * (1 - p_hat)\n      Xmat  &lt;- cbind(1, X)\n      grad  &lt;- t(Xmat) %*% (Y - p_hat)\n      H     &lt;- -t(Xmat) %*% (Xmat * W_nr)\n      beta_mle &lt;- beta_mle - solve(H) %*% grad\n    }\n    beta_mle &lt;- as.numeric(beta_mle)\n\n    # MLE loss\n    eta_mle  &lt;- beta_mle[1] + beta_mle[2] * X\n    p_mle    &lt;- 1 / (1 + exp(-eta_mle))\n    p_mle    &lt;- pmax(pmin(p_mle, 1 - 1e-10), 1e-10)\n    loss_mle &lt;- -mean(Y * log(p_mle) + (1 - Y) * log(1 - p_mle))\n\n    # --- Full-batch gradient descent ---\n    beta_fb    &lt;- c(0, 0)\n    loss_fb    &lt;- numeric(n_iter)\n    beta1_fb   &lt;- numeric(n_iter)\n\n    for (t in 1:n_iter) {\n      eta   &lt;- beta_fb[1] + beta_fb[2] * X\n      p_hat &lt;- 1 / (1 + exp(-eta))\n      grad  &lt;- c(-mean(Y - p_hat), -mean((Y - p_hat) * X))\n      beta_fb &lt;- beta_fb - lr * grad\n\n      eta2  &lt;- beta_fb[1] + beta_fb[2] * X\n      p2    &lt;- 1 / (1 + exp(-eta2))\n      p2    &lt;- pmax(pmin(p2, 1 - 1e-10), 1e-10)\n      loss_fb[t]  &lt;- -mean(Y * log(p2) + (1 - Y) * log(1 - p2))\n      beta1_fb[t] &lt;- beta_fb[2]\n    }\n\n    # --- Mini-batch SGD ---\n    beta_mb    &lt;- c(0, 0)\n    loss_mb    &lt;- numeric(n_iter)\n    beta1_mb   &lt;- numeric(n_iter)\n\n    for (t in 1:n_iter) {\n      idx   &lt;- sample(1:n, bs, replace = TRUE)\n      x_b   &lt;- X[idx]\n      y_b   &lt;- Y[idx]\n      eta   &lt;- beta_mb[1] + beta_mb[2] * x_b\n      p_hat &lt;- 1 / (1 + exp(-eta))\n      grad  &lt;- c(-mean(y_b - p_hat), -mean((y_b - p_hat) * x_b))\n      beta_mb &lt;- beta_mb - lr * grad\n\n      # Full-sample loss for tracking\n      eta2  &lt;- beta_mb[1] + beta_mb[2] * X\n      p2    &lt;- 1 / (1 + exp(-eta2))\n      p2    &lt;- pmax(pmin(p2, 1 - 1e-10), 1e-10)\n      loss_mb[t]  &lt;- -mean(Y * log(p2) + (1 - Y) * log(1 - p2))\n      beta1_mb[t] &lt;- beta_mb[2]\n    }\n\n    list(loss_fb = loss_fb, loss_mb = loss_mb,\n         beta1_fb = beta1_fb, beta1_mb = beta1_mb,\n         loss_mle = loss_mle, beta_mle = beta_mle,\n         n_iter = n_iter, bs = bs, n = n)\n  })\n\n  output$loss_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ylims &lt;- range(c(d$loss_fb, d$loss_mb, d$loss_mle), na.rm = TRUE)\n    ylims[2] &lt;- min(ylims[2], ylims[1] + (ylims[2] - ylims[1]) * 2)\n\n    plot(1:d$n_iter, d$loss_fb, type = \"l\", col = \"#3498db\", lwd = 2,\n         main = \"Loss vs Iteration\",\n         xlab = \"Iteration\", ylab = \"Negative log-likelihood\",\n         ylim = ylims)\n\n    lines(1:d$n_iter, d$loss_mb, col = \"#e74c3c\", lwd = 1.5)\n\n    abline(h = d$loss_mle, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Full-batch GD\", paste0(\"Mini-batch SGD (bs=\", d$bs, \")\"),\n                      \"MLE optimum\"),\n           col = c(\"#3498db\", \"#e74c3c\", \"#2c3e50\"),\n           lwd = c(2, 1.5, 2), lty = c(1, 1, 2))\n  })\n\n  output$beta_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ylims &lt;- range(c(d$beta1_fb, d$beta1_mb, d$beta_mle[2]), na.rm = TRUE)\n\n    plot(1:d$n_iter, d$beta1_fb, type = \"l\", col = \"#3498db\", lwd = 2,\n         main = expression(\"Parameter \" * hat(beta)[1] * \" vs Iteration\"),\n         xlab = \"Iteration\", ylab = expression(hat(beta)[1]),\n         ylim = ylims)\n\n    lines(1:d$n_iter, d$beta1_mb, col = \"#e74c3c\", lwd = 1.5)\n\n    abline(h = d$beta_mle[2], lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"bottomright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Full-batch GD\", paste0(\"Mini-batch SGD (bs=\", d$bs, \")\"),\n                      expression(\"MLE \" * hat(beta)[1])),\n           col = c(\"#3498db\", \"#e74c3c\", \"#2c3e50\"),\n           lwd = c(2, 1.5, 2), lty = c(1, 1, 2))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n\n    fb_final  &lt;- d$beta1_fb[d$n_iter]\n    mb_final  &lt;- d$beta1_mb[d$n_iter]\n    mle_val   &lt;- d$beta_mle[2]\n    fb_err    &lt;- abs(fb_final - mle_val)\n    mb_err    &lt;- abs(mb_final - mle_val)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;MLE solution:&lt;/b&gt;&lt;br&gt;\",\n        \"&nbsp; \\u03b2\\u0302\\u2080 = \", round(d$beta_mle[1], 3),\n        \", \\u03b2\\u0302\\u2081 = \", round(mle_val, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Full-batch final:&lt;/b&gt;&lt;br&gt;\",\n        \"&nbsp; \\u03b2\\u0302\\u2081 = \", round(fb_final, 3),\n        \" (|error| = \", round(fb_err, 4), \")&lt;br&gt;\",\n        \"&lt;b&gt;Mini-batch final:&lt;/b&gt;&lt;br&gt;\",\n        \"&nbsp; \\u03b2\\u0302\\u2081 = \", round(mb_final, 3),\n        \" (|error| = \", round(mb_err, 4), \")\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Statistical Foundations of AI",
      "Training as Maximum Likelihood"
    ]
  },
  {
    "objectID": "training-as-mle.html#what-training-optimizes-and-what-it-does-not",
    "href": "training-as-mle.html#what-training-optimizes-and-what-it-does-not",
    "title": "Training Neural Networks as Maximum Likelihood",
    "section": "What training optimizes — and what it does not",
    "text": "What training optimizes — and what it does not\nTraining a neural network finds parameters \\(\\hat{\\theta}\\) that minimize prediction error on the training distribution. This is optimization of a statistical objective: \\(\\hat{\\theta} = \\arg\\min_\\theta \\mathcal{L}(\\theta)\\).\nBut prediction is not the only thing you might care about. The MLE page noted that if the model is misspecified, MLE converges to the distribution closest to the truth in Kullback-Leibler divergence — not necessarily the “right” answer.\nFor neural networks, this matters acutely:\n\nThe model learns \\(P(Y \\mid X)\\) — the conditional distribution. It does not learn \\(P(Y \\mid do(X))\\) — the interventional distribution. This distinction is explored in Prediction vs Causation in Foundation Models.\nThe model minimizes loss on the training distribution. If the deployment distribution differs (distribution shift), the guarantees vanish.\nThe model has no notion of identification. It finds a good predictor, not a causally interpretable parameter.\n\n\n\n\n\n\n\nThe key distinction. Training minimizes prediction error. Causal inference asks whether the parameter is identified — whether you can recover the quantity of interest from the data at all, regardless of sample size or model complexity. These are different questions, and a powerful model that answers the first does not automatically answer the second.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Training as Maximum Likelihood"
    ]
  },
  {
    "objectID": "training-as-mle.html#connecting-to-the-course",
    "href": "training-as-mle.html#connecting-to-the-course",
    "title": "Training Neural Networks as Maximum Likelihood",
    "section": "Connecting to the course",
    "text": "Connecting to the course\nThis page bridges two frameworks:\n\nMLE provides the estimation principle. Neural network training is MLE (or regularized MLE) with a flexible function class.\nRegularization as Bayesian inference shows that weight decay, dropout, and other regularization techniques have principled statistical interpretations — they are not ad hoc tricks.\nThe Algebra Behind OLS derived standard errors from \\((X'X)^{-1}\\). Neural networks lack this closed-form machinery, which is why uncertainty quantification requires separate tools.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Training as Maximum Likelihood"
    ]
  },
  {
    "objectID": "model-selection.html",
    "href": "model-selection.html",
    "title": "Model Selection",
    "section": "",
    "text": "Training error always favors complexity — a more flexible model can always fit the training data at least as well. But out-of-sample, complex models overfit. Model selection asks: how do we choose the right level of complexity?\nThere are two main approaches: information criteria (penalize the log-likelihood) and cross-validation (directly estimate out-of-sample error).",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Model Selection"
    ]
  },
  {
    "objectID": "model-selection.html#the-problem",
    "href": "model-selection.html#the-problem",
    "title": "Model Selection",
    "section": "",
    "text": "Training error always favors complexity — a more flexible model can always fit the training data at least as well. But out-of-sample, complex models overfit. Model selection asks: how do we choose the right level of complexity?\nThere are two main approaches: information criteria (penalize the log-likelihood) and cross-validation (directly estimate out-of-sample error).",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Model Selection"
    ]
  },
  {
    "objectID": "model-selection.html#information-criteria",
    "href": "model-selection.html#information-criteria",
    "title": "Model Selection",
    "section": "Information criteria",
    "text": "Information criteria\nBoth AIC and BIC start from the maximized log-likelihood \\(\\hat{\\ell}\\) and add a penalty for the number of parameters \\(k\\):\n\\[\\text{AIC} = -2\\hat{\\ell} + 2k\\]\n\\[\\text{BIC} = -2\\hat{\\ell} + k \\log n\\]\nLower is better. The key difference:\n\n\n\n\n\n\n\n\n\nAIC\nBIC\n\n\n\n\nPenalty\n\\(2k\\)\n\\(k \\log n\\)\n\n\nFor \\(n &gt; 8\\)\nLighter penalty\nHeavier penalty\n\n\nSelects\nLarger models\nMore parsimonious models\n\n\nGoal\nBest prediction\nConsistent model selection\n\n\nAsymptotic property\nMinimizes KL divergence to truth\nSelects true model (if in candidate set)\n\n\n\nAIC targets prediction — it estimates the out-of-sample KL divergence. BIC targets model identification — as \\(n \\to \\infty\\), it selects the true model with probability 1 (if the true model is among the candidates).",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Model Selection"
    ]
  },
  {
    "objectID": "model-selection.html#cross-validation",
    "href": "model-selection.html#cross-validation",
    "title": "Model Selection",
    "section": "Cross-validation",
    "text": "Cross-validation\nK-fold cross-validation directly estimates the out-of-sample error without distributional assumptions:\n\nSplit data into \\(K\\) folds.\nFor each fold: train on the other \\(K-1\\) folds, predict on the held-out fold.\nAverage the prediction errors across folds.\n\nCommon choices: \\(K = 5\\) or \\(K = 10\\). Leave-one-out CV (\\(K = n\\)) is approximately equivalent to AIC for linear models.\n\n\n\n\n\n\nThe Oracle View. In the simulation below, we know the true polynomial degree that generated the data. We can compare what AIC, BIC, and cross-validation select against the truth. In practice, the “true degree” is unknown — you’re using these tools to approximate an answer you can’t observe.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Model Selection"
    ]
  },
  {
    "objectID": "model-selection.html#simulation",
    "href": "model-selection.html#simulation",
    "title": "Model Selection",
    "section": "Simulation",
    "text": "Simulation\nTrue polynomial DGP of degree \\(d\\). Fit degrees 1–10. Training error decreases monotonically; test error is U-shaped. AIC and BIC pick different models.\n#| standalone: true\n#| viewerHeight: 750\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .eq-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-bottom: 14px; font-size: 14px; line-height: 1.9;\n    }\n    .eq-box b { color: #2c3e50; }\n    .match  { color: #27ae60; font-weight: bold; }\n    .coef   { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 4,\n\n      sliderInput(\"true_d\", \"True polynomial degree:\",\n                  min = 1, max = 6, value = 3, step = 1),\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 30, max = 300, value = 80, step = 10),\n\n      sliderInput(\"sigma\", HTML(\"Noise level (&sigma;):\"),\n                  min = 0.5, max = 5, value = 1.5, step = 0.5),\n\n      actionButton(\"resim\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_box\")\n    ),\n\n    mainPanel(\n      width = 8,\n      fluidRow(\n        column(6, plotOutput(\"plot_fit\", height = \"420px\")),\n        column(6, plotOutput(\"plot_ic\", height = \"420px\"))\n      ),\n      uiOutput(\"note_box\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$resim\n    true_d &lt;- input$true_d\n    n      &lt;- input$n\n    sigma  &lt;- input$sigma\n\n    # Generate true polynomial coefficients\n    set.seed(NULL)\n    true_coefs &lt;- runif(true_d, -1, 1)\n\n    x_train &lt;- sort(runif(n, -2, 2))\n    x_test  &lt;- sort(runif(n, -2, 2))\n\n    # True function\n    f_true &lt;- function(x) {\n      val &lt;- rep(0, length(x))\n      for (j in seq_along(true_coefs)) {\n        val &lt;- val + true_coefs[j] * x^j\n      }\n      val\n    }\n\n    y_train &lt;- f_true(x_train) + rnorm(n, sd = sigma)\n    y_test  &lt;- f_true(x_test) + rnorm(n, sd = sigma)\n\n    max_d &lt;- 10\n    train_mse &lt;- numeric(max_d)\n    test_mse  &lt;- numeric(max_d)\n    aic_vals  &lt;- numeric(max_d)\n    bic_vals  &lt;- numeric(max_d)\n    cv_mse    &lt;- numeric(max_d)\n\n    for (d in 1:max_d) {\n      fit &lt;- lm(y_train ~ poly(x_train, d, raw = TRUE))\n\n      # Training MSE\n      train_mse[d] &lt;- mean(resid(fit)^2)\n\n      # Test MSE\n      pred_test &lt;- predict(fit, newdata = data.frame(x_train = x_test))\n      test_mse[d] &lt;- mean((y_test - pred_test)^2)\n\n      # AIC / BIC\n      k &lt;- d + 1  # coefficients + intercept\n      log_lik &lt;- -n/2 * log(2 * pi * train_mse[d]) - n/2\n      aic_vals[d] &lt;- -2 * log_lik + 2 * k\n      bic_vals[d] &lt;- -2 * log_lik + k * log(n)\n\n      # 5-fold CV\n      folds &lt;- sample(rep(1:5, length.out = n))\n      cv_errs &lt;- numeric(5)\n      for (fold in 1:5) {\n        train_idx &lt;- folds != fold\n        test_idx  &lt;- folds == fold\n        cv_fit &lt;- lm(y_train[train_idx] ~\n                     poly(x_train[train_idx], d, raw = TRUE))\n        cv_pred &lt;- predict(cv_fit,\n                          newdata = data.frame(\n                            \"x_train[train_idx]\" = x_train[test_idx]))\n        # Manual prediction for robustness\n        cv_coefs &lt;- coef(cv_fit)\n        cv_pred2 &lt;- cv_coefs[1]\n        for (j in 1:d) {\n          cv_pred2 &lt;- cv_pred2 + cv_coefs[j + 1] * x_train[test_idx]^j\n        }\n        cv_errs[fold] &lt;- mean((y_train[test_idx] - cv_pred2)^2)\n      }\n      cv_mse[d] &lt;- mean(cv_errs)\n    }\n\n    list(x_train = x_train, y_train = y_train,\n         f_true = f_true,\n         train_mse = train_mse, test_mse = test_mse,\n         aic_vals = aic_vals, bic_vals = bic_vals,\n         cv_mse = cv_mse,\n         true_d = true_d, sigma = sigma, n = n)\n  })\n\n  output$plot_fit &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(5, 5, 4, 2))\n\n    plot(d$x_train, d$y_train, pch = 16,\n         col = adjustcolor(\"#3498db\", 0.5), cex = 0.7,\n         xlab = \"x\", ylab = \"y\",\n         main = paste0(\"True degree: \", d$true_d))\n\n    x_grid &lt;- seq(-2, 2, length.out = 300)\n    lines(x_grid, d$f_true(x_grid), col = \"#2c3e50\", lwd = 2, lty = 2)\n\n    # Show AIC and BIC selected fits\n    aic_d &lt;- which.min(d$aic_vals)\n    bic_d &lt;- which.min(d$bic_vals)\n\n    fit_aic &lt;- lm(d$y_train ~ poly(d$x_train, aic_d, raw = TRUE))\n    c_aic &lt;- coef(fit_aic)\n    pred_aic &lt;- c_aic[1]\n    for (j in 1:aic_d) pred_aic &lt;- pred_aic + c_aic[j + 1] * x_grid^j\n    lines(x_grid, pred_aic, col = \"#e74c3c\", lwd = 2)\n\n    if (bic_d != aic_d) {\n      fit_bic &lt;- lm(d$y_train ~ poly(d$x_train, bic_d, raw = TRUE))\n      c_bic &lt;- coef(fit_bic)\n      pred_bic &lt;- c_bic[1]\n      for (j in 1:bic_d) pred_bic &lt;- pred_bic + c_bic[j + 1] * x_grid^j\n      lines(x_grid, pred_bic, col = \"#27ae60\", lwd = 2)\n    }\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"True function\",\n                      paste0(\"AIC pick (d=\", aic_d, \")\"),\n                      paste0(\"BIC pick (d=\", bic_d, \")\")),\n           col = c(\"#2c3e50\", \"#e74c3c\", \"#27ae60\"),\n           lwd = 2, lty = c(2, 1, 1))\n  })\n\n  output$plot_ic &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(5, 5, 4, 2))\n\n    degrees &lt;- 1:10\n\n    # Normalize for plotting on same scale\n    test_norm &lt;- (d$test_mse - min(d$test_mse)) /\n                 (max(d$test_mse) - min(d$test_mse) + 1e-10)\n    aic_norm  &lt;- (d$aic_vals - min(d$aic_vals)) /\n                 (max(d$aic_vals) - min(d$aic_vals) + 1e-10)\n    bic_norm  &lt;- (d$bic_vals - min(d$bic_vals)) /\n                 (max(d$bic_vals) - min(d$bic_vals) + 1e-10)\n    cv_norm   &lt;- (d$cv_mse - min(d$cv_mse)) /\n                 (max(d$cv_mse) - min(d$cv_mse) + 1e-10)\n    train_norm &lt;- (d$train_mse - min(d$train_mse)) /\n                  (max(d$train_mse) - min(d$train_mse) + 1e-10)\n\n    plot(degrees, test_norm, type = \"b\", pch = 17, cex = 0.9,\n         col = \"#e74c3c\", lwd = 2,\n         xlab = \"Polynomial degree\", ylab = \"Normalized score\",\n         main = \"Model selection criteria\",\n         ylim = c(-0.05, 1.2), xaxt = \"n\")\n    axis(1, at = 1:10)\n\n    lines(degrees, train_norm, type = \"b\", pch = 19, cex = 0.7,\n          col = \"#95a5a6\", lwd = 1.5, lty = 2)\n    lines(degrees, aic_norm, type = \"b\", pch = 15, cex = 0.9,\n          col = \"#3498db\", lwd = 2)\n    lines(degrees, bic_norm, type = \"b\", pch = 18, cex = 1.1,\n          col = \"#27ae60\", lwd = 2)\n    lines(degrees, cv_norm, type = \"b\", pch = 4, cex = 0.9,\n          col = \"#9b59b6\", lwd = 2)\n\n    # Mark selections\n    abline(v = d$true_d, lty = 3, col = \"#2c3e50\", lwd = 1.5)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Test MSE\", \"Training MSE\",\n                      paste0(\"AIC (picks \", which.min(d$aic_vals), \")\"),\n                      paste0(\"BIC (picks \", which.min(d$bic_vals), \")\"),\n                      paste0(\"CV (picks \", which.min(d$cv_mse), \")\"),\n                      paste0(\"True degree: \", d$true_d)),\n           col = c(\"#e74c3c\", \"#95a5a6\", \"#3498db\",\n                   \"#27ae60\", \"#9b59b6\", \"#2c3e50\"),\n           pch = c(17, 19, 15, 18, 4, NA),\n           lwd = c(2, 1.5, 2, 2, 2, 1.5),\n           lty = c(1, 2, 1, 1, 1, 3))\n  })\n\n  output$results_box &lt;- renderUI({\n    d &lt;- dat()\n\n    tags$div(class = \"eq-box\", style = \"margin-top: 16px;\",\n      HTML(paste0(\n        \"&lt;b&gt;Selections:&lt;/b&gt;&lt;br&gt;\",\n        \"AIC picks: degree &lt;b&gt;\", which.min(d$aic_vals), \"&lt;/b&gt;&lt;br&gt;\",\n        \"BIC picks: degree &lt;b&gt;\", which.min(d$bic_vals), \"&lt;/b&gt;&lt;br&gt;\",\n        \"CV picks: degree &lt;b&gt;\", which.min(d$cv_mse), \"&lt;/b&gt;&lt;br&gt;\",\n        \"Min test MSE: degree &lt;b&gt;\", which.min(d$test_mse), \"&lt;/b&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;True degree:&lt;/b&gt; \", d$true_d\n      ))\n    )\n  })\n\n  output$note_box &lt;- renderUI({\n    tags$div(class = \"eq-box\", style = \"margin-top: 8px;\",\n      HTML(paste0(\n        \"&lt;b&gt;Left:&lt;/b&gt; the true function (dashed black) with the AIC and BIC selected fits. \",\n        \"&lt;b&gt;Right:&lt;/b&gt; all criteria normalized to [0, 1] for visual comparison. \",\n        \"Training error (gray) always decreases — the other criteria penalize complexity.\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nTrue degree 3, low noise: AIC and BIC both nail it. Easy problem.\nTrue degree 3, high noise (\\(\\sigma = 4\\)+): BIC may pick degree 1 or 2 (too conservative). AIC stays closer to 3. Neither is always right.\nTrue degree 1 (linear): BIC’s conservatism pays off. AIC sometimes overfits to degree 2 or 3.\nLarge \\(n\\) (200+): BIC becomes very accurate at identifying the true degree. AIC may still pick slightly more complex models.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Model Selection"
    ]
  },
  {
    "objectID": "model-selection.html#when-they-disagree",
    "href": "model-selection.html#when-they-disagree",
    "title": "Model Selection",
    "section": "When they disagree",
    "text": "When they disagree\n\n\n\nGoal\nUse\n\n\n\n\nPrediction (minimize forecast error)\nAIC or CV\n\n\nInference (identify the true model)\nBIC\n\n\nNo distributional assumptions\nCV\n\n\nSafe default\nCV\n\n\n\nAIC and cross-validation are asymptotically equivalent for linear models. BIC is more conservative and will select simpler models, especially for large \\(n\\).\n\n\nConnections\n\nBias-Variance Tradeoff — Model selection is the practical response to the bias-variance tradeoff\nMaximum Likelihood — AIC and BIC are both based on the maximized log-likelihood\nBayesian Model Comparison — BIC approximates the Bayes factor; Bayesian model selection formalizes the complexity penalty\nRegularization as Bayesian Inference — Regularization is a continuous alternative to discrete model selection\n\n\n\n\nDid you know?\n\nHirotugu Akaike proposed the AIC in 1973. The key insight: the expected log-likelihood overestimates the out-of-sample performance by approximately \\(k\\) (the number of parameters), so subtracting \\(2k\\) corrects for optimism. Akaike won the first Kyoto Prize in Basic Sciences for this work.\nGideon Schwarz proposed the BIC in 1978 from a Bayesian perspective: it approximates the log marginal likelihood (Bayes factor) under certain regularity conditions. Despite its Bayesian motivation, it’s used by frequentists everywhere.\nThe AIC vs BIC debate is still unresolved because they answer different questions. AIC asks “which model predicts best?” BIC asks “which model is true?” These are different questions with different answers.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Model Selection"
    ]
  },
  {
    "objectID": "gmm.html",
    "href": "gmm.html",
    "title": "Generalized Method of Moments",
    "section": "",
    "text": "Method of Moments is simple but leaves a key question unanswered: which moments should you match? And what if you have more moment conditions than parameters? GMM formalizes this. It takes a set of moment conditions — potentially more than you need — and finds the parameters that satisfy them as closely as possible in a precise, optimal sense.",
    "crumbs": [
      "Estimation",
      "Generalized Method of Moments"
    ]
  },
  {
    "objectID": "gmm.html#from-mom-to-gmm",
    "href": "gmm.html#from-mom-to-gmm",
    "title": "Generalized Method of Moments",
    "section": "From MoM to GMM",
    "text": "From MoM to GMM\nRecall how MoM works: you have \\(k\\) parameters and you pick \\(k\\) moment conditions. Set the sample moments equal to zero and solve. When you have exactly as many conditions as parameters — the just-identified case — there’s a unique solution and everything is clean.\nBut in practice, economic theory or statistical reasoning often gives you more moment conditions than parameters. Suppose you have \\(m\\) conditions for \\(k\\) parameters, with \\(m &gt; k\\). Now the system is over-identified: you can’t satisfy all \\(m\\) conditions exactly, so you need a way to get as close as possible.\nThat’s what GMM does. Instead of solving a system of equations, it minimizes a measure of how far the sample moments are from zero.",
    "crumbs": [
      "Estimation",
      "Generalized Method of Moments"
    ]
  },
  {
    "objectID": "gmm.html#the-gmm-estimator",
    "href": "gmm.html#the-gmm-estimator",
    "title": "Generalized Method of Moments",
    "section": "The GMM estimator",
    "text": "The GMM estimator\nStart with a vector of moment conditions. The theory says that at the true parameter \\(\\theta_0\\):\n\\[\nE[g(X_i, \\theta_0)] = 0\n\\]\nwhere \\(g\\) is an \\(m\\)-dimensional vector (one entry per moment condition). The sample analog is:\n\\[\n\\bar{g}(\\theta) = \\frac{1}{n}\\sum_{i=1}^n g(X_i, \\theta)\n\\]\nIf \\(m = k\\) (just-identified), you can set \\(\\bar{g}(\\theta) = 0\\) and solve — that’s MoM. If \\(m &gt; k\\) (over-identified), you can’t make all entries of \\(\\bar{g}\\) exactly zero. GMM minimizes a weighted quadratic form:\n\\[\n\\hat{\\theta}_{\\text{GMM}} = \\arg\\min_\\theta \\; \\bar{g}(\\theta)' \\, W \\, \\bar{g}(\\theta)\n\\]\nwhere \\(W\\) is an \\(m \\times m\\) positive definite weighting matrix. This is like a weighted sum of squared moment violations — GMM finds the \\(\\theta\\) that makes the sample moments as close to zero as possible, with \\(W\\) determining how much each moment condition matters.\n\nChoosing \\(W\\)\nThe choice of \\(W\\) affects efficiency but not consistency — any positive definite \\(W\\) gives a consistent estimator. But some choices are better than others.\n\n\n\n\n\n\nWhat is efficiency? An estimator is more efficient if it has smaller variance (tighter sampling distribution) for the same sample size. Two estimators can both be consistent — converging to the truth as \\(n \\to \\infty\\) — but one can get there with less noise along the way. That’s efficiency: how much information the estimator squeezes out of each observation. MLE is asymptotically efficient — no consistent estimator can have smaller variance in large samples. MoM and GMM are consistent but may be less efficient, depending on which moments you use and how you weight them.\n\n\n\nFeasible two-step GMM. The most common approach:\n\nStart with \\(W = I\\) (or any reasonable \\(W\\)) and get a preliminary estimate \\(\\hat{\\theta}_1\\)\nUse \\(\\hat{\\theta}_1\\) to estimate the optimal weighting matrix: \\(\\hat{W} = \\left[\\frac{1}{n}\\sum_i g(X_i, \\hat{\\theta}_1) \\, g(X_i, \\hat{\\theta}_1)'\\right]^{-1}\\)\nRe-estimate with \\(\\hat{W}\\) to get \\(\\hat{\\theta}_2\\)\n\nThe optimal \\(W\\) is the inverse of the variance of the moment conditions. This weights precisely-estimated moments more heavily and noisy moments less — exactly the right thing to do.\n\n\n\n\n\n\nIntuition. If one moment condition has a lot of sampling noise and another is very precise, you should trust the precise one more. The optimal weighting matrix does this automatically.\n\n\n\nTry the simulation below — compare the sampling distribution of the IV estimator with 1 instrument (just-identified) versus more instruments (over-identified), and watch the standard deviation shrink as you add instruments.\n\n\nThings to try\n\n1 instrument: just-identified IV. The sampling distribution can be wide, especially with weak instruments.\nIncrease instruments: the distribution tightens — more valid moment conditions means more information. The efficiency gain shows in the lower SD.\nWeak instruments (low strength): the distribution becomes wide and potentially biased. Even with many instruments, weak instruments give poor estimates.\nJ-test (right panel): with valid instruments, the J-statistics follow the \\(\\chi^2\\) reference closely and the rejection rate is near 5%. If you modified the DGP to include an invalid instrument, the J-test would reject more often.\n\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 200, max = 2000, value = 500, step = 200),\n\n      sliderInput(\"m\", \"Number of instruments:\",\n                  min = 1, max = 6, value = 3, step = 1),\n\n      sliderInput(\"strength\", \"Instrument strength:\",\n                  min = 0.1, max = 0.9, value = 0.5, step = 0.1),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(7, plotOutput(\"hist_plot\", height = \"470px\")),\n        column(5, plotOutput(\"jtest_plot\", height = \"470px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  sim &lt;- reactive({\n    input$go\n    n      &lt;- input$n\n    m      &lt;- input$m\n    pi_str &lt;- input$strength\n    reps   &lt;- 500\n    beta_true &lt;- 2\n\n    est_1 &lt;- numeric(reps)\n    est_m &lt;- numeric(reps)\n    j_stats &lt;- if (m &gt; 1) numeric(reps) else NULL\n\n    for (r in 1:reps) {\n      # Generate instruments\n      Z_all &lt;- matrix(rnorm(n * m), n, m)\n\n      # Endogenous regressor: X correlated with error\n      v   &lt;- rnorm(n)\n      eps &lt;- 0.6 * v + rnorm(n) * 0.8\n      X   &lt;- Z_all %*% rep(pi_str / sqrt(m), m) + v\n      Y   &lt;- 1 + beta_true * X + eps\n\n      Xmat &lt;- cbind(1, X)\n\n      # --- Just-identified: 1 instrument ---\n      Z1   &lt;- cbind(1, Z_all[, 1])\n      Pz1  &lt;- Z1 %*% solve(t(Z1) %*% Z1) %*% t(Z1)\n      Xh1  &lt;- Pz1 %*% Xmat\n      b1   &lt;- solve(t(Xh1) %*% Xmat) %*% (t(Xh1) %*% Y)\n      est_1[r] &lt;- b1[2]\n\n      # --- Over-identified: m instruments ---\n      Zm   &lt;- cbind(1, Z_all[, 1:m, drop = FALSE])\n      Pzm  &lt;- Zm %*% solve(t(Zm) %*% Zm) %*% t(Zm)\n      Xhm  &lt;- Pzm %*% Xmat\n      bm   &lt;- solve(t(Xhm) %*% Xmat) %*% (t(Xhm) %*% Y)\n      est_m[r] &lt;- bm[2]\n\n      # J-test (Sargan) when over-identified\n      if (m &gt; 1) {\n        e_hat  &lt;- as.numeric(Y - Xmat %*% bm)\n        sigma2 &lt;- sum(e_hat^2) / (n - 2)\n        Z_exc  &lt;- Z_all[, 1:m, drop = FALSE]\n        PZ_exc &lt;- Z_exc %*% solve(t(Z_exc) %*% Z_exc) %*% t(Z_exc)\n        j_stats[r] &lt;- as.numeric(t(e_hat) %*% PZ_exc %*% e_hat) / sigma2\n      }\n    }\n\n    list(est_1 = est_1, est_m = est_m, j_stats = j_stats,\n         beta_true = beta_true, m = m, n = n,\n         sd_1 = sd(est_1), sd_m = sd(est_m),\n         mean_1 = mean(est_1), mean_m = mean(est_m))\n  })\n\n  output$hist_plot &lt;- renderPlot({\n    d &lt;- sim()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    all_est &lt;- c(d$est_1, d$est_m)\n    spread  &lt;- max(abs(all_est - d$beta_true), na.rm = TRUE) * 1.2\n    xlims   &lt;- d$beta_true + c(-1, 1) * spread\n    breaks_seq &lt;- seq(xlims[1], xlims[2], length.out = 50)\n\n    hist(d$est_1, breaks = breaks_seq,\n         col = adjustcolor(\"#3498db\", 0.4),\n         border = adjustcolor(\"#3498db\", 0.6),\n         probability = TRUE,\n         main = expression(\"Sampling Distribution of \" * hat(beta)[1]),\n         xlab = expression(hat(beta)[1]),\n         xlim = xlims)\n\n    if (d$m &gt; 1) {\n      hist(d$est_m, breaks = breaks_seq,\n           col = adjustcolor(\"#e74c3c\", 0.4),\n           border = adjustcolor(\"#e74c3c\", 0.6),\n           probability = TRUE, add = TRUE)\n    }\n\n    abline(v = d$beta_true, lwd = 2.5, col = \"#2c3e50\", lty = 2)\n\n    leg_labels &lt;- c(\"1 instrument (just-identified)\",\n                     paste0(d$m, \" instruments (over-identified)\"),\n                     expression(\"True \" * beta))\n    leg_cols &lt;- c(adjustcolor(\"#3498db\", 0.6),\n                  adjustcolor(\"#e74c3c\", 0.6),\n                  \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = leg_labels[if (d$m &gt; 1) 1:3 else c(1, 3)],\n           fill   = c(adjustcolor(\"#3498db\", 0.4),\n                      if (d$m &gt; 1) adjustcolor(\"#e74c3c\", 0.4) else NULL,\n                      NA),\n           border = c(adjustcolor(\"#3498db\", 0.6),\n                      if (d$m &gt; 1) adjustcolor(\"#e74c3c\", 0.6) else NULL,\n                      NA),\n           lwd    = c(NA, if (d$m &gt; 1) NA else NULL, 2.5),\n           lty    = c(NA, if (d$m &gt; 1) NA else NULL, 2))\n  })\n\n  output$jtest_plot &lt;- renderPlot({\n    d &lt;- sim()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    if (d$m &lt;= 1) {\n      plot.new()\n      text(0.5, 0.5, \"J-test not available\\n(just-identified: m = k)\",\n           cex = 1.2, col = \"#7f8c8d\")\n    } else {\n      df &lt;- d$m - 1\n\n      hist(d$j_stats, breaks = 30,\n           col = adjustcolor(\"#9b59b6\", 0.4),\n           border = \"#9b59b6\", probability = TRUE,\n           main = paste0(\"J-test Statistics (df = \", df, \")\"),\n           xlab = \"J-statistic\",\n           xlim = c(0, max(d$j_stats, qchisq(0.99, df)) * 1.1))\n\n      x_seq &lt;- seq(0, max(d$j_stats, qchisq(0.99, df)) * 1.1,\n                    length.out = 200)\n      lines(x_seq, dchisq(x_seq, df), col = \"#2c3e50\", lwd = 2, lty = 2)\n\n      cv &lt;- qchisq(0.95, df)\n      abline(v = cv, col = \"#e74c3c\", lwd = 1.5, lty = 3)\n\n      rej_rate &lt;- mean(d$j_stats &gt; cv)\n\n      legend(\"topright\", bty = \"n\", cex = 0.8,\n             legend = c(\n               as.expression(bquote(chi^2 * \"(\" * .(df) * \") reference\")),\n               \"5% critical value\",\n               paste0(\"Rejection rate: \", round(rej_rate * 100, 1), \"%\")),\n             col = c(\"#2c3e50\", \"#e74c3c\", NA),\n             lwd = c(2, 1.5, NA), lty = c(2, 3, NA))\n    }\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- sim()\n    eff_gain &lt;- if (d$m &gt; 1) round((1 - d$sd_m / d$sd_1) * 100, 1) else NA\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;1 instrument:&lt;/b&gt;&lt;br&gt;\",\n        \"&nbsp; Mean = \", round(d$mean_1, 3),\n        \", SD = \", round(d$sd_1, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;\", d$m, \" instruments:&lt;/b&gt;&lt;br&gt;\",\n        \"&nbsp; Mean = \", round(d$mean_m, 3),\n        \", SD = \", round(d$sd_m, 3),\n        if (!is.na(eff_gain)) paste0(\n          \"&lt;hr style='margin:6px 0'&gt;\",\n          \"&lt;b&gt;Efficiency gain:&lt;/b&gt; \", eff_gain, \"%&lt;br&gt;\",\n          \"&lt;span style='font-size:12px;color:#7f8c8d'&gt;\",\n          \"(reduction in SD from 1→\", d$m, \" instruments)&lt;/span&gt;\"\n        ) else \"\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Estimation",
      "Generalized Method of Moments"
    ]
  },
  {
    "objectID": "gmm.html#examples-that-build-on-the-course",
    "href": "gmm.html#examples-that-build-on-the-course",
    "title": "Generalized Method of Moments",
    "section": "Examples that build on the course",
    "text": "Examples that build on the course\n\nOLS as GMM\nThe population moment condition behind OLS is:\n\\[\nE[X_i(Y_i - X_i'\\beta)] = 0\n\\]\nThis says the regressors are uncorrelated with the error — exactly the identification assumption from Regression & the CEF. The sample analog is:\n\\[\n\\bar{g}(\\beta) = \\frac{1}{n}\\sum_{i=1}^n X_i(Y_i - X_i'\\beta) = \\frac{1}{n}X'(y - X\\beta)\n\\]\nSetting this to zero and solving gives \\(X'X\\hat{\\beta} = X'y\\), or \\(\\hat{\\beta} = (X'X)^{-1}X'y\\) — the OLS formula from The Algebra Behind OLS. OLS is just-identified GMM: \\(k\\) regressors, \\(k\\) moment conditions, and \\(W\\) drops out because you can solve exactly.\n\n\nIV/2SLS as GMM\nWhen regressors are endogenous, you need instruments \\(Z_i\\) that are correlated with \\(X_i\\) but uncorrelated with \\(\\varepsilon_i\\). The moment condition becomes:\n\\[\nE[Z_i(Y_i - X_i'\\beta)] = 0\n\\]\nIf you have more instruments than endogenous regressors (\\(m &gt; k\\)), this is over-identified — and GMM handles it naturally. Two-stage least squares (2SLS) is a specific GMM estimator with a particular choice of weighting matrix.",
    "crumbs": [
      "Estimation",
      "Generalized Method of Moments"
    ]
  },
  {
    "objectID": "gmm.html#the-j-test-overidentification-test",
    "href": "gmm.html#the-j-test-overidentification-test",
    "title": "Generalized Method of Moments",
    "section": "The J-test (overidentification test)",
    "text": "The J-test (overidentification test)\nWhen you’re over-identified (\\(m &gt; k\\)), you have a built-in specification test. The logic: if the model is correct and all moment conditions are valid, the minimized GMM objective should be “small” (close to zero). If it’s “large,” at least some moment conditions are being violated, which suggests the model is misspecified.\nFormally, under the null that the model is correct:\n\\[\nJ = n \\cdot \\bar{g}(\\hat{\\theta})' \\hat{W} \\, \\bar{g}(\\hat{\\theta}) \\;\\xrightarrow{d}\\; \\chi^2_{m-k}\n\\]\nwhere \\(m - k\\) is the number of “extra” moment conditions (the degree of over-identification). A large \\(J\\) (relative to the \\(\\chi^2_{m-k}\\) distribution) rejects the model.\n\n\n\n\n\n\nWhat the J-test can and can’t do. The J-test can detect when your moment conditions are mutually inconsistent — they can’t all be true at the same time. But it can’t tell you which condition is wrong. And if you’re just-identified (\\(m = k\\)), \\(J = 0\\) by construction — there’s no overidentification to test.",
    "crumbs": [
      "Estimation",
      "Generalized Method of Moments"
    ]
  },
  {
    "objectID": "gmm.html#when-to-use-gmm",
    "href": "gmm.html#when-to-use-gmm",
    "title": "Generalized Method of Moments",
    "section": "When to use GMM",
    "text": "When to use GMM\nWhen economic theory gives you moment conditions but not a full likelihood. Many structural models in economics specify relationships between variables (Euler equations, equilibrium conditions) that translate directly into moment conditions. You don’t need to know the full distribution of the data — just these conditions. This is much less demanding than MLE.\nWhen you want robustness to distributional assumptions. GMM only assumes the moment conditions hold; it’s agnostic about the rest of the distribution. If you use MLE with the wrong distributional assumption, your estimator converges to the wrong thing. GMM avoids this risk by not making that assumption in the first place.\nWhen you have instruments or panel data. These settings naturally produce moment conditions (instrument exogeneity, sequential exogeneity), making GMM a natural framework.\nThe tradeoff. GMM is more robust than MLE but less efficient when MLE’s distributional assumptions actually hold. If you know the distribution and it’s correctly specified, MLE squeezes out every last bit of information. GMM leaves some information on the table by not using the full distribution.",
    "crumbs": [
      "Estimation",
      "Generalized Method of Moments"
    ]
  },
  {
    "objectID": "gmm.html#connecting-to-mle",
    "href": "gmm.html#connecting-to-mle",
    "title": "Generalized Method of Moments",
    "section": "Connecting to MLE",
    "text": "Connecting to MLE\nMLE is actually a special case of GMM. The MLE score equations — the first-order conditions from maximizing the log-likelihood — are moment conditions:\n\\[\nE\\!\\left[\\frac{\\partial \\log f(X_i \\mid \\theta_0)}{\\partial \\theta}\\right] = 0\n\\]\nThese are the “moment conditions” that MLE implicitly uses. When you use these specific conditions with the optimal GMM weighting matrix, GMM produces exactly the MLE. So the hierarchy is:\n\\[\n\\text{MoM} \\;\\subset\\; \\text{GMM} \\;\\supset\\; \\text{MLE}\n\\]\nMoM is GMM with a specific (often suboptimal) choice of moments and \\(W = I\\). MLE is GMM with the score as the moment condition and the optimal \\(W\\). GMM is the general framework that nests both.\nThis is why GMM achieves MLE-level efficiency when the likelihood is correctly specified and the optimal weighting matrix is used — it’s doing MLE, just expressed in the language of moments.",
    "crumbs": [
      "Estimation",
      "Generalized Method of Moments"
    ]
  },
  {
    "objectID": "test-statistics.html",
    "href": "test-statistics.html",
    "title": "Test Statistics",
    "section": "",
    "text": "p-values & Confidence Intervals explained what a p-value means. This page explains the machinery that produces them — the test statistics. There are many, but they almost all reduce to the same core idea: how far is the estimate from the null, measured in units of its uncertainty?",
    "crumbs": [
      "Inference",
      "Test Statistics"
    ]
  },
  {
    "objectID": "test-statistics.html#the-universal-structure",
    "href": "test-statistics.html#the-universal-structure",
    "title": "Test Statistics",
    "section": "The universal structure",
    "text": "The universal structure\nNearly every classical test statistic has this shape:\n\\[\n\\text{test statistic} = \\frac{(\\text{estimate} - \\text{null value})^2}{\\text{variance of estimate}}\n\\]\nThat’s a quadratic form — a squared distance, scaled by uncertainty. This is why the \\(\\chi^2\\) distribution shows up everywhere: a squared standard normal is \\(\\chi^2_1\\) by definition, and sums of squared standard normals are \\(\\chi^2\\) with more degrees of freedom.\n\n\n\n\n\n\nThe deep pattern. Linear forms (estimate \\(-\\) null, unscaled) give you \\(Z\\) or \\(t\\) statistics. Quadratic forms (squared and variance-scaled) give you \\(\\chi^2\\) or \\(F\\) statistics. Likelihood comparisons give you LR statistics. Rank-based forms give you nonparametric tests. But asymptotically, most of them converge to normal or chi-squared distributions.",
    "crumbs": [
      "Inference",
      "Test Statistics"
    ]
  },
  {
    "objectID": "test-statistics.html#i.-mean-based-tests",
    "href": "test-statistics.html#i.-mean-based-tests",
    "title": "Test Statistics",
    "section": "I. Mean-based tests",
    "text": "I. Mean-based tests\n\nZ-statistic\nThe simplest test statistic. Used when the variance is known (rare) or the sample is large enough for the CLT to kick in:\n\\[\nZ = \\frac{\\hat{\\theta} - \\theta_0}{\\text{SE}(\\hat{\\theta})}\n\\]\nUnder \\(H_0\\): \\(Z \\sim N(0, 1)\\).\nYou’ll see Z-tests in large-sample regressions and most asymptotic tests in econometrics. When someone reports a “coefficient divided by its standard error” in a large sample, that’s a Z-statistic.\n\n\nt-statistic\nSame structure as \\(Z\\), but the standard error is estimated rather than known:\n\\[\nt = \\frac{\\hat{\\theta} - \\theta_0}{\\widehat{\\text{SE}}(\\hat{\\theta})}\n\\]\nUnder \\(H_0\\): \\(t \\sim t_{df}\\), where \\(df\\) depends on the sample size and number of parameters.\nThis is what standard regression output reports. Every coefficient row in a regression table has a t-statistic and a p-value derived from it.\nThe t-distribution has heavier tails than the normal — it’s more conservative, accounting for the extra uncertainty from estimating the variance. As \\(n \\to \\infty\\), the t-distribution converges to the normal, so \\(t \\to Z\\).",
    "crumbs": [
      "Inference",
      "Test Statistics"
    ]
  },
  {
    "objectID": "test-statistics.html#ii.-variance-and-distribution-tests",
    "href": "test-statistics.html#ii.-variance-and-distribution-tests",
    "title": "Test Statistics",
    "section": "II. Variance and distribution tests",
    "text": "II. Variance and distribution tests\n\nChi-squared (\\(\\chi^2\\))\nThe chi-squared statistic is a general quadratic form:\n\\[\n\\chi^2 = \\sum \\frac{(O - E)^2}{E}\n\\]\nwhere \\(O\\) is observed and \\(E\\) is expected under the null. Used for goodness-of-fit tests, independence in contingency tables, and likelihood ratio tests.\nA key fact that connects everything:\n\\[\n\\text{If } Z \\sim N(0, 1), \\text{ then } Z^2 \\sim \\chi^2_1\n\\]\nThis is why \\(\\chi^2\\) shows up everywhere — many test statistics are squared normals, or sums of squared normals. When you have \\(k\\) independent squared standard normals, you get \\(\\chi^2_k\\).\n\n\nF-statistic\nUsed to test multiple restrictions simultaneously — “are all these coefficients jointly zero?”:\n\\[\nF = \\frac{(\\text{RSS}_r - \\text{RSS}_u) / q}{\\text{RSS}_u / (n - k)}\n\\]\nwhere \\(\\text{RSS}_r\\) is the residual sum of squares from the restricted model, \\(\\text{RSS}_u\\) from the unrestricted model, \\(q\\) is the number of restrictions, and \\(n - k\\) is the degrees of freedom.\nUnder \\(H_0\\): \\(F \\sim F_{q, \\, n-k}\\).\nUsed for joint hypothesis tests (“are all pre-trends zero?” in DiD), ANOVA, and overall model significance.\nThe F-statistic and the t-statistic are directly related:\n\\[\nF_{1, \\, df} = t^2\n\\]\nAn F-test with one restriction is exactly the square of the corresponding t-test. The F generalizes the t to multiple restrictions. And just as \\(t \\to Z\\) as \\(n \\to \\infty\\), we have \\(q \\cdot F_{q, n-k} \\to \\chi^2_q\\) — the F-test converges to a scaled chi-squared test in large samples.\n\n\nSimulation: Watch t converge to Z and F converge to chi-squared as n grows\nDrag the degrees-of-freedom slider and watch two convergences happen simultaneously: the t distribution collapses onto the standard normal, and F(1, df) collapses onto chi-squared(1).\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"df\", \"Degrees of freedom (df):\",\n                  min = 2, max = 200, value = 10, step = 1),\n\n      uiOutput(\"stats\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"conv_plot\", height = \"540px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  output$conv_plot &lt;- renderPlot({\n    df &lt;- input$df\n    par(mfrow = c(2, 1), mar = c(4.5, 4.5, 3, 1))\n\n    # --- Top panel: t vs Z ---\n    xseq &lt;- seq(-4.5, 4.5, length.out = 500)\n    y_t &lt;- dt(xseq, df = df)\n    y_z &lt;- dnorm(xseq)\n\n    plot(xseq, y_z, type = \"l\", lwd = 2, lty = 2, col = \"black\",\n         xlab = \"x\", ylab = \"Density\",\n         main = paste0(\"t(\", df, \") vs Standard Normal\"),\n         ylim = c(0, max(y_z) * 1.15))\n    lines(xseq, y_t, lwd = 3, col = \"#3498db\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.9,\n           legend = c(paste0(\"t(\", df, \")\"), \"N(0, 1)\"),\n           col = c(\"#3498db\", \"black\"),\n           lwd = c(3, 2), lty = c(1, 2))\n\n    # --- Bottom panel: F(1, df) vs chi-sq(1) ---\n    xf &lt;- seq(0.001, 8, length.out = 500)\n    y_f  &lt;- df(xf, df1 = 1, df2 = df)\n    y_ch &lt;- dchisq(xf, df = 1)\n\n    ymax &lt;- min(max(c(y_f, y_ch), na.rm = TRUE) * 1.1, 5)\n\n    plot(xf, y_f, type = \"l\", lwd = 3, col = \"#27ae60\",\n         xlab = \"x\", ylab = \"Density\",\n         main = paste0(\"F(1, \", df, \") vs \\u03c7\\u00b2(1)\"),\n         ylim = c(0, ymax))\n    lines(xf, y_ch, lwd = 2, lty = 2, col = \"#e74c3c\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.9,\n           legend = c(paste0(\"F(1, \", df, \")\"),\n                      expression(chi^2 * \"(1)\")),\n           col = c(\"#27ae60\", \"#e74c3c\"),\n           lwd = c(3, 2), lty = c(1, 2))\n  })\n\n  output$stats &lt;- renderUI({\n    df &lt;- input$df\n    p_t  &lt;- 2 * pt(-1.96, df = df)\n    p_z  &lt;- 0.05\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;df:&lt;/b&gt; \", df, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;P(|t| &gt; 1.96):&lt;/b&gt; \", round(p_t, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;P(|Z| &gt; 1.96):&lt;/b&gt; \", round(p_z, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Difference:&lt;/b&gt; \", round(abs(p_t - p_z), 4), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;As df &rarr; &infin;, the t-tail&lt;br&gt;\",\n        \"probability converges to 0.05&lt;br&gt;\",\n        \"and F(1, df) &rarr; &chi;&sup2;(1).&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Inference",
      "Test Statistics"
    ]
  },
  {
    "objectID": "test-statistics.html#iii.-the-likelihood-based-trinity",
    "href": "test-statistics.html#iii.-the-likelihood-based-trinity",
    "title": "Test Statistics",
    "section": "III. The likelihood-based trinity",
    "text": "III. The likelihood-based trinity\nThese three tests are the workhorses of MLE-based inference. They all test the same null hypothesis and are asymptotically equivalent — but they differ in what you need to estimate.\n\nLikelihood Ratio (LR) test\nEstimate both the restricted and unrestricted models, then compare their log-likelihoods:\n\\[\nLR = -2\\left(\\ell_{\\text{restricted}} - \\ell_{\\text{unrestricted}}\\right)\n\\]\nUnder \\(H_0\\): \\(LR \\sim \\chi^2_q\\), where \\(q\\) is the number of restrictions.\nIntuition: does the likelihood drop much when you impose the restriction? If the restriction is true, the restricted model shouldn’t fit much worse, so \\(LR\\) should be small.\nUsed in logit/probit, structural estimation, and any MLE comparison.\n\n\nWald test\nEstimate only the unrestricted model, then check whether the estimates are far from the restriction:\n\\[\nW = \\frac{(\\hat{\\theta} - \\theta_0)^2}{\\text{Var}(\\hat{\\theta})}\n\\]\nUnder \\(H_0\\): \\(W \\sim \\chi^2_q\\).\nIntuition: if the restriction is true, the unrestricted estimate should land close to \\(\\theta_0\\). A large \\(W\\) means the estimate is far from the null, in variance-scaled units.\nThis is what most regression software gives you by default. In fact, t-tests and F-tests are special cases of Wald tests: the t-statistic is \\(\\sqrt{W}\\) for a single restriction, and the F-statistic is \\(W/q\\) with a finite-sample correction.\n\n\nScore (Lagrange Multiplier) test\nEstimate only the restricted model, then check whether the score (the gradient of the log-likelihood) is far from zero at the restricted estimate:\n\\[\nLM = \\left.\\frac{\\partial \\ell}{\\partial \\theta}\\right|_{\\theta = \\hat{\\theta}_r}^{\\!\\!2} \\bigg/ \\mathcal{I}(\\hat{\\theta}_r)\n\\]\nUnder \\(H_0\\): \\(LM \\sim \\chi^2_q\\).\nIntuition: if the restriction is correct, the restricted estimate should be near the peak of the log-likelihood, so the slope (score) should be near zero. A steep slope at the restricted estimate means the restriction is pushing you away from the peak.\nThe Score test is useful when the unrestricted model is hard to estimate — you only need the restricted model.\n\n\nThe trinity compared\n\n\n\nTest\nRequires estimating\nChecks\n\n\n\n\nLikelihood Ratio\nBoth models\nDid the likelihood drop?\n\n\nWald\nUnrestricted only\nIs the estimate far from the null?\n\n\nScore (LM)\nRestricted only\nIs the score far from zero?\n\n\n\nAll three converge to \\(\\chi^2_q\\) under the null. In finite samples they can disagree, but the ranking is typically: Wald \\(\\geq\\) LR \\(\\geq\\) Score (Wald tends to over-reject, Score tends to under-reject).\n\n\n\n\n\n\nWhen to use which. If you’ve already estimated the full model, use Wald — it’s what your regression output gives you. If you’re comparing nested models via MLE, use LR. If the unrestricted model is computationally expensive or doesn’t converge, use Score.",
    "crumbs": [
      "Inference",
      "Test Statistics"
    ]
  },
  {
    "objectID": "test-statistics.html#iv.-nonparametric-tests",
    "href": "test-statistics.html#iv.-nonparametric-tests",
    "title": "Test Statistics",
    "section": "IV. Nonparametric tests",
    "text": "IV. Nonparametric tests\nThese don’t assume normality or any specific distribution.\n\nRank-based tests\nWilcoxon and Mann-Whitney tests replace raw values with ranks. Instead of asking “is the mean different?”, they ask “do observations from one group tend to have higher ranks?” Robust to outliers and non-normal distributions.\n\n\nKolmogorov-Smirnov (KS) test\nTests whether two distributions are equal by finding the maximum distance between their CDFs:\n\\[\nKS = \\sup_x |F_1(x) - F_2(x)|\n\\]\nThis is a whole-distribution test — it detects differences in shape, location, and spread simultaneously.",
    "crumbs": [
      "Inference",
      "Test Statistics"
    ]
  },
  {
    "objectID": "test-statistics.html#v.-meta-analysis-and-multiple-testing",
    "href": "test-statistics.html#v.-meta-analysis-and-multiple-testing",
    "title": "Test Statistics",
    "section": "V. Meta-analysis and multiple testing",
    "text": "V. Meta-analysis and multiple testing\n\nFisher’s combined test\nWhen you have \\(k\\) independent p-values from separate studies and want to test whether there’s any signal across them:\n\\[\nX^2 = -2\\sum_{i=1}^k \\ln(p_i)\n\\]\nUnder the global null (all \\(H_0\\)’s are true): \\(X^2 \\sim \\chi^2_{2k}\\).\nThe intuition: under the null, p-values are uniform on \\([0, 1]\\), so \\(-2\\ln(p_i) \\sim \\chi^2_2\\). Summing them gives a \\(\\chi^2\\) with \\(2k\\) degrees of freedom.\n\n\nMultiple testing corrections\nNot a new statistic — instead, these adjust p-values or rejection thresholds when you’re running many tests simultaneously. Bonferroni (divide \\(\\alpha\\) by the number of tests), Holm, Benjamini-Hochberg (FDR control). Covered in Multiple Testing.",
    "crumbs": [
      "Inference",
      "Test Statistics"
    ]
  },
  {
    "objectID": "test-statistics.html#the-relationships-between-everything",
    "href": "test-statistics.html#the-relationships-between-everything",
    "title": "Test Statistics",
    "section": "The relationships between everything",
    "text": "The relationships between everything\nHere’s how these test statistics connect to each other:\n\\[\nZ^2 = \\chi^2_1 = F_{1,\\infty} = W \\text{ (one restriction, large sample)}\n\\]\n\\[\nt^2 = F_{1, df}\n\\]\n\\[\nq \\cdot F_{q, n-k} \\;\\xrightarrow{n \\to \\infty}\\; \\chi^2_q\n\\]\n\\[\n\\text{Wald} \\approx \\text{LR} \\approx \\text{Score} \\;\\sim\\; \\chi^2_q \\text{ (asymptotically)}\n\\]\nAnd in causal inference contexts:\n\n\n\n\n\n\n\n\n\nSetting\nIndividual coefficient\nJoint test\nRobust version\n\n\n\n\nRegression\nt-test\nF-test\nCluster-robust t or F\n\n\nDiD / event study\nt (each period)\nF (all pre-trends)\nCluster-robust\n\n\nMLE (logit, probit)\nWald (z)\nLR or Wald \\(\\chi^2\\)\nSandwich-robust Wald\n\n\nBootstrap\n—\n—\nEmpirical distribution\n\n\n\n\n\n\n\n\n\nThe takeaway. Different test statistics differ in their small-sample properties, robustness, and whether they require variance estimation or rely on asymptotics. But the unifying idea is always the same: measure how far the data are from what the null hypothesis predicts, in units that account for sampling variability. The bigger that distance, the stronger the evidence against the null.",
    "crumbs": [
      "Inference",
      "Test Statistics"
    ]
  },
  {
    "objectID": "pvalues-ci.html",
    "href": "pvalues-ci.html",
    "title": "p-values, Confidence Intervals & What They Actually Mean",
    "section": "",
    "text": "These are two separate steps, and confusing them is where most misunderstanding begins.\nStep 1 — Estimation is about computing numbers from your data:\n\nYou collect a sample and calculate a point estimate — your best guess at the true parameter. For example, \\(\\hat{\\beta} = -5\\) or \\(\\bar{x} = 12.3\\).\nYou also calculate a standard error (SE) — how much that estimate would bounce around if you repeated the experiment. A small SE means your estimate is precise; a large SE means it’s noisy.\n\nEstimation gives you a number. It does not tell you what to conclude.\nStep 2 — Inference is about drawing conclusions from those numbers:\n\nIs the effect real, or could it be noise? → p-value\nWhat range of values is plausible for the true parameter? → confidence interval\nBoth are built from the same two ingredients: the estimate and its SE.\n\n\\[\\text{test statistic} = \\frac{\\text{estimate}}{\\text{SE}} \\qquad \\qquad \\text{CI} = \\text{estimate} \\pm 1.96 \\times \\text{SE}\\]\nEverything on this page is Step 2. If the estimation step is wrong (biased estimate, wrong SE), then the inference is wrong too — no matter how sophisticated the test. Good inference starts with good estimation.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we set the true \\(\\mu\\) — so we know whether \\(H_0\\) is true, whether each CI covers the truth, and whether each rejection is a correct detection or a false alarm. In practice, you never know if \\(H_0\\) is true. That’s the whole point of the test.",
    "crumbs": [
      "Inference",
      "p-values & Confidence Intervals"
    ]
  },
  {
    "objectID": "pvalues-ci.html#first-estimation.-then-inference.",
    "href": "pvalues-ci.html#first-estimation.-then-inference.",
    "title": "p-values, Confidence Intervals & What They Actually Mean",
    "section": "",
    "text": "These are two separate steps, and confusing them is where most misunderstanding begins.\nStep 1 — Estimation is about computing numbers from your data:\n\nYou collect a sample and calculate a point estimate — your best guess at the true parameter. For example, \\(\\hat{\\beta} = -5\\) or \\(\\bar{x} = 12.3\\).\nYou also calculate a standard error (SE) — how much that estimate would bounce around if you repeated the experiment. A small SE means your estimate is precise; a large SE means it’s noisy.\n\nEstimation gives you a number. It does not tell you what to conclude.\nStep 2 — Inference is about drawing conclusions from those numbers:\n\nIs the effect real, or could it be noise? → p-value\nWhat range of values is plausible for the true parameter? → confidence interval\nBoth are built from the same two ingredients: the estimate and its SE.\n\n\\[\\text{test statistic} = \\frac{\\text{estimate}}{\\text{SE}} \\qquad \\qquad \\text{CI} = \\text{estimate} \\pm 1.96 \\times \\text{SE}\\]\nEverything on this page is Step 2. If the estimation step is wrong (biased estimate, wrong SE), then the inference is wrong too — no matter how sophisticated the test. Good inference starts with good estimation.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we set the true \\(\\mu\\) — so we know whether \\(H_0\\) is true, whether each CI covers the truth, and whether each rejection is a correct detection or a false alarm. In practice, you never know if \\(H_0\\) is true. That’s the whole point of the test.",
    "crumbs": [
      "Inference",
      "p-values & Confidence Intervals"
    ]
  },
  {
    "objectID": "pvalues-ci.html#part-1-p-values",
    "href": "pvalues-ci.html#part-1-p-values",
    "title": "p-values, Confidence Intervals & What They Actually Mean",
    "section": "Part 1: p-values",
    "text": "Part 1: p-values\n\nWhat is a test statistic?\nBefore we can talk about p-values, we need the test statistic — a single number that measures how far your estimate is from what the null hypothesis predicts, in units of standard error.\nThe most common one is the z-statistic (or t-statistic for small samples):\n\\[z = \\frac{\\bar{x} - \\mu_0}{\\text{SE}} = \\frac{\\text{estimate} - \\text{null value}}{\\text{standard error}}\\]\nIt answers: “How many standard errors is my estimate from the null?” A \\(z\\) of 2 means your estimate is 2 SEs away from what H₀ predicts — that’s unusual enough to start doubting H₀.\n\n\nWhat a p-value actually is\nThe “p” stands for probability.\n\nThe p-value is the probability of getting a test statistic as large as (or larger than) yours, assuming the null hypothesis is true.\n\nThat’s it. Large \\(|z|\\) → small p-value → more evidence against H₀.\nIt is not:\n\nThe probability that H₀ is true\nThe probability that you made a mistake\nThe probability that the result will replicate\n\nIt’s a statement about the data, not about the hypothesis. The p-value asks: “If there really were no effect, how surprising would my data be?”\n\n\nA concrete example\nSay you’re testing whether a drug lowers blood pressure. You run a regression and get \\(\\hat{\\beta} = -5\\) (blood pressure drops 5 points).\nH₀: \\(\\beta = 0\\) — the drug does nothing.\nThe p-value asks: if the drug truly does nothing (\\(\\beta = 0\\)), what’s the probability of observing \\(\\hat{\\beta}\\) as extreme as \\(-5\\) or more?\np = 0.03 means: there’s only a 3% chance of seeing an estimate this large purely from random noise. So either:\n\nH₀ is true and you got unlucky (3% chance), or\nH₀ is false — the drug actually works\n\nYou reject H₀ because 3% feels too unlikely to be just noise.\nBut the p-value never tells you \\(\\beta\\)’s actual value. It only tells you the data would be surprising if \\(\\beta\\) were exactly zero. This matters:\n\np = 0.03 with \\(\\hat{\\beta} = -5\\) → probably a real, meaningful effect\np = 0.03 with \\(\\hat{\\beta} = -0.001\\) and \\(n = 10{,}000{,}000\\) → “statistically significant” but practically meaningless\n\nThat’s why you should always look at the estimate (\\(\\hat{\\beta}\\)) and confidence interval together with the p-value — not just whether p &lt; 0.05.\n\n\nSimulation: The p-value machine\nDraw a sample, compute a test statistic, and see where it falls on the null distribution. The shaded area is the p-value. Under H₀, p-values are uniformly distributed — every value between 0 and 1 is equally likely.\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 10, max = 200, value = 30, step = 5),\n\n      sliderInput(\"true_mu\", HTML(\"True &mu; (for data generation):\"),\n                  min = -1, max = 1, value = 0, step = 0.1),\n\n      sliderInput(\"reps\", \"Number of experiments:\",\n                  min = 100, max = 2000, value = 500, step = 100),\n\n      helpText(\"Set true \\u03bc = 0 to see p-values under H\\u2080.\n               Set it away from 0 to see p-values under H\\u2081.\"),\n\n      actionButton(\"go\", \"Run experiments\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"null_dist\", height = \"400px\")),\n        column(6, plotOutput(\"pval_hist\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n      &lt;- input$n\n    mu     &lt;- input$true_mu\n    reps   &lt;- input$reps\n\n    # Draw one sample for display\n    one_sample &lt;- rnorm(n, mean = mu, sd = 1)\n    one_z &lt;- mean(one_sample) / (1 / sqrt(n))\n    one_p &lt;- 2 * pnorm(-abs(one_z))\n\n    # Draw many samples\n    z_stats &lt;- replicate(reps, {\n      x &lt;- rnorm(n, mean = mu, sd = 1)\n      mean(x) / (1 / sqrt(n))\n    })\n    p_vals &lt;- 2 * pnorm(-abs(z_stats))\n\n    reject_rate &lt;- mean(p_vals &lt; 0.05)\n\n    list(one_z = one_z, one_p = one_p, z_stats = z_stats, p_vals = p_vals,\n         n = n, mu = mu, reps = reps, reject_rate = reject_rate)\n  })\n\n  output$null_dist &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    x &lt;- seq(-4, 4, length.out = 300)\n    y &lt;- dnorm(x)\n\n    plot(x, y, type = \"l\", lwd = 2.5, col = \"#2c3e50\",\n         xlab = \"Test statistic (z)\", ylab = \"Density\",\n         main = \"Null distribution & your test statistic\")\n\n    # Shade p-value area (two-tailed)\n    z_abs &lt;- abs(d$one_z)\n    if (z_abs &lt; 4) {\n      x_right &lt;- seq(z_abs, 4, length.out = 100)\n      polygon(c(z_abs, x_right, 4), c(0, dnorm(x_right), 0),\n              col = adjustcolor(\"#e74c3c\", 0.4), border = NA)\n      x_left &lt;- seq(-4, -z_abs, length.out = 100)\n      polygon(c(-4, x_left, -z_abs), c(0, dnorm(x_left), 0),\n              col = adjustcolor(\"#e74c3c\", 0.4), border = NA)\n    }\n\n    abline(v = d$one_z, lwd = 2.5, col = \"#3498db\", lty = 1)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Null distribution (H\\u2080)\",\n                      paste0(\"Your z = \", round(d$one_z, 3)),\n                      paste0(\"p-value = \", round(d$one_p, 4))),\n           col = c(\"#2c3e50\", \"#3498db\", adjustcolor(\"#e74c3c\", 0.6)),\n           lwd = c(2.5, 2.5, 8), lty = c(1, 1, 1))\n  })\n\n  output$pval_hist &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$p_vals, breaks = 20, probability = TRUE,\n         col = \"#3498db30\", border = \"#3498db80\",\n         main = paste0(\"Distribution of p-values (\",\n                       d$reps, \" experiments)\"),\n         xlab = \"p-value\", ylab = \"Density\",\n         xlim = c(0, 1))\n\n    if (abs(d$mu) &lt; 0.001) {\n      abline(h = 1, lty = 2, lwd = 2, col = \"#e74c3c\")\n      legend(\"topright\", bty = \"n\", cex = 0.85,\n             legend = c(\"p-value histogram\",\n                        \"Uniform(0,1) reference\"),\n             col = c(\"#3498db80\", \"#e74c3c\"),\n             lwd = c(8, 2), lty = c(1, 2))\n    } else {\n      legend(\"topright\", bty = \"n\", cex = 0.85,\n             legend = paste0(\"Under H\\u2081 (\\u03bc = \", d$mu, \"):\\n\",\n                            \"p-values pile up near 0\"))\n    }\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;This sample:&lt;/b&gt;&lt;br&gt;\",\n        \"z = \", round(d$one_z, 3), \"&lt;br&gt;\",\n        \"p = \", round(d$one_p, 4), \" \",\n        ifelse(d$one_p &lt; 0.05,\n               \"&lt;span class='bad'&gt;(&lt; 0.05)&lt;/span&gt;\",\n               \"&lt;span class='good'&gt;(\\u2265 0.05)&lt;/span&gt;\"), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Across \", d$reps, \" experiments:&lt;/b&gt;&lt;br&gt;\",\n        \"Rejection rate: \", round(d$reject_rate * 100, 1), \"%&lt;br&gt;\",\n        \"&lt;small&gt;\",\n        ifelse(abs(d$mu) &lt; 0.001,\n               \"Under H\\u2080, this should be ~5%.\",\n               paste0(\"Under H\\u2081, this is the power.\")),\n        \"&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nTrue \\(\\mu\\) = 0: p-values are uniform — flat histogram. This is the defining property of a valid test. About 5% of p-values fall below 0.05 purely by chance.\nTrue \\(\\mu\\) = 0.5: p-values pile up near 0. The further \\(\\mu\\) is from 0, the more powerful the test, and the smaller the p-values tend to be.",
    "crumbs": [
      "Inference",
      "p-values & Confidence Intervals"
    ]
  },
  {
    "objectID": "pvalues-ci.html#part-2-confidence-intervals",
    "href": "pvalues-ci.html#part-2-confidence-intervals",
    "title": "p-values, Confidence Intervals & What They Actually Mean",
    "section": "Part 2: Confidence intervals",
    "text": "Part 2: Confidence intervals\n\nWhat a confidence interval actually is\n\nA 95% CI is constructed by a procedure that, in repeated sampling, captures the true parameter 95% of the time.\n\nIt is not:\n\nA 95% probability that the true parameter is inside this particular interval\nA range where 95% of the data falls\nA range where 95% of sample means fall\n\nOnce you compute a specific CI, the true parameter is either in it or it isn’t — there’s no probability about it. The 95% refers to the method, not to any single interval.\n\n\nHow a CI is constructed\nThe recipe is simple. You need three ingredients:\n\nA point estimate — your best guess (e.g., \\(\\bar{x}\\) or \\(\\hat{\\beta}\\))\nA standard error — how much the estimate bounces around due to sampling\nA critical value — how many SEs to go out for your desired confidence level\n\nThe formula for a 95% CI for a mean:\n\\[\\text{CI} = \\bar{x} \\pm z_{0.975} \\times \\text{SE} = \\bar{x} \\pm 1.96 \\times \\frac{s}{\\sqrt{n}}\\]\nThat’s it: estimate ± margin of error. The margin of error is just a scaled-up standard error.\n\n\n\nConfidence level\nCritical value (\\(z\\))\nMargin of error\n\n\n\n\n90%\n1.645\nNarrower CI\n\n\n95%\n1.960\nStandard\n\n\n99%\n2.576\nWider CI\n\n\n\nWhere does the 1.96 come from? The middle 95% of a standard normal distribution falls between \\(-1.96\\) and \\(+1.96\\). So if the sampling distribution of \\(\\bar{x}\\) is approximately normal (CLT), going out 1.96 SEs in each direction captures the true mean 95% of the time.\nFor small samples (roughly \\(n &lt; 30\\)), replace the \\(z\\) critical value with a \\(t\\) critical value from the \\(t\\)-distribution with \\(n - 1\\) degrees of freedom. The \\(t\\)-distribution has heavier tails, making the CI wider to account for the extra uncertainty in estimating the SE from small samples. As \\(n\\) grows, the \\(t\\) and \\(z\\) values converge.\nThe key insight: a CI is wide when (a) the data is noisy (large \\(s\\)), (b) the sample is small (small \\(n\\)), or (c) you demand more confidence (larger critical value). You can’t have precision, small samples, and high confidence all at once — pick two.\n\n\nSimulation: CI coverage\nDraw 100 confidence intervals. Each one either contains the true \\(\\mu\\) (blue) or misses it (red). Over many experiments, about 95% contain \\(\\mu\\) — but any single CI either does or doesn’t. The 95% is about the procedure, not about your interval.\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n2\", \"Sample size (n):\",\n                  min = 5, max = 100, value = 25, step = 5),\n\n      sliderInput(\"conf\", \"Confidence level:\",\n                  min = 0.80, max = 0.99, value = 0.95, step = 0.01),\n\n      sliderInput(\"n_ci\", \"Number of CIs to draw:\",\n                  min = 20, max = 200, value = 100, step = 10),\n\n      actionButton(\"go2\", \"Draw new CIs\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"ci_plot\", height = \"550px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go2\n    n     &lt;- input$n2\n    conf  &lt;- input$conf\n    n_ci  &lt;- input$n_ci\n    mu    &lt;- 5  # true mean\n    sigma &lt;- 2\n\n    z &lt;- qnorm(1 - (1 - conf) / 2)\n\n    results &lt;- t(replicate(n_ci, {\n      x &lt;- rnorm(n, mean = mu, sd = sigma)\n      xbar &lt;- mean(x)\n      se &lt;- sd(x) / sqrt(n)\n      lo &lt;- xbar - z * se\n      hi &lt;- xbar + z * se\n      c(xbar = xbar, lo = lo, hi = hi, covers = (lo &lt;= mu & mu &lt;= hi))\n    }))\n\n    list(results = results, mu = mu, n = n, conf = conf, n_ci = n_ci)\n  })\n\n  output$ci_plot &lt;- renderPlot({\n    d &lt;- dat()\n    res &lt;- d$results\n    n_ci &lt;- d$n_ci\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    covers &lt;- as.logical(res[, \"covers\"])\n    cols &lt;- ifelse(covers, \"#3498db\", \"#e74c3c\")\n\n    plot(NULL, xlim = range(res[, c(\"lo\", \"hi\")]),\n         ylim = c(0.5, n_ci + 0.5),\n         xlab = expression(\"Value of \" * mu),\n         ylab = \"Experiment number\",\n         main = paste0(n_ci, \" Confidence Intervals (\",\n                       d$conf * 100, \"% level)\"),\n         yaxt = \"n\")\n\n    # Draw CIs\n    segments(res[, \"lo\"], seq_len(n_ci), res[, \"hi\"], seq_len(n_ci),\n             col = cols, lwd = 1.5)\n    points(res[, \"xbar\"], seq_len(n_ci), pch = 16, cex = 0.5, col = cols)\n\n    # True mu\n    abline(v = d$mu, lty = 2, lwd = 2.5, col = \"#2c3e50\")\n\n    n_miss &lt;- sum(!covers)\n    legend(\"topright\", bty = \"n\", cex = 0.9,\n           legend = c(paste0(\"Contains \\u03bc (\", sum(covers), \")\"),\n                      paste0(\"Misses \\u03bc (\", n_miss, \")\"),\n                      paste0(\"True \\u03bc = \", d$mu)),\n           col = c(\"#3498db\", \"#e74c3c\", \"#2c3e50\"),\n           lwd = c(3, 3, 2.5), lty = c(1, 1, 2))\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- dat()\n    covers &lt;- as.logical(d$results[, \"covers\"])\n    cover_rate &lt;- mean(covers)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Coverage rate:&lt;/b&gt; &lt;span class='\",\n        ifelse(abs(cover_rate - d$conf) &lt; 0.05, \"good\", \"bad\"), \"'&gt;\",\n        round(cover_rate * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Target:&lt;/b&gt; \", d$conf * 100, \"%&lt;br&gt;\",\n        \"&lt;b&gt;Missed:&lt;/b&gt; \", sum(!covers), \" of \", d$n_ci, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;Each red line is a CI that&lt;br&gt;\",\n        \"does not contain the true \\u03bc.&lt;br&gt;\",\n        \"The method gets it right ~\",\n        d$conf * 100, \"% of the time.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\n95% level: roughly 5 out of 100 CIs miss \\(\\mu\\) (shown in red). Click “Draw new CIs” several times — the exact number varies, but it averages to 5%.\nChange confidence to 99%: fewer CIs miss \\(\\mu\\), but each CI is wider. There’s always a tradeoff between coverage and precision.\nSmall n (n = 5): the CIs are very wide. With little data, you can’t be precise. This is the price of honesty.",
    "crumbs": [
      "Inference",
      "p-values & Confidence Intervals"
    ]
  },
  {
    "objectID": "pvalues-ci.html#the-bottom-line",
    "href": "pvalues-ci.html#the-bottom-line",
    "title": "p-values, Confidence Intervals & What They Actually Mean",
    "section": "The bottom line",
    "text": "The bottom line\n\nA p-value is not the probability the null is true. It’s the probability of seeing data this extreme if the null were true.\nA confidence interval is not a probability statement about \\(\\mu\\). It’s a statement about the procedure: if you repeated the experiment many times, 95% of your CIs would contain \\(\\mu\\).\nBoth concepts are statements about long-run frequency, not about any single experiment. This is what makes frequentist inference subtle — and why Bayesian methods (which can make probability statements about parameters) are appealing to many.\n\n\n\nDid you know?\n\nThe p-value was popularized by R.A. Fisher in the 1920s as an informal measure of evidence against the null. But the rigid “reject if p &lt; 0.05” framework came from Jerzy Neyman and Egon Pearson in the 1930s. Fisher and Neyman-Pearson fundamentally disagreed about what statistical testing means, and the debate was never resolved — we use an awkward hybrid of both frameworks to this day.\nIn 2016, the American Statistical Association issued its first-ever formal statement on p-values, warning against common misinterpretations. Statement #2: “P-values do not measure the probability that the studied hypothesis is true.”\nConfidence intervals were invented by Neyman in 1937. He was explicit that the 95% refers to the procedure, not to any single interval. He wrote: “It is not possible to say that the probability of the true value falling in any particular interval is 95%.”",
    "crumbs": [
      "Inference",
      "p-values & Confidence Intervals"
    ]
  },
  {
    "objectID": "bootstrap.html",
    "href": "bootstrap.html",
    "title": "The Bootstrap",
    "section": "",
    "text": "You want a confidence interval for some statistic (mean, median, regression coefficient), but you don’t know the sampling distribution. Maybe the formula is complicated. Maybe there is no formula.\nThe bootstrap says: pretend your sample is the population. Then simulate the sampling process by resampling with replacement from your data, over and over. Each resample gives you a new estimate. The distribution of those estimates approximates the true sampling distribution.\n\n\n\nYou have a sample of \\(n\\) observations.\nDraw \\(n\\) observations with replacement from your sample (some points get picked twice, some not at all).\nCompute your statistic on this resample.\nRepeat B times (typically 1,000–10,000).\nThe spread of those B estimates gives you a standard error and a confidence interval.\n\nThat’s it. No formulas, no distributional assumptions. Just resampling.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we know the true population and the true parameter — so we can check whether the bootstrap CIs actually cover the truth. In practice, the bootstrap is all you have: you resample from your one sample and trust that it approximates the population well enough.\n\n\n\n\n\n\nNo — and this is a crucial distinction. There are two different distributions at play:\n\nThe distribution of the data = what individual observations look like (their histogram). This tells you about spread, skew, outliers in your raw data.\nThe sampling distribution = what your statistic (mean, median, slope) would look like if you could repeat the entire experiment many times. This is what you need for standard errors and confidence intervals.\n\nYou can always plot your data. But you cannot see the sampling distribution — you only ran the experiment once. You have one mean, not a distribution of means.\nThe bootstrap bridges that gap. It simulates the “what if I repeated this experiment?” question using only the data you have. The left panel below shows your data distribution. The right panel shows the bootstrap sampling distribution. Notice: they look completely different. Your data might be skewed and spread out, but the sampling distribution of the mean is narrow and roughly normal (thanks to the CLT). The bootstrap discovers this for you without any formulas.\n#| standalone: true\n#| viewerHeight: 900\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"stat\", \"Statistic to bootstrap:\",\n                  choices = c(\"Mean\", \"Median\", \"SD\",\n                              \"Correlation\", \"Regression slope\")),\n\n      selectInput(\"pop\", \"Population shape:\",\n                  choices = c(\"Normal\", \"Skewed (exponential)\",\n                              \"Heavy-tailed\", \"Bimodal\")),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 10, max = 200, value = 30, step = 10),\n\n      sliderInput(\"B\", \"Bootstrap resamples (B):\",\n                  min = 100, max = 5000, value = 1000, step = 100),\n\n      actionButton(\"go\", \"New sample + bootstrap\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"sample_plot\", height = \"350px\")),\n        column(6, plotOutput(\"boot_plot\", height = \"350px\"))\n      ),\n      fluidRow(\n        column(12, plotOutput(\"compare_plot\", height = \"350px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  draw_pop &lt;- function(n, pop) {\n    switch(pop,\n      \"Normal\" = rnorm(n, mean = 5, sd = 2),\n      \"Skewed (exponential)\" = rexp(n, rate = 0.5),\n      \"Heavy-tailed\" = rt(n, df = 3) * 2 + 5,\n      \"Bimodal\" = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, 2, 0.8) + (1 - k) * rnorm(n, 7, 0.8)\n      }\n    )\n  }\n\n  compute_stat &lt;- function(x, y = NULL, stat) {\n    switch(stat,\n      \"Mean\" = mean(x),\n      \"Median\" = median(x),\n      \"SD\" = sd(x),\n      \"Correlation\" = cor(x, y),\n      \"Regression slope\" = coef(lm(y ~ x))[2]\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    B    &lt;- input$B\n    stat &lt;- input$stat\n    pop  &lt;- input$pop\n\n    x &lt;- draw_pop(n, pop)\n    y &lt;- NULL\n    if (stat %in% c(\"Correlation\", \"Regression slope\")) {\n      y &lt;- 1 + 0.8 * x + rnorm(n, sd = 2)\n    }\n\n    obs_stat &lt;- compute_stat(x, y, stat)\n\n    # Bootstrap\n    boot_stats &lt;- replicate(B, {\n      idx &lt;- sample(n, n, replace = TRUE)\n      x_b &lt;- x[idx]\n      y_b &lt;- if (!is.null(y)) y[idx] else NULL\n      compute_stat(x_b, y_b, stat)\n    })\n\n    boot_se &lt;- sd(boot_stats)\n    boot_ci &lt;- quantile(boot_stats, c(0.025, 0.975))\n\n    list(x = x, y = y, obs_stat = obs_stat,\n         boot_stats = boot_stats, boot_se = boot_se,\n         boot_ci = boot_ci, stat = stat, n = n, B = B)\n  })\n\n  output$sample_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    if (d$stat %in% c(\"Correlation\", \"Regression slope\")) {\n      plot(d$x, d$y, pch = 16, col = \"#3498db80\", cex = 0.8,\n           xlab = \"X\", ylab = \"Y\", main = \"Your sample\")\n      if (d$stat == \"Regression slope\") {\n        abline(lm(d$y ~ d$x), col = \"#e74c3c\", lwd = 2)\n      }\n    } else {\n      hist(d$x, breaks = 20, col = \"#d5e8d4\", border = \"#82b366\",\n           main = \"Your sample\", xlab = \"X\", ylab = \"Frequency\")\n      abline(v = d$obs_stat, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n      legend(\"topright\", bty = \"n\", cex = 0.85,\n             legend = paste0(d$stat, \" = \", round(d$obs_stat, 3)),\n             col = \"#e74c3c\", lty = 2, lwd = 2)\n    }\n  })\n\n  output$boot_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$boot_stats, breaks = 40, probability = TRUE,\n         col = \"#dae8fc\", border = \"#6c8ebf\",\n         main = paste0(\"Bootstrap distribution (B = \", d$B, \")\"),\n         xlab = paste0(\"Bootstrap \", tolower(d$stat)),\n         ylab = \"Density\")\n\n    abline(v = d$obs_stat, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n    abline(v = d$boot_ci, col = \"#27ae60\", lwd = 2, lty = 3)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(paste0(\"Observed: \", round(d$obs_stat, 3)),\n                      paste0(\"95% CI: [\", round(d$boot_ci[1], 3),\n                             \", \", round(d$boot_ci[2], 3), \"]\")),\n           col = c(\"#e74c3c\", \"#27ae60\"),\n           lty = c(2, 3), lwd = 2)\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    if (d$stat %in% c(\"Correlation\", \"Regression slope\")) {\n      # For bivariate stats, just show bootstrap dist with CI\n      hist(d$boot_stats, breaks = 40, probability = TRUE,\n           col = \"#dae8fc\", border = \"#6c8ebf\",\n           main = \"Bootstrap sampling dist.\",\n           xlab = paste0(\"Bootstrap \", tolower(d$stat)),\n           ylab = \"Density\")\n      abline(v = d$boot_ci, col = \"#27ae60\", lwd = 2, lty = 3)\n      abline(v = d$obs_stat, col = \"#e74c3c\", lwd = 2, lty = 2)\n    } else {\n      # Overlay: data distribution (wide) vs bootstrap distribution (narrow)\n      # Rescale data density to fit on same axes\n      dens_data &lt;- density(d$x)\n      dens_boot &lt;- density(d$boot_stats)\n\n      # Normalize both to peak at 1 for visual comparison\n      dens_data$y &lt;- dens_data$y / max(dens_data$y)\n      dens_boot$y &lt;- dens_boot$y / max(dens_boot$y)\n\n      xlim &lt;- range(c(dens_data$x, dens_boot$x))\n\n      plot(dens_data, col = \"#e74c3c\", lwd = 2.5, xlim = xlim,\n           ylim = c(0, 1.25), main = \"Data vs Sampling Distribution\",\n           xlab = \"Value\", ylab = \"Scaled density\")\n      polygon(dens_data$x, dens_data$y,\n              col = adjustcolor(\"#e74c3c\", 0.15), border = NA)\n\n      lines(dens_boot, col = \"#3498db\", lwd = 2.5)\n      polygon(dens_boot$x, dens_boot$y,\n              col = adjustcolor(\"#3498db\", 0.15), border = NA)\n\n      legend(\"topright\", bty = \"n\", cex = 0.8,\n             legend = c(\"Your data (wide, raw obs.)\",\n                        paste0(\"Sampling dist. of \", tolower(d$stat),\n                               \" (narrow)\")),\n             col = c(\"#e74c3c\", \"#3498db\"), lwd = 2.5)\n    }\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Statistic:&lt;/b&gt; \", d$stat, \"&lt;br&gt;\",\n        \"&lt;b&gt;Observed:&lt;/b&gt; \", round(d$obs_stat, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bootstrap SE:&lt;/b&gt; \", round(d$boot_se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;95% CI:&lt;/b&gt; [\", round(d$boot_ci[1], 4),\n        \", \", round(d$boot_ci[2], 4), \"]&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;CI uses percentile method:&lt;br&gt;\",\n        \"2.5th and 97.5th percentiles of&lt;br&gt;\",\n        \"bootstrap distribution.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nMean with Normal population: the bootstrap distribution looks normal, similar to what CLT gives you analytically.\nMedian with skewed population: no easy formula for the SE of a median. But the bootstrap handles it effortlessly.\nSD with heavy-tailed data: the bootstrap distribution is skewed — the percentile CI is asymmetric, which is exactly right.\nRegression slope: bootstrap the slope to get a CI without assuming homoskedasticity.\nSmall n (10) vs large n (200): with small samples the bootstrap distribution is choppy (limited unique resamples). With more data it smooths out.\n\n\n\n\n\n\n\n\n\n\n\nUse bootstrap when…\nDon’t bother when…\n\n\n\n\nNo formula for the SE exists\nStandard formulas are available and valid\n\n\nThe statistic is complicated (ratios, quantiles)\nYou’re computing a simple mean\n\n\nYou suspect non-normality\nn is large and CLT applies\n\n\nYou want a quick, assumption-free CI\nYou need exact small-sample inference\n\n\n\nThe bootstrap is not magic — it can fail with very small samples or non-smooth statistics. But for most practical situations, it’s the easiest path to a valid confidence interval.\n\n\n\n\n\nBradley Efron invented the bootstrap in 1979 at Stanford. The name comes from the expression “pulling yourself up by your bootstraps” — attributed to the tall tales of Baron Munchausen, who claimed to have pulled himself out of a swamp by his own hair (later versions say bootstraps). The idea that a sample can estimate its own sampling distribution seemed equally absurd at first.\nWhen Efron first presented the bootstrap, many statisticians were skeptical. One famously asked: “You’re just sampling from your sample — how can that tell you anything new?” The answer: it tells you about the variability of your estimate, not about new data points.\nThe bootstrap is now one of the most cited statistical methods ever. Efron’s 1979 paper has over 20,000 citations. He received the International Prize in Statistics (the statistics equivalent of the Nobel) in 2019 for this work.\nBefore the bootstrap, getting standard errors for complex statistics (ratios, quantiles, eigenvalues) required either painful analytical derivations or the delta method — a Taylor expansion trick that often required heroic assumptions. The bootstrap made all of that unnecessary.",
    "crumbs": [
      "Inference",
      "The Bootstrap"
    ]
  },
  {
    "objectID": "bootstrap.html#the-idea",
    "href": "bootstrap.html#the-idea",
    "title": "The Bootstrap",
    "section": "",
    "text": "You want a confidence interval for some statistic (mean, median, regression coefficient), but you don’t know the sampling distribution. Maybe the formula is complicated. Maybe there is no formula.\nThe bootstrap says: pretend your sample is the population. Then simulate the sampling process by resampling with replacement from your data, over and over. Each resample gives you a new estimate. The distribution of those estimates approximates the true sampling distribution.\n\n\n\nYou have a sample of \\(n\\) observations.\nDraw \\(n\\) observations with replacement from your sample (some points get picked twice, some not at all).\nCompute your statistic on this resample.\nRepeat B times (typically 1,000–10,000).\nThe spread of those B estimates gives you a standard error and a confidence interval.\n\nThat’s it. No formulas, no distributional assumptions. Just resampling.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we know the true population and the true parameter — so we can check whether the bootstrap CIs actually cover the truth. In practice, the bootstrap is all you have: you resample from your one sample and trust that it approximates the population well enough.\n\n\n\n\n\n\nNo — and this is a crucial distinction. There are two different distributions at play:\n\nThe distribution of the data = what individual observations look like (their histogram). This tells you about spread, skew, outliers in your raw data.\nThe sampling distribution = what your statistic (mean, median, slope) would look like if you could repeat the entire experiment many times. This is what you need for standard errors and confidence intervals.\n\nYou can always plot your data. But you cannot see the sampling distribution — you only ran the experiment once. You have one mean, not a distribution of means.\nThe bootstrap bridges that gap. It simulates the “what if I repeated this experiment?” question using only the data you have. The left panel below shows your data distribution. The right panel shows the bootstrap sampling distribution. Notice: they look completely different. Your data might be skewed and spread out, but the sampling distribution of the mean is narrow and roughly normal (thanks to the CLT). The bootstrap discovers this for you without any formulas.\n#| standalone: true\n#| viewerHeight: 900\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"stat\", \"Statistic to bootstrap:\",\n                  choices = c(\"Mean\", \"Median\", \"SD\",\n                              \"Correlation\", \"Regression slope\")),\n\n      selectInput(\"pop\", \"Population shape:\",\n                  choices = c(\"Normal\", \"Skewed (exponential)\",\n                              \"Heavy-tailed\", \"Bimodal\")),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 10, max = 200, value = 30, step = 10),\n\n      sliderInput(\"B\", \"Bootstrap resamples (B):\",\n                  min = 100, max = 5000, value = 1000, step = 100),\n\n      actionButton(\"go\", \"New sample + bootstrap\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"sample_plot\", height = \"350px\")),\n        column(6, plotOutput(\"boot_plot\", height = \"350px\"))\n      ),\n      fluidRow(\n        column(12, plotOutput(\"compare_plot\", height = \"350px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  draw_pop &lt;- function(n, pop) {\n    switch(pop,\n      \"Normal\" = rnorm(n, mean = 5, sd = 2),\n      \"Skewed (exponential)\" = rexp(n, rate = 0.5),\n      \"Heavy-tailed\" = rt(n, df = 3) * 2 + 5,\n      \"Bimodal\" = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, 2, 0.8) + (1 - k) * rnorm(n, 7, 0.8)\n      }\n    )\n  }\n\n  compute_stat &lt;- function(x, y = NULL, stat) {\n    switch(stat,\n      \"Mean\" = mean(x),\n      \"Median\" = median(x),\n      \"SD\" = sd(x),\n      \"Correlation\" = cor(x, y),\n      \"Regression slope\" = coef(lm(y ~ x))[2]\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    B    &lt;- input$B\n    stat &lt;- input$stat\n    pop  &lt;- input$pop\n\n    x &lt;- draw_pop(n, pop)\n    y &lt;- NULL\n    if (stat %in% c(\"Correlation\", \"Regression slope\")) {\n      y &lt;- 1 + 0.8 * x + rnorm(n, sd = 2)\n    }\n\n    obs_stat &lt;- compute_stat(x, y, stat)\n\n    # Bootstrap\n    boot_stats &lt;- replicate(B, {\n      idx &lt;- sample(n, n, replace = TRUE)\n      x_b &lt;- x[idx]\n      y_b &lt;- if (!is.null(y)) y[idx] else NULL\n      compute_stat(x_b, y_b, stat)\n    })\n\n    boot_se &lt;- sd(boot_stats)\n    boot_ci &lt;- quantile(boot_stats, c(0.025, 0.975))\n\n    list(x = x, y = y, obs_stat = obs_stat,\n         boot_stats = boot_stats, boot_se = boot_se,\n         boot_ci = boot_ci, stat = stat, n = n, B = B)\n  })\n\n  output$sample_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    if (d$stat %in% c(\"Correlation\", \"Regression slope\")) {\n      plot(d$x, d$y, pch = 16, col = \"#3498db80\", cex = 0.8,\n           xlab = \"X\", ylab = \"Y\", main = \"Your sample\")\n      if (d$stat == \"Regression slope\") {\n        abline(lm(d$y ~ d$x), col = \"#e74c3c\", lwd = 2)\n      }\n    } else {\n      hist(d$x, breaks = 20, col = \"#d5e8d4\", border = \"#82b366\",\n           main = \"Your sample\", xlab = \"X\", ylab = \"Frequency\")\n      abline(v = d$obs_stat, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n      legend(\"topright\", bty = \"n\", cex = 0.85,\n             legend = paste0(d$stat, \" = \", round(d$obs_stat, 3)),\n             col = \"#e74c3c\", lty = 2, lwd = 2)\n    }\n  })\n\n  output$boot_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$boot_stats, breaks = 40, probability = TRUE,\n         col = \"#dae8fc\", border = \"#6c8ebf\",\n         main = paste0(\"Bootstrap distribution (B = \", d$B, \")\"),\n         xlab = paste0(\"Bootstrap \", tolower(d$stat)),\n         ylab = \"Density\")\n\n    abline(v = d$obs_stat, col = \"#e74c3c\", lwd = 2.5, lty = 2)\n    abline(v = d$boot_ci, col = \"#27ae60\", lwd = 2, lty = 3)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(paste0(\"Observed: \", round(d$obs_stat, 3)),\n                      paste0(\"95% CI: [\", round(d$boot_ci[1], 3),\n                             \", \", round(d$boot_ci[2], 3), \"]\")),\n           col = c(\"#e74c3c\", \"#27ae60\"),\n           lty = c(2, 3), lwd = 2)\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    if (d$stat %in% c(\"Correlation\", \"Regression slope\")) {\n      # For bivariate stats, just show bootstrap dist with CI\n      hist(d$boot_stats, breaks = 40, probability = TRUE,\n           col = \"#dae8fc\", border = \"#6c8ebf\",\n           main = \"Bootstrap sampling dist.\",\n           xlab = paste0(\"Bootstrap \", tolower(d$stat)),\n           ylab = \"Density\")\n      abline(v = d$boot_ci, col = \"#27ae60\", lwd = 2, lty = 3)\n      abline(v = d$obs_stat, col = \"#e74c3c\", lwd = 2, lty = 2)\n    } else {\n      # Overlay: data distribution (wide) vs bootstrap distribution (narrow)\n      # Rescale data density to fit on same axes\n      dens_data &lt;- density(d$x)\n      dens_boot &lt;- density(d$boot_stats)\n\n      # Normalize both to peak at 1 for visual comparison\n      dens_data$y &lt;- dens_data$y / max(dens_data$y)\n      dens_boot$y &lt;- dens_boot$y / max(dens_boot$y)\n\n      xlim &lt;- range(c(dens_data$x, dens_boot$x))\n\n      plot(dens_data, col = \"#e74c3c\", lwd = 2.5, xlim = xlim,\n           ylim = c(0, 1.25), main = \"Data vs Sampling Distribution\",\n           xlab = \"Value\", ylab = \"Scaled density\")\n      polygon(dens_data$x, dens_data$y,\n              col = adjustcolor(\"#e74c3c\", 0.15), border = NA)\n\n      lines(dens_boot, col = \"#3498db\", lwd = 2.5)\n      polygon(dens_boot$x, dens_boot$y,\n              col = adjustcolor(\"#3498db\", 0.15), border = NA)\n\n      legend(\"topright\", bty = \"n\", cex = 0.8,\n             legend = c(\"Your data (wide, raw obs.)\",\n                        paste0(\"Sampling dist. of \", tolower(d$stat),\n                               \" (narrow)\")),\n             col = c(\"#e74c3c\", \"#3498db\"), lwd = 2.5)\n    }\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Statistic:&lt;/b&gt; \", d$stat, \"&lt;br&gt;\",\n        \"&lt;b&gt;Observed:&lt;/b&gt; \", round(d$obs_stat, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Bootstrap SE:&lt;/b&gt; \", round(d$boot_se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;95% CI:&lt;/b&gt; [\", round(d$boot_ci[1], 4),\n        \", \", round(d$boot_ci[2], 4), \"]&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;CI uses percentile method:&lt;br&gt;\",\n        \"2.5th and 97.5th percentiles of&lt;br&gt;\",\n        \"bootstrap distribution.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nMean with Normal population: the bootstrap distribution looks normal, similar to what CLT gives you analytically.\nMedian with skewed population: no easy formula for the SE of a median. But the bootstrap handles it effortlessly.\nSD with heavy-tailed data: the bootstrap distribution is skewed — the percentile CI is asymmetric, which is exactly right.\nRegression slope: bootstrap the slope to get a CI without assuming homoskedasticity.\nSmall n (10) vs large n (200): with small samples the bootstrap distribution is choppy (limited unique resamples). With more data it smooths out.\n\n\n\n\n\n\n\n\n\n\n\nUse bootstrap when…\nDon’t bother when…\n\n\n\n\nNo formula for the SE exists\nStandard formulas are available and valid\n\n\nThe statistic is complicated (ratios, quantiles)\nYou’re computing a simple mean\n\n\nYou suspect non-normality\nn is large and CLT applies\n\n\nYou want a quick, assumption-free CI\nYou need exact small-sample inference\n\n\n\nThe bootstrap is not magic — it can fail with very small samples or non-smooth statistics. But for most practical situations, it’s the easiest path to a valid confidence interval.\n\n\n\n\n\nBradley Efron invented the bootstrap in 1979 at Stanford. The name comes from the expression “pulling yourself up by your bootstraps” — attributed to the tall tales of Baron Munchausen, who claimed to have pulled himself out of a swamp by his own hair (later versions say bootstraps). The idea that a sample can estimate its own sampling distribution seemed equally absurd at first.\nWhen Efron first presented the bootstrap, many statisticians were skeptical. One famously asked: “You’re just sampling from your sample — how can that tell you anything new?” The answer: it tells you about the variability of your estimate, not about new data points.\nThe bootstrap is now one of the most cited statistical methods ever. Efron’s 1979 paper has over 20,000 citations. He received the International Prize in Statistics (the statistics equivalent of the Nobel) in 2019 for this work.\nBefore the bootstrap, getting standard errors for complex statistics (ratios, quantiles, eigenvalues) required either painful analytical derivations or the delta method — a Taylor expansion trick that often required heroic assumptions. The bootstrap made all of that unnecessary.",
    "crumbs": [
      "Inference",
      "The Bootstrap"
    ]
  },
  {
    "objectID": "experimental-design-ai.html",
    "href": "experimental-design-ai.html",
    "title": "Experimental Design for AI Systems",
    "section": "",
    "text": "Evaluating AI systems — which model is better, which prompt works, whether a feature improves user outcomes — is an empirical question. And the methods for answering empirical questions are the ones already in this course: randomized experiments, power analysis, and multiple testing corrections. The fact that the treatment is an algorithm rather than a drug or a policy does not change the statistical logic.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Experimental Design for AI Systems"
    ]
  },
  {
    "objectID": "experimental-design-ai.html#ab-testing-is-a-randomized-controlled-trial",
    "href": "experimental-design-ai.html#ab-testing-is-a-randomized-controlled-trial",
    "title": "Experimental Design for AI Systems",
    "section": "A/B testing is a randomized controlled trial",
    "text": "A/B testing is a randomized controlled trial\nWhen a technology company tests two versions of a product feature (version A vs version B), it is running a randomized controlled trial. Users are randomly assigned to conditions, outcomes are measured, and a treatment effect is estimated.\nThe statistical framework is identical to what’s covered in Power, Alpha, Beta & MDE:\n\nUnit of randomization: users, sessions, or requests\nTreatment: the new model, prompt, or feature\nOutcome: click-through rate, user satisfaction, task completion\nEstimand: the average treatment effect (ATE)\n\nThe estimator is a simple difference in means — or, equivalently, a regression of the outcome on the treatment indicator. This is OLS, which is MLE under normality, which is MoM with the moment condition \\(E[D_i(Y_i - \\alpha - \\beta D_i)] = 0\\).\n\n\n\n\n\n\nThe point. A/B tests in AI are not a new methodology. They are the same randomized experiments that have been used in medicine, economics, and social science for decades. The statistical principles — randomization for identification, power for design, multiple testing for honesty — carry over directly.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Experimental Design for AI Systems"
    ]
  },
  {
    "objectID": "experimental-design-ai.html#power-analysis-for-ai-experiments",
    "href": "experimental-design-ai.html#power-analysis-for-ai-experiments",
    "title": "Experimental Design for AI Systems",
    "section": "Power analysis for AI experiments",
    "text": "Power analysis for AI experiments\nAI experiments face specific power challenges:\nHigh-dimensional outcomes. A language model change might affect response quality, latency, safety, user engagement, and revenue simultaneously. Testing all of these increases the multiple testing burden (see Multiple Testing).\nSmall effect sizes. Mature AI systems are already highly optimized. The marginal improvement from a new model or prompt variant may be small — perhaps a 0.5% improvement in a key metric. Detecting small effects requires large samples and precise estimation.\nInterference between units. If users interact with each other (social networks, marketplaces), treating one user may affect the outcomes of untreated users. This violates SUTVA (the stable unit treatment value assumption) and biases the ATE estimate. Cluster randomization — randomizing at the group or region level — is the standard fix, with clustered standard errors for inference.\nNon-stationarity. User behavior changes over time (weekday vs weekend, novelty effects, seasonal trends). This makes pre-post comparisons unreliable and motivates concurrent randomization — running treatment and control simultaneously rather than sequentially.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Experimental Design for AI Systems"
    ]
  },
  {
    "objectID": "experimental-design-ai.html#multiple-testing-in-model-evaluation",
    "href": "experimental-design-ai.html#multiple-testing-in-model-evaluation",
    "title": "Experimental Design for AI Systems",
    "section": "Multiple testing in model evaluation",
    "text": "Multiple testing in model evaluation\nEvaluating an LLM typically involves many benchmarks: reasoning, coding, math, safety, factual accuracy, instruction following. Each benchmark produces a p-value (or a confidence interval for the performance difference). Testing across many benchmarks without correction inflates the false discovery rate — the fraction of “improvements” that are actually noise.\nThe tools from Multiple Testing apply directly:\n\nBonferroni correction: divide \\(\\alpha\\) by the number of benchmarks. Conservative but simple.\nBenjamini-Hochberg: controls the false discovery rate (FDR) rather than the family-wise error rate. Less conservative, more appropriate when you expect some true improvements.\nPre-registration: specify the primary benchmark before running the evaluation. Secondary benchmarks are exploratory and should be flagged as such.\n\n\n\n\n\n\n\nA common pattern in AI evaluation. A paper reports improvements on 12 out of 15 benchmarks. But if the improvements are small and no multiple testing correction is applied, several of those 12 may be false positives. The statistical bar for claiming improvement should scale with the number of comparisons — exactly the lesson from Multiple Testing.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Experimental Design for AI Systems"
    ]
  },
  {
    "objectID": "experimental-design-ai.html#observational-evaluation-and-its-limits",
    "href": "experimental-design-ai.html#observational-evaluation-and-its-limits",
    "title": "Experimental Design for AI Systems",
    "section": "Observational evaluation and its limits",
    "text": "Observational evaluation and its limits\nNot all evaluations can be randomized. Sometimes you want to know: “Did deploying this AI system improve outcomes?” without a controlled experiment. This is an observational causal inference problem, and the standard tools apply:\n\nDifference-in-differences: compare outcomes before and after deployment, relative to a control group that didn’t receive the system\nRegression discontinuity: if deployment was based on a threshold (e.g., rolled out to users above a certain engagement level), exploit the discontinuity\nSynthetic control: construct a counterfactual from similar units that weren’t treated\n\nEach of these requires identification assumptions — parallel trends, continuity, no anticipation — that must be argued for on substantive grounds. The methods are covered in the causal inference course.\nThe key point from Prediction vs Causation applies here: measuring the causal impact of an AI system requires an identification strategy, not just a before-after comparison. A correlation between AI adoption and improved outcomes may reflect selection (better organizations adopt AI first), not a causal effect.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Experimental Design for AI Systems"
    ]
  },
  {
    "objectID": "experimental-design-ai.html#connecting-to-the-course",
    "href": "experimental-design-ai.html#connecting-to-the-course",
    "title": "Experimental Design for AI Systems",
    "section": "Connecting to the course",
    "text": "Connecting to the course\nThis page applies tools from throughout the course to a specific domain:\n\nPower and p-values: the foundation of experimental design, whether the treatment is a drug or a prompt\nMultiple Testing: essential when evaluating across many benchmarks or metrics\nClustered SEs: necessary when randomization is at the cluster level or users are not independent\nHeteroskedasticity: treatment effects may vary across user segments, requiring robust standard errors\nPrediction vs Causation: the overarching distinction between what AI systems optimize and what causal evaluations require",
    "crumbs": [
      "Statistical Foundations of AI",
      "Experimental Design for AI Systems"
    ]
  },
  {
    "objectID": "multiple-testing.html",
    "href": "multiple-testing.html",
    "title": "Multiple Testing & the Replication Crisis",
    "section": "",
    "text": "If you test one null hypothesis at \\(\\alpha = 0.05\\), there’s a 5% chance of a false positive. But if you test 20 null hypotheses, the chance that at least one is falsely significant is:\n\\[P(\\text{at least one false positive}) = 1 - (1 - 0.05)^{20} \\approx 0.64\\]\nThat’s a 64% chance of finding something “significant” even when nothing is real. This is the multiple testing problem, and it’s at the heart of the replication crisis.\n\n\n\nA researcher tests 20 outcome variables and reports the one with p &lt; 0.05\nA genetics study tests 500,000 SNPs for association with a disease\nAn A/B test checks conversions, clicks, revenue, time on page, bounce rate…\nA psychology experiment tests the effect on 10 different mood scales\n\nIn each case, the nominal \\(\\alpha = 0.05\\) no longer controls the actual false positive rate. You need a correction.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we know which effects are real and which are null — so we can count exactly how many “significant” results are true discoveries vs false alarms. In practice, when you get a significant p-value, you never know which kind it is. That’s why corrections matter.",
    "crumbs": [
      "Inference",
      "Multiple Testing"
    ]
  },
  {
    "objectID": "multiple-testing.html#the-problem-test-enough-things-and-something-will-be-significant",
    "href": "multiple-testing.html#the-problem-test-enough-things-and-something-will-be-significant",
    "title": "Multiple Testing & the Replication Crisis",
    "section": "",
    "text": "If you test one null hypothesis at \\(\\alpha = 0.05\\), there’s a 5% chance of a false positive. But if you test 20 null hypotheses, the chance that at least one is falsely significant is:\n\\[P(\\text{at least one false positive}) = 1 - (1 - 0.05)^{20} \\approx 0.64\\]\nThat’s a 64% chance of finding something “significant” even when nothing is real. This is the multiple testing problem, and it’s at the heart of the replication crisis.\n\n\n\nA researcher tests 20 outcome variables and reports the one with p &lt; 0.05\nA genetics study tests 500,000 SNPs for association with a disease\nAn A/B test checks conversions, clicks, revenue, time on page, bounce rate…\nA psychology experiment tests the effect on 10 different mood scales\n\nIn each case, the nominal \\(\\alpha = 0.05\\) no longer controls the actual false positive rate. You need a correction.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we know which effects are real and which are null — so we can count exactly how many “significant” results are true discoveries vs false alarms. In practice, when you get a significant p-value, you never know which kind it is. That’s why corrections matter.",
    "crumbs": [
      "Inference",
      "Multiple Testing"
    ]
  },
  {
    "objectID": "multiple-testing.html#simulation-1-false-discoveries-from-multiple-tests",
    "href": "multiple-testing.html#simulation-1-false-discoveries-from-multiple-tests",
    "title": "Multiple Testing & the Replication Crisis",
    "section": "Simulation 1: False discoveries from multiple tests",
    "text": "Simulation 1: False discoveries from multiple tests\nTest 20 true null hypotheses at \\(\\alpha = 0.05\\). Watch how many come up “significant” purely by chance. Repeat to see the pattern.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n_tests\", \"Number of hypotheses tested:\",\n                  min = 5, max = 50, value = 20, step = 5),\n\n      sliderInput(\"n_obs\", \"Sample size per test:\",\n                  min = 20, max = 200, value = 50, step = 10),\n\n      sliderInput(\"alpha\", HTML(\"&alpha; (per-test):\"),\n                  min = 0.01, max = 0.10, value = 0.05, step = 0.01),\n\n      sliderInput(\"n_real\", \"Number with real effects:\",\n                  min = 0, max = 10, value = 0, step = 1),\n\n      actionButton(\"go\", \"Run tests\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"pval_plot\", height = \"520px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    m     &lt;- input$n_tests\n    n     &lt;- input$n_obs\n    alpha &lt;- input$alpha\n    n_real &lt;- min(input$n_real, m)\n\n    p_vals &lt;- numeric(m)\n    is_real &lt;- rep(FALSE, m)\n\n    for (i in seq_len(m)) {\n      if (i &lt;= n_real) {\n        # Real effect (d = 0.5)\n        x &lt;- rnorm(n, mean = 0.5, sd = 1)\n        is_real[i] &lt;- TRUE\n      } else {\n        # Null is true\n        x &lt;- rnorm(n, mean = 0, sd = 1)\n      }\n      p_vals[i] &lt;- t.test(x, mu = 0)$p.value\n    }\n\n    # Bonferroni correction\n    bonf_alpha &lt;- alpha / m\n    bonf_reject &lt;- p_vals &lt; bonf_alpha\n\n    # BH (FDR) correction\n    sorted_idx &lt;- order(p_vals)\n    sorted_p &lt;- p_vals[sorted_idx]\n    bh_threshold &lt;- (seq_len(m) / m) * alpha\n    bh_max &lt;- max(c(0, which(sorted_p &lt;= bh_threshold)))\n    bh_reject &lt;- rep(FALSE, m)\n    if (bh_max &gt; 0) bh_reject[sorted_idx[seq_len(bh_max)]] &lt;- TRUE\n\n    list(p_vals = p_vals, is_real = is_real, m = m, n = n,\n         alpha = alpha, n_real = n_real,\n         raw_reject = p_vals &lt; alpha,\n         bonf_reject = bonf_reject,\n         bh_reject = bh_reject,\n         bonf_alpha = bonf_alpha)\n  })\n\n  output$pval_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 6, 3, 1))\n\n    ord &lt;- order(d$p_vals)\n    p_sorted &lt;- d$p_vals[ord]\n    real_sorted &lt;- d$is_real[ord]\n\n    cols &lt;- ifelse(real_sorted, \"#3498db\", \"#95a5a6\")\n    pch_vals &lt;- ifelse(real_sorted, 17, 16)\n\n    plot(p_sorted, seq_len(d$m), pch = pch_vals, cex = 1.5,\n         col = cols, xlim = c(0, 1),\n         xlab = \"p-value\", ylab = \"Test (sorted by p-value)\",\n         main = paste0(d$m, \" hypothesis tests (\", d$n_real,\n                      \" real effects)\"),\n         yaxt = \"n\")\n    axis(2, at = seq_len(d$m), labels = seq_len(d$m),\n         las = 1, cex.axis = 0.7)\n\n    # Alpha threshold\n    abline(v = d$alpha, lty = 2, lwd = 2, col = \"#e74c3c\")\n    text(d$alpha + 0.02, d$m, paste0(\"\\u03b1 = \", d$alpha),\n         col = \"#e74c3c\", cex = 0.8, adj = 0)\n\n    # Bonferroni threshold\n    abline(v = d$bonf_alpha, lty = 2, lwd = 2, col = \"#e67e22\")\n    text(d$bonf_alpha + 0.02, d$m - 1,\n         paste0(\"Bonf = \", round(d$bonf_alpha, 4)),\n         col = \"#e67e22\", cex = 0.8, adj = 0)\n\n    # BH line\n    bh_line &lt;- (seq_len(d$m) / d$m) * d$alpha\n    lines(bh_line[ord], seq_len(d$m), lty = 3, lwd = 2, col = \"#27ae60\")\n\n    # Highlight rejections\n    raw_sig &lt;- which(p_sorted &lt; d$alpha)\n    if (length(raw_sig) &gt; 0) {\n      points(p_sorted[raw_sig], raw_sig, pch = 1, cex = 2.5,\n             col = \"#e74c3c\", lwd = 2)\n    }\n\n    legend(\"bottomright\", bty = \"n\", cex = 0.8,\n           legend = c(\n             ifelse(d$n_real &gt; 0, \"Real effect\", \"\"),\n             \"Null (no effect)\",\n             paste0(\"Uncorrected \\u03b1 = \", d$alpha),\n             paste0(\"Bonferroni \\u03b1 = \", round(d$bonf_alpha, 4)),\n             \"BH (FDR) threshold\"\n           )[c(d$n_real &gt; 0, TRUE, TRUE, TRUE, TRUE)],\n           col = c(\n             if (d$n_real &gt; 0) \"#3498db\",\n             \"#95a5a6\", \"#e74c3c\", \"#e67e22\", \"#27ae60\"\n           ),\n           pch = c(\n             if (d$n_real &gt; 0) 17,\n             16, NA, NA, NA\n           ),\n           lwd = c(\n             if (d$n_real &gt; 0) NA,\n             NA, 2, 2, 2\n           ),\n           lty = c(\n             if (d$n_real &gt; 0) NA,\n             NA, 2, 2, 3\n           ))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n\n    raw_fp &lt;- sum(d$raw_reject & !d$is_real)\n    bonf_fp &lt;- sum(d$bonf_reject & !d$is_real)\n    bh_fp &lt;- sum(d$bh_reject & !d$is_real)\n\n    raw_tp &lt;- sum(d$raw_reject & d$is_real)\n    bonf_tp &lt;- sum(d$bonf_reject & d$is_real)\n    bh_tp &lt;- sum(d$bh_reject & d$is_real)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Uncorrected:&lt;/b&gt;&lt;br&gt;\",\n        \"&nbsp; Reject: \", sum(d$raw_reject),\n        \" (FP: &lt;span class='bad'&gt;\", raw_fp, \"&lt;/span&gt;\",\n        if (d$n_real &gt; 0) paste0(\", TP: \", raw_tp), \")&lt;br&gt;\",\n        \"&lt;b&gt;Bonferroni:&lt;/b&gt;&lt;br&gt;\",\n        \"&nbsp; Reject: \", sum(d$bonf_reject),\n        \" (FP: \", bonf_fp,\n        if (d$n_real &gt; 0) paste0(\", TP: \", bonf_tp), \")&lt;br&gt;\",\n        \"&lt;b&gt;BH (FDR):&lt;/b&gt;&lt;br&gt;\",\n        \"&nbsp; Reject: \", sum(d$bh_reject),\n        \" (FP: \", bh_fp,\n        if (d$n_real &gt; 0) paste0(\", TP: \", bh_tp), \")&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;FP = false positive&lt;br&gt;\",\n        \"TP = true positive&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Inference",
      "Multiple Testing"
    ]
  },
  {
    "objectID": "multiple-testing.html#simulation-2-correction-methods-compared",
    "href": "multiple-testing.html#simulation-2-correction-methods-compared",
    "title": "Multiple Testing & the Replication Crisis",
    "section": "Simulation 2: Correction methods compared",
    "text": "Simulation 2: Correction methods compared\nRun the multiple testing experiment many times to see the long-run performance of each correction method. Track the family-wise error rate (FWER: any false positive?) and false discovery rate (FDR: what fraction of discoveries are false?).\n#| standalone: true\n#| viewerHeight: 520\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"m2\", \"Number of tests:\",\n                  min = 5, max = 50, value = 20, step = 5),\n\n      sliderInput(\"n_real2\", \"Tests with real effects:\",\n                  min = 0, max = 10, value = 3, step = 1),\n\n      sliderInput(\"sims2\", \"Simulations:\",\n                  min = 200, max = 1000, value = 500, step = 100),\n\n      actionButton(\"go2\", \"Run simulations\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"compare_plot\", height = \"420px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go2\n    m      &lt;- input$m2\n    n_real &lt;- min(input$n_real2, m)\n    sims   &lt;- input$sims2\n    n      &lt;- 50\n    alpha  &lt;- 0.05\n\n    fwer &lt;- c(raw = 0, bonf = 0, bh = 0)\n    power &lt;- c(raw = 0, bonf = 0, bh = 0)\n    fdr_sum &lt;- c(raw = 0, bonf = 0, bh = 0)\n    fdr_count &lt;- c(raw = 0, bonf = 0, bh = 0)\n\n    for (s in seq_len(sims)) {\n      p_vals &lt;- numeric(m)\n      is_real &lt;- rep(FALSE, m)\n\n      for (i in seq_len(m)) {\n        if (i &lt;= n_real) {\n          x &lt;- rnorm(n, mean = 0.5, sd = 1)\n          is_real[i] &lt;- TRUE\n        } else {\n          x &lt;- rnorm(n, mean = 0, sd = 1)\n        }\n        p_vals[i] &lt;- t.test(x, mu = 0)$p.value\n      }\n\n      # Raw\n      raw_rej &lt;- p_vals &lt; alpha\n\n      # Bonferroni\n      bonf_rej &lt;- p_vals &lt; (alpha / m)\n\n      # BH\n      sorted_idx &lt;- order(p_vals)\n      sorted_p &lt;- p_vals[sorted_idx]\n      bh_thresh &lt;- (seq_len(m) / m) * alpha\n      bh_max &lt;- max(c(0, which(sorted_p &lt;= bh_thresh)))\n      bh_rej &lt;- rep(FALSE, m)\n      if (bh_max &gt; 0) bh_rej[sorted_idx[seq_len(bh_max)]] &lt;- TRUE\n\n      for (method in list(list(\"raw\", raw_rej),\n                          list(\"bonf\", bonf_rej),\n                          list(\"bh\", bh_rej))) {\n        nm &lt;- method[[1]]\n        rej &lt;- method[[2]]\n\n        # FWER\n        if (any(rej & !is_real)) fwer[nm] &lt;- fwer[nm] + 1\n\n        # Power (if any real effects)\n        if (n_real &gt; 0) {\n          power[nm] &lt;- power[nm] + sum(rej & is_real)\n        }\n\n        # FDR\n        if (sum(rej) &gt; 0) {\n          fdr_sum[nm] &lt;- fdr_sum[nm] + sum(rej & !is_real) / sum(rej)\n          fdr_count[nm] &lt;- fdr_count[nm] + 1\n        }\n      }\n    }\n\n    fwer &lt;- fwer / sims\n    if (n_real &gt; 0) power &lt;- power / (sims * n_real)\n    fdr &lt;- ifelse(fdr_count &gt; 0, fdr_sum / fdr_count, 0)\n\n    list(fwer = fwer, power = power, fdr = fdr,\n         m = m, n_real = n_real, sims = sims)\n  })\n\n  output$compare_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 6), xpd = TRUE)\n\n    methods &lt;- c(\"Uncorrected\", \"Bonferroni\", \"BH (FDR)\")\n    cols &lt;- c(\"#e74c3c\", \"#e67e22\", \"#27ae60\")\n    x_pos &lt;- seq_along(methods)\n\n    if (d$n_real &gt; 0) {\n      # Show both FWER and power\n      mat &lt;- rbind(d$fwer * 100, d$power * 100)\n      bp &lt;- barplot(mat, beside = TRUE, names.arg = methods,\n                    col = c(\"#e74c3c80\", \"#3498db80\"),\n                    border = c(\"#e74c3c\", \"#3498db\"),\n                    ylim = c(0, 100),\n                    ylab = \"Rate (%)\",\n                    main = paste0(\"FWER & Power (\", d$sims,\n                                 \" simulations, \", d$n_real,\n                                 \" real effects)\"))\n\n      abline(h = 5, lty = 2, lwd = 2, col = \"#7f8c8d\")\n      text(max(bp) + 1, 7, \"\\u03b1 = 5%\", cex = 0.8, col = \"#7f8c8d\")\n\n      legend(\"topright\", inset = c(-0.18, 0), bty = \"n\", cex = 0.85,\n             legend = c(\"FWER\", \"Power\"),\n             fill = c(\"#e74c3c80\", \"#3498db80\"),\n             border = c(\"#e74c3c\", \"#3498db\"))\n    } else {\n      bp &lt;- barplot(d$fwer * 100, names.arg = methods,\n                    col = cols, border = NA,\n                    ylim = c(0, max(d$fwer * 100) * 1.3),\n                    ylab = \"Family-wise error rate (%)\",\n                    main = paste0(\"FWER under H\\u2080 (\",\n                                 d$sims, \" simulations)\"))\n\n      abline(h = 5, lty = 2, lwd = 2, col = \"#7f8c8d\")\n      text(max(bp) + 0.3, 7, \"Target: 5%\", cex = 0.8, col = \"#7f8c8d\")\n\n      text(bp, d$fwer * 100 + 2,\n           paste0(round(d$fwer * 100, 1), \"%\"), cex = 0.9)\n    }\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- dat()\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;FWER (any false positive):&lt;/b&gt;&lt;br&gt;\",\n        \"&nbsp; Uncorrected: &lt;span class='bad'&gt;\",\n        round(d$fwer[\"raw\"] * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&nbsp; Bonferroni: &lt;span class='good'&gt;\",\n        round(d$fwer[\"bonf\"] * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&nbsp; BH: \", round(d$fwer[\"bh\"] * 100, 1), \"%&lt;br&gt;\",\n        if (d$n_real &gt; 0) paste0(\n          \"&lt;hr style='margin:8px 0'&gt;\",\n          \"&lt;b&gt;Power (detect real effects):&lt;/b&gt;&lt;br&gt;\",\n          \"&nbsp; Uncorrected: \", round(d$power[\"raw\"] * 100, 1), \"%&lt;br&gt;\",\n          \"&nbsp; Bonferroni: \", round(d$power[\"bonf\"] * 100, 1), \"%&lt;br&gt;\",\n          \"&nbsp; BH: \", round(d$power[\"bh\"] * 100, 1), \"%\"\n        ) else \"\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\n20 tests, 0 real effects: uncorrected FWER is ~64%. Bonferroni brings it to ~5%. BH is somewhere in between.\n20 tests, 3 real effects: Bonferroni controls FWER tightly but has lower power. BH controls the false discovery rate (what fraction of your discoveries are wrong) while maintaining better power.\n50 tests, 0 real effects: uncorrected FWER is over 90%. The more you test, the worse it gets.\nSim 1 with 0 real effects: click “Run tests” repeatedly. Each time, a different set of null hypotheses appears “significant.” This is exactly how researchers accidentally find spurious results.\n\n\n\nWhen to use which correction\n\n\n\n\n\n\n\n\nMethod\nControls\nBest for\n\n\n\n\nNone\nPer-comparison \\(\\alpha\\)\nSingle pre-registered hypothesis\n\n\nBonferroni\nFWER (any false positive)\nSafety-critical decisions; few tests\n\n\nBH (Benjamini-Hochberg)\nFDR (false discovery proportion)\nExploratory analysis; many tests\n\n\nPre-registration\nEverything\nSpecify your hypothesis before seeing data\n\n\n\n\n\nThe bottom line\n\nIf you test enough things, something will be significant. The question is whether it’s real.\nBonferroni is conservative — it controls the probability of any false positive but sacrifices power.\nBH is more permissive — it controls the proportion of discoveries that are false, giving you better power.\nThe best correction is not testing everything. Pre-register your primary hypothesis and keep the number of tests small.\n\n\n\n\nDid you know?\n\nThe XKCD comic “Significant” perfectly illustrates the multiple testing problem: researchers test whether jelly beans cause acne, testing 20 different colors. Green jelly beans come up significant at p &lt; 0.05 — and the newspaper headline reads “Green Jelly Beans Linked to Acne!”\nJohn Ioannidis published “Why Most Published Research Findings Are False” in 2005. His argument: when studies are underpowered, when many hypotheses are tested, and when researchers have flexibility in their analysis, the majority of “significant” findings are likely false positives. The paper has been cited over 10,000 times and helped launch the replication crisis movement.\nBenjamini and Hochberg published their FDR procedure in 1995. It was initially met with skepticism — why would you allow some false discoveries? But in fields like genomics, where you test thousands of genes simultaneously, controlling the proportion of false discoveries turns out to be much more practical (and powerful) than trying to prevent any single false positive.",
    "crumbs": [
      "Inference",
      "Multiple Testing"
    ]
  },
  {
    "objectID": "bayesian-updating.html",
    "href": "bayesian-updating.html",
    "title": "Bayesian Updating: One Gentle Introduction",
    "section": "",
    "text": "Bayesian inference has exactly one formula:\n\\[\\text{Posterior} \\propto \\text{Prior} \\times \\text{Likelihood}\\]\nThat’s it. Everything else is details.\n\nPrior: what you believed before seeing data\nLikelihood: how probable the data is under each possible parameter value\nPosterior: your updated belief after seeing data\n\nThe key insight: your belief gets updated as data arrives. Start with a prior (maybe vague, maybe informed), observe data, and the posterior combines both. With enough data, the prior gets overwhelmed — the data speaks for itself.\n\n\n\n\n\n\n\n\n\n\n\nFrequentist\nBayesian\n\n\n\n\nParameters\nFixed but unknown\nRandom variables with distributions\n\n\nProbability\nLong-run frequency\nDegree of belief\n\n\nCan say\n“If H₀ is true, data this extreme has p = 0.03”\n“Given the data, P(parameter &gt; 0) = 0.97”\n\n\nCI / CrI\n“95% of intervals from this procedure contain \\(\\theta\\)”\n“There’s a 95% probability \\(\\theta\\) is in this interval”\n\n\n\nThe Bayesian interpretation is often what people think a confidence interval means — but it requires a prior.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we set the true coin bias — so we can watch the posterior converge to the right answer. In practice, you don’t know the true parameter. You choose a prior, observe data, and update. The posterior is your best belief — but you never get to check it against the truth.",
    "crumbs": [
      "Bayesian Thinking",
      "Bayesian Updating"
    ]
  },
  {
    "objectID": "bayesian-updating.html#the-bayesian-recipe",
    "href": "bayesian-updating.html#the-bayesian-recipe",
    "title": "Bayesian Updating: One Gentle Introduction",
    "section": "",
    "text": "Bayesian inference has exactly one formula:\n\\[\\text{Posterior} \\propto \\text{Prior} \\times \\text{Likelihood}\\]\nThat’s it. Everything else is details.\n\nPrior: what you believed before seeing data\nLikelihood: how probable the data is under each possible parameter value\nPosterior: your updated belief after seeing data\n\nThe key insight: your belief gets updated as data arrives. Start with a prior (maybe vague, maybe informed), observe data, and the posterior combines both. With enough data, the prior gets overwhelmed — the data speaks for itself.\n\n\n\n\n\n\n\n\n\n\n\nFrequentist\nBayesian\n\n\n\n\nParameters\nFixed but unknown\nRandom variables with distributions\n\n\nProbability\nLong-run frequency\nDegree of belief\n\n\nCan say\n“If H₀ is true, data this extreme has p = 0.03”\n“Given the data, P(parameter &gt; 0) = 0.97”\n\n\nCI / CrI\n“95% of intervals from this procedure contain \\(\\theta\\)”\n“There’s a 95% probability \\(\\theta\\) is in this interval”\n\n\n\nThe Bayesian interpretation is often what people think a confidence interval means — but it requires a prior.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we set the true coin bias — so we can watch the posterior converge to the right answer. In practice, you don’t know the true parameter. You choose a prior, observe data, and update. The posterior is your best belief — but you never get to check it against the truth.",
    "crumbs": [
      "Bayesian Thinking",
      "Bayesian Updating"
    ]
  },
  {
    "objectID": "bayesian-updating.html#simulation-1-coin-flipping-watch-the-posterior-update",
    "href": "bayesian-updating.html#simulation-1-coin-flipping-watch-the-posterior-update",
    "title": "Bayesian Updating: One Gentle Introduction",
    "section": "Simulation 1: Coin flipping — watch the posterior update",
    "text": "Simulation 1: Coin flipping — watch the posterior update\nStart with a prior belief about a coin’s bias. Flip the coin one at a time and watch the posterior distribution update in real time. With enough flips, the posterior concentrates around the true bias — regardless of the prior.\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_p\", \"True coin bias:\",\n                  min = 0.1, max = 0.9, value = 0.7, step = 0.05),\n\n      selectInput(\"prior\", \"Prior belief:\",\n                  choices = c(\"Uniform (a=1, b=1)\" = \"uniform\",\n                              \"Skeptical of bias (a=5, b=5)\" = \"skeptical\",\n                              \"Believe heads-biased (a=8, b=2)\" = \"heads\",\n                              \"Believe tails-biased (a=2, b=8)\" = \"tails\",\n                              \"Very strong fair (a=50, b=50)\" = \"strong_fair\")),\n\n      sliderInput(\"n_flips\", \"Number of flips:\",\n                  min = 1, max = 200, value = 10, step = 1),\n\n      actionButton(\"go\", \"Flip coins\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(12, plotOutput(\"posterior_plot\", height = \"400px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  get_prior &lt;- function(prior) {\n    switch(prior,\n      \"uniform\"      = c(a = 1, b = 1),\n      \"skeptical\"    = c(a = 5, b = 5),\n      \"heads\"        = c(a = 8, b = 2),\n      \"tails\"        = c(a = 2, b = 8),\n      \"strong_fair\"  = c(a = 50, b = 50)\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go\n    true_p  &lt;- input$true_p\n    prior   &lt;- input$prior\n    n_flips &lt;- input$n_flips\n\n    ab &lt;- get_prior(prior)\n    a0 &lt;- ab[\"a\"]; b0 &lt;- ab[\"b\"]\n\n    # Generate flips\n    flips &lt;- rbinom(n_flips, 1, true_p)\n    heads &lt;- sum(flips)\n    tails &lt;- n_flips - heads\n\n    # Posterior parameters\n    a_post &lt;- a0 + heads\n    b_post &lt;- b0 + tails\n\n    list(a0 = a0, b0 = b0, a_post = a_post, b_post = b_post,\n         true_p = true_p, n_flips = n_flips, heads = heads,\n         tails = tails, flips = flips)\n  })\n\n  output$posterior_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    x &lt;- seq(0, 1, length.out = 500)\n    y_prior &lt;- dbeta(x, d$a0, d$b0)\n    y_post  &lt;- dbeta(x, d$a_post, d$b_post)\n\n    ylim &lt;- c(0, max(c(y_prior, y_post)) * 1.1)\n\n    plot(x, y_post, type = \"l\", lwd = 3, col = \"#3498db\",\n         xlab = expression(\"Coin bias (\" * theta * \")\"),\n         ylab = \"Density\",\n         main = paste0(\"Bayesian updating after \", d$n_flips,\n                      \" flips (\", d$heads, \"H, \", d$tails, \"T)\"),\n         ylim = ylim)\n\n    # Prior\n    lines(x, y_prior, lwd = 2, col = \"#95a5a6\", lty = 2)\n\n    # Shade posterior\n    polygon(c(0, x, 1), c(0, y_post, 0),\n            col = adjustcolor(\"#3498db\", 0.2), border = NA)\n\n    # True value\n    abline(v = d$true_p, lwd = 2.5, col = \"#e74c3c\", lty = 2)\n\n    # Posterior mean\n    post_mean &lt;- d$a_post / (d$a_post + d$b_post)\n    abline(v = post_mean, lwd = 2, col = \"#27ae60\", lty = 3)\n\n    # 95% credible interval\n    ci_lo &lt;- qbeta(0.025, d$a_post, d$b_post)\n    ci_hi &lt;- qbeta(0.975, d$a_post, d$b_post)\n    x_ci &lt;- seq(ci_lo, ci_hi, length.out = 200)\n    y_ci &lt;- dbeta(x_ci, d$a_post, d$b_post)\n    polygon(c(ci_lo, x_ci, ci_hi), c(0, y_ci, 0),\n            col = adjustcolor(\"#3498db\", 0.3), border = NA)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Prior\",\n                      \"Posterior\",\n                      \"95% credible interval\",\n                      paste0(\"True \\u03b8 = \", d$true_p),\n                      paste0(\"Posterior mean = \", round(post_mean, 3))),\n           col = c(\"#95a5a6\", \"#3498db\",\n                   adjustcolor(\"#3498db\", 0.4),\n                   \"#e74c3c\", \"#27ae60\"),\n           lwd = c(2, 3, 8, 2.5, 2),\n           lty = c(2, 1, 1, 2, 3))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    post_mean &lt;- d$a_post / (d$a_post + d$b_post)\n    ci_lo &lt;- qbeta(0.025, d$a_post, d$b_post)\n    ci_hi &lt;- qbeta(0.975, d$a_post, d$b_post)\n\n    prior_mean &lt;- d$a0 / (d$a0 + d$b0)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Prior:&lt;/b&gt; Beta(\", d$a0, \", \", d$b0, \")&lt;br&gt;\",\n        \"Prior mean: \", round(prior_mean, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Data:&lt;/b&gt; \", d$heads, \"H / \", d$tails, \"T&lt;br&gt;\",\n        \"MLE: \", round(d$heads / d$n_flips, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Posterior:&lt;/b&gt; Beta(\", d$a_post, \", \", d$b_post, \")&lt;br&gt;\",\n        \"Post. mean: \", round(post_mean, 3), \"&lt;br&gt;\",\n        \"95% CrI: [\", round(ci_lo, 3), \", \", round(ci_hi, 3), \"]&lt;br&gt;\",\n        \"&lt;b&gt;True \\u03b8:&lt;/b&gt; \", d$true_p\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Bayesian Thinking",
      "Bayesian Updating"
    ]
  },
  {
    "objectID": "bayesian-updating.html#simulation-2-prior-sensitivity-different-priors-same-data",
    "href": "bayesian-updating.html#simulation-2-prior-sensitivity-different-priors-same-data",
    "title": "Bayesian Updating: One Gentle Introduction",
    "section": "Simulation 2: Prior sensitivity — different priors, same data",
    "text": "Simulation 2: Prior sensitivity — different priors, same data\nSame coin, same flips, but different starting beliefs. With little data, the prior matters a lot. With enough data, all priors converge to the same posterior. The data overwhelms the prior.\n#| standalone: true\n#| viewerHeight: 580\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"true_p2\", \"True coin bias:\",\n                  min = 0.1, max = 0.9, value = 0.7, step = 0.05),\n\n      sliderInput(\"n_flips2\", \"Number of flips:\",\n                  min = 1, max = 500, value = 10, step = 1),\n\n      actionButton(\"go2\", \"Flip coins\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"sensitivity_plot\", height = \"450px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go2\n    true_p  &lt;- input$true_p2\n    n_flips &lt;- input$n_flips2\n\n    # Generate one set of flips (shared across priors)\n    flips &lt;- rbinom(n_flips, 1, true_p)\n    heads &lt;- sum(flips)\n    tails &lt;- n_flips - heads\n\n    priors &lt;- list(\n      \"Uniform (1,1)\"        = c(1, 1),\n      \"Fair-biased (10,10)\"  = c(10, 10),\n      \"Heads-biased (8,2)\"   = c(8, 2),\n      \"Tails-biased (2,8)\"   = c(2, 8),\n      \"Strong fair (50,50)\"  = c(50, 50)\n    )\n\n    list(priors = priors, heads = heads, tails = tails,\n         true_p = true_p, n_flips = n_flips)\n  })\n\n  output$sensitivity_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    x &lt;- seq(0, 1, length.out = 500)\n    cols &lt;- c(\"#3498db\", \"#e74c3c\", \"#27ae60\", \"#9b59b6\", \"#e67e22\")\n\n    # Find y range\n    ymax &lt;- 0\n    for (i in seq_along(d$priors)) {\n      ab &lt;- d$priors[[i]]\n      y &lt;- dbeta(x, ab[1] + d$heads, ab[2] + d$tails)\n      ymax &lt;- max(ymax, max(y))\n    }\n\n    plot(NULL, xlim = c(0, 1), ylim = c(0, ymax * 1.1),\n         xlab = expression(\"Coin bias (\" * theta * \")\"),\n         ylab = \"Posterior density\",\n         main = paste0(\"5 priors, same data (\",\n                      d$heads, \"H / \", d$tails, \"T out of \",\n                      d$n_flips, \" flips)\"))\n\n    for (i in seq_along(d$priors)) {\n      ab &lt;- d$priors[[i]]\n      y &lt;- dbeta(x, ab[1] + d$heads, ab[2] + d$tails)\n      lines(x, y, lwd = 2.5, col = cols[i])\n    }\n\n    abline(v = d$true_p, lty = 2, lwd = 2.5, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(names(d$priors),\n                      paste0(\"True \\u03b8 = \", d$true_p)),\n           col = c(cols, \"#2c3e50\"),\n           lwd = c(rep(2.5, 5), 2.5),\n           lty = c(rep(1, 5), 2))\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- dat()\n\n    lines &lt;- sapply(seq_along(d$priors), function(i) {\n      ab &lt;- d$priors[[i]]\n      a_post &lt;- ab[1] + d$heads\n      b_post &lt;- ab[2] + d$tails\n      post_mean &lt;- a_post / (a_post + b_post)\n      paste0(names(d$priors)[i], \": \",\n             round(post_mean, 3))\n    })\n\n    mle &lt;- d$heads / d$n_flips\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Posterior means:&lt;/b&gt;&lt;br&gt;\",\n        paste(lines, collapse = \"&lt;br&gt;\"), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;MLE:&lt;/b&gt; \", round(mle, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;True \\u03b8:&lt;/b&gt; \", d$true_p, \"&lt;br&gt;\",\n        \"&lt;small&gt;With more flips, all posteriors&lt;br&gt;\",\n        \"converge to the MLE.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nSim 1, 5 flips: the posterior is wide and heavily influenced by the prior. The 95% credible interval is broad.\nSim 1, 100 flips: the posterior is sharp and centered near the true bias. The prior barely matters anymore.\nSim 1, strong fair prior (a=50, b=50) with true bias = 0.7: the prior pulls the posterior toward 0.5. You need many flips to overcome a strong prior.\nSim 2, 10 flips: the five posteriors are spread apart — the prior matters a lot.\nSim 2, 200 flips: all five posteriors are nearly identical and centered on the true value. The data overwhelms the prior.\nSim 2, slide from 1 to 500 flips: watch the posteriors converge in real time. This is the key Bayesian result — with enough data, the prior washes out.\n\n\n\nThe bottom line\n\nBayesian inference = prior × likelihood → posterior. That’s the entire framework.\nThe prior encodes what you know before seeing data. It can be informative (based on previous studies) or vague (letting the data speak).\nThe posterior is your updated belief. It’s a full distribution, not just a point estimate — you get a complete picture of uncertainty.\nWith enough data, the prior doesn’t matter. All reasonable priors converge to the same posterior. This is reassuring: Bayesian inference isn’t “subjective” in the long run.\nThe credible interval has the interpretation people want from a confidence interval: “There’s a 95% probability that \\(\\theta\\) is in this interval.” But this requires accepting the Bayesian framework (and a prior).",
    "crumbs": [
      "Bayesian Thinking",
      "Bayesian Updating"
    ]
  },
  {
    "objectID": "bayesian-updating.html#how-the-pieces-fit-together",
    "href": "bayesian-updating.html#how-the-pieces-fit-together",
    "title": "Bayesian Updating: One Gentle Introduction",
    "section": "How the pieces fit together",
    "text": "How the pieces fit together\nPeople sometimes use updating, estimation, and inference interchangeably. They’re related but distinct:\n\n\n\n\n\nflowchart TD\n    INF(\"&lt;b&gt;Bayesian Inference&lt;/b&gt;&lt;br/&gt;&lt;i&gt;The whole framework&lt;/i&gt;\")\n    UPD(\"&lt;b&gt;Updating&lt;/b&gt;&lt;br/&gt;Prior × Likelihood → Posterior&lt;br/&gt;&lt;i&gt;this page&lt;/i&gt;\")\n    EST(\"&lt;b&gt;Estimation&lt;/b&gt;&lt;br/&gt;Posterior → point estimates&lt;br/&gt;MAP · posterior mean\")\n    UNC(\"&lt;b&gt;Uncertainty&lt;/b&gt;&lt;br/&gt;Posterior → credible intervals\")\n    PRED(\"&lt;b&gt;Prediction&lt;/b&gt;&lt;br/&gt;Posterior → forecasts\")\n\n    INF --&gt; UPD\n    UPD --&gt; EST\n    UPD --&gt; UNC\n    UPD --&gt; PRED\n\n    style INF fill:#2c3e50,color:#fff,stroke:#2c3e50\n    style UPD fill:#3498db,color:#fff,stroke:#3498db\n    style EST fill:#27ae60,color:#fff,stroke:#27ae60\n    style UNC fill:#8e44ad,color:#fff,stroke:#8e44ad\n    style PRED fill:#e67e22,color:#fff,stroke:#e67e22\n\n\n\n\n\n\n\nInference is the umbrella — the entire Bayesian approach to learning from data.\nUpdating is the engine inside: take a prior, combine it with the likelihood, get a posterior. That’s what the simulations above demonstrate.\nEstimation takes the posterior and extracts a single “best guess” — the MAP (the peak) or the posterior mean (the center of mass). With a flat prior, MAP reduces to MLE. With a normal prior, MAP gives you ridge regression. This is covered in Bayesian Estimation.\nUncertainty and prediction also flow from the posterior — credible intervals, posterior predictive checks, and forecasting all start from the same updated distribution.\n\nThe key insight: updating comes first. You can’t estimate or quantify uncertainty until you have a posterior. Everything downstream depends on the prior-to-posterior step.\n\n\nDid you know?\n\nThomas Bayes was an English Presbyterian minister who never published his theorem. His friend Richard Price found the manuscript after Bayes’ death and published it in 1763 as “An Essay towards solving a Problem in the Doctrine of Chances.” Bayes’ original example was essentially the coin flipping problem in this simulation.\nBayesian methods were largely abandoned in the 20th century because they were computationally intractable — computing posteriors for realistic problems required integrals that couldn’t be solved analytically. The revival came with Markov Chain Monte Carlo (MCMC) algorithms in the 1990s, which made it possible to sample from posteriors instead of computing them exactly.\nPierre-Simon Laplace independently discovered Bayes’ theorem and used it extensively — including to estimate the mass of Saturn, the probability that the sun would rise tomorrow, and the bias of a coin. Laplace was arguably the first true Bayesian; Bayes himself only scratched the surface.\nThis page is a teaser. The full Bayesian course covers MCMC, hierarchical models, prior selection, and model comparison in depth.",
    "crumbs": [
      "Bayesian Thinking",
      "Bayesian Updating"
    ]
  },
  {
    "objectID": "heteroskedasticity.html",
    "href": "heteroskedasticity.html",
    "title": "Homoskedasticity & Heteroskedasticity",
    "section": "",
    "text": "Homoskedasticity (homo = same, skedasis = spread): the variance of the errors is constant across all values of \\(X\\).\nHeteroskedasticity (hetero = different): the variance changes with \\(X\\).\n\nThink of income vs. age. Young people’s incomes are clustered (entry-level jobs). But at age 50, some people earn $40k and others earn $500k. The spread of income increases with age — that’s heteroskedasticity.\n\n\nOLS is still unbiased under heteroskedasticity — the coefficient estimates are fine. But the standard errors are wrong. And wrong standard errors mean wrong confidence intervals, wrong t-statistics, wrong p-values. You might declare something significant when it isn’t (or miss something real).\nThe fix: use robust standard errors (HC, or “sandwich” standard errors) that don’t assume constant variance.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we control whether the errors are homoskedastic or heteroskedastic — we set the variance function. In practice, you don’t know the error structure. You test for heteroskedasticity and use robust SEs as insurance.\n\n\n\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"type\", \"Error structure:\",\n                  choices = c(\"Homoskedastic\",\n                              \"Fan-shaped (variance grows with X)\",\n                              \"U-shaped variance\",\n                              \"Group heteroskedasticity\")),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 50, max = 500, value = 200, step = 50),\n\n      sliderInput(\"b1\", HTML(\"True &beta;&lt;sub&gt;1&lt;/sub&gt;:\"),\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(4, plotOutput(\"scatter\", height = \"380px\")),\n        column(4, plotOutput(\"resid_plot\", height = \"380px\")),\n        column(4, plotOutput(\"ci_plot\", height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    b1   &lt;- input$b1\n    type &lt;- input$type\n\n    x &lt;- runif(n, 0.5, 10)\n\n    if (type == \"Homoskedastic\") {\n      eps &lt;- rnorm(n, sd = 2)\n    } else if (type == \"Fan-shaped (variance grows with X)\") {\n      eps &lt;- rnorm(n, sd = 0.3 * x)\n    } else if (type == \"U-shaped variance\") {\n      eps &lt;- rnorm(n, sd = 0.5 + 0.8 * abs(x - 5))\n    } else {\n      eps &lt;- rnorm(n, sd = ifelse(x &gt; 5, 4, 0.8))\n    }\n\n    y &lt;- 2 + b1 * x + eps\n    fit &lt;- lm(y ~ x)\n\n    # OLS SE\n    ols_se &lt;- summary(fit)$coefficients[2, 2]\n\n    # HC robust SE (manual HC1)\n    r &lt;- resid(fit)\n    X &lt;- cbind(1, x)\n    bread &lt;- solve(t(X) %*% X)\n    meat &lt;- t(X) %*% diag(r^2 * n / (n - 2)) %*% X\n    robust_vcov &lt;- bread %*% meat %*% bread\n    robust_se &lt;- sqrt(robust_vcov[2, 2])\n\n    # Simulation: repeat 500 times, see how often CI covers\n    covers_ols &lt;- 0\n    covers_robust &lt;- 0\n    reps &lt;- 500\n    for (i in seq_len(reps)) {\n      xx &lt;- runif(n, 0.5, 10)\n      if (type == \"Homoskedastic\") {\n        ee &lt;- rnorm(n, sd = 2)\n      } else if (type == \"Fan-shaped (variance grows with X)\") {\n        ee &lt;- rnorm(n, sd = 0.3 * xx)\n      } else if (type == \"U-shaped variance\") {\n        ee &lt;- rnorm(n, sd = 0.5 + 0.8 * abs(xx - 5))\n      } else {\n        ee &lt;- rnorm(n, sd = ifelse(xx &gt; 5, 4, 0.8))\n      }\n      yy &lt;- 2 + b1 * xx + ee\n      ff &lt;- lm(yy ~ xx)\n      b_hat &lt;- coef(ff)[2]\n      ols_s &lt;- summary(ff)$coefficients[2, 2]\n\n      rr &lt;- resid(ff)\n      XX &lt;- cbind(1, xx)\n      br &lt;- solve(t(XX) %*% XX)\n      mt &lt;- t(XX) %*% diag(rr^2 * n / (n - 2)) %*% XX\n      rob_s &lt;- sqrt((br %*% mt %*% br)[2, 2])\n\n      if (b_hat - 1.96 * ols_s &lt;= b1 && b1 &lt;= b_hat + 1.96 * ols_s)\n        covers_ols &lt;- covers_ols + 1\n      if (b_hat - 1.96 * rob_s &lt;= b1 && b1 &lt;= b_hat + 1.96 * rob_s)\n        covers_robust &lt;- covers_robust + 1\n    }\n\n    list(x = x, y = y, fit = fit, type = type, b1 = b1,\n         ols_se = ols_se, robust_se = robust_se,\n         cover_ols = covers_ols / reps,\n         cover_robust = covers_robust / reps)\n  })\n\n  output$scatter &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$x, d$y, pch = 16, col = \"#3498db60\", cex = 0.7,\n         xlab = \"X\", ylab = \"Y\", main = \"Data + OLS fit\")\n    abline(d$fit, col = \"#e74c3c\", lwd = 2.5)\n  })\n\n  output$resid_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    r  &lt;- resid(d$fit)\n    fv &lt;- fitted(d$fit)\n    plot(fv, r, pch = 16, col = \"#9b59b660\", cex = 0.7,\n         xlab = \"Fitted values\", ylab = \"Residuals\",\n         main = \"Residuals vs Fitted\")\n    abline(h = 0, lty = 2, col = \"gray40\", lwd = 1.5)\n\n    # Envelope\n    lo_abs &lt;- loess(abs(r) ~ fv)\n    ox &lt;- order(fv)\n    env &lt;- predict(lo_abs)[ox]\n    lines(fv[ox], env, col = \"#e67e22\", lwd = 2, lty = 1)\n    lines(fv[ox], -env, col = \"#e67e22\", lwd = 2, lty = 1)\n  })\n\n  output$ci_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 6, 3, 1))\n\n    b_hat &lt;- coef(d$fit)[2]\n\n    ci_ols &lt;- c(b_hat - 1.96 * d$ols_se, b_hat + 1.96 * d$ols_se)\n    ci_rob &lt;- c(b_hat - 1.96 * d$robust_se, b_hat + 1.96 * d$robust_se)\n\n    xlim &lt;- range(c(ci_ols, ci_rob, d$b1)) + c(-0.3, 0.3)\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, 2.5),\n         yaxt = \"n\", ylab = \"\", xlab = expression(beta[1]),\n         main = \"95% Confidence Intervals\")\n    axis(2, at = 1:2, labels = c(\"OLS SE\", \"Robust SE\"), las = 1, cex.axis = 0.85)\n\n    segments(ci_ols[1], 1, ci_ols[2], 1, lwd = 4, col = \"#e74c3c\")\n    points(b_hat, 1, pch = 19, cex = 1.5, col = \"#e74c3c\")\n\n    segments(ci_rob[1], 2, ci_rob[2], 2, lwd = 4, col = \"#27ae60\")\n    points(b_hat, 2, pch = 19, cex = 1.5, col = \"#27ae60\")\n\n    abline(v = d$b1, lty = 2, lwd = 2, col = \"#2c3e50\")\n    text(d$b1, 2.4, expression(\"True \" * beta[1]), cex = 0.9)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;OLS SE:&lt;/b&gt; \", round(d$ols_se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Robust SE:&lt;/b&gt; \", round(d$robust_se, 4), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;OLS CI coverage:&lt;/b&gt; &lt;span class='\",\n        ifelse(abs(d$cover_ols - 0.95) &gt; 0.03, \"bad\", \"good\"), \"'&gt;\",\n        round(d$cover_ols * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Robust CI coverage:&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$cover_robust * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;small&gt;Target: 95%. Over 500 simulations.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nHomoskedastic: both SEs are similar, both CIs have ~95% coverage. Constant variance = OLS SEs are fine.\nFan-shaped: the residual plot shows a clear funnel. OLS SEs are wrong — coverage drops below 95%. Robust SEs fix it.\nGroup heteroskedasticity: one group (X &gt; 5) is much noisier. Look at the residual plot — the right side has wider scatter. OLS pretends the variance is the same everywhere.\nCompare the right panel: under heteroskedasticity, the OLS CI (red) is often the wrong width. The robust CI (green) adjusts.\n\n\n\n\nIn applied work, always use robust standard errors (or clustered SEs). They are valid whether or not heteroskedasticity is present. If the errors happen to be homoskedastic, robust SEs give you the same answer anyway — there’s no downside.\n\n\n\nSoftware\nHow to get robust SEs\n\n\n\n\nR\nlmtest::coeftest(fit, vcov = sandwich::vcovHC)\n\n\nStata\nreg y x, robust\n\n\nPython\nsm.OLS(y, X).fit(cov_type='HC3')\n\n\n\n\n\n\n\n\nNobody can agree on how to spell it. “Heteroskedasticity” (with a k) is the original Greek-derived spelling (skedasis = scattering). “Heteroscedasticity” (with a c) is the Latinized version. Both are correct. Econometricians tend to use k; other fields use c. The important thing is that your standard errors are right, not your spelling.\nHalbert White published his famous robust standard error estimator (HC0) in 1980. It was so influential that “White standard errors” became the default in applied economics. The paper has over 30,000 citations.\nThe HC in HC0, HC1, HC2, HC3 stands for “heteroskedasticity-consistent.” HC3 is generally preferred for small samples because it gives each observation a slightly larger weight, correcting for leverage points.\nFun debugging tip: if your robust SEs are much larger than your OLS SEs, you likely have heteroskedasticity. If they’re much smaller, something is probably wrong with your data.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Heteroskedasticity"
    ]
  },
  {
    "objectID": "heteroskedasticity.html#what-do-these-words-mean",
    "href": "heteroskedasticity.html#what-do-these-words-mean",
    "title": "Homoskedasticity & Heteroskedasticity",
    "section": "",
    "text": "Homoskedasticity (homo = same, skedasis = spread): the variance of the errors is constant across all values of \\(X\\).\nHeteroskedasticity (hetero = different): the variance changes with \\(X\\).\n\nThink of income vs. age. Young people’s incomes are clustered (entry-level jobs). But at age 50, some people earn $40k and others earn $500k. The spread of income increases with age — that’s heteroskedasticity.\n\n\nOLS is still unbiased under heteroskedasticity — the coefficient estimates are fine. But the standard errors are wrong. And wrong standard errors mean wrong confidence intervals, wrong t-statistics, wrong p-values. You might declare something significant when it isn’t (or miss something real).\nThe fix: use robust standard errors (HC, or “sandwich” standard errors) that don’t assume constant variance.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we control whether the errors are homoskedastic or heteroskedastic — we set the variance function. In practice, you don’t know the error structure. You test for heteroskedasticity and use robust SEs as insurance.\n\n\n\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"type\", \"Error structure:\",\n                  choices = c(\"Homoskedastic\",\n                              \"Fan-shaped (variance grows with X)\",\n                              \"U-shaped variance\",\n                              \"Group heteroskedasticity\")),\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 50, max = 500, value = 200, step = 50),\n\n      sliderInput(\"b1\", HTML(\"True &beta;&lt;sub&gt;1&lt;/sub&gt;:\"),\n                  min = 0, max = 3, value = 1.5, step = 0.25),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(4, plotOutput(\"scatter\", height = \"380px\")),\n        column(4, plotOutput(\"resid_plot\", height = \"380px\")),\n        column(4, plotOutput(\"ci_plot\", height = \"380px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    b1   &lt;- input$b1\n    type &lt;- input$type\n\n    x &lt;- runif(n, 0.5, 10)\n\n    if (type == \"Homoskedastic\") {\n      eps &lt;- rnorm(n, sd = 2)\n    } else if (type == \"Fan-shaped (variance grows with X)\") {\n      eps &lt;- rnorm(n, sd = 0.3 * x)\n    } else if (type == \"U-shaped variance\") {\n      eps &lt;- rnorm(n, sd = 0.5 + 0.8 * abs(x - 5))\n    } else {\n      eps &lt;- rnorm(n, sd = ifelse(x &gt; 5, 4, 0.8))\n    }\n\n    y &lt;- 2 + b1 * x + eps\n    fit &lt;- lm(y ~ x)\n\n    # OLS SE\n    ols_se &lt;- summary(fit)$coefficients[2, 2]\n\n    # HC robust SE (manual HC1)\n    r &lt;- resid(fit)\n    X &lt;- cbind(1, x)\n    bread &lt;- solve(t(X) %*% X)\n    meat &lt;- t(X) %*% diag(r^2 * n / (n - 2)) %*% X\n    robust_vcov &lt;- bread %*% meat %*% bread\n    robust_se &lt;- sqrt(robust_vcov[2, 2])\n\n    # Simulation: repeat 500 times, see how often CI covers\n    covers_ols &lt;- 0\n    covers_robust &lt;- 0\n    reps &lt;- 500\n    for (i in seq_len(reps)) {\n      xx &lt;- runif(n, 0.5, 10)\n      if (type == \"Homoskedastic\") {\n        ee &lt;- rnorm(n, sd = 2)\n      } else if (type == \"Fan-shaped (variance grows with X)\") {\n        ee &lt;- rnorm(n, sd = 0.3 * xx)\n      } else if (type == \"U-shaped variance\") {\n        ee &lt;- rnorm(n, sd = 0.5 + 0.8 * abs(xx - 5))\n      } else {\n        ee &lt;- rnorm(n, sd = ifelse(xx &gt; 5, 4, 0.8))\n      }\n      yy &lt;- 2 + b1 * xx + ee\n      ff &lt;- lm(yy ~ xx)\n      b_hat &lt;- coef(ff)[2]\n      ols_s &lt;- summary(ff)$coefficients[2, 2]\n\n      rr &lt;- resid(ff)\n      XX &lt;- cbind(1, xx)\n      br &lt;- solve(t(XX) %*% XX)\n      mt &lt;- t(XX) %*% diag(rr^2 * n / (n - 2)) %*% XX\n      rob_s &lt;- sqrt((br %*% mt %*% br)[2, 2])\n\n      if (b_hat - 1.96 * ols_s &lt;= b1 && b1 &lt;= b_hat + 1.96 * ols_s)\n        covers_ols &lt;- covers_ols + 1\n      if (b_hat - 1.96 * rob_s &lt;= b1 && b1 &lt;= b_hat + 1.96 * rob_s)\n        covers_robust &lt;- covers_robust + 1\n    }\n\n    list(x = x, y = y, fit = fit, type = type, b1 = b1,\n         ols_se = ols_se, robust_se = robust_se,\n         cover_ols = covers_ols / reps,\n         cover_robust = covers_robust / reps)\n  })\n\n  output$scatter &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    plot(d$x, d$y, pch = 16, col = \"#3498db60\", cex = 0.7,\n         xlab = \"X\", ylab = \"Y\", main = \"Data + OLS fit\")\n    abline(d$fit, col = \"#e74c3c\", lwd = 2.5)\n  })\n\n  output$resid_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n    r  &lt;- resid(d$fit)\n    fv &lt;- fitted(d$fit)\n    plot(fv, r, pch = 16, col = \"#9b59b660\", cex = 0.7,\n         xlab = \"Fitted values\", ylab = \"Residuals\",\n         main = \"Residuals vs Fitted\")\n    abline(h = 0, lty = 2, col = \"gray40\", lwd = 1.5)\n\n    # Envelope\n    lo_abs &lt;- loess(abs(r) ~ fv)\n    ox &lt;- order(fv)\n    env &lt;- predict(lo_abs)[ox]\n    lines(fv[ox], env, col = \"#e67e22\", lwd = 2, lty = 1)\n    lines(fv[ox], -env, col = \"#e67e22\", lwd = 2, lty = 1)\n  })\n\n  output$ci_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 6, 3, 1))\n\n    b_hat &lt;- coef(d$fit)[2]\n\n    ci_ols &lt;- c(b_hat - 1.96 * d$ols_se, b_hat + 1.96 * d$ols_se)\n    ci_rob &lt;- c(b_hat - 1.96 * d$robust_se, b_hat + 1.96 * d$robust_se)\n\n    xlim &lt;- range(c(ci_ols, ci_rob, d$b1)) + c(-0.3, 0.3)\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, 2.5),\n         yaxt = \"n\", ylab = \"\", xlab = expression(beta[1]),\n         main = \"95% Confidence Intervals\")\n    axis(2, at = 1:2, labels = c(\"OLS SE\", \"Robust SE\"), las = 1, cex.axis = 0.85)\n\n    segments(ci_ols[1], 1, ci_ols[2], 1, lwd = 4, col = \"#e74c3c\")\n    points(b_hat, 1, pch = 19, cex = 1.5, col = \"#e74c3c\")\n\n    segments(ci_rob[1], 2, ci_rob[2], 2, lwd = 4, col = \"#27ae60\")\n    points(b_hat, 2, pch = 19, cex = 1.5, col = \"#27ae60\")\n\n    abline(v = d$b1, lty = 2, lwd = 2, col = \"#2c3e50\")\n    text(d$b1, 2.4, expression(\"True \" * beta[1]), cex = 0.9)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;OLS SE:&lt;/b&gt; \", round(d$ols_se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Robust SE:&lt;/b&gt; \", round(d$robust_se, 4), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;OLS CI coverage:&lt;/b&gt; &lt;span class='\",\n        ifelse(abs(d$cover_ols - 0.95) &gt; 0.03, \"bad\", \"good\"), \"'&gt;\",\n        round(d$cover_ols * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Robust CI coverage:&lt;/b&gt; &lt;span class='good'&gt;\",\n        round(d$cover_robust * 100, 1), \"%&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;small&gt;Target: 95%. Over 500 simulations.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\nHomoskedastic: both SEs are similar, both CIs have ~95% coverage. Constant variance = OLS SEs are fine.\nFan-shaped: the residual plot shows a clear funnel. OLS SEs are wrong — coverage drops below 95%. Robust SEs fix it.\nGroup heteroskedasticity: one group (X &gt; 5) is much noisier. Look at the residual plot — the right side has wider scatter. OLS pretends the variance is the same everywhere.\nCompare the right panel: under heteroskedasticity, the OLS CI (red) is often the wrong width. The robust CI (green) adjusts.\n\n\n\n\nIn applied work, always use robust standard errors (or clustered SEs). They are valid whether or not heteroskedasticity is present. If the errors happen to be homoskedastic, robust SEs give you the same answer anyway — there’s no downside.\n\n\n\nSoftware\nHow to get robust SEs\n\n\n\n\nR\nlmtest::coeftest(fit, vcov = sandwich::vcovHC)\n\n\nStata\nreg y x, robust\n\n\nPython\nsm.OLS(y, X).fit(cov_type='HC3')\n\n\n\n\n\n\n\n\nNobody can agree on how to spell it. “Heteroskedasticity” (with a k) is the original Greek-derived spelling (skedasis = scattering). “Heteroscedasticity” (with a c) is the Latinized version. Both are correct. Econometricians tend to use k; other fields use c. The important thing is that your standard errors are right, not your spelling.\nHalbert White published his famous robust standard error estimator (HC0) in 1980. It was so influential that “White standard errors” became the default in applied economics. The paper has over 30,000 citations.\nThe HC in HC0, HC1, HC2, HC3 stands for “heteroskedasticity-consistent.” HC3 is generally preferred for small samples because it gives each observation a slightly larger weight, correcting for leverage points.\nFun debugging tip: if your robust SEs are much larger than your OLS SEs, you likely have heteroskedasticity. If they’re much smaller, something is probably wrong with your data.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "Heteroskedasticity"
    ]
  },
  {
    "objectID": "correlation-causation.html",
    "href": "correlation-causation.html",
    "title": "From Correlation to Causation",
    "section": "",
    "text": "On the Regression & CEF page, you learned that OLS estimates the best linear approximation to \\(E[Y \\mid X]\\). The slope \\(\\hat{\\beta}\\) tells you: when X is one unit higher, Y is on average \\(\\hat{\\beta}\\) units higher. But “higher” is not “caused by.” The slope measures how X and Y move together — that’s correlation. In fact:\n\\[\\hat{\\beta} = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)} = r_{XY} \\cdot \\frac{SD(Y)}{SD(X)}\\]\nIt’s literally a rescaled correlation coefficient. A significant slope means X and Y are associated. It says nothing about why.",
    "crumbs": [
      "Causal Thinking",
      "From Correlation to Causation"
    ]
  },
  {
    "objectID": "correlation-causation.html#the-regression-slope-is-a-correlation",
    "href": "correlation-causation.html#the-regression-slope-is-a-correlation",
    "title": "From Correlation to Causation",
    "section": "",
    "text": "On the Regression & CEF page, you learned that OLS estimates the best linear approximation to \\(E[Y \\mid X]\\). The slope \\(\\hat{\\beta}\\) tells you: when X is one unit higher, Y is on average \\(\\hat{\\beta}\\) units higher. But “higher” is not “caused by.” The slope measures how X and Y move together — that’s correlation. In fact:\n\\[\\hat{\\beta} = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)} = r_{XY} \\cdot \\frac{SD(Y)}{SD(X)}\\]\nIt’s literally a rescaled correlation coefficient. A significant slope means X and Y are associated. It says nothing about why.",
    "crumbs": [
      "Causal Thinking",
      "From Correlation to Causation"
    ]
  },
  {
    "objectID": "correlation-causation.html#so-you-see-a-correlation.-now-what",
    "href": "correlation-causation.html#so-you-see-a-correlation.-now-what",
    "title": "From Correlation to Causation",
    "section": "So you see a correlation. Now what?",
    "text": "So you see a correlation. Now what?\nYour slope is positive, your p-value is small. But why are X and Y correlated?\nThere are exactly five possibilities:\n\n\n\nReason\nStructure\nExample\n\n\n\n\nX causes Y\n\\(X \\to Y\\)\nExercise → lower blood pressure\n\n\nY causes X\n\\(Y \\to X\\)\nHigher income → more education (you can afford grad school)\n\n\nSomething else causes both\n\\(Z \\to X\\), \\(Z \\to Y\\)\nFamily background → education AND income\n\n\nSpurious correlation\nNo real connection\nUS spending on science correlates with suicides by hanging\n\n\nYou conditioned on a common effect\nCollider bias\nAmong hospitalized patients, two unrelated diseases look negatively correlated\n\n\n\nHere’s the problem: all five produce the same scatter plot. A positive slope between X and Y looks identical whether X actually causes Y, or a confounder drives both, or it’s pure coincidence. The data can’t tell you which story is true — only theory, study design, and assumptions can.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we build the causal structure — we decide what causes what. We can compare the true causal effect with what a naive regression estimates. In practice, you never see the causal structure. You see the data and have to argue — using design, theory, and assumptions — that your estimate is causal.",
    "crumbs": [
      "Causal Thinking",
      "From Correlation to Causation"
    ]
  },
  {
    "objectID": "correlation-causation.html#simulation-1-same-scatter-plot-different-causal-stories",
    "href": "correlation-causation.html#simulation-1-same-scatter-plot-different-causal-stories",
    "title": "From Correlation to Causation",
    "section": "Simulation 1: Same scatter plot, different causal stories",
    "text": "Simulation 1: Same scatter plot, different causal stories\nThree different causal structures — all producing the same correlation between X and Y. Drag the sliders and watch: the scatter plots are indistinguishable, but the true causal effect of X on Y is completely different in each case.\n#| standalone: true\n#| viewerHeight: 520\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 8px; font-size: 13px; line-height: 1.7;\n    }\n    .stats-box b { color: #2c3e50; }\n    .causal  { color: #27ae60; font-weight: bold; }\n    .mislead { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 4,\n\n      sliderInput(\"n1\", \"Sample size:\",\n                  min = 100, max = 500, value = 300, step = 50),\n\n      sliderInput(\"true_effect\", \"True causal effect of X on Y:\",\n                  min = 0, max = 2, value = 0, step = 0.1),\n\n      sliderInput(\"confound\", \"Confounding strength (Z on both):\",\n                  min = 0, max = 3, value = 2, step = 0.25),\n\n      uiOutput(\"results1\")\n    ),\n\n    mainPanel(\n      width = 8,\n      fluidRow(\n        column(4, plotOutput(\"plot_causal\", height = \"260px\")),\n        column(4, plotOutput(\"plot_confound\", height = \"260px\")),\n        column(4, plotOutput(\"plot_reverse\", height = \"260px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  sim &lt;- reactive({\n    n  &lt;- input$n1\n    b  &lt;- input$true_effect\n    cf &lt;- input$confound\n\n    set.seed(42)\n    z &lt;- rnorm(n)\n    noise_x &lt;- rnorm(n)\n    noise_y &lt;- rnorm(n)\n\n    # Story 1: X causes Y (true effect = b, no confounding)\n    x1 &lt;- noise_x\n    y1 &lt;- b * x1 + noise_y\n\n    # Story 2: Confounding — Z causes both X and Y\n    x2 &lt;- cf * z + noise_x\n    y2 &lt;- b * x2 + cf * z + noise_y\n\n    # Story 3: Reverse causality — Y causes X\n    y3 &lt;- cf * z + noise_y\n    x3 &lt;- 1.5 * y3 + noise_x\n\n    fit1 &lt;- lm(y1 ~ x1)\n    fit2 &lt;- lm(y2 ~ x2)\n    fit3 &lt;- lm(y3 ~ x3)\n\n    list(x1 = x1, y1 = y1, x2 = x2, y2 = y2, x3 = x3, y3 = y3,\n         b1 = coef(fit1)[2], b2 = coef(fit2)[2], b3 = coef(fit3)[2],\n         true_b = b, confound = cf)\n  })\n\n  output$plot_causal &lt;- renderPlot({\n    d &lt;- sim()\n    par(mar = c(3, 3, 2, 0.5), cex.axis = 0.8, cex.lab = 0.9, cex.main = 0.85)\n    plot(d$x1, d$y1, pch = 16, cex = 0.5,\n         col = adjustcolor(\"#3498db\", 0.4),\n         xlab = \"X\", ylab = \"Y\",\n         main = expression(\"Story 1: X causes Y  (\" * X %-&gt;% Y * \")\"))\n    abline(lm(d$y1 ~ d$x1), col = \"#e74c3c\", lwd = 3)\n    legend(\"topleft\", bty = \"n\", cex = 0.8,\n           legend = paste0(\"OLS slope = \", round(d$b1, 2),\n                           \"  |  True effect = \", d$true_b),\n           text.col = if (abs(d$b1 - d$true_b) &lt; 0.15) \"#27ae60\" else \"#e74c3c\",\n           text.font = 2)\n  })\n\n  output$plot_confound &lt;- renderPlot({\n    d &lt;- sim()\n    par(mar = c(3, 3, 2, 0.5), cex.axis = 0.8, cex.lab = 0.9, cex.main = 0.85)\n    plot(d$x2, d$y2, pch = 16, cex = 0.5,\n         col = adjustcolor(\"#3498db\", 0.4),\n         xlab = \"X\", ylab = \"Y\",\n         main = expression(\"Story 2: Confounding  (\" * Z %-&gt;% X * \",\" ~~ Z %-&gt;% Y * \")\"))\n    abline(lm(d$y2 ~ d$x2), col = \"#e74c3c\", lwd = 3)\n    legend(\"topleft\", bty = \"n\", cex = 0.8,\n           legend = paste0(\"OLS slope = \", round(d$b2, 2),\n                           \"  |  True effect = \", d$true_b,\n                           \"  [BIASED]\"),\n           text.col = \"#e74c3c\", text.font = 2)\n  })\n\n  output$plot_reverse &lt;- renderPlot({\n    d &lt;- sim()\n    par(mar = c(3, 3, 2, 0.5), cex.axis = 0.8, cex.lab = 0.9, cex.main = 0.85)\n    plot(d$x3, d$y3, pch = 16, cex = 0.5,\n         col = adjustcolor(\"#3498db\", 0.4),\n         xlab = \"X\", ylab = \"Y\",\n         main = expression(\"Story 3: Reverse causality  (\" * Y %-&gt;% X * \")\"))\n    abline(lm(d$y3 ~ d$x3), col = \"#e74c3c\", lwd = 3)\n    legend(\"topleft\", bty = \"n\", cex = 0.8,\n           legend = paste0(\"OLS slope = \", round(d$b3, 2),\n                           \"  |  True effect = 0  [MISLEADING]\"),\n           text.col = \"#e74c3c\", text.font = 2)\n  })\n\n  output$results1 &lt;- renderUI({\n    d &lt;- sim()\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;X \\u2192 Y (no confounding):&lt;/b&gt;&lt;br&gt;\",\n        \"OLS = \", round(d$b1, 3),\n        \" | True = \", d$true_b,\n        if (abs(d$b1 - d$true_b) &lt; 0.15)\n          \" &lt;span class='causal'&gt;\\u2713&lt;/span&gt;\" else \"\", \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Z \\u2192 X, Z \\u2192 Y (confounding):&lt;/b&gt;&lt;br&gt;\",\n        \"OLS = \", round(d$b2, 3),\n        \" | True = \", d$true_b,\n        \" &lt;span class='mislead'&gt;biased&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Y \\u2192 X (reverse causality):&lt;/b&gt;&lt;br&gt;\",\n        \"OLS = \", round(d$b3, 3),\n        \" | True = 0\",\n        \" &lt;span class='mislead'&gt;misleading&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nTrue effect = 0, confounding = 2: all three panels show a positive correlation. But only in panel 1 is the causal effect actually zero and the OLS slope correct. In panels 2 and 3, the correlation is entirely spurious — driven by confounding or reverse causality.\nTrue effect = 1, confounding = 2: panel 1 recovers the true effect. Panel 2 overestimates it (confounding adds to the slope). Panel 3 shows something completely unrelated.\nIncrease sample size: all three slopes get more precise — but the biased ones stay biased. More data doesn’t fix confounding. It just gives you a more precise wrong answer.",
    "crumbs": [
      "Causal Thinking",
      "From Correlation to Causation"
    ]
  },
  {
    "objectID": "correlation-causation.html#simulation-2-randomization-breaks-confounding",
    "href": "correlation-causation.html#simulation-2-randomization-breaks-confounding",
    "title": "From Correlation to Causation",
    "section": "Simulation 2: Randomization breaks confounding",
    "text": "Simulation 2: Randomization breaks confounding\nThis is the fundamental insight of causal inference: randomization makes X independent of everything else, so the only reason Y changes with X is the causal effect.\nCompare an observational study (where confounding biases the estimate) with a randomized experiment (where the bias disappears).\n#| standalone: true\n#| viewerHeight: 700\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 4,\n\n      sliderInput(\"n2\", \"Sample size:\",\n                  min = 100, max = 500, value = 200, step = 50),\n\n      sliderInput(\"true_b\", \"True causal effect of treatment:\",\n                  min = 0, max = 5, value = 2, step = 0.25),\n\n      sliderInput(\"confound2\", \"Confounding strength:\",\n                  min = 0, max = 5, value = 3, step = 0.25),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 8,\n      plotOutput(\"rct_plot\", height = \"550px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  sim &lt;- reactive({\n    n  &lt;- input$n2\n    b  &lt;- input$true_b\n    cf &lt;- input$confound2\n\n    # Confounder: ability, motivation, etc.\n    z &lt;- rnorm(n)\n\n    # Observational: treatment correlated with confounder\n    # (motivated people self-select into treatment)\n    prob_treat &lt;- plogis(cf * z)\n    treat_obs  &lt;- rbinom(n, 1, prob_treat)\n    y_obs      &lt;- b * treat_obs + 2 * z + rnorm(n)\n    fit_obs    &lt;- lm(y_obs ~ treat_obs)\n\n    # RCT: treatment is random (coin flip)\n    treat_rct &lt;- rbinom(n, 1, 0.5)\n    y_rct     &lt;- b * treat_rct + 2 * z + rnorm(n)\n    fit_rct   &lt;- lm(y_rct ~ treat_rct)\n\n    list(treat_obs = treat_obs, y_obs = y_obs,\n         treat_rct = treat_rct, y_rct = y_rct,\n         z = z, b_obs = coef(fit_obs)[2], b_rct = coef(fit_rct)[2],\n         true_b = b, confound = cf)\n  })\n\n  output$rct_plot &lt;- renderPlot({\n    d &lt;- sim()\n    par(mfrow = c(1, 2), mar = c(5, 5, 4, 2))\n\n    # Left: Observational\n    stripchart(d$y_obs ~ d$treat_obs,\n               method = \"jitter\", jitter = 0.15,\n               vertical = TRUE, pch = 16, cex = 0.6,\n               col = adjustcolor(c(\"#3498db\", \"#e74c3c\"), 0.4),\n               group.names = c(\"Control\", \"Treated\"),\n               xlab = \"\", ylab = \"Outcome (Y)\",\n               main = \"Observational study\")\n\n    means_obs &lt;- tapply(d$y_obs, d$treat_obs, mean)\n    segments(0.8, means_obs[1], 1.2, means_obs[1],\n             col = \"#3498db\", lwd = 3)\n    segments(1.8, means_obs[2], 2.2, means_obs[2],\n             col = \"#e74c3c\", lwd = 3)\n\n    mtext(paste0(\"Difference = \", round(d$b_obs, 2),\n                 \"  (true = \", d$true_b, \")\"),\n          side = 3, line = 0, cex = 1, font = 2, col = \"#e74c3c\")\n\n    # Right: RCT\n    stripchart(d$y_rct ~ d$treat_rct,\n               method = \"jitter\", jitter = 0.15,\n               vertical = TRUE, pch = 16, cex = 0.6,\n               col = adjustcolor(c(\"#3498db\", \"#e74c3c\"), 0.4),\n               group.names = c(\"Control\", \"Treated\"),\n               xlab = \"\", ylab = \"Outcome (Y)\",\n               main = \"Randomized experiment\")\n\n    means_rct &lt;- tapply(d$y_rct, d$treat_rct, mean)\n    segments(0.8, means_rct[1], 1.2, means_rct[1],\n             col = \"#3498db\", lwd = 3)\n    segments(1.8, means_rct[2], 2.2, means_rct[2],\n             col = \"#e74c3c\", lwd = 3)\n\n    mtext(paste0(\"Difference = \", round(d$b_rct, 2),\n                 \"  (true = \", d$true_b, \")\"),\n          side = 3, line = 0, cex = 1, font = 2, col = \"#27ae60\")\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- sim()\n\n    bias_obs &lt;- d$b_obs - d$true_b\n    bias_rct &lt;- d$b_rct - d$true_b\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True causal effect:&lt;/b&gt; \", d$true_b, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Observational:&lt;/b&gt; \", round(d$b_obs, 3),\n        \"&lt;br&gt;Bias: &lt;span class='bad'&gt;\",\n        round(bias_obs, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;small&gt;Treated group has higher ability\",\n        \" (selection bias)&lt;/small&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Randomized:&lt;/b&gt; \", round(d$b_rct, 3),\n        \"&lt;br&gt;Bias: &lt;span class='good'&gt;\",\n        round(bias_rct, 3), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;small&gt;Treatment is independent of\",\n        \" everything \\u2192 unbiased&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nTrue effect = 2, confounding = 3: the observational study overestimates the effect (maybe by 50–100%) because motivated people self-select into treatment. The RCT nails it.\nTrue effect = 0, confounding = 3: the treatment does nothing, but the observational study finds a large “effect” — entirely driven by selection bias. The RCT correctly shows ~0.\nConfounding = 0: both studies agree. When there’s no confounding, observational data works fine. The problem is you never know if confounding is zero in practice.\nIncrease n: both estimates get more precise, but the observational one stays biased. Sample size doesn’t fix confounding.",
    "crumbs": [
      "Causal Thinking",
      "From Correlation to Causation"
    ]
  },
  {
    "objectID": "correlation-causation.html#digging-deeper-why-correlations-mislead",
    "href": "correlation-causation.html#digging-deeper-why-correlations-mislead",
    "title": "From Correlation to Causation",
    "section": "Digging deeper: why correlations mislead",
    "text": "Digging deeper: why correlations mislead\n\nSpurious correlation: why unrelated things look related\nA spurious correlation is a correlation between two variables that have no causal connection whatsoever. It’s not confounding (no hidden Z), not reverse causality — it’s pure noise that looks like signal.\nWhy does it happen? If \\(X\\) and \\(Y\\) are truly independent, then \\(\\text{Cor}(X, Y) = 0\\) in the population. But in a finite sample, the sample correlation \\(r_{XY}\\) is almost never exactly zero. It has sampling variability:\n\\[\\text{Var}(r_{XY}) \\approx \\frac{1}{n}\\]\nWith small \\(n\\), the sample correlation can be large purely by chance. With \\(n = 10\\), a correlation of \\(r = 0.5\\) between two completely independent variables isn’t unusual at all.\nMultiple testing makes it worse. If you compute correlations between \\(k\\) pairs of unrelated variables and test each at \\(\\alpha = 0.05\\), the probability that at least one comes up “significant” is:\n\\[P(\\text{at least one false positive}) = 1 - (1 - 0.05)^k\\]\nWith \\(k = 20\\) pairs, that’s a 64% chance. With \\(k = 100\\), it’s 99.4%. Test enough things and you will find “significant” correlations between variables that have nothing to do with each other. (This ties directly to the Multiple Testing page.)\nTrending variables are the worst offenders. If two time series both trend upward over time — say, US GDP and the number of cat videos on YouTube — they’ll be highly correlated even though neither causes the other. Mathematically, if \\(X_t = a + bt + \\varepsilon_t\\) and \\(Y_t = c + dt + \\eta_t\\) where \\(\\varepsilon\\) and \\(\\eta\\) are completely independent, the sample correlation between \\(X\\) and \\(Y\\) will be close to 1 because both share the time trend \\(t\\). The correlation is real in the data but meaningless — it’s driven by the common trend, not by any relationship between \\(X\\) and \\(Y\\).\nThe fix: large samples (reduces \\(\\text{Var}(r)\\)), pre-registration (decide what to test before looking at data), multiple testing corrections (Bonferroni, BH), and skepticism about correlations found by searching through many variables.\n\n\nCollider bias: the trickiest one\nThe first four reasons are intuitive. The fifth — collider bias — is subtle and catches even experienced researchers.\nStart with the name. In causal diagrams, arrows show what causes what. A collider is a variable where two arrows point into it:\n\\[X \\to C \\leftarrow Y\\]\nThe variable \\(C\\) “collides” — it’s caused by both \\(X\\) and \\(Y\\). On its own, that’s fine. \\(X\\) and \\(Y\\) can be completely independent. The problem starts when you condition on the collider — by restricting your sample to specific values of \\(C\\), or by controlling for \\(C\\) in a regression. That act of conditioning opens a fake pathway between \\(X\\) and \\(Y\\) that doesn’t exist in the real world.\nWhy does conditioning on a collider create a fake association? Think of it as an information leak. If you know someone is in a specific group (defined by \\(C\\)), then learning about \\(X\\) tells you something about \\(Y\\) — not because they’re related, but because both contributed to getting into that group. Knowing one cause of \\(C\\) changes what you infer about the other cause.\nThe hospital example, step by step.\n\nDisease A and disease B are completely unrelated in the general population. Knowing someone has A tells you nothing about whether they have B.\nBoth diseases can put you in the hospital. “Hospitalization” is a collider: A → Hospital ← B.\nNow you study only hospitalized patients. You’ve conditioned on the collider.\nPick a hospitalized patient. You learn they don’t have disease A. But they’re in the hospital — so something put them there. That something is more likely to be disease B.\nResult: among hospitalized patients, A and B look negatively correlated. Not because one prevents the other, but because you restricted your sample to people who have at least one of them.\n\nIn the general population: no correlation. In the hospital: negative correlation. The correlation is entirely an artifact of how you selected your sample.\nThe talent-attractiveness tradeoff.\n\nIn the general population, acting talent and physical attractiveness are unrelated. Plenty of beautiful untalented people, plenty of talented plain-looking people.\nHollywood selects actors who are talented OR attractive OR both. “Getting cast” is a collider: Talent → Cast ← Attractiveness.\nAmong successful actors (conditioned on being cast), you notice: the beautiful ones seem less talented, and the talented ones seem less attractive.\nWhy? If an actor isn’t particularly talented, they probably got cast because they’re attractive (and vice versa). Knowing one cause tells you about the other — but only within the selected group.\nYou conclude “there’s a tradeoff between talent and looks.” There isn’t. You’re seeing collider bias.\n\nHow it shows up in research. Collider bias often sneaks in through:\n\nSample selection: studying only college graduates, only employed people, only published studies, only survivors\nControlling for post-treatment variables: if you run a regression of X on Y “controlling for” a variable that X and Y both cause, you’ve conditioned on a collider\nAttrition: if people drop out of your study for reasons related to both treatment and outcome, the remaining sample is conditioned on a collider\n\nThe fix: think carefully about the causal structure before you run your regression. Draw the diagram. If a variable is caused by your treatment and your outcome, don’t control for it and don’t select on it. Condition only on variables that are causes (confounders), never on variables that are consequences (colliders).",
    "crumbs": [
      "Causal Thinking",
      "From Correlation to Causation"
    ]
  },
  {
    "objectID": "correlation-causation.html#when-does-correlation-become-causation",
    "href": "correlation-causation.html#when-does-correlation-become-causation",
    "title": "From Correlation to Causation",
    "section": "When does correlation become causation?",
    "text": "When does correlation become causation?\nCorrelation becomes causal when you can rule out reasons 2–5. That’s what the entire field of causal inference is about.\n\n\n\n\n\n\n\n\nThreat\nWhat it means\nHow to address it\n\n\n\n\nConfounding\nA third variable drives both X and Y\nRandomize, or control for all confounders (hard)\n\n\nReverse causality\nY causes X, not the other way around\nRandomize, use timing/lags, or find an instrument\n\n\nSelection / collider bias\nYour sample is selected in a way that creates a spurious relationship\nThink carefully about your sample; don’t condition on post-treatment variables\n\n\nCoincidence\nSmall samples, multiple testing\nLarge samples, pre-registration, corrections\n\n\n\nRandomization solves all of them at once — that’s why RCTs are the gold standard. But randomization isn’t always possible (you can’t randomly assign smoking, education, or recessions). That’s where the causal inference toolkit comes in:\n\nInstrumental variables (IV): find something that affects X but has no other path to Y\nDifference-in-differences (DiD): compare before/after changes between treated and untreated groups\nRegression discontinuity (RDD): exploit a cutoff that creates near-random assignment\nMatching / propensity scores: create comparable treated and control groups from observational data\n\nEach method makes different assumptions to rule out the threats above. None is universally valid — the right method depends on your setting.\nThese methods are covered in depth in the Causal Inference course.",
    "crumbs": [
      "Causal Thinking",
      "From Correlation to Causation"
    ]
  },
  {
    "objectID": "correlation-causation.html#simpsons-paradox",
    "href": "correlation-causation.html#simpsons-paradox",
    "title": "From Correlation to Causation",
    "section": "Simpson’s Paradox",
    "text": "Simpson’s Paradox\nA striking example of confounding: a trend that appears in aggregate data reverses when you look within subgroups.\n\nThe UC Berkeley admissions case (1973)\nBerkeley’s graduate admissions data showed an apparent bias against women: overall, 44% of men were admitted vs 35% of women. Lawsuit-worthy? Not so fast.\nWhen researchers looked within departments, women were admitted at equal or higher rates than men in most departments. The aggregate gap existed because women applied disproportionately to more competitive departments (like English) while men applied to less competitive ones (like Engineering). Department was a confounder: it drove both the gender composition of applicants and the admission rate.\n\n\n\n\n\n\n\n\n\nLevel\nMen admitted\nWomen admitted\nLooks like…\n\n\n\n\nAggregate\n44%\n35%\nBias against women\n\n\nWithin departments\nSimilar or lower\nSimilar or higher\nNo bias (or slight bias for women)\n\n\n\nThis is Simpson’s paradox: the aggregate trend is the opposite of the within-group trend. It’s not a paradox in logic — it’s a consequence of confounding. The aggregate numbers mix together departments with very different admission rates and very different gender ratios.\n\n\nThe causal lesson\nSimpson’s paradox is resolved by asking a causal question: what should we condition on?\n\nIf department choice is a pre-treatment variable (a confounder), you should condition on it — and the “bias” disappears.\nIf department choice is a consequence of discrimination (e.g., women are steered away from certain fields), conditioning on it would hide the discrimination.\n\nThe data alone can’t tell you which is correct. You need a causal model — a story about what causes what. This is the same lesson as the rest of this page: correlation (the aggregate gap) doesn’t imply causation (discrimination), and the right adjustment depends on the causal structure.\n#| standalone: true\n#| viewerHeight: 580\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 8px; font-size: 13px; line-height: 1.7;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 4,\n\n      sliderInput(\"n_simpson\", \"Applicants per group-dept cell:\",\n                  min = 50, max = 500, value = 200, step = 50),\n\n      sliderInput(\"n_groups\", \"Number of departments:\",\n                  min = 2, max = 6, value = 3, step = 1),\n\n      sliderInput(\"confound_strength\", \"Confounding strength:\",\n                  min = 0, max = 0.4, value = 0.25, step = 0.05),\n\n      actionButton(\"go_simp\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_simpson\")\n    ),\n\n    mainPanel(\n      width = 8,\n      fluidRow(\n        column(6, plotOutput(\"simpson_agg\", height = \"350px\")),\n        column(6, plotOutput(\"simpson_dept\", height = \"350px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  sim &lt;- reactive({\n    input$go_simp\n    n_cell &lt;- input$n_simpson\n    K &lt;- input$n_groups\n    cf &lt;- input$confound_strength\n\n    # Department base admission rates (decreasing)\n    base_rates &lt;- seq(0.7, 0.2, length.out = K)\n\n    # Group A (e.g., men) applies more to easy departments\n    # Group B (e.g., women) applies more to hard departments\n    weight_a &lt;- seq(1 + cf * K, 1 - cf * K, length.out = K)\n    weight_a &lt;- pmax(weight_a, 0.1)\n    weight_a &lt;- weight_a / sum(weight_a)\n\n    weight_b &lt;- seq(1 - cf * K, 1 + cf * K, length.out = K)\n    weight_b &lt;- pmax(weight_b, 0.1)\n    weight_b &lt;- weight_b / sum(weight_b)\n\n    n_a &lt;- round(weight_a * n_cell * K)\n    n_b &lt;- round(weight_b * n_cell * K)\n\n    # Generate admits with same or slightly higher rate for group B within each dept\n    dept_results &lt;- data.frame(\n      dept = integer(), group = character(),\n      n = integer(), admitted = integer(),\n      stringsAsFactors = FALSE\n    )\n\n    for (k in 1:K) {\n      rate_a &lt;- base_rates[k]\n      rate_b &lt;- base_rates[k] + 0.02  # Slight advantage for group B within dept\n\n      adm_a &lt;- rbinom(1, n_a[k], rate_a)\n      adm_b &lt;- rbinom(1, n_b[k], rate_b)\n\n      dept_results &lt;- rbind(dept_results,\n        data.frame(dept = k, group = \"A\", n = n_a[k], admitted = adm_a),\n        data.frame(dept = k, group = \"B\", n = n_b[k], admitted = adm_b))\n    }\n\n    dept_results$rate &lt;- dept_results$admitted / dept_results$n\n\n    # Aggregate rates\n    agg_a &lt;- sum(dept_results$admitted[dept_results$group == \"A\"]) /\n             sum(dept_results$n[dept_results$group == \"A\"])\n    agg_b &lt;- sum(dept_results$admitted[dept_results$group == \"B\"]) /\n             sum(dept_results$n[dept_results$group == \"B\"])\n\n    list(dept_results = dept_results, agg_a = agg_a, agg_b = agg_b,\n         K = K, base_rates = base_rates,\n         n_a = n_a, n_b = n_b)\n  })\n\n  output$simpson_agg &lt;- renderPlot({\n    d &lt;- sim()\n    par(mar = c(4, 4.5, 3, 1))\n\n    vals &lt;- c(d$agg_a, d$agg_b)\n    cols &lt;- c(\"#3498db\", \"#e74c3c\")\n    bp &lt;- barplot(vals * 100, col = cols, border = NA,\n                  names.arg = c(\"Group A\", \"Group B\"),\n                  main = \"Aggregate Admission Rate\",\n                  ylab = \"Admission rate (%)\",\n                  ylim = c(0, max(vals * 100) * 1.3))\n    text(bp, vals * 100 + 2, paste0(round(vals * 100, 1), \"%\"),\n         cex = 1, font = 2)\n\n    if (d$agg_a &gt; d$agg_b) {\n      mtext(\"Group A admitted at higher rate\", side = 1, line = 2.5,\n            col = \"#e74c3c\", font = 2, cex = 0.9)\n    } else {\n      mtext(\"Group B admitted at higher rate\", side = 1, line = 2.5,\n            col = \"#27ae60\", font = 2, cex = 0.9)\n    }\n  })\n\n  output$simpson_dept &lt;- renderPlot({\n    d &lt;- sim()\n    dr &lt;- d$dept_results\n    K &lt;- d$K\n    par(mar = c(4, 4.5, 3, 1))\n\n    at_a &lt;- (1:K) * 3 - 1.5\n    at_b &lt;- (1:K) * 3 - 0.5\n\n    rates_a &lt;- dr$rate[dr$group == \"A\"]\n    rates_b &lt;- dr$rate[dr$group == \"B\"]\n\n    ylim &lt;- c(0, max(c(rates_a, rates_b) * 100) * 1.25)\n\n    plot(NULL, xlim = c(0, K * 3 + 0.5), ylim = ylim,\n         xaxt = \"n\", xlab = \"\", ylab = \"Admission rate (%)\",\n         main = \"Within-Department Rates\")\n\n    rect(at_a - 0.4, 0, at_a + 0.4, rates_a * 100,\n         col = \"#3498db\", border = NA)\n    rect(at_b - 0.4, 0, at_b + 0.4, rates_b * 100,\n         col = \"#e74c3c\", border = NA)\n\n    text(at_a, rates_a * 100 + 2, paste0(round(rates_a * 100, 0), \"%\"),\n         cex = 0.7, col = \"#3498db\", font = 2)\n    text(at_b, rates_b * 100 + 2, paste0(round(rates_b * 100, 0), \"%\"),\n         cex = 0.7, col = \"#e74c3c\", font = 2)\n\n    axis(1, at = (1:K) * 3 - 1, labels = paste(\"Dept\", 1:K), cex.axis = 0.85)\n\n    # Show sample sizes\n    n_a &lt;- dr$n[dr$group == \"A\"]\n    n_b &lt;- dr$n[dr$group == \"B\"]\n    mtext(paste0(\"n: \", paste(n_a, collapse = \"/\")), side = 1,\n          line = 2.3, cex = 0.7, col = \"#3498db\", adj = 0)\n    mtext(paste0(\"n: \", paste(n_b, collapse = \"/\")), side = 1,\n          line = 3, cex = 0.7, col = \"#e74c3c\", adj = 0)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Group A\", \"Group B\"),\n           fill = c(\"#3498db\", \"#e74c3c\"), border = NA)\n\n    b_wins &lt;- sum(rates_b &gt;= rates_a)\n    mtext(paste0(\"Group B higher in \", b_wins, \"/\", K, \" departments\"),\n          side = 1, line = 2.5, col = \"#27ae60\", font = 2, cex = 0.85,\n          adj = 1)\n  })\n\n  output$results_simpson &lt;- renderUI({\n    d &lt;- sim()\n    dr &lt;- d$dept_results\n    rates_a &lt;- dr$rate[dr$group == \"A\"]\n    rates_b &lt;- dr$rate[dr$group == \"B\"]\n    b_higher &lt;- sum(rates_b &gt;= rates_a)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Aggregate:&lt;/b&gt;&lt;br&gt;\",\n        \"Group A: \", round(d$agg_a * 100, 1), \"%&lt;br&gt;\",\n        \"Group B: \", round(d$agg_b * 100, 1), \"%&lt;br&gt;\",\n        if (d$agg_a &gt; d$agg_b)\n          \"&lt;span class='bad'&gt;A looks favored overall&lt;/span&gt;\"\n        else\n          \"&lt;span class='good'&gt;No aggregate reversal&lt;/span&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Within departments:&lt;/b&gt;&lt;br&gt;\",\n        \"B higher in &lt;span class='good'&gt;\", b_higher, \"/\", d$K,\n        \"&lt;/span&gt; depts&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;small&gt;The reversal happens because Group B applies more to \",\n        \"harder departments. Department is a confounder.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nConfounding strength = 0.25: Group A has a higher aggregate rate, but Group B has a higher (or equal) rate in most individual departments. That’s the paradox.\nConfounding strength = 0: both groups apply to departments at the same rates. The paradox disappears — aggregate and within-department trends agree.\nIncrease confounding: the aggregate gap widens even though within-department rates stay the same. More confounding = more misleading aggregate data.\n2 departments: the paradox is easy to see. With 6 departments, it’s harder to spot without the visualization.",
    "crumbs": [
      "Causal Thinking",
      "From Correlation to Causation"
    ]
  },
  {
    "objectID": "correlation-causation.html#did-you-know",
    "href": "correlation-causation.html#did-you-know",
    "title": "From Correlation to Causation",
    "section": "Did you know?",
    "text": "Did you know?\n\nThe phrase “correlation does not imply causation” is one of the most cited principles in statistics, yet researchers violate it constantly. A 2019 study found that over 30% of observational studies in top medical journals used causal language (like “X reduces Y”) without any causal identification strategy.\nRonald Fisher, the father of modern statistics, spent decades arguing that the correlation between smoking and lung cancer was not causal — it could be confounding by a genetic factor that causes both the desire to smoke and susceptibility to cancer. He was wrong, but his argument was statistically valid. It took decades of converging evidence from multiple study designs to establish causality.\nThe 2021 Nobel Prize in Economics went to David Card, Joshua Angrist, and Guido Imbens for developing methods to establish causal relationships from observational data — essentially for building the toolkit described above. Their key insight: you don’t need a perfect experiment; you need a credible natural experiment where something approximates random assignment.",
    "crumbs": [
      "Causal Thinking",
      "From Correlation to Causation"
    ]
  },
  {
    "objectID": "cef.html",
    "href": "cef.html",
    "title": "Regression & the CEF",
    "section": "",
    "text": "What is the CEF?\nThe conditional expectation function (CEF) answers a simple question: “What do you expect Y to be, given that you know X?”\nSay you’re looking at income (\\(Y\\)) vs. years of education (\\(X\\)). The CEF answers: among everyone with exactly 12 years of education, what’s their average income? What about 16 years? 20 years? If you plot those averages, you get a curve — that’s the CEF. It could be a straight line, or it could bend, flatten out, jump — whatever the data actually does.\n\n\nHow does regression fit in?\nOLS draws a straight line through that. Two cases:\n\nCEF is already a straight line — OLS gets it exactly right. Each extra unit of \\(X\\) adds the same bump to expected \\(Y\\). The line is the CEF.\nCEF is curved — maybe the first few years of education matter a lot, but returns flatten after a PhD. OLS can’t bend, so it draws the best straight line it can through that curve. It’s a useful summary, but it misses the shape.\n\nRegression doesn’t assume the world is linear. It gives you the best linear approximation to whatever the true relationship is. The simulator below lets you see exactly where that approximation works and where it breaks down.\nIn the plots: the red dots show the conditional mean of \\(Y\\) in each bin of \\(X\\) (the empirical CEF), the green curve is the true CEF, and the blue line is OLS. Switch DGPs to see when they agree and when they diverge.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we set the true CEF — the green curve. We know exactly how \\(E[Y \\mid X]\\) behaves, so we can see where OLS gets it right and where it misses. In practice, you never see the true CEF. You only see the data and the regression line, and you hope they’re close.\n\n\n\n\n\nReading the residual plot\nThe right panel shows residuals vs. fitted values with a LOESS smoother (red curve). LOESS fits a tiny weighted regression at each point using only nearby observations, producing a flexible curve that follows local patterns. If OLS is correctly specified, the LOESS line should be flat at zero. If it curves, that’s visual evidence of nonlinearity that OLS is missing.\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\ndgp_choices &lt;- c(\n  \"Linear\",\n  \"Quadratic\",\n  \"Log (diminishing returns)\",\n  \"Step function\",\n  \"Sine wave\"\n)\n\n# True CEF for each DGP\ncef_fun &lt;- function(x, dgp) {\n  switch(dgp,\n    \"Linear\"                   = 2 + 1.5 * x,\n    \"Quadratic\"                = 1 + 0.8 * x - 0.15 * x^2,\n    \"Log (diminishing returns)\" = 3 * log(x + 1),\n    \"Step function\"            = ifelse(x &lt; 0, -1, 2),\n    \"Sine wave\"                = 2 * sin(x),\n    1.5 * x\n  )\n}\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .info-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.8;\n    }\n    .info-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"dgp\", \"True DGP:\",\n                  choices = dgp_choices),\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 50, max = 1000, value = 300, step = 50),\n\n      sliderInput(\"sigma\", HTML(\"Error SD (&sigma;):\"),\n                  min = 0.5, max = 4, value = 1.5, step = 0.5),\n\n      sliderInput(\"nbins\", \"Bins for CEF:\",\n                  min = 5, max = 30, value = 15, step = 1),\n\n      actionButton(\"redraw\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"info_box\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"scatter_plot\", height = \"420px\")),\n        column(6, plotOutput(\"resid_plot\",   height = \"420px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$redraw\n    n     &lt;- input$n\n    sigma &lt;- input$sigma\n    dgp   &lt;- input$dgp\n\n    # X range depends on DGP\n    if (dgp == \"Log (diminishing returns)\") {\n      x &lt;- runif(n, 0, 6)\n    } else if (dgp == \"Step function\") {\n      x &lt;- runif(n, -3, 3)\n    } else if (dgp == \"Sine wave\") {\n      x &lt;- runif(n, -pi, 2 * pi)\n    } else {\n      x &lt;- runif(n, -3, 5)\n    }\n\n    mu &lt;- cef_fun(x, dgp)\n    y  &lt;- mu + rnorm(n, sd = sigma)\n\n    ols &lt;- lm(y ~ x)\n\n    # Bin X and compute conditional means\n    nbins &lt;- input$nbins\n    breaks &lt;- seq(min(x), max(x), length.out = nbins + 1)\n    bin    &lt;- cut(x, breaks, include.lowest = TRUE)\n    bin_mid &lt;- tapply(x, bin, mean)\n    bin_cef &lt;- tapply(y, bin, mean)\n    bin_n   &lt;- tapply(y, bin, length)\n\n    list(x = x, y = y, mu = mu, ols = ols, dgp = dgp,\n         bin_mid = bin_mid, bin_cef = bin_cef, bin_n = bin_n)\n  })\n\n  # --- Main scatter plot ---\n  output$scatter_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(d$x, d$y, pch = 16, col = \"#bdc3c780\", cex = 0.6,\n         xlab = \"X\", ylab = \"Y\",\n         main = paste(\"DGP:\", d$dgp))\n\n    # True CEF curve\n    xo &lt;- sort(d$x)\n    lines(xo, cef_fun(xo, d$dgp), col = \"#2ecc71\", lwd = 2.5)\n\n    # OLS line\n    abline(d$ols, col = \"#3498db\", lwd = 2.5)\n\n    # Binned conditional means (empirical CEF)\n    keep &lt;- !is.na(d$bin_cef) & !is.na(d$bin_mid)\n    points(d$bin_mid[keep], d$bin_cef[keep],\n           pch = 19, col = \"#e74c3c\", cex = 1.6)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(expression(\"True \" * E * \"[Y|X]\"),\n                      \"OLS regression\",\n                      expression(\"Binned \" * bar(Y) * \" (empirical CEF)\")),\n           col = c(\"#2ecc71\", \"#3498db\", \"#e74c3c\"),\n           lwd = c(2.5, 2.5, NA),\n           pch = c(NA, NA, 19),\n           pt.cex = c(NA, NA, 1.4))\n  })\n\n  # --- Residual plot ---\n  output$resid_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    r &lt;- resid(d$ols)\n    fv &lt;- fitted(d$ols)\n\n    plot(fv, r, pch = 16, col = \"#9b59b680\", cex = 0.6,\n         xlab = \"Fitted values\", ylab = \"Residuals\",\n         main = \"Residuals vs Fitted\")\n    abline(h = 0, lty = 2, col = \"gray40\", lwd = 1.5)\n\n    # Loess smoother to reveal nonlinearity\n    lo &lt;- loess(r ~ fv)\n    ox &lt;- order(fv)\n    lines(fv[ox], predict(lo)[ox], col = \"#e74c3c\", lwd = 2)\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Loess smoother\", \"Zero line\"),\n           col = c(\"#e74c3c\", \"gray40\"),\n           lwd = c(2, 1.5),\n           lty = c(1, 2))\n  })\n\n  # --- Info box ---\n  output$info_box &lt;- renderUI({\n    d &lt;- dat()\n    b0 &lt;- round(coef(d$ols)[1], 3)\n    b1 &lt;- round(coef(d$ols)[2], 3)\n    r2 &lt;- round(summary(d$ols)$r.squared, 3)\n\n    linear &lt;- d$dgp == \"Linear\"\n\n    tags$div(class = \"info-box\",\n      HTML(paste0(\n        \"&lt;b&gt;OLS:&lt;/b&gt; Y = \", b0, \" + \", b1, \"X&lt;br&gt;\",\n        \"&lt;b&gt;R&sup2;:&lt;/b&gt; \", r2, \"&lt;br&gt;&lt;br&gt;\",\n        if (linear) {\n          \"&lt;span style='color:#27ae60;'&gt;&lt;b&gt;&#10003; CEF is linear &mdash; OLS recovers it exactly.&lt;/b&gt;&lt;/span&gt;\"\n        } else {\n          \"&lt;span style='color:#e67e22;'&gt;&lt;b&gt;CEF is nonlinear &mdash; OLS is the best linear approximation.&lt;/b&gt;&lt;br&gt;Check the residual plot for the pattern.&lt;/span&gt;\"\n        }\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nDid you know?\n\nThe word “regression” comes from Francis Galton’s 1886 study of heights. He noticed that tall parents tended to have children who were tall — but not as tall. Short parents had children who were short — but not as short. Children’s heights “regressed toward the mean.” The statistical technique kept the name, even though modern regression has nothing to do with reverting to averages.\nGalton was also Charles Darwin’s half-cousin. He applied statistical thinking to heredity, fingerprints, and even the optimal way to brew tea.\nThe conditional expectation function is sometimes called the “regression function” — which makes sense once you know Galton’s story. OLS literally estimates the function that tells you the expected value of \\(Y\\) given \\(X\\).",
    "crumbs": [
      "Regression",
      "Regression & the CEF"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Interactive simulations for statistics, econometrics, and causal inference. The idea is to see how things like sampling distributions, p-values, and OLS actually work, not just read about them.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#start-here",
    "href": "index.html#start-here",
    "title": "Statistical Inference",
    "section": "Start Here",
    "text": "Start Here\n\nProbability & Uncertainty\n\nFoundations — Distributions, Sampling & Confidence Intervals\nVariance, SD & Standard Error — Population vs Sample, SD vs SE & Why n − 1\nThe Sampling Distribution — Data vs Sampling Distribution & When to Assume Normality\n\n\n\nThe Central Limit Theorem\n\nCLT Simulator — Interactive CLT Simulator\nLLN vs CLT — Why Averages Stabilize Before They Become Normal\n\n\n\nInference\n\np-values & Confidence Intervals — What They Actually Mean\nTest Statistics — Z, t, F, Chi-Squared, Wald, LR & Score\nThe Bootstrap — Resampling-Based Inference\nPower, Alpha, Beta & MDE — Hypothesis Testing & Experiment Design\nMonte Carlo Experiments — How We Understand Estimators\nMultiple Testing — False Discoveries & the Replication Crisis\n\n\n\nRegression\n\nRegression & the CEF — OLS and the Conditional Expectation Function\nResiduals & Controls — Diagnostics, Partialling Out & Omitted Variable Bias\nFrisch-Waugh-Lovell — Partialling Out & OVB\nOmitted Variable Bias — The OVB formula, sign-of-bias table & when controls eliminate confounding\n\n\n\nAlgebra of Regression\n\nThe Algebra Behind OLS — Matrix notation, the VCV matrix, and where standard errors come from\n\n\n\nEstimation\n\nMethod of Moments — Match sample moments to population moments\nMaximum Likelihood — Find the parameters that make the data most probable\nLimited Dependent Variables — Logit, probit, marginal effects & the Tobit model\nGeneralized Method of Moments — When you have more moment conditions than parameters\nBayesian Estimation — MAP, posterior mean, and the link to regularization\n\n\n\nStandard Errors & Diagnostics\n\nHeteroskedasticity — Constant vs Non-Constant Variance & Robust SEs\nClustered SEs — When Observations Aren’t Independent\nThe Delta Method — Standard errors for nonlinear transformations of estimates\nMeasurement Error — Why Noisy Regressors Bias You Toward Zero\nBias-Variance Tradeoff — Underfitting, Overfitting & MSE Decomposition\nModel Selection — AIC, BIC, cross-validation & choosing the right complexity\nWhen Inference Breaks Down — Why Variation Is the Fuel of Inference\n\n\n\nCausal Thinking\n\nFrom Correlation to Causation — Why Correlation Isn’t Enough & When It Becomes Causal\n\n\n\nBayesian Thinking\n\nBayesian Updating — One Gentle Introduction\n\n\n\nStatistical Foundations of AI\n\nTraining as Maximum Likelihood — Cross-entropy, SGD, and what training optimizes\nRegularization as Bayesian Inference — Weight decay, dropout, and the RLHF penalty as priors\nPrediction vs Causation in Foundation Models — \\(P(Y \\mid X)\\) vs \\(P(Y \\mid do(X))\\) and identification vs training\nExperimental Design for AI Systems — A/B testing, power, and multiple testing for model evaluation\nCalibration & Uncertainty Quantification — When to trust model confidence",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "delta-method.html",
    "href": "delta-method.html",
    "title": "The Delta Method",
    "section": "",
    "text": "You have an estimator \\(\\hat{\\theta}\\) with known (or estimated) variance, and you want the variance of some nonlinear transformation \\(g(\\hat{\\theta})\\). Examples:\n\nYou estimated \\(\\hat{\\beta}\\) from a logistic regression, but you want the SE of the odds ratio \\(e^{\\hat{\\beta}}\\).\nYou have two coefficients \\(\\hat{\\beta}_1, \\hat{\\beta}_2\\) and want the SE of their ratio \\(\\hat{\\beta}_1 / \\hat{\\beta}_2\\).\nYou want marginal effects from a probit model — nonlinear functions of \\(\\hat{\\beta}\\).",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "The Delta Method"
    ]
  },
  {
    "objectID": "delta-method.html#the-problem",
    "href": "delta-method.html#the-problem",
    "title": "The Delta Method",
    "section": "",
    "text": "You have an estimator \\(\\hat{\\theta}\\) with known (or estimated) variance, and you want the variance of some nonlinear transformation \\(g(\\hat{\\theta})\\). Examples:\n\nYou estimated \\(\\hat{\\beta}\\) from a logistic regression, but you want the SE of the odds ratio \\(e^{\\hat{\\beta}}\\).\nYou have two coefficients \\(\\hat{\\beta}_1, \\hat{\\beta}_2\\) and want the SE of their ratio \\(\\hat{\\beta}_1 / \\hat{\\beta}_2\\).\nYou want marginal effects from a probit model — nonlinear functions of \\(\\hat{\\beta}\\).",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "The Delta Method"
    ]
  },
  {
    "objectID": "delta-method.html#the-formula",
    "href": "delta-method.html#the-formula",
    "title": "The Delta Method",
    "section": "The formula",
    "text": "The formula\nIf \\(\\hat{\\theta} \\xrightarrow{d} N(\\theta, \\sigma^2/n)\\), then a first-order Taylor expansion gives:\n\\[g(\\hat{\\theta}) \\approx g(\\theta) + g'(\\theta)(\\hat{\\theta} - \\theta)\\]\nso:\n\\[\\text{Var}(g(\\hat{\\theta})) \\approx [g'(\\theta)]^2 \\cdot \\text{Var}(\\hat{\\theta})\\]\nMultivariate version: if \\(\\hat{\\boldsymbol{\\theta}}\\) is a vector with covariance matrix \\(\\Sigma\\), then:\n\\[\\text{Var}(g(\\hat{\\boldsymbol{\\theta}})) \\approx \\nabla g(\\boldsymbol{\\theta})' \\, \\Sigma \\, \\nabla g(\\boldsymbol{\\theta})\\]\nwhere \\(\\nabla g\\) is the gradient of \\(g\\) evaluated at \\(\\hat{\\boldsymbol{\\theta}}\\).",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "The Delta Method"
    ]
  },
  {
    "objectID": "delta-method.html#worked-examples",
    "href": "delta-method.html#worked-examples",
    "title": "The Delta Method",
    "section": "Worked examples",
    "text": "Worked examples\n\nExample 1: Odds ratio \\(e^{\\hat{\\beta}}\\)\n\\(g(\\beta) = e^\\beta\\), so \\(g'(\\beta) = e^\\beta\\). Therefore:\n\\[\\text{SE}(e^{\\hat{\\beta}}) \\approx e^{\\hat{\\beta}} \\cdot \\text{SE}(\\hat{\\beta})\\]\n\n\nExample 2: Ratio \\(\\hat{\\beta}_1 / \\hat{\\beta}_2\\)\n\\(g(\\beta_1, \\beta_2) = \\beta_1 / \\beta_2\\). The gradient is \\(\\nabla g = (1/\\beta_2, \\; -\\beta_1/\\beta_2^2)\\), so:\n\\[\\text{Var}\\!\\left(\\frac{\\hat{\\beta}_1}{\\hat{\\beta}_2}\\right) \\approx \\frac{1}{\\beta_2^2}\\text{Var}(\\hat{\\beta}_1) + \\frac{\\beta_1^2}{\\beta_2^4}\\text{Var}(\\hat{\\beta}_2) - \\frac{2\\beta_1}{\\beta_2^3}\\text{Cov}(\\hat{\\beta}_1, \\hat{\\beta}_2)\\]\n\n\nExample 3: Logit marginal effects\nFor logit with \\(P(Y=1|X) = \\Lambda(X'\\beta)\\), the marginal effect of \\(X_j\\) is \\(\\Lambda'(X'\\beta) \\cdot \\beta_j\\). The delta method gives its SE using the gradient with respect to \\(\\beta\\) — see limited dependent variables.\n\n\n\n\n\n\nThe Oracle View. In the simulation below, we know the true \\(\\beta\\) and can compute the exact sampling distribution of \\(e^{\\hat{\\beta}}\\) by Monte Carlo. We compare three approaches: the delta method SE, the bootstrap SE, and the true SD from simulation. In practice, you’d rely on the delta method or bootstrap since you can’t simulate from the truth.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "The Delta Method"
    ]
  },
  {
    "objectID": "delta-method.html#simulation",
    "href": "delta-method.html#simulation",
    "title": "The Delta Method",
    "section": "Simulation",
    "text": "Simulation\nEstimate \\(\\hat{\\beta}\\) from a simple regression, then compute \\(e^{\\hat{\\beta}}\\) (the odds ratio). Compare the delta method SE vs bootstrap SE vs the true sampling distribution.\n#| standalone: true\n#| viewerHeight: 750\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .eq-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-bottom: 14px; font-size: 14px; line-height: 1.9;\n    }\n    .eq-box b { color: #2c3e50; }\n    .match  { color: #27ae60; font-weight: bold; }\n    .coef   { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 4,\n\n      sliderInput(\"beta\", HTML(\"True &beta;:\"),\n                  min = 0.1, max = 2, value = 0.5, step = 0.1),\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 30, max = 500, value = 100, step = 10),\n\n      sliderInput(\"sigma\", HTML(\"Error SD (&sigma;):\"),\n                  min = 0.5, max = 5, value = 2, step = 0.5),\n\n      actionButton(\"resim\", \"Run simulations\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_box\")\n    ),\n\n    mainPanel(\n      width = 8,\n      fluidRow(\n        column(6, plotOutput(\"plot_beta\", height = \"400px\")),\n        column(6, plotOutput(\"plot_exp\", height = \"400px\"))\n      ),\n      uiOutput(\"compare_box\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  sim_results &lt;- reactive({\n    input$resim\n    beta  &lt;- input$beta\n    n     &lt;- input$n\n    sigma &lt;- input$sigma\n    n_sims &lt;- 1000\n    n_boot &lt;- 200\n\n    beta_hats &lt;- numeric(n_sims)\n    exp_betas &lt;- numeric(n_sims)\n    delta_ses &lt;- numeric(n_sims)\n    boot_ses  &lt;- numeric(n_sims)\n\n    for (i in seq_len(n_sims)) {\n      x &lt;- rnorm(n)\n      y &lt;- beta * x + rnorm(n, sd = sigma)\n      fit &lt;- lm(y ~ x)\n\n      b_hat &lt;- coef(fit)[\"x\"]\n      se_b  &lt;- summary(fit)$coefficients[\"x\", \"Std. Error\"]\n\n      beta_hats[i] &lt;- b_hat\n      exp_betas[i] &lt;- exp(b_hat)\n\n      # Delta method SE for exp(beta)\n      delta_ses[i] &lt;- exp(b_hat) * se_b\n\n      # Bootstrap SE\n      boot_exp &lt;- numeric(n_boot)\n      for (j in seq_len(n_boot)) {\n        idx &lt;- sample(n, n, replace = TRUE)\n        boot_fit &lt;- lm(y[idx] ~ x[idx])\n        boot_exp[j] &lt;- exp(coef(boot_fit)[\"x[idx]\"])\n      }\n      boot_ses[i] &lt;- sd(boot_exp)\n    }\n\n    list(beta_hats = beta_hats, exp_betas = exp_betas,\n         delta_ses = delta_ses, boot_ses = boot_ses,\n         true_beta = beta, true_exp = exp(beta))\n  })\n\n  output$plot_beta &lt;- renderPlot({\n    d &lt;- sim_results()\n    par(mar = c(5, 5, 4, 2))\n\n    hist(d$beta_hats, breaks = 40,\n         col = adjustcolor(\"#3498db\", 0.5), border = \"white\",\n         main = expression(\"Sampling dist. of \" * hat(beta)),\n         xlab = expression(hat(beta)), freq = FALSE)\n    abline(v = d$true_beta, col = \"#e74c3c\", lwd = 2.5)\n    legend(\"topright\", bty = \"n\",\n           legend = paste(\"True =\", d$true_beta),\n           col = \"#e74c3c\", lwd = 2.5)\n  })\n\n  output$plot_exp &lt;- renderPlot({\n    d &lt;- sim_results()\n    par(mar = c(5, 5, 4, 2))\n\n    hist(d$exp_betas, breaks = 40,\n         col = adjustcolor(\"#27ae60\", 0.5), border = \"white\",\n         main = expression(\"Sampling dist. of \" * e^{hat(beta)}),\n         xlab = expression(e^{hat(beta)}), freq = FALSE)\n    abline(v = d$true_exp, col = \"#e74c3c\", lwd = 2.5)\n\n    # Overlay delta method normal approximation\n    x_seq &lt;- seq(min(d$exp_betas), max(d$exp_betas), length.out = 200)\n    avg_delta_se &lt;- mean(d$delta_ses)\n    lines(x_seq, dnorm(x_seq, mean = d$true_exp, sd = avg_delta_se),\n          col = \"#9b59b6\", lwd = 2, lty = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(paste(\"True =\", round(d$true_exp, 3)),\n                      \"Delta method approx\"),\n           col = c(\"#e74c3c\", \"#9b59b6\"),\n           lwd = c(2.5, 2), lty = c(1, 2))\n  })\n\n  output$results_box &lt;- renderUI({\n    d &lt;- sim_results()\n\n    true_sd &lt;- sd(d$exp_betas)\n    avg_delta &lt;- mean(d$delta_ses)\n    avg_boot  &lt;- mean(d$boot_ses)\n\n    tags$div(class = \"eq-box\", style = \"margin-top: 16px;\",\n      HTML(paste0(\n        \"&lt;b&gt;SE comparison for e&lt;sup&gt;&beta;&lt;/sup&gt;:&lt;/b&gt;&lt;br&gt;\",\n        \"True SD (simulation): &lt;span class='coef'&gt;\",\n        round(true_sd, 4), \"&lt;/span&gt;&lt;br&gt;\",\n        \"Delta method (avg): &lt;span class='match'&gt;\",\n        round(avg_delta, 4), \"&lt;/span&gt;&lt;br&gt;\",\n        \"Bootstrap (avg): &lt;span class='match'&gt;\",\n        round(avg_boot, 4), \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;small&gt;All three should agree when n is large \",\n        \"and &beta; is moderate.&lt;/small&gt;\"\n      ))\n    )\n  })\n\n  output$compare_box &lt;- renderUI({\n    tags$div(class = \"eq-box\", style = \"margin-top: 8px;\",\n      HTML(paste0(\n        \"&lt;b&gt;Formula check:&lt;/b&gt; SE(e&lt;sup&gt;&beta;&lt;/sup&gt;) &asymp; \",\n        \"e&lt;sup&gt;&beta;&lt;/sup&gt; &times; SE(&beta;). \",\n        \"The delta method uses a linear approximation at the estimate — \",\n        \"it works well when the sampling distribution of &beta; is \",\n        \"approximately normal and g is smooth.\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nSmall \\(\\beta\\) (0.1–0.3): \\(e^\\beta \\approx 1 + \\beta\\), nearly linear. Delta method is excellent.\nLarge \\(\\beta\\) (1.5–2.0): the exponential is more curved. The sampling distribution of \\(e^{\\hat{\\beta}}\\) becomes right-skewed. Delta method SE is still close but the normal approximation is less accurate.\nSmall \\(n\\) (30–50): bootstrap and delta method may diverge because \\(\\hat{\\beta}\\) isn’t yet well-approximated by a normal.\nLarge \\(n\\) (300+): all three agree closely.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "The Delta Method"
    ]
  },
  {
    "objectID": "delta-method.html#when-the-delta-method-fails",
    "href": "delta-method.html#when-the-delta-method-fails",
    "title": "The Delta Method",
    "section": "When the delta method fails",
    "text": "When the delta method fails\nThe approximation relies on \\(g'(\\theta_0) \\neq 0\\) and on \\(\\hat{\\theta}\\) being approximately normal. It breaks down when:\n\n\\(g'(\\theta_0) = 0\\): the linear term vanishes and you need a second-order expansion. Example: \\(g(\\theta) = \\theta^2\\) at \\(\\theta_0 = 0\\).\nSmall samples: \\(\\hat{\\theta}\\) isn’t close enough to normal for the Taylor expansion to be accurate.\nHighly nonlinear \\(g\\): the curvature of \\(g\\) matters over the range where \\(\\hat{\\theta}\\) varies. The sampling distribution of \\(g(\\hat{\\theta})\\) may be skewed even though \\(\\hat{\\theta}\\) is symmetric.\n\nIn these cases, use the bootstrap instead.\n\n\nConnections\n\nFisher Information — The delta method starts from the asymptotic normality that Fisher information provides\nThe Bootstrap — The nonparametric alternative when the delta method’s assumptions fail\nLimited Dependent Variables — Marginal effect SEs in logit/probit require the delta method\n\n\n\n\nDid you know?\n\nThe delta method is one of the oldest tools in statistics, going back to R.A. Fisher in the 1920s and even earlier to Friedrich Bessel. The idea is simple — linearize and propagate — but it appears everywhere.\nIn physics, the same technique is called error propagation or propagation of uncertainty. Every lab course teaches it: if you measure the radius \\(r\\) with uncertainty \\(\\sigma_r\\), the area \\(\\pi r^2\\) has uncertainty approximately \\(2\\pi r \\cdot \\sigma_r\\).\nThe “delta” in the name refers to the small perturbation \\(\\delta\\theta =\n\\hat{\\theta} - \\theta\\) in the Taylor expansion, not to any particular Greek letter in the formula.",
    "crumbs": [
      "Standard Errors & Diagnostics",
      "The Delta Method"
    ]
  },
  {
    "objectID": "simulations.html",
    "href": "simulations.html",
    "title": "CLT Simulator",
    "section": "",
    "text": "Drag the sample size slider to watch the Central Limit Theorem in action. The left plot shows the true population; the right shows the sampling distribution of the mean — notice how it becomes normal as n grows, regardless of the population shape.\n\n\n\n\n\n\nThe Oracle View. We choose the population distribution and can draw unlimited samples to build the sampling distribution. In practice, you have one sample from an unknown distribution and rely on the CLT to tell you the sampling distribution is approximately normal.\n\n\n\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\n\n# ---------------------------------------------------------------------------\n# Helper: draw a single random sample from the chosen distribution\n# ---------------------------------------------------------------------------\ndraw_sample &lt;- function(n, dist) {\n  switch(dist,\n    \"Uniform(0, 1)\"      = runif(n),\n    \"Exponential(1)\"     = rexp(n, rate = 1),\n    \"Right-skewed\"       = rchisq(n, df = 3),\n    \"Bimodal\"            = {\n      k &lt;- rbinom(n, 1, 0.5)\n      k * rnorm(n, mean = -2, sd = 0.6) + (1 - k) * rnorm(n, mean = 2, sd = 0.6)\n    },\n    \"Bernoulli(0.3)\"     = rbinom(n, size = 1, prob = 0.3),\n    runif(n)\n  )\n}\n\n# Theoretical mean & sd of each population distribution\npop_params &lt;- list(\n  \"Uniform(0, 1)\"  = list(mu = 0.5, sigma = sqrt(1 / 12)),\n  \"Exponential(1)\" = list(mu = 1,   sigma = 1),\n  \"Right-skewed\"   = list(mu = 3,   sigma = sqrt(6)),\n  \"Bimodal\"        = list(mu = 0,   sigma = sqrt(0.6^2 + 4)),\n  \"Bernoulli(0.3)\" = list(mu = 0.3, sigma = sqrt(0.3 * 0.7))\n)\n\n# ---------------------------------------------------------------------------\n# UI\n# ---------------------------------------------------------------------------\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #eaf2f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.8;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"dist\", \"Population distribution:\",\n                  choices = names(pop_params)),\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 1, max = 200, value = 5, step = 1),\n\n      sliderInput(\"reps\", \"Number of samples:\",\n                  min = 100, max = 3000, value = 1000, step = 100),\n\n      actionButton(\"resample\", \"Draw new samples\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"stats_box\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"parent_plot\", height = \"380px\")),\n        column(6, plotOutput(\"sampling_plot\", height = \"380px\"))\n      )\n    )\n  )\n)\n\n# ---------------------------------------------------------------------------\n# Server\n# ---------------------------------------------------------------------------\nserver &lt;- function(input, output, session) {\n\n  sim &lt;- reactive({\n    input$resample\n    n    &lt;- input$n\n    reps &lt;- input$reps\n    dist &lt;- input$dist\n\n    means &lt;- replicate(reps, mean(draw_sample(n, dist)))\n\n    params &lt;- pop_params[[dist]]\n    theo_mu &lt;- params$mu\n    theo_se &lt;- params$sigma / sqrt(n)\n\n    list(means = means, dist = dist, n = n, reps = reps,\n         theo_mu = theo_mu, theo_se = theo_se)\n  })\n\n  output$parent_plot &lt;- renderPlot({\n    dist &lt;- input$dist\n    big  &lt;- draw_sample(10000, dist)\n\n    par(mar = c(4.5, 4, 3, 1))\n    hist(big, breaks = 60, probability = TRUE,\n         col = \"#d5e8d4\", border = \"#82b366\",\n         main = paste(\"True Population:\", dist),\n         xlab = \"x\", ylab = \"Density\")\n  })\n\n  output$sampling_plot &lt;- renderPlot({\n    s &lt;- sim()\n\n    par(mar = c(4.5, 4, 3, 1))\n    hist(s$means, breaks = 40, probability = TRUE,\n         col = \"#dae8fc\", border = \"#6c8ebf\",\n         main = paste0(\"Sampling Distribution of the Mean (n = \", s$n, \")\"),\n         xlab = \"Sample mean\", ylab = \"Density\")\n\n    x_seq &lt;- seq(min(s$means), max(s$means), length.out = 300)\n    lines(x_seq, dnorm(x_seq, mean = s$theo_mu, sd = s$theo_se),\n          col = \"#e74c3c\", lwd = 2.5)\n\n    abline(v = s$theo_mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\",\n           legend = c(\"Normal approximation\", \"Theoretical mean\"),\n           col    = c(\"#e74c3c\", \"#2c3e50\"),\n           lwd    = c(2.5, 2),\n           lty    = c(1, 2),\n           bty    = \"n\", cex = 0.9)\n  })\n\n  output$stats_box &lt;- renderUI({\n    s &lt;- sim()\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Theoretical mean:&lt;/b&gt; \",   round(s$theo_mu, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Observed mean:&lt;/b&gt; \",      round(mean(s$means), 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Theoretical SE:&lt;/b&gt; \",     round(s$theo_se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Observed SD:&lt;/b&gt; \",        round(sd(s$means), 4)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nDid you know?\n\nThe CLT was first glimpsed by Abraham de Moivre in 1733, who showed that the binomial distribution approaches a bell curve. Laplace generalized it in 1812. But the rigorous proof for arbitrary distributions came from Aleksandr Lyapunov in 1901 — over 150 years after de Moivre’s insight.\nThe normal distribution is sometimes called the “Gaussian” distribution after Carl Friedrich Gauss, but Gauss wasn’t the first to describe it — de Moivre was. Gauss just got better publicity.\nThe CLT explains why so many things in nature look bell-shaped: human heights, blood pressure, measurement errors, IQ scores. Whenever an outcome is the sum of many small independent factors, the CLT kicks in.",
    "crumbs": [
      "The Central Limit Theorem",
      "CLT Simulator"
    ]
  },
  {
    "objectID": "calibration-uncertainty.html",
    "href": "calibration-uncertainty.html",
    "title": "Calibration and Uncertainty Quantification",
    "section": "",
    "text": "Classical statistical estimators come with standard errors. OLS gives you \\(\\sigma^2(X'X)^{-1}\\) (Algebra of Regression). MLE gives you the inverse Fisher information (MLE). Bayesian inference gives you the full posterior distribution (Bayesian Updating). Modern machine learning models typically give you none of these. This page asks: when a model says it’s 90% confident, is it right 90% of the time? And if not, what can you do about it?",
    "crumbs": [
      "Statistical Foundations of AI",
      "Calibration & Uncertainty Quantification"
    ]
  },
  {
    "objectID": "calibration-uncertainty.html#what-calibration-means",
    "href": "calibration-uncertainty.html#what-calibration-means",
    "title": "Calibration and Uncertainty Quantification",
    "section": "What calibration means",
    "text": "What calibration means\nA model is calibrated if its predicted probabilities match empirical frequencies:\n\\[\nP(Y = 1 \\mid \\hat{p}(X) = q) = q \\qquad \\text{for all } q \\in [0, 1]\n\\]\nIf the model says “there’s a 70% chance of rain” for 1,000 days, it should rain on approximately 700 of them. If it rains on 850, the model is overconfident at the 70% level. If it rains on 550, the model is underconfident.\nCalibration is a property of the predicted probabilities, not the predicted classes. A model can have high accuracy (correctly classifying most examples) while being poorly calibrated (systematically overconfident or underconfident in its probability estimates).\n\nWhy calibration matters\nCalibration matters whenever you use predicted probabilities as inputs to decisions:\n\nRisk assessment: if a medical model says “30% chance of disease,” a doctor’s decision depends on that number being trustworthy\nRanking and prioritization: if a fraud detection model scores transactions, the ordering depends on calibrated probabilities\nBayesian updating: if you use a model’s output as a likelihood in Bayesian Updating, miscalibration corrupts the posterior\n\nAn uncalibrated model’s predictions are relative orderings at best — “this example is more likely than that one” — but the actual probability values are unreliable.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Calibration & Uncertainty Quantification"
    ]
  },
  {
    "objectID": "calibration-uncertainty.html#neural-networks-are-typically-miscalibrated",
    "href": "calibration-uncertainty.html#neural-networks-are-typically-miscalibrated",
    "title": "Calibration and Uncertainty Quantification",
    "section": "Neural networks are typically miscalibrated",
    "text": "Neural networks are typically miscalibrated\nModern deep neural networks tend to be overconfident: they assign high probabilities to their predictions even when they are wrong. This is a well-documented empirical finding (Guo et al., 2017).\nThe cause is related to training as MLE. The cross-entropy loss encourages the model to push predicted probabilities toward 0 or 1 — the loss is minimized when the model is maximally confident on every training example. With enough capacity, the model memorizes the training set and becomes overconfident on the test set.\nThis is the bias-variance tradeoff manifesting in probability space: an overparameterized model fits the training data perfectly (low bias) but produces overconfident predictions on new data (high variance in the probability estimates).\n\nHow to read a calibration curve\nA reliability diagram (calibration curve) plots predicted probability on the x-axis against observed frequency on the y-axis. Here’s what to look for:\n  Observed frequency\n  1.0 |                          /\n      |                        / ← Perfect calibration\n      |                      /     (diagonal)\n      |                    /\n  0.5 |         ____------\n      |       /  ↑ Overconfident model:\n      |     /    predicts 0.8, actual is 0.6\n      |   /\n  0.0 |_/___________________________\n      0.0              0.5        1.0\n              Predicted probability\n\nOn the diagonal: the model is perfectly calibrated. Predicted 70% → actually happens 70% of the time.\nBelow the diagonal: the model is overconfident. It predicts higher probabilities than the actual rate. Says 80%, reality is 60%.\nAbove the diagonal: the model is underconfident. It predicts lower probabilities than the actual rate. Says 30%, reality is 50%.\n\nThe gap between the curve and the diagonal is the calibration error at that probability level. The Expected Calibration Error (ECE) is the weighted average of these gaps across all bins.\nTry it in the simulation below — set miscalibration to 0 to see the diagonal, then increase it to watch the curve bow away.\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n    .good { color: #27ae60; font-weight: bold; }\n    .bad  { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 500, max = 5000, value = 1000, step = 500),\n\n      sliderInput(\"miscal\", \"Miscalibration level:\",\n                  min = 0, max = 1, value = 0.5, step = 0.1),\n\n      actionButton(\"go\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(7, plotOutput(\"reliability_plot\", height = \"470px\")),\n        column(5, plotOutput(\"hist_plot\", height = \"470px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    n      &lt;- input$n\n    miscal &lt;- input$miscal\n\n    # True probabilities and outcomes\n    p_true &lt;- runif(n)\n    Y      &lt;- rbinom(n, 1, p_true)\n\n    # Model predictions: push toward 0/1 when miscal &gt; 0\n    q &lt;- p_true^(1 / (1 + miscal))\n\n    # Bin predictions into 10 bins\n    n_bins &lt;- 10\n    breaks &lt;- seq(0, 1, length.out = n_bins + 1)\n    bins   &lt;- cut(q, breaks = breaks, include.lowest = TRUE, labels = FALSE)\n\n    bin_mid  &lt;- numeric(n_bins)\n    bin_acc  &lt;- numeric(n_bins)\n    bin_conf &lt;- numeric(n_bins)\n    bin_n    &lt;- numeric(n_bins)\n\n    for (j in 1:n_bins) {\n      idx &lt;- which(bins == j)\n      bin_n[j]    &lt;- length(idx)\n      bin_conf[j] &lt;- if (length(idx) &gt; 0) mean(q[idx]) else NA\n      bin_acc[j]  &lt;- if (length(idx) &gt; 0) mean(Y[idx]) else NA\n      bin_mid[j]  &lt;- (breaks[j] + breaks[j + 1]) / 2\n    }\n\n    # Brier score\n    brier &lt;- mean((q - Y)^2)\n\n    # ECE: weighted average of |accuracy - confidence| per bin\n    valid   &lt;- bin_n &gt; 0\n    ece     &lt;- sum(bin_n[valid] * abs(bin_acc[valid] - bin_conf[valid])) / n\n\n    list(q = q, Y = Y, n = n, miscal = miscal,\n         bin_conf = bin_conf, bin_acc = bin_acc, bin_n = bin_n,\n         breaks = breaks, n_bins = n_bins,\n         mean_pred = mean(q), pos_rate = mean(Y),\n         brier = brier, ece = ece)\n  })\n\n  output$reliability_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    valid &lt;- d$bin_n &gt; 0\n\n    plot(NULL, xlim = c(0, 1), ylim = c(0, 1),\n         xlab = \"Mean predicted probability\",\n         ylab = \"Fraction of positives\",\n         main = \"Reliability (Calibration) Diagram\",\n         asp = 1)\n\n    # Perfect calibration diagonal\n    abline(0, 1, lty = 2, lwd = 2, col = \"#7f8c8d\")\n\n    # Reliability curve\n    lines(d$bin_conf[valid], d$bin_acc[valid],\n          col = \"#e74c3c\", lwd = 2.5, type = \"o\", pch = 19, cex = 1.3)\n\n    # Shaded gap between curve and diagonal\n    for (j in which(valid)) {\n      if (!is.na(d$bin_conf[j]) && !is.na(d$bin_acc[j])) {\n        segments(d$bin_conf[j], d$bin_conf[j],\n                 d$bin_conf[j], d$bin_acc[j],\n                 col = adjustcolor(\"#e74c3c\", 0.3), lwd = 6)\n      }\n    }\n\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = c(\"Perfect calibration\", \"Model\"),\n           col = c(\"#7f8c8d\", \"#e74c3c\"),\n           lwd = c(2, 2.5), lty = c(2, 1), pch = c(NA, 19))\n\n    # Annotate miscalibration level\n    text(0.95, 0.05, paste0(\"Miscal = \", d$miscal),\n         cex = 0.85, font = 2, col = \"#2c3e50\", pos = 2)\n  })\n\n  output$hist_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$q, breaks = 20, col = adjustcolor(\"#3498db\", 0.5),\n         border = \"#3498db\", main = \"Predicted Probabilities\",\n         xlab = \"Predicted probability\", ylab = \"Frequency\",\n         xlim = c(0, 1))\n\n    abline(v = mean(d$q), lty = 2, lwd = 2, col = \"#2c3e50\")\n    text(mean(d$q), par(\"usr\")[4] * 0.9,\n         paste0(\"Mean = \", round(mean(d$q), 3)),\n         cex = 0.8, font = 2, col = \"#2c3e50\", pos = 4)\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n\n    ece_class &lt;- if (d$ece &lt; 0.05) \"good\" else if (d$ece &lt; 0.1) \"bad\" else \"bad\"\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Mean predicted prob:&lt;/b&gt; \", round(d$mean_pred, 3), \"&lt;br&gt;\",\n        \"&lt;b&gt;Actual positive rate:&lt;/b&gt; \", round(d$pos_rate, 3), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:6px 0'&gt;\",\n        \"&lt;b&gt;Brier score:&lt;/b&gt; \", round(d$brier, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;ECE:&lt;/b&gt; &lt;span class='\", ece_class, \"'&gt;\",\n        round(d$ece, 4), \"&lt;/span&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nThings to try\n\nMiscalibration = 0: the reliability curve lies on the diagonal — the model is perfectly calibrated. The histogram of predictions is roughly uniform (matching the true probability distribution).\nIncrease miscalibration: the reliability curve bows below the diagonal. The model predicts probabilities that are too extreme — when it says 80%, the actual rate is lower. This is overconfidence. The ECE increases.\nLarge n: the reliability curve becomes smoother and the ECE estimate becomes more precise. With small \\(n\\), the binned estimates are noisy even for a well-calibrated model.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Calibration & Uncertainty Quantification"
    ]
  },
  {
    "objectID": "calibration-uncertainty.html#post-hoc-calibration",
    "href": "calibration-uncertainty.html#post-hoc-calibration",
    "title": "Calibration and Uncertainty Quantification",
    "section": "Post-hoc calibration",
    "text": "Post-hoc calibration\nThe most common fix is temperature scaling: divide the logits (pre-softmax outputs) by a scalar \\(T &gt; 0\\) before applying the softmax:\n\\[\n\\hat{p}_i = \\text{softmax}(z_i / T)\n\\]\n\n\\(T = 1\\): original model\n\\(T &gt; 1\\): softer probabilities (less confident)\n\\(T &lt; 1\\): sharper probabilities (more confident)\n\nThe temperature \\(T\\) is tuned on a held-out validation set to minimize the negative log-likelihood — which is, once again, MLE. Temperature scaling does not change the model’s rankings (which example is “most likely”) — it only adjusts the magnitudes of the probabilities.\n\n\n\n\n\n\nWhy “temperature”? The name comes from statistical mechanics. In the Boltzmann distribution, temperature controls how peaked or flat the distribution is over energy states. High temperature → uniform (maximum entropy). Low temperature → concentrated on the lowest-energy state. The analogy to softmax is exact.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Calibration & Uncertainty Quantification"
    ]
  },
  {
    "objectID": "calibration-uncertainty.html#bayesian-approaches-to-uncertainty",
    "href": "calibration-uncertainty.html#bayesian-approaches-to-uncertainty",
    "title": "Calibration and Uncertainty Quantification",
    "section": "Bayesian approaches to uncertainty",
    "text": "Bayesian approaches to uncertainty\nThe Bayesian Estimation page showed that Bayesian inference naturally produces uncertainty estimates — the posterior is a full distribution, not a point. Applying this to neural networks:\nBayesian neural networks place priors on the weights and compute (or approximate) the posterior \\(p(\\theta \\mid \\text{data})\\). Predictions integrate over weight uncertainty:\n\\[\np(Y \\mid X, \\text{data}) = \\int p(Y \\mid X, \\theta) \\, p(\\theta \\mid \\text{data}) \\, d\\theta\n\\]\nThis integral is typically intractable, motivating approximations:\n\nMonte Carlo dropout: as discussed in Regularization as Bayesian Inference, running the network with dropout at test time approximates sampling from the posterior. The spread of predictions estimates epistemic uncertainty.\nDeep ensembles: train multiple networks with different initializations. The disagreement between them estimates uncertainty. This is not formally Bayesian but captures a similar intuition — uncertainty is where the models disagree.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Calibration & Uncertainty Quantification"
    ]
  },
  {
    "objectID": "calibration-uncertainty.html#two-kinds-of-uncertainty",
    "href": "calibration-uncertainty.html#two-kinds-of-uncertainty",
    "title": "Calibration and Uncertainty Quantification",
    "section": "Two kinds of uncertainty",
    "text": "Two kinds of uncertainty\nA useful decomposition:\nAleatoric uncertainty is inherent randomness in the data — the noise \\(\\varepsilon\\) in \\(Y = f(X) + \\varepsilon\\). This cannot be reduced by collecting more data. It is the \\(\\sigma^2\\) in \\(\\text{Var}(\\hat{\\beta}) = \\sigma^2(X'X)^{-1}\\) from The Algebra Behind OLS.\nEpistemic uncertainty is uncertainty about the model or parameters — what you don’t know because you have finite data. This can be reduced with more data. It is what Bayesian Updating reduces as the posterior concentrates.\nClassical statistics separates these naturally: \\(\\sigma^2\\) is aleatoric; \\(\\text{Var}(\\hat{\\beta})\\) is epistemic. Neural networks typically conflate them, reporting a single predicted probability that mixes both sources.\n\n\n\n\n\n\nWhy the distinction matters. If uncertainty is aleatoric, the model should say “I don’t know because no one can know — the outcome is inherently random.” If it is epistemic, the model should say “I don’t know because I haven’t seen enough data — more training data or a better model might help.” These have different implications for decision-making: aleatoric uncertainty calls for risk management; epistemic uncertainty calls for more data.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Calibration & Uncertainty Quantification"
    ]
  },
  {
    "objectID": "calibration-uncertainty.html#conformal-prediction",
    "href": "calibration-uncertainty.html#conformal-prediction",
    "title": "Calibration and Uncertainty Quantification",
    "section": "Conformal prediction",
    "text": "Conformal prediction\nA recent and increasingly popular approach that provides distribution-free prediction intervals. The idea:\n\nFit any model (neural network, random forest, anything)\nOn a calibration set, compute the residuals (or nonconformity scores)\nUse the quantiles of these residuals to construct prediction intervals\n\nThe guarantee: with probability \\(1 - \\alpha\\), the true outcome falls within the prediction interval — regardless of the model or the distribution of the data. This requires only the assumption that calibration and test data are exchangeable (essentially, drawn from the same distribution).\nConformal prediction is attractive because:\n\nIt works with any underlying model — no Bayesian assumptions needed\nIt provides finite-sample coverage guarantees (not just asymptotic)\nIt is computationally simple — just sort residuals and take quantiles\n\nThe connection to the course: conformal prediction produces objects that resemble confidence intervals (they have a coverage guarantee) but are computed without assuming normality or using Fisher information. They are closer in spirit to the bootstrap — using the empirical distribution of residuals rather than parametric assumptions.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Calibration & Uncertainty Quantification"
    ]
  },
  {
    "objectID": "calibration-uncertainty.html#connecting-to-the-course",
    "href": "calibration-uncertainty.html#connecting-to-the-course",
    "title": "Calibration and Uncertainty Quantification",
    "section": "Connecting to the course",
    "text": "Connecting to the course\nThis page ties together uncertainty quantification across frameworks:\n\n\n\nFramework\nSource of uncertainty\nTool\n\n\n\n\nOLS\n\\(\\sigma^2(X'X)^{-1}\\)\nStandard errors\n\n\nMLE\nFisher information\nAsymptotic SEs\n\n\nBayesian\nFull posterior\nCredible intervals\n\n\nBootstrap\nResampled distribution\nBootstrap CIs\n\n\nNeural networks\n(typically missing)\nCalibration, ensembles, conformal\n\n\n\nThe fundamental question is always the same: how much should you trust the estimate? Classical statistics answers this with standard errors and confidence intervals. Bayesian inference answers it with posteriors and credible intervals. For machine learning models, the answer requires additional machinery — calibration, ensembles, or conformal prediction — because the training procedure does not provide uncertainty estimates by default.",
    "crumbs": [
      "Statistical Foundations of AI",
      "Calibration & Uncertainty Quantification"
    ]
  },
  {
    "objectID": "lln.html",
    "href": "lln.html",
    "title": "LLN vs CLT: Why Averages Stabilize Before They Become Normal",
    "section": "",
    "text": "The Law of Large Numbers (LLN) and the Central Limit Theorem (CLT) are the two pillars of statistics, but they say different things:\n\n\n\n\n\n\n\n\n\nLLN\nCLT\n\n\n\n\nWhat it says\nThe sample mean converges to \\(\\mu\\)\nThe distribution of sample means is approximately normal\n\n\nAbout\nOne running average\nMany sample averages\n\n\nPromise\nYour estimate gets close to the truth\nYou know the shape of the uncertainty\n\n\nRequires\nFinite mean\nFinite variance\n\n\n\nLLN says the mean settles down. CLT says where it settles follows a normal distribution.\nThink of it this way: LLN tells you that if you flip a fair coin enough times, the fraction of heads approaches 0.5. CLT tells you that if you repeat the entire experiment many times, the distribution of those fractions is bell-shaped.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we know the true mean \\(\\mu\\) — the horizontal line the running average converges to. In practice, you don’t know \\(\\mu\\). You watch your estimate stabilize but never know exactly what it’s converging toward. That’s the gap between theory and practice: LLN guarantees convergence, but the target is invisible.",
    "crumbs": [
      "The Central Limit Theorem",
      "LLN vs CLT"
    ]
  },
  {
    "objectID": "lln.html#two-theorems-two-different-promises",
    "href": "lln.html#two-theorems-two-different-promises",
    "title": "LLN vs CLT: Why Averages Stabilize Before They Become Normal",
    "section": "",
    "text": "The Law of Large Numbers (LLN) and the Central Limit Theorem (CLT) are the two pillars of statistics, but they say different things:\n\n\n\n\n\n\n\n\n\nLLN\nCLT\n\n\n\n\nWhat it says\nThe sample mean converges to \\(\\mu\\)\nThe distribution of sample means is approximately normal\n\n\nAbout\nOne running average\nMany sample averages\n\n\nPromise\nYour estimate gets close to the truth\nYou know the shape of the uncertainty\n\n\nRequires\nFinite mean\nFinite variance\n\n\n\nLLN says the mean settles down. CLT says where it settles follows a normal distribution.\nThink of it this way: LLN tells you that if you flip a fair coin enough times, the fraction of heads approaches 0.5. CLT tells you that if you repeat the entire experiment many times, the distribution of those fractions is bell-shaped.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we know the true mean \\(\\mu\\) — the horizontal line the running average converges to. In practice, you don’t know \\(\\mu\\). You watch your estimate stabilize but never know exactly what it’s converging toward. That’s the gap between theory and practice: LLN guarantees convergence, but the target is invisible.",
    "crumbs": [
      "The Central Limit Theorem",
      "LLN vs CLT"
    ]
  },
  {
    "objectID": "lln.html#simulation-1-the-running-average",
    "href": "lln.html#simulation-1-the-running-average",
    "title": "LLN vs CLT: Why Averages Stabilize Before They Become Normal",
    "section": "Simulation 1: The running average",
    "text": "Simulation 1: The running average\nWatch one running average converge to the true mean as you add observations one at a time. This is LLN in action — the cumulative mean stabilizes. Try different distributions and notice that convergence happens regardless of the population shape.\n#| standalone: true\n#| viewerHeight: 620\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"pop\", \"Population:\",\n                  choices = c(\"Uniform(0, 1)\",\n                              \"Exponential(1)\",\n                              \"Bimodal\",\n                              \"Bernoulli(0.3)\",\n                              \"Heavy-tailed (t, df=2)\")),\n\n      sliderInput(\"n\", \"Number of observations:\",\n                  min = 50, max = 5000, value = 500, step = 50),\n\n      sliderInput(\"paths\", \"Number of paths:\",\n                  min = 1, max = 10, value = 3, step = 1),\n\n      actionButton(\"go\", \"Draw new paths\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      plotOutput(\"running_avg\", height = \"500px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  draw_pop &lt;- function(n, pop) {\n    switch(pop,\n      \"Uniform(0, 1)\"          = runif(n),\n      \"Exponential(1)\"         = rexp(n, rate = 1),\n      \"Bimodal\"                = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, -2, 0.6) + (1 - k) * rnorm(n, 2, 0.6)\n      },\n      \"Bernoulli(0.3)\"         = rbinom(n, 1, 0.3),\n      \"Heavy-tailed (t, df=2)\" = rt(n, df = 2)\n    )\n  }\n\n  pop_mu &lt;- function(pop) {\n    switch(pop,\n      \"Uniform(0, 1)\"          = 0.5,\n      \"Exponential(1)\"         = 1,\n      \"Bimodal\"                = 0,\n      \"Bernoulli(0.3)\"         = 0.3,\n      \"Heavy-tailed (t, df=2)\" = 0\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n     &lt;- input$n\n    paths &lt;- input$paths\n    pop   &lt;- input$pop\n    mu    &lt;- pop_mu(pop)\n\n    all_paths &lt;- lapply(seq_len(paths), function(i) {\n      x &lt;- draw_pop(n, pop)\n      cumsum(x) / seq_along(x)\n    })\n\n    list(all_paths = all_paths, n = n, mu = mu, pop = pop, paths = paths)\n  })\n\n  output$running_avg &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ylim &lt;- range(unlist(d$all_paths))\n    ylim &lt;- ylim + c(-1, 1) * 0.1 * diff(ylim)\n\n    cols &lt;- c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"#9b59b6\", \"#e67e22\",\n              \"#1abc9c\", \"#34495e\", \"#f39c12\", \"#2ecc71\", \"#c0392b\")\n\n    plot(NULL, xlim = c(1, d$n), ylim = ylim,\n         xlab = \"Number of observations\",\n         ylab = \"Cumulative mean\",\n         main = paste0(\"LLN: Running average converges to \\u03bc = \", d$mu))\n\n    abline(h = d$mu, lty = 2, lwd = 2.5, col = \"#2c3e50\")\n\n    for (i in seq_along(d$all_paths)) {\n      lines(seq_along(d$all_paths[[i]]), d$all_paths[[i]],\n            col = adjustcolor(cols[i], 0.7), lwd = 1.5)\n    }\n\n    legend(\"topright\", bty = \"n\", cex = 0.9,\n           legend = c(paste0(\"True mean (\\u03bc = \", d$mu, \")\"),\n                      paste0(d$paths, \" sample path(s)\")),\n           col = c(\"#2c3e50\", cols[1]),\n           lwd = c(2.5, 1.5), lty = c(2, 1))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    final_means &lt;- sapply(d$all_paths, function(p) p[length(p)])\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True mean:&lt;/b&gt; \", d$mu, \"&lt;br&gt;\",\n        \"&lt;b&gt;Final running avg(s):&lt;/b&gt;&lt;br&gt;\",\n        paste(round(final_means, 4), collapse = \", \"), \"&lt;br&gt;\",\n        \"&lt;b&gt;Max deviation:&lt;/b&gt; \",\n        round(max(abs(final_means - d$mu)), 4)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "The Central Limit Theorem",
      "LLN vs CLT"
    ]
  },
  {
    "objectID": "lln.html#simulation-2-lln-vs-clt-side-by-side",
    "href": "lln.html#simulation-2-lln-vs-clt-side-by-side",
    "title": "LLN vs CLT: Why Averages Stabilize Before They Become Normal",
    "section": "Simulation 2: LLN vs CLT side by side",
    "text": "Simulation 2: LLN vs CLT side by side\nNow see both theorems at once. The left panel shows LLN — one running average converging to \\(\\mu\\). The right panel shows CLT — the histogram of many sample means forming a normal distribution. Same data, different questions.\n#| standalone: true\n#| viewerHeight: 580\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"pop2\", \"Population:\",\n                  choices = c(\"Uniform(0, 1)\",\n                              \"Exponential(1)\",\n                              \"Bimodal\",\n                              \"Bernoulli(0.3)\")),\n\n      sliderInput(\"n2\", \"Sample size per experiment:\",\n                  min = 5, max = 200, value = 30, step = 5),\n\n      sliderInput(\"reps2\", \"Number of experiments:\",\n                  min = 200, max = 3000, value = 1000, step = 100),\n\n      actionButton(\"go2\", \"Run experiments\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results2\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"lln_plot\", height = \"430px\")),\n        column(6, plotOutput(\"clt_plot\", height = \"430px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  draw_pop &lt;- function(n, pop) {\n    switch(pop,\n      \"Uniform(0, 1)\"  = runif(n),\n      \"Exponential(1)\" = rexp(n, rate = 1),\n      \"Bimodal\"        = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, -2, 0.6) + (1 - k) * rnorm(n, 2, 0.6)\n      },\n      \"Bernoulli(0.3)\" = rbinom(n, 1, 0.3)\n    )\n  }\n\n  pop_mu &lt;- function(pop) {\n    switch(pop,\n      \"Uniform(0, 1)\"  = 0.5,\n      \"Exponential(1)\" = 1,\n      \"Bimodal\"        = 0,\n      \"Bernoulli(0.3)\" = 0.3\n    )\n  }\n\n  pop_sigma &lt;- function(pop) {\n    switch(pop,\n      \"Uniform(0, 1)\"  = sqrt(1 / 12),\n      \"Exponential(1)\" = 1,\n      \"Bimodal\"        = sqrt(0.6^2 + 4),\n      \"Bernoulli(0.3)\" = sqrt(0.3 * 0.7)\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go2\n    n    &lt;- input$n2\n    reps &lt;- input$reps2\n    pop  &lt;- input$pop2\n    mu   &lt;- pop_mu(pop)\n    sig  &lt;- pop_sigma(pop)\n\n    # One long draw for LLN\n    long_draw &lt;- draw_pop(n * 5, pop)\n    running   &lt;- cumsum(long_draw) / seq_along(long_draw)\n\n    # Many experiments for CLT\n    means &lt;- replicate(reps, mean(draw_pop(n, pop)))\n\n    list(running = running, means = means, mu = mu, sig = sig,\n         n = n, reps = reps, pop = pop)\n  })\n\n  output$lln_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    plot(seq_along(d$running), d$running, type = \"l\",\n         col = \"#3498db\", lwd = 1.5,\n         xlab = \"Number of observations\",\n         ylab = \"Cumulative mean\",\n         main = \"LLN: One running average\")\n    abline(h = d$mu, lty = 2, lwd = 2.5, col = \"#e74c3c\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Running average\",\n                      paste0(\"True \\u03bc = \", d$mu)),\n           col = c(\"#3498db\", \"#e74c3c\"),\n           lwd = c(1.5, 2.5), lty = c(1, 2))\n  })\n\n  output$clt_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$means, breaks = 40, probability = TRUE,\n         col = \"#3498db30\", border = \"#3498db80\",\n         main = paste0(\"CLT: \", d$reps, \" sample means (n=\", d$n, \")\"),\n         xlab = \"Sample mean\", ylab = \"Density\")\n\n    x_seq &lt;- seq(min(d$means), max(d$means), length.out = 300)\n    se &lt;- d$sig / sqrt(d$n)\n    lines(x_seq, dnorm(x_seq, mean = d$mu, sd = se),\n          col = \"#e74c3c\", lwd = 2.5)\n\n    abline(v = d$mu, lty = 2, lwd = 2, col = \"#2c3e50\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Sampling distribution\",\n                      paste0(\"N(\\u03bc, \\u03c3/\\u221an) overlay\"),\n                      paste0(\"True \\u03bc = \", d$mu)),\n           col = c(\"#3498db80\", \"#e74c3c\", \"#2c3e50\"),\n           lwd = c(8, 2.5, 2), lty = c(1, 1, 2))\n  })\n\n  output$results2 &lt;- renderUI({\n    d &lt;- dat()\n    se &lt;- d$sig / sqrt(d$n)\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;LLN (left):&lt;/b&gt;&lt;br&gt;\",\n        \"Final avg: \", round(d$running[length(d$running)], 4), \"&lt;br&gt;\",\n        \"True \\u03bc: \", d$mu, \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;CLT (right):&lt;/b&gt;&lt;br&gt;\",\n        \"Mean of means: \", round(mean(d$means), 4), \"&lt;br&gt;\",\n        \"SD of means: \", round(sd(d$means), 4), \"&lt;br&gt;\",\n        \"Theoretical SE: \", round(se, 4)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThings to try\n\nExponential, n = 5: the LLN panel shows the running average still wobbling, while the CLT histogram is visibly skewed. Neither theorem has fully “kicked in” yet — but they will at different rates.\nExponential, n = 100: the running average has settled (LLN), and the histogram is now bell-shaped (CLT). Both theorems have done their job.\nBimodal: the running average converges to 0 (the midpoint), even though no individual observation is near 0. LLN doesn’t care about the shape.\nMultiple paths (Sim 1): all paths converge to the same \\(\\mu\\), but they take different routes. The early wobble is sampling variability; the eventual convergence is LLN.\n\n\n\nThe bottom line\n\nLLN = your estimate gets closer to the truth as \\(n\\) grows. It’s about accuracy of a single estimate.\nCLT = the shape of the sampling distribution is approximately normal. It’s about the distribution of the estimator across repeated experiments.\nYou need LLN before CLT matters. If the mean hasn’t stabilized, knowing its distribution is normal doesn’t help much.\n\n\n\n\nDid you know?\n\nJacob Bernoulli proved the first version of the Law of Large Numbers in his book Ars Conjectandi, published posthumously in 1713. He called it his “golden theorem” and spent over 20 years working on the proof. The result seems obvious in hindsight — of course averages converge — but rigorously proving why required entirely new mathematical machinery.\nThere are actually two versions: the Weak LLN (convergence in probability, proved by Bernoulli and later Chebyshev) and the Strong LLN (almost sure convergence, proved by Kolmogorov in 1933). The strong version says that convergence happens with probability 1, not just “usually.”\nThe heavy-tailed \\(t\\)-distribution with \\(\\text{df} = 1\\) (the Cauchy distribution) has no finite mean, so LLN doesn’t apply. The running average never settles down, no matter how much data you collect. Try it in the simulation above — it keeps jumping around forever.",
    "crumbs": [
      "The Central Limit Theorem",
      "LLN vs CLT"
    ]
  },
  {
    "objectID": "sampling-distribution.html",
    "href": "sampling-distribution.html",
    "title": "The Sampling Distribution",
    "section": "",
    "text": "You run an experiment and compute a statistic — say the sample mean. You get \\(\\bar{x} = 4.7\\). But if you ran the experiment again with new data, you’d get \\(\\bar{x} = 5.1\\). And again: \\(\\bar{x} = 4.3\\).\nThe sampling distribution is the distribution of all possible values your statistic could take across every possible sample you could have drawn. It tells you: how much does my estimate bounce around due to the randomness of sampling?\nYou never observe it directly — you only ran the experiment once. But it’s the foundation of everything: standard errors, confidence intervals, p-values, and hypothesis tests all depend on it.\n\n\nThis is the most common confusion in statistics:\n\n\n\n\n\n\n\n\n\nDistribution of the data\nSampling distribution\n\n\n\n\nWhat is it?\nHistogram of individual observations\nHistogram of the statistic across repeated samples\n\n\nWhat does it show?\nHow spread out individual values are\nHow much the estimate varies\n\n\nShape\nCould be anything (skewed, bimodal, etc.)\nOften approximately normal for averages, by the CLT\n\n\nWidth\nDepends on \\(\\sigma\\) (population SD)\nDepends on \\(\\sigma / \\sqrt{n}\\) (standard error)\n\n\nYou can see it?\nYes — plot your data\nNo — you only have one sample\n\n\n\nYour data might be wildly skewed. But the sampling distribution of the mean can still be normal, because averaging smooths things out. That’s the CLT.\nThe simulation below makes this concrete. The left panel shows your data (one sample). The right panel shows what happens when you repeat the experiment 1,000 times — the sampling distribution.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we can repeat the experiment 1,000 times and directly see the sampling distribution. In practice, you run the experiment once. You never observe the sampling distribution — you infer its properties (via the CLT or the bootstrap) to build standard errors and confidence intervals.\n\n\n\n#| standalone: true\n#| viewerHeight: 1050\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"pop\", \"Population shape:\",\n                  choices = c(\"Normal\",\n                              \"Uniform\",\n                              \"Skewed (exponential)\",\n                              \"Heavily skewed (chi-sq 2)\",\n                              \"Bimodal\",\n                              \"Heavy-tailed (t, df=3)\")),\n\n      selectInput(\"stat\", \"Statistic:\",\n                  choices = c(\"Mean\", \"Median\", \"SD\",\n                              \"Max\", \"IQR\")),\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 2, max = 200, value = 5, step = 1),\n\n      sliderInput(\"reps\", \"Repeated experiments:\",\n                  min = 100, max = 5000, value = 1000, step = 100),\n\n      actionButton(\"go\", \"Run experiments\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"data_plot\", height = \"350px\")),\n        column(6, plotOutput(\"samp_dist_plot\", height = \"350px\"))\n      ),\n      fluidRow(\n        column(12, plotOutput(\"convergence_plot\", height = \"450px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  draw_pop &lt;- function(n, pop) {\n    switch(pop,\n      \"Normal\" = rnorm(n, mean = 5, sd = 2),\n      \"Uniform\" = runif(n, 0, 10),\n      \"Skewed (exponential)\" = rexp(n, rate = 0.5),\n      \"Heavily skewed (chi-sq 2)\" = rchisq(n, df = 2),\n      \"Bimodal\" = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, 2, 0.7) + (1 - k) * rnorm(n, 8, 0.7)\n      },\n      \"Heavy-tailed (t, df=3)\" = rt(n, df = 3) * 2 + 5\n    )\n  }\n\n  compute_stat &lt;- function(x, stat) {\n    switch(stat,\n      \"Mean\"   = mean(x),\n      \"Median\" = median(x),\n      \"SD\"     = sd(x),\n      \"Max\"    = max(x),\n      \"IQR\"    = IQR(x)\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    reps &lt;- input$reps\n    pop  &lt;- input$pop\n    stat &lt;- input$stat\n\n    # One sample (for display)\n    one_sample &lt;- draw_pop(n, pop)\n    one_stat   &lt;- compute_stat(one_sample, stat)\n\n    # Big population draw (to show true shape)\n    big_pop &lt;- draw_pop(10000, pop)\n\n    # Repeated experiments\n    samp_stats &lt;- replicate(reps, compute_stat(draw_pop(n, pop), stat))\n\n    # Check normality (Shapiro on subset)\n    shap_sub &lt;- samp_stats[seq_len(min(5000, length(samp_stats)))]\n    shap_p &lt;- tryCatch(shapiro.test(shap_sub)$p.value, error = function(e) NA)\n\n    list(one_sample = one_sample, one_stat = one_stat,\n         big_pop = big_pop, samp_stats = samp_stats,\n         n = n, reps = reps, stat = stat, pop = pop,\n         shap_p = shap_p)\n  })\n\n  # --- Left: data distribution ---\n  output$data_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$big_pop, breaks = 60, probability = TRUE,\n         col = \"#e74c3c20\", border = \"#e74c3c60\",\n         main = paste(\"Population:\", d$pop),\n         xlab = \"X\", ylab = \"Density\")\n\n    # Overlay one sample as rug\n    rug(d$one_sample, col = \"#2c3e50\", lwd = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Population shape\",\n                      paste0(\"Your sample (n = \", d$n, \")\")),\n           col = c(\"#e74c3c60\", \"#2c3e50\"),\n           lwd = c(8, 2), lty = c(1, 1))\n  })\n\n  # --- Right: sampling distribution ---\n  output$samp_dist_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$samp_stats, breaks = 50, probability = TRUE,\n         col = \"#3498db30\", border = \"#3498db80\",\n         main = paste0(\"Sampling dist. of \", d$stat, \" (n = \", d$n, \")\"),\n         xlab = paste0(\"Sample \", tolower(d$stat)),\n         ylab = \"Density\")\n\n    # Normal overlay\n    x_seq &lt;- seq(min(d$samp_stats), max(d$samp_stats), length.out = 300)\n    lines(x_seq,\n          dnorm(x_seq, mean = mean(d$samp_stats), sd = sd(d$samp_stats)),\n          col = \"#e74c3c\", lwd = 2, lty = 2)\n\n    # Mark your one estimate\n    abline(v = d$one_stat, col = \"#2c3e50\", lwd = 2.5, lty = 1)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Sampling distribution\",\n                      \"Normal approximation\",\n                      paste0(\"Your estimate: \", round(d$one_stat, 3))),\n           col = c(\"#3498db80\", \"#e74c3c\", \"#2c3e50\"),\n           lwd = c(8, 2, 2.5), lty = c(1, 2, 1))\n  })\n\n  # --- Bottom: how normality improves with n ---\n  output$convergence_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ns &lt;- c(2, 5, 10, 30, 50, 100)\n    ns &lt;- ns[ns &lt;= 200]\n\n    colors &lt;- colorRampPalette(c(\"#e74c3c\", \"#3498db\"))(length(ns))\n\n    # First pass to get x range\n    all_stats &lt;- list()\n    for (i in seq_along(ns)) {\n      all_stats[[i]] &lt;- replicate(500, compute_stat(draw_pop(ns[i], d$pop), d$stat))\n    }\n    xlim &lt;- range(unlist(all_stats))\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, length(ns) * 1.5 + 1),\n         yaxt = \"n\", ylab = \"\", xlab = paste0(\"Sample \", tolower(d$stat)),\n         main = paste0(\"Sampling distribution of \", d$stat, \" at different n\"))\n    positions &lt;- seq_along(ns) * 1.5\n    axis(2, at = positions, labels = paste0(\"n=\", ns), las = 1, cex.axis = 0.9)\n\n    for (i in seq_along(ns)) {\n      dens &lt;- density(all_stats[[i]])\n      # Scale density to fit in a strip\n      dens$y &lt;- dens$y / max(dens$y) * 0.7\n      polygon(dens$x, dens$y + positions[i], col = adjustcolor(colors[i], 0.4),\n              border = colors[i], lwd = 1.5)\n    }\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    se &lt;- sd(d$samp_stats)\n    normal_enough &lt;- !is.na(d$shap_p) && d$shap_p &gt; 0.05\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Your estimate:&lt;/b&gt; \", round(d$one_stat, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;SE (from sampling dist):&lt;/b&gt; \", round(se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Sampling dist SD:&lt;/b&gt; \", round(sd(d$samp_stats), 4), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Looks normal?&lt;/b&gt; \",\n        if (normal_enough)\n          \"&lt;span style='color:#27ae60;'&gt;Yes (Shapiro p = \" else\n          \"&lt;span style='color:#e74c3c;'&gt;No (Shapiro p = \",\n        round(d$shap_p, 4), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;small&gt;Shapiro-Wilk tests whether the&lt;br&gt;\",\n        \"sampling distribution is normal.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nThe red dashed line on the right panel is the normal approximation. When it fits the histogram well, you can safely use normal-based inference (z-tests, t-tests, standard CIs).\nRules of thumb:\n\n\n\nPopulation\nStatistic\nn needed for normality\n\n\n\n\nSymmetric (normal, uniform)\nMean\n~10–15\n\n\nModerately skewed (exponential)\nMean\n~30\n\n\nHeavily skewed (chi-sq 2)\nMean\n~50–100\n\n\nAny shape\nMean\n~30 is the textbook answer, but check\n\n\nAny shape\nMedian\nSlower — needs larger n\n\n\nAny shape\nMax\nVery slow — often never normal\n\n\nAny shape\nSD\nModerate — ~30–50\n\n\n\n\n\n\n\nExponential, Mean, n = 5: the sampling distribution is visibly skewed. Normal approximation is poor. Shapiro test says “No.”\nSame but n = 50: now it’s nearly bell-shaped. CLT has kicked in.\nAny population, Max, n = 100: the sampling distribution of the maximum is not normal even with large n. It has its own distribution (extreme value theory). Normal approximation fails.\nBimodal, Mean, n = 2: the sampling distribution itself is bimodal! With only 2 observations, averaging doesn’t smooth enough.\nBottom panel (ridgeline): watch the sampling distribution narrow and become more normal as n increases, all on one plot.\n\n\n\n\nThe sampling distribution is not your data. It’s the distribution of your estimate. Whether it’s normal depends on three things:\n\nThe population shape — the uglier it is, the more data you need\nThe sample size — larger n → CLT → normality\nThe statistic — means converge fast, medians slower, maxima barely at all\n\nWhen in doubt, don’t assume — bootstrap it. The bootstrap page shows you how to build the sampling distribution empirically.\n\n\n\n\n\nWilliam Sealy Gosset, a chemist at the Guinness brewery, invented the t-distribution in 1908 because he was working with tiny samples of barley. Guinness didn’t let employees publish under their own names, so he used the pen name “Student” — hence “Student’s t-test.”\nGosset’s problem: with only 3–4 observations, you can’t assume the sampling distribution is normal. The t-distribution has heavier tails to account for the extra uncertainty when \\(n\\) is small. As \\(n\\) grows, the t-distribution converges to the normal — because with enough data, the sampling distribution is normal (CLT).\nThe concept of a sampling distribution was so confusing to early statisticians that R.A. Fisher reportedly said it was the most difficult idea in all of statistics to teach clearly.",
    "crumbs": [
      "Probability & Uncertainty",
      "The Sampling Distribution"
    ]
  },
  {
    "objectID": "sampling-distribution.html#what-is-a-sampling-distribution",
    "href": "sampling-distribution.html#what-is-a-sampling-distribution",
    "title": "The Sampling Distribution",
    "section": "",
    "text": "You run an experiment and compute a statistic — say the sample mean. You get \\(\\bar{x} = 4.7\\). But if you ran the experiment again with new data, you’d get \\(\\bar{x} = 5.1\\). And again: \\(\\bar{x} = 4.3\\).\nThe sampling distribution is the distribution of all possible values your statistic could take across every possible sample you could have drawn. It tells you: how much does my estimate bounce around due to the randomness of sampling?\nYou never observe it directly — you only ran the experiment once. But it’s the foundation of everything: standard errors, confidence intervals, p-values, and hypothesis tests all depend on it.\n\n\nThis is the most common confusion in statistics:\n\n\n\n\n\n\n\n\n\nDistribution of the data\nSampling distribution\n\n\n\n\nWhat is it?\nHistogram of individual observations\nHistogram of the statistic across repeated samples\n\n\nWhat does it show?\nHow spread out individual values are\nHow much the estimate varies\n\n\nShape\nCould be anything (skewed, bimodal, etc.)\nOften approximately normal for averages, by the CLT\n\n\nWidth\nDepends on \\(\\sigma\\) (population SD)\nDepends on \\(\\sigma / \\sqrt{n}\\) (standard error)\n\n\nYou can see it?\nYes — plot your data\nNo — you only have one sample\n\n\n\nYour data might be wildly skewed. But the sampling distribution of the mean can still be normal, because averaging smooths things out. That’s the CLT.\nThe simulation below makes this concrete. The left panel shows your data (one sample). The right panel shows what happens when you repeat the experiment 1,000 times — the sampling distribution.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we can repeat the experiment 1,000 times and directly see the sampling distribution. In practice, you run the experiment once. You never observe the sampling distribution — you infer its properties (via the CLT or the bootstrap) to build standard errors and confidence intervals.\n\n\n\n#| standalone: true\n#| viewerHeight: 1050\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      selectInput(\"pop\", \"Population shape:\",\n                  choices = c(\"Normal\",\n                              \"Uniform\",\n                              \"Skewed (exponential)\",\n                              \"Heavily skewed (chi-sq 2)\",\n                              \"Bimodal\",\n                              \"Heavy-tailed (t, df=3)\")),\n\n      selectInput(\"stat\", \"Statistic:\",\n                  choices = c(\"Mean\", \"Median\", \"SD\",\n                              \"Max\", \"IQR\")),\n\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 2, max = 200, value = 5, step = 1),\n\n      sliderInput(\"reps\", \"Repeated experiments:\",\n                  min = 100, max = 5000, value = 1000, step = 100),\n\n      actionButton(\"go\", \"Run experiments\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"data_plot\", height = \"350px\")),\n        column(6, plotOutput(\"samp_dist_plot\", height = \"350px\"))\n      ),\n      fluidRow(\n        column(12, plotOutput(\"convergence_plot\", height = \"450px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  draw_pop &lt;- function(n, pop) {\n    switch(pop,\n      \"Normal\" = rnorm(n, mean = 5, sd = 2),\n      \"Uniform\" = runif(n, 0, 10),\n      \"Skewed (exponential)\" = rexp(n, rate = 0.5),\n      \"Heavily skewed (chi-sq 2)\" = rchisq(n, df = 2),\n      \"Bimodal\" = {\n        k &lt;- rbinom(n, 1, 0.5)\n        k * rnorm(n, 2, 0.7) + (1 - k) * rnorm(n, 8, 0.7)\n      },\n      \"Heavy-tailed (t, df=3)\" = rt(n, df = 3) * 2 + 5\n    )\n  }\n\n  compute_stat &lt;- function(x, stat) {\n    switch(stat,\n      \"Mean\"   = mean(x),\n      \"Median\" = median(x),\n      \"SD\"     = sd(x),\n      \"Max\"    = max(x),\n      \"IQR\"    = IQR(x)\n    )\n  }\n\n  dat &lt;- reactive({\n    input$go\n    n    &lt;- input$n\n    reps &lt;- input$reps\n    pop  &lt;- input$pop\n    stat &lt;- input$stat\n\n    # One sample (for display)\n    one_sample &lt;- draw_pop(n, pop)\n    one_stat   &lt;- compute_stat(one_sample, stat)\n\n    # Big population draw (to show true shape)\n    big_pop &lt;- draw_pop(10000, pop)\n\n    # Repeated experiments\n    samp_stats &lt;- replicate(reps, compute_stat(draw_pop(n, pop), stat))\n\n    # Check normality (Shapiro on subset)\n    shap_sub &lt;- samp_stats[seq_len(min(5000, length(samp_stats)))]\n    shap_p &lt;- tryCatch(shapiro.test(shap_sub)$p.value, error = function(e) NA)\n\n    list(one_sample = one_sample, one_stat = one_stat,\n         big_pop = big_pop, samp_stats = samp_stats,\n         n = n, reps = reps, stat = stat, pop = pop,\n         shap_p = shap_p)\n  })\n\n  # --- Left: data distribution ---\n  output$data_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$big_pop, breaks = 60, probability = TRUE,\n         col = \"#e74c3c20\", border = \"#e74c3c60\",\n         main = paste(\"Population:\", d$pop),\n         xlab = \"X\", ylab = \"Density\")\n\n    # Overlay one sample as rug\n    rug(d$one_sample, col = \"#2c3e50\", lwd = 2)\n\n    legend(\"topright\", bty = \"n\", cex = 0.85,\n           legend = c(\"Population shape\",\n                      paste0(\"Your sample (n = \", d$n, \")\")),\n           col = c(\"#e74c3c60\", \"#2c3e50\"),\n           lwd = c(8, 2), lty = c(1, 1))\n  })\n\n  # --- Right: sampling distribution ---\n  output$samp_dist_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$samp_stats, breaks = 50, probability = TRUE,\n         col = \"#3498db30\", border = \"#3498db80\",\n         main = paste0(\"Sampling dist. of \", d$stat, \" (n = \", d$n, \")\"),\n         xlab = paste0(\"Sample \", tolower(d$stat)),\n         ylab = \"Density\")\n\n    # Normal overlay\n    x_seq &lt;- seq(min(d$samp_stats), max(d$samp_stats), length.out = 300)\n    lines(x_seq,\n          dnorm(x_seq, mean = mean(d$samp_stats), sd = sd(d$samp_stats)),\n          col = \"#e74c3c\", lwd = 2, lty = 2)\n\n    # Mark your one estimate\n    abline(v = d$one_stat, col = \"#2c3e50\", lwd = 2.5, lty = 1)\n\n    legend(\"topright\", bty = \"n\", cex = 0.8,\n           legend = c(\"Sampling distribution\",\n                      \"Normal approximation\",\n                      paste0(\"Your estimate: \", round(d$one_stat, 3))),\n           col = c(\"#3498db80\", \"#e74c3c\", \"#2c3e50\"),\n           lwd = c(8, 2, 2.5), lty = c(1, 2, 1))\n  })\n\n  # --- Bottom: how normality improves with n ---\n  output$convergence_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    ns &lt;- c(2, 5, 10, 30, 50, 100)\n    ns &lt;- ns[ns &lt;= 200]\n\n    colors &lt;- colorRampPalette(c(\"#e74c3c\", \"#3498db\"))(length(ns))\n\n    # First pass to get x range\n    all_stats &lt;- list()\n    for (i in seq_along(ns)) {\n      all_stats[[i]] &lt;- replicate(500, compute_stat(draw_pop(ns[i], d$pop), d$stat))\n    }\n    xlim &lt;- range(unlist(all_stats))\n\n    plot(NULL, xlim = xlim, ylim = c(0.5, length(ns) * 1.5 + 1),\n         yaxt = \"n\", ylab = \"\", xlab = paste0(\"Sample \", tolower(d$stat)),\n         main = paste0(\"Sampling distribution of \", d$stat, \" at different n\"))\n    positions &lt;- seq_along(ns) * 1.5\n    axis(2, at = positions, labels = paste0(\"n=\", ns), las = 1, cex.axis = 0.9)\n\n    for (i in seq_along(ns)) {\n      dens &lt;- density(all_stats[[i]])\n      # Scale density to fit in a strip\n      dens$y &lt;- dens$y / max(dens$y) * 0.7\n      polygon(dens$x, dens$y + positions[i], col = adjustcolor(colors[i], 0.4),\n              border = colors[i], lwd = 1.5)\n    }\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n    se &lt;- sd(d$samp_stats)\n    normal_enough &lt;- !is.na(d$shap_p) && d$shap_p &gt; 0.05\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;Your estimate:&lt;/b&gt; \", round(d$one_stat, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;SE (from sampling dist):&lt;/b&gt; \", round(se, 4), \"&lt;br&gt;\",\n        \"&lt;b&gt;Sampling dist SD:&lt;/b&gt; \", round(sd(d$samp_stats), 4), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Looks normal?&lt;/b&gt; \",\n        if (normal_enough)\n          \"&lt;span style='color:#27ae60;'&gt;Yes (Shapiro p = \" else\n          \"&lt;span style='color:#e74c3c;'&gt;No (Shapiro p = \",\n        round(d$shap_p, 4), \")&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;small&gt;Shapiro-Wilk tests whether the&lt;br&gt;\",\n        \"sampling distribution is normal.&lt;/small&gt;\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\n\nThe red dashed line on the right panel is the normal approximation. When it fits the histogram well, you can safely use normal-based inference (z-tests, t-tests, standard CIs).\nRules of thumb:\n\n\n\nPopulation\nStatistic\nn needed for normality\n\n\n\n\nSymmetric (normal, uniform)\nMean\n~10–15\n\n\nModerately skewed (exponential)\nMean\n~30\n\n\nHeavily skewed (chi-sq 2)\nMean\n~50–100\n\n\nAny shape\nMean\n~30 is the textbook answer, but check\n\n\nAny shape\nMedian\nSlower — needs larger n\n\n\nAny shape\nMax\nVery slow — often never normal\n\n\nAny shape\nSD\nModerate — ~30–50\n\n\n\n\n\n\n\nExponential, Mean, n = 5: the sampling distribution is visibly skewed. Normal approximation is poor. Shapiro test says “No.”\nSame but n = 50: now it’s nearly bell-shaped. CLT has kicked in.\nAny population, Max, n = 100: the sampling distribution of the maximum is not normal even with large n. It has its own distribution (extreme value theory). Normal approximation fails.\nBimodal, Mean, n = 2: the sampling distribution itself is bimodal! With only 2 observations, averaging doesn’t smooth enough.\nBottom panel (ridgeline): watch the sampling distribution narrow and become more normal as n increases, all on one plot.\n\n\n\n\nThe sampling distribution is not your data. It’s the distribution of your estimate. Whether it’s normal depends on three things:\n\nThe population shape — the uglier it is, the more data you need\nThe sample size — larger n → CLT → normality\nThe statistic — means converge fast, medians slower, maxima barely at all\n\nWhen in doubt, don’t assume — bootstrap it. The bootstrap page shows you how to build the sampling distribution empirically.\n\n\n\n\n\nWilliam Sealy Gosset, a chemist at the Guinness brewery, invented the t-distribution in 1908 because he was working with tiny samples of barley. Guinness didn’t let employees publish under their own names, so he used the pen name “Student” — hence “Student’s t-test.”\nGosset’s problem: with only 3–4 observations, you can’t assume the sampling distribution is normal. The t-distribution has heavier tails to account for the extra uncertainty when \\(n\\) is small. As \\(n\\) grows, the t-distribution converges to the normal — because with enough data, the sampling distribution is normal (CLT).\nThe concept of a sampling distribution was so confusing to early statisticians that R.A. Fisher reportedly said it was the most difficult idea in all of statistics to teach clearly.",
    "crumbs": [
      "Probability & Uncertainty",
      "The Sampling Distribution"
    ]
  },
  {
    "objectID": "fwl.html",
    "href": "fwl.html",
    "title": "Frisch-Waugh-Lovell Theorem",
    "section": "",
    "text": "The Frisch-Waugh-Lovell (FWL) theorem says: the coefficient on \\(X_1\\) in \\(Y = \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon\\) is identical to the slope from regressing the residualized \\(Y\\) on the residualized \\(X_1\\) — after partialling out \\(X_2\\) from both.\nDrag the sliders to see it hold for any DGP.\n\n\n\n\n\n\nThe Oracle View. In these simulations, we set the true \\(\\beta_1\\), \\(\\beta_2\\), and the correlation between \\(X_1\\) and \\(X_2\\). We can verify FWL gives the same answer as the full regression. In practice, you don’t know the true coefficients — but FWL holds mechanically regardless, which is why it’s useful for understanding what “controlling for” actually does.\n\n\n\n#| standalone: true\n#| viewerHeight: 900\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .eq-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-bottom: 14px; font-size: 14px; line-height: 1.9;\n    }\n    .eq-box b { color: #2c3e50; }\n    .match  { color: #27ae60; font-weight: bold; }\n    .coef   { color: #e74c3c; font-weight: bold; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 4,\n\n      sliderInput(\"n\", \"Sample size:\",\n                  min = 50, max = 500, value = 200, step = 50),\n\n      sliderInput(\"b1\", HTML(\"True &beta;&lt;sub&gt;1&lt;/sub&gt;:\"),\n                  min = -3, max = 3, value = 1.5, step = 0.1),\n\n      sliderInput(\"b2\", HTML(\"True &beta;&lt;sub&gt;2&lt;/sub&gt;:\"),\n                  min = -3, max = 3, value = -1, step = 0.1),\n\n      sliderInput(\"rho\", HTML(\"Corr(X&lt;sub&gt;1&lt;/sub&gt;, X&lt;sub&gt;2&lt;/sub&gt;):\"),\n                  min = -0.9, max = 0.9, value = 0.6, step = 0.1),\n\n      sliderInput(\"sigma\", HTML(\"Error SD (&sigma;):\"),\n                  min = 0.5, max = 5, value = 1, step = 0.5),\n\n      actionButton(\"resim\", \"New draw\", class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results_box\")\n    ),\n\n    mainPanel(\n      width = 8,\n      plotOutput(\"plot_full\",    height = \"350px\"),\n      fluidRow(\n        column(6, plotOutput(\"plot_partial\", height = \"350px\")),\n        column(6, plotOutput(\"plot_fwl\",     height = \"350px\"))\n      ),\n      uiOutput(\"step_text\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$resim\n    n     &lt;- input$n\n    b1    &lt;- input$b1\n    b2    &lt;- input$b2\n    rho   &lt;- input$rho\n    sigma &lt;- input$sigma\n\n    # Generate correlated X1, X2\n    z1 &lt;- rnorm(n)\n    z2 &lt;- rnorm(n)\n    x1 &lt;- z1\n    x2 &lt;- rho * z1 + sqrt(1 - rho^2) * z2\n\n    eps &lt;- rnorm(n, sd = sigma)\n    y   &lt;- b1 * x1 + b2 * x2 + eps\n\n    # Full OLS\n    full_fit &lt;- lm(y ~ x1 + x2)\n\n    # FWL steps\n    ey &lt;- resid(lm(y  ~ x2))   # residualise Y on X2\n    ex &lt;- resid(lm(x1 ~ x2))   # residualise X1 on X2\n    fwl_fit &lt;- lm(ey ~ ex)\n\n    list(x1 = x1, x2 = x2, y = y,\n         ey = ey, ex = ex,\n         full_fit = full_fit, fwl_fit = fwl_fit,\n         b1 = b1, b2 = b2)\n  })\n\n  # --- Plot 1: Y vs X1 (naive scatter) ---\n  output$plot_full &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(5, 5, 4, 2))\n    plot(d$x1, d$y, pch = 16, col = \"#3498db80\", cex = 0.8,\n         xlab = expression(X[1]), ylab = \"Y\",\n         main = expression(\"Y vs \" * X[1] * \" (raw)\"))\n    abline(lm(d$y ~ d$x1), col = \"#e74c3c\", lwd = 2.5)\n    naive_b &lt;- round(coef(lm(d$y ~ d$x1))[2], 4)\n    legend(\"topleft\", bty = \"n\", cex = 0.9,\n           legend = paste(\"Naive slope =\", naive_b))\n  })\n\n  # --- Plot 2: residualised X1 (partial out X2) ---\n  output$plot_partial &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(5, 5, 4, 2))\n    plot(d$x1, d$ex, pch = 16, col = \"#9b59b680\", cex = 0.8,\n         xlab = expression(X[1]),\n         ylab = expression(e[X[1]]),\n         main = expression(\"Residualise \" * X[1] * \" on \" * X[2]))\n    abline(h = 0, lty = 2, col = \"gray50\")\n    abline(lm(d$ex ~ d$x1), col = \"#8e44ad\", lwd = 2)\n    legend(\"topleft\", bty = \"n\", cex = 0.85,\n           legend = expression(\"Variation in \" * X[1] * \" independent of \" * X[2]))\n  })\n\n  # --- Plot 3: FWL regression ---\n  output$plot_fwl &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(5, 5, 4, 2))\n    plot(d$ex, d$ey, pch = 16, col = \"#2ecc7180\", cex = 0.8,\n         xlab = expression(e[X[1]]),\n         ylab = expression(e[Y]),\n         main = \"FWL: Residual Y vs Residual X1\")\n    abline(d$fwl_fit, col = \"#e74c3c\", lwd = 2.5)\n    fwl_b &lt;- round(coef(d$fwl_fit)[2], 4)\n    legend(\"topleft\", bty = \"n\", cex = 0.9,\n           legend = paste(\"FWL slope =\", fwl_b))\n  })\n\n  # --- Results comparison ---\n  output$results_box &lt;- renderUI({\n    d &lt;- dat()\n    full_b1 &lt;- round(coef(d$full_fit)[\"x1\"], 4)\n    fwl_b1  &lt;- round(coef(d$fwl_fit)[2], 4)\n    naive_b &lt;- round(coef(lm(d$y ~ d$x1))[2], 4)\n\n    tags$div(class = \"eq-box\", style = \"margin-top: 16px;\",\n      HTML(paste0(\n        \"&lt;b&gt;True &beta;&lt;sub&gt;1&lt;/sub&gt;:&lt;/b&gt; \", d$b1, \"&lt;br&gt;\",\n        \"&lt;b&gt;Full OLS &beta;&lt;sub&gt;1&lt;/sub&gt;:&lt;/b&gt; &lt;span class='coef'&gt;\", full_b1, \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;b&gt;FWL &beta;&lt;sub&gt;1&lt;/sub&gt;:&lt;/b&gt; &lt;span class='coef'&gt;\", fwl_b1, \"&lt;/span&gt;&lt;br&gt;\",\n        \"&lt;span class='match'&gt;&#10003; They match!&lt;/span&gt;&lt;br&gt;&lt;br&gt;\",\n        \"&lt;b&gt;Naive slope:&lt;/b&gt; \", naive_b, \"&lt;br&gt;\",\n        \"&lt;small&gt;(biased by omitting X&lt;sub&gt;2&lt;/sub&gt;)&lt;/small&gt;\"\n      ))\n    )\n  })\n\n  # --- Step explanation ---\n  output$step_text &lt;- renderUI({\n    tags$div(class = \"eq-box\", style = \"margin-top: 8px;\",\n      HTML(paste0(\n        \"&lt;b&gt;Steps:&lt;/b&gt; \",\n        \"(1) Regress Y on X&lt;sub&gt;2&lt;/sub&gt; &rarr; residuals &lt;i&gt;e&lt;sub&gt;Y&lt;/sub&gt;&lt;/i&gt; &nbsp;|&nbsp; \",\n        \"(2) Regress X&lt;sub&gt;1&lt;/sub&gt; on X&lt;sub&gt;2&lt;/sub&gt; &rarr; residuals &lt;i&gt;e&lt;sub&gt;X₁&lt;/sub&gt;&lt;/i&gt; &nbsp;|&nbsp; \",\n        \"(3) Regress &lt;i&gt;e&lt;sub&gt;Y&lt;/sub&gt;&lt;/i&gt; on &lt;i&gt;e&lt;sub&gt;X₁&lt;/sub&gt;&lt;/i&gt; &rarr; \",\n        \"slope = &beta;&lt;sub&gt;1&lt;/sub&gt; from full regression\"\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)\n\n\nDid you know?\n\nRagnar Frisch and Jan Tinbergen won the very first Nobel Prize in Economics in 1969. Frisch coined the terms “econometrics,” “microeconomics,” and “macroeconomics.” The FWL theorem appeared in Frisch & Waugh (1933).\nMichael Lovell extended the result in 1963, showing it applies to any partitioned regression — not just the two-variable case. That’s why it’s FWL, not just FW.\nFWL is the theoretical foundation behind “partialling out” and “controlling for” variables. Every time you add a control to a regression, you’re implicitly doing the residualization that FWL describes.\nIn machine learning, the same idea appears as “residualization” in double/debiased ML (Chernozhukov et al., 2018) — one of the most important recent developments in causal ML.",
    "crumbs": [
      "Regression",
      "Frisch-Waugh-Lovell"
    ]
  },
  {
    "objectID": "method-of-moments.html",
    "href": "method-of-moments.html",
    "title": "Method of Moments",
    "section": "",
    "text": "The simplest estimation idea you’ll encounter: set sample moments equal to population moments and solve. It won’t always give you the best estimator, but it’s often the most transparent one — and it’s the starting point for understanding everything else in this section.",
    "crumbs": [
      "Estimation",
      "Method of Moments"
    ]
  },
  {
    "objectID": "method-of-moments.html#the-idea",
    "href": "method-of-moments.html#the-idea",
    "title": "Method of Moments",
    "section": "The idea",
    "text": "The idea\nA moment is just an expected value of some function of the data. The first moment of \\(X\\) is \\(E[X]\\) (the mean), the second moment is \\(E[X^2]\\), and so on. Many parameters you care about can be written as functions of moments:\n\\[\n\\mu = E[X], \\qquad \\sigma^2 = E[X^2] - (E[X])^2\n\\]\nThe population moments involve the true distribution, which you don’t know. But you do have data, so you can compute sample moments:\n\\[\n\\hat{m}_1 = \\frac{1}{n}\\sum_{i=1}^n X_i, \\qquad \\hat{m}_2 = \\frac{1}{n}\\sum_{i=1}^n X_i^2\n\\]\nThe Method of Moments (MoM) strategy is dead simple:\n\nWrite the parameters as functions of population moments\nReplace population moments with sample moments\nSolve for the parameters\n\nFor the normal distribution, this gives you \\(\\hat{\\mu} = \\bar{X}\\) and \\(\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_i (X_i - \\bar{X})^2\\). The sample mean and sample variance are MoM estimators — you’ve been using Method of Moments all along without calling it that.\n\n\n\n\n\n\nIntuition. MoM says: “I don’t know the population, but if the model is right, the population moments should look like the sample moments. So I’ll find the parameters that make them match.”",
    "crumbs": [
      "Estimation",
      "Method of Moments"
    ]
  },
  {
    "objectID": "method-of-moments.html#a-worked-example",
    "href": "method-of-moments.html#a-worked-example",
    "title": "Method of Moments",
    "section": "A worked example",
    "text": "A worked example\nSuppose \\(X_1, \\ldots, X_n\\) are drawn from a Uniform\\((0, \\theta)\\) distribution. You want to estimate \\(\\theta\\).\nThe population mean is:\n\\[\nE[X] = \\frac{\\theta}{2}\n\\]\nSet the sample moment equal to the population moment:\n\\[\n\\bar{X} = \\frac{\\hat{\\theta}}{2}\n\\]\nSolve:\n\\[\n\\hat{\\theta}_{\\text{MoM}} = 2\\bar{X}\n\\]\nThat’s it — double the sample mean. Now here’s what’s interesting: the MLE for the same problem is \\(\\hat{\\theta}_{\\text{MLE}} = \\max(X_1, \\ldots, X_n)\\) — the largest observation. Same data, same model, completely different estimator.\nThis is important. Different estimation principles can give you different answers. MoM matched a moment; MLE maximized a likelihood. Neither is “wrong” — they’re optimizing different things.\n\nSimulation: MoM vs MLE for Uniform(0, θ)\nSee this difference in action. Both estimators target the same parameter, but the MLE (maximum of the sample) has dramatically lower variance than the MoM estimator (twice the sample mean). The right panel shows the sampling distributions across 500 repeated samples — watch how much tighter the MLE distribution is.\n#| standalone: true\n#| viewerHeight: 680\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$head(tags$style(HTML(\"\n    .stats-box {\n      background: #f0f4f8; border-radius: 6px; padding: 14px;\n      margin-top: 12px; font-size: 14px; line-height: 1.9;\n    }\n    .stats-box b { color: #2c3e50; }\n  \"))),\n\n  sidebarLayout(\n    sidebarPanel(\n      width = 3,\n\n      sliderInput(\"theta\", \"True \\u03b8:\",\n                  min = 1, max = 10, value = 5, step = 0.5),\n\n      sliderInput(\"n\", \"Sample size n:\",\n                  min = 5, max = 500, value = 30, step = 5),\n\n      actionButton(\"go\", \"New draw\",\n                   class = \"btn-primary\", width = \"100%\"),\n\n      uiOutput(\"results\")\n    ),\n\n    mainPanel(\n      width = 9,\n      fluidRow(\n        column(6, plotOutput(\"hist_plot\", height = \"520px\")),\n        column(6, plotOutput(\"sampling_plot\", height = \"520px\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  dat &lt;- reactive({\n    input$go\n    theta &lt;- input$theta\n    n     &lt;- input$n\n\n    # One draw for the left panel\n    x &lt;- runif(n, min = 0, max = theta)\n    mom_est &lt;- 2 * mean(x)\n    mle_est &lt;- max(x)\n\n    # 500 repeated samples for the right panel\n    mom_reps &lt;- replicate(500, {\n      xs &lt;- runif(n, min = 0, max = theta)\n      2 * mean(xs)\n    })\n    mle_reps &lt;- replicate(500, {\n      xs &lt;- runif(n, min = 0, max = theta)\n      max(xs)\n    })\n\n    list(x = x, theta = theta, n = n,\n         mom_est = mom_est, mle_est = mle_est,\n         mom_reps = mom_reps, mle_reps = mle_reps)\n  })\n\n  output$hist_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    hist(d$x, breaks = 25, probability = TRUE,\n         col = \"#95a5a620\", border = \"#95a5a680\",\n         main = paste0(\"One sample (n = \", d$n, \")\"),\n         xlab = \"x\", ylab = \"Density\",\n         xlim = c(0, max(d$theta * 1.15, d$mom_est * 1.05)))\n\n    abline(v = d$theta, lty = 2, lwd = 2.5, col = \"#2c3e50\")\n    abline(v = d$mom_est, lwd = 2.5, col = \"#3498db\")\n    abline(v = d$mle_est, lwd = 2.5, col = \"#e74c3c\")\n\n    legend(\"topright\", bty = \"n\", cex = 0.9,\n           legend = c(paste0(\"True \\u03b8 = \", d$theta),\n                      paste0(\"MoM = 2x\\u0305 = \", round(d$mom_est, 3)),\n                      paste0(\"MLE = max = \", round(d$mle_est, 3))),\n           col = c(\"#2c3e50\", \"#3498db\", \"#e74c3c\"),\n           lwd = 2.5, lty = c(2, 1, 1))\n  })\n\n  output$sampling_plot &lt;- renderPlot({\n    d &lt;- dat()\n    par(mar = c(4.5, 4.5, 3, 1))\n\n    all_vals &lt;- c(d$mom_reps, d$mle_reps)\n    xlim &lt;- range(all_vals)\n    xlim &lt;- xlim + c(-0.1, 0.1) * diff(xlim)\n\n    mom_dens &lt;- density(d$mom_reps, from = xlim[1], to = xlim[2])\n    mle_dens &lt;- density(d$mle_reps, from = xlim[1], to = xlim[2])\n\n    ylim &lt;- c(0, max(mom_dens$y, mle_dens$y) * 1.1)\n\n    plot(NULL, xlim = xlim, ylim = ylim,\n         xlab = \"Estimate of \\u03b8\", ylab = \"Density\",\n         main = \"Sampling distributions (500 reps)\")\n\n    polygon(mom_dens$x, mom_dens$y,\n            col = adjustcolor(\"#3498db\", 0.25), border = \"#3498db\", lwd = 2)\n    polygon(mle_dens$x, mle_dens$y,\n            col = adjustcolor(\"#e74c3c\", 0.25), border = \"#e74c3c\", lwd = 2)\n\n    abline(v = d$theta, lty = 2, lwd = 2.5, col = \"#2c3e50\")\n\n    legend(\"topleft\", bty = \"n\", cex = 0.9,\n           legend = c(paste0(\"MoM (SD = \", round(sd(d$mom_reps), 3), \")\"),\n                      paste0(\"MLE (SD = \", round(sd(d$mle_reps), 3), \")\"),\n                      paste0(\"True \\u03b8 = \", d$theta)),\n           col = c(\"#3498db\", \"#e74c3c\", \"#2c3e50\"),\n           lwd = c(2, 2, 2.5), lty = c(1, 1, 2),\n           fill = c(adjustcolor(\"#3498db\", 0.25),\n                    adjustcolor(\"#e74c3c\", 0.25), NA),\n           border = c(\"#3498db\", \"#e74c3c\", NA))\n  })\n\n  output$results &lt;- renderUI({\n    d &lt;- dat()\n\n    tags$div(class = \"stats-box\",\n      HTML(paste0(\n        \"&lt;b&gt;True \\u03b8:&lt;/b&gt; \", d$theta, \"&lt;br&gt;\",\n        \"&lt;b&gt;This draw:&lt;/b&gt;&lt;br&gt;\",\n        \"MoM = 2x\\u0305 = \", round(d$mom_est, 4), \"&lt;br&gt;\",\n        \"MLE = max = \", round(d$mle_est, 4), \"&lt;br&gt;\",\n        \"&lt;hr style='margin:8px 0'&gt;\",\n        \"&lt;b&gt;Across 500 reps:&lt;/b&gt;&lt;br&gt;\",\n        \"MoM mean: \", round(mean(d$mom_reps), 4),\n        \" &nbsp; SD: \", round(sd(d$mom_reps), 4), \"&lt;br&gt;\",\n        \"MLE mean: \", round(mean(d$mle_reps), 4),\n        \" &nbsp; SD: \", round(sd(d$mle_reps), 4)\n      ))\n    )\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Estimation",
      "Method of Moments"
    ]
  },
  {
    "objectID": "method-of-moments.html#when-mom-works-well",
    "href": "method-of-moments.html#when-mom-works-well",
    "title": "Method of Moments",
    "section": "When MoM works well",
    "text": "When MoM works well\nSimple and closed-form. You write down moment equations and solve. No optimization, no iterative algorithms. For many common distributions, the estimators are formulas you can compute by hand.\nConsistent under mild conditions. As long as the law of large numbers applies (sample moments converge to population moments), MoM estimators converge to the true parameter values. You need very little for this — just finite moments and independent data.\nA good starting point. Even when MoM isn’t the most efficient estimator, it’s often used as an initial value for more complex procedures (like numerical MLE or GMM).",
    "crumbs": [
      "Estimation",
      "Method of Moments"
    ]
  },
  {
    "objectID": "method-of-moments.html#limitations",
    "href": "method-of-moments.html#limitations",
    "title": "Method of Moments",
    "section": "Limitations",
    "text": "Limitations\nInefficient. MoM estimators often have higher variance than MLE estimators. In the Uniform example above, \\(\\hat{\\theta}_{\\text{MoM}} = 2\\bar{X}\\) has variance that shrinks at rate \\(1/n\\), while \\(\\hat{\\theta}_{\\text{MLE}} = \\max(X_i)\\) has variance that shrinks at rate \\(1/n^2\\). The MLE is dramatically better because it uses the shape of the distribution, not just a single summary statistic.\nCan produce inadmissible estimates. A MoM estimator for a variance could turn out negative. A MoM estimator for a probability could land outside \\([0, 1]\\). The method has no built-in mechanism to enforce constraints on the parameter space.\nAmbiguity with many parameters. If you have \\(k\\) parameters, you need \\(k\\) moment conditions. But there are infinitely many moments to choose from — \\(E[X]\\), \\(E[X^2]\\), \\(E[X^3]\\), \\(E[\\log X]\\), \\(E[1/X]\\), and so on. Different choices give different estimators, and MoM doesn’t tell you which is best. This ambiguity is exactly what GMM resolves.",
    "crumbs": [
      "Estimation",
      "Method of Moments"
    ]
  },
  {
    "objectID": "method-of-moments.html#connecting-forward",
    "href": "method-of-moments.html#connecting-forward",
    "title": "Method of Moments",
    "section": "Connecting forward",
    "text": "Connecting forward\nMoM is the simplest member of a family of estimation strategies. Each one builds on the same intuition — matching features of the data to features of the model — but adds sophistication:\n\nMaximum Likelihood picks parameters to maximize the probability of the observed data. It’s often more efficient than MoM because it uses the full distributional shape, not just selected moments.\nGMM formalizes “which moments to match” and handles the case where you have more moment conditions than parameters (over-identification). It also tells you the best way to weight those conditions.\nOLS as MoM. The normal equations \\(X'y = X'X\\hat{\\beta}\\) are really just moment conditions in disguise: \\(E[X_i(Y_i - X_i'\\beta)] = 0\\) says “the regressors are uncorrelated with the error.” Setting the sample analog to zero and solving gives you OLS. For more on the algebra, see The Algebra Behind OLS.",
    "crumbs": [
      "Estimation",
      "Method of Moments"
    ]
  }
]